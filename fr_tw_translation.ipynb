{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/fr_tw_translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n9YjovaQRksS"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "\n",
        "MODE = 'train'\n",
        "NUM_EPOCHS = 100\n",
        "NUMBER_OF_DATASET = 10000\n",
        "BATCH_SIZE = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dxf5AuezRmUE",
        "outputId": "b2a8f920-c3a9-443c-db3f-7def56b295c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ],
      "source": [
        "def read_dataset(number):\n",
        "\n",
        "    french_data = []\n",
        "    with open('/content/verified.french.txt') as file:\n",
        "\n",
        "        line = file.readline()\n",
        "        cnt = 1\n",
        "        while line:\n",
        "            french_data.append(line.strip())\n",
        "            line = file.readline()\n",
        "            cnt += 1\n",
        "\n",
        "\n",
        "    twi_data = []\n",
        "    with open('/content/verified.twi.txt') as file:\n",
        "\n",
        "        line = file.readline()\n",
        "        cnt = 1\n",
        "        while line:\n",
        "            twi_data.append(line.strip())\n",
        "            line = file.readline()\n",
        "            cnt += 1\n",
        "\n",
        "    return french_data[:number],twi_data[:number]\n",
        "\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def normalize_fr(s):\n",
        "    s = unicode_to_ascii(s)\n",
        "    s = re.sub(r'([!.?])', r' \\1', s)\n",
        "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "    s = re.sub(r'\\s+', r' ', s)\n",
        "    return s\n",
        "\n",
        "def normalize_twi(s):\n",
        "    s = unicode_to_ascii(s)\n",
        "    s = re.sub(r'([!.?])', r' \\1', s)\n",
        "    s = re.sub(r'[^a-zA-Z.ƆɔɛƐ!?’]+', r' ', s)\n",
        "    s = re.sub(r'\\s+', r' ', s)\n",
        "    return s\n",
        "\n",
        "raw_data_fr,raw_data_twi = read_dataset(NUMBER_OF_DATASET)\n",
        "raw_data_fr = [normalize_fr(data) for data in raw_data_fr]\n",
        "raw_data_twi = [normalize_twi(data) for data in raw_data_twi]\n",
        "raw_data_twi_in = ['<start> ' + normalize_twi(data) for data in raw_data_twi]\n",
        "raw_data_twi_out = [normalize_twi(data) + ' <end>' for data in raw_data_twi]\n",
        "print(len(raw_data_twi_in))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qi5J85P3U5zC"
      },
      "outputs": [],
      "source": [
        "fr_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "fr_tokenizer.fit_on_texts(raw_data_fr)\n",
        "data_fr = fr_tokenizer.texts_to_sequences(raw_data_fr)\n",
        "data_fr = tf.keras.preprocessing.sequence.pad_sequences(data_fr,\n",
        "                                                        padding='post')\n",
        "\n",
        "twi_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "twi_tokenizer.fit_on_texts(raw_data_twi_in)\n",
        "twi_tokenizer.fit_on_texts(raw_data_twi_out)\n",
        "data_twi_in = twi_tokenizer.texts_to_sequences(raw_data_twi_in)\n",
        "data_twi_in = tf.keras.preprocessing.sequence.pad_sequences(data_twi_in,\n",
        "                                                           padding='post')\n",
        "\n",
        "data_twi_out = twi_tokenizer.texts_to_sequences(raw_data_twi_out)\n",
        "data_twi_out = tf.keras.preprocessing.sequence.pad_sequences(data_twi_out,\n",
        "                                                            padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vwdwNS-W--m",
        "outputId": "fbecc043-df64-4a6d-a84d-5f1c55a5eac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<BatchDataset element_spec=(TensorSpec(shape=(None, 43), dtype=tf.int32, name=None), TensorSpec(shape=(None, 32), dtype=tf.int32, name=None), TensorSpec(shape=(None, 32), dtype=tf.int32, name=None))>\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (data_fr, data_twi_in, data_twi_out))\n",
        "dataset = dataset.shuffle(len(data_fr)).batch(BATCH_SIZE)\n",
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czULaqqTYdO-"
      },
      "outputs": [],
      "source": [
        "\"\"\"## Create the Positional Embedding\"\"\"\n",
        "\n",
        "\n",
        "def positional_encoding(pos, model_size):\n",
        "    \"\"\" Compute positional encoding for a particular position\n",
        "\n",
        "    Args:\n",
        "        pos: position of a token in the sequence\n",
        "        model_size: depth size of the model\n",
        "    \n",
        "    Returns:\n",
        "        The positional encoding for the given token\n",
        "    \"\"\"\n",
        "    PE = np.zeros((1, model_size))\n",
        "    for i in range(model_size):\n",
        "        if i % 2 == 0:\n",
        "            PE[:, i] = np.sin(pos / 10000 ** (i / model_size))\n",
        "        else:\n",
        "            PE[:, i] = np.cos(pos / 10000 ** ((i - 1) / model_size))\n",
        "    return PE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6ZzL6_-Z4bT",
        "outputId": "c6714ee2-1a61-4043-f180-eeb04b91a67f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(43, 256)\n",
            "(10000, 43)\n",
            "(10000, 32)\n"
          ]
        }
      ],
      "source": [
        "max_length = max(len(data_fr[0]), len(data_twi_in[0]))\n",
        "MODEL_SIZE = 256\n",
        "\n",
        "pes = []\n",
        "for i in range(max_length):\n",
        "    pes.append(positional_encoding(i, MODEL_SIZE))\n",
        "\n",
        "pes = np.concatenate(pes, axis=0)\n",
        "pes = tf.constant(pes, dtype=tf.float32)\n",
        "\n",
        "\n",
        "print(pes.shape)\n",
        "print(data_fr.shape)\n",
        "print(data_twi_in.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOBTwtZVYdaP"
      },
      "outputs": [],
      "source": [
        "\"\"\"## Create the Multihead Attention layer\"\"\"\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.Model):\n",
        "    \"\"\" Class for Multi-Head Attention layer\n",
        "\n",
        "    Attributes:\n",
        "        key_size: d_key in the paper\n",
        "        h: number of attention heads\n",
        "        wq: the Linear layer for Q\n",
        "        wk: the Linear layer for K\n",
        "        wv: the Linear layer for V\n",
        "        wo: the Linear layer for the output\n",
        "    \"\"\"\n",
        "    def __init__(self, model_size, h):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.key_size = model_size // h\n",
        "        self.h = h\n",
        "        self.wq = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(key_size) for _ in range(h)]\n",
        "        self.wk = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(key_size) for _ in range(h)]\n",
        "        self.wv = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(value_size) for _ in range(h)]\n",
        "        self.wo = tf.keras.layers.Dense(model_size)\n",
        "\n",
        "    def call(self, query, value, mask=None):\n",
        "        \"\"\" The forward pass for Multi-Head Attention layer\n",
        "\n",
        "        Args:\n",
        "            query: the Q matrix\n",
        "            value: the V matrix, acts as V and K\n",
        "            mask: mask to filter out unwanted tokens\n",
        "                  - zero mask: mask for padded tokens\n",
        "                  - right-side mask: mask to prevent attention towards tokens on the right-hand side\n",
        "        \n",
        "        Returns:\n",
        "            The concatenated context vector\n",
        "            The alignment (attention) vectors of all heads\n",
        "        \"\"\"\n",
        "        # query has shape (batch, query_len, model_size)\n",
        "        # value has shape (batch, value_len, model_size)\n",
        "        query = self.wq(query)\n",
        "        key = self.wk(value)\n",
        "        value = self.wv(value)\n",
        "        \n",
        "        # Split matrices for multi-heads attention\n",
        "        batch_size = query.shape[0]\n",
        "        \n",
        "        # Originally, query has shape (batch, query_len, model_size)\n",
        "        # We need to reshape to (batch, query_len, h, key_size)\n",
        "        query = tf.reshape(query, [batch_size, -1, self.h, self.key_size])\n",
        "        # In order to compute matmul, the dimensions must be transposed to (batch, h, query_len, key_size)\n",
        "        query = tf.transpose(query, [0, 2, 1, 3])\n",
        "        \n",
        "        # Do the same for key and value\n",
        "        key = tf.reshape(key, [batch_size, -1, self.h, self.key_size])\n",
        "        key = tf.transpose(key, [0, 2, 1, 3])\n",
        "        value = tf.reshape(value, [batch_size, -1, self.h, self.key_size])\n",
        "        value = tf.transpose(value, [0, 2, 1, 3])\n",
        "        \n",
        "        # Compute the dot score\n",
        "        # and divide the score by square root of key_size (as stated in paper)\n",
        "        # (must convert key_size to float32 otherwise an error would occur)\n",
        "        score = tf.matmul(query, key, transpose_b=True) / tf.math.sqrt(tf.dtypes.cast(self.key_size, dtype=tf.float32))\n",
        "        # score will have shape of (batch, h, query_len, value_len)\n",
        "        \n",
        "        # Mask out the score if a mask is provided\n",
        "        # There are two types of mask:\n",
        "        # - Padding mask (batch, 1, 1, value_len): to prevent attention being drawn to padded token (i.e. 0)\n",
        "        # - Look-left mask (batch, 1, query_len, value_len): to prevent decoder to draw attention to tokens to the right\n",
        "        if mask is not None:\n",
        "            score *= mask\n",
        "\n",
        "            # We want the masked out values to be zeros when applying softmax\n",
        "            # One way to accomplish that is assign them to a very large negative value\n",
        "            score = tf.where(tf.equal(score, 0), tf.ones_like(score) * -1e9, score)\n",
        "        \n",
        "        # Alignment vector: (batch, h, query_len, value_len)\n",
        "        alignment = tf.nn.softmax(score, axis=-1)\n",
        "        \n",
        "        # Context vector: (batch, h, query_len, key_size)\n",
        "        context = tf.matmul(alignment, value)\n",
        "        \n",
        "        # Finally, do the opposite to have a tensor of shape (batch, query_len, model_size)\n",
        "        context = tf.transpose(context, [0, 2, 1, 3])\n",
        "        context = tf.reshape(context, [batch_size, -1, self.key_size * self.h])\n",
        "        \n",
        "        # Apply one last full connected layer (WO)\n",
        "        heads = self.wo(context)\n",
        "        \n",
        "        return heads, alignment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGJUEC_qbBqL",
        "outputId": "f2bacbf8-bda2-4361-fbf7-48ffb447674f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6267\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([1, 5, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "\"\"\"## Create the Encoder\"\"\"\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "    \"\"\" Class for the Encoder\n",
        "\n",
        "    Args:\n",
        "        model_size: d_model in the paper (depth size of the model)\n",
        "        num_layers: number of layers (Multi-Head Attention + FNN)\n",
        "        h: number of attention heads\n",
        "        embedding: Embedding layer\n",
        "        embedding_dropout: Dropout layer for Embedding\n",
        "        attention: array of Multi-Head Attention layers\n",
        "        attention_dropout: array of Dropout layers for Multi-Head Attention\n",
        "        attention_norm: array of LayerNorm layers for Multi-Head Attention\n",
        "        dense_1: array of first Dense layers for FFN\n",
        "        dense_2: array of second Dense layers for FFN\n",
        "        ffn_dropout: array of Dropout layers for FFN\n",
        "        ffn_norm: array of LayerNorm layers for FFN\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, model_size, num_layers, h):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.model_size = model_size\n",
        "        self.num_layers = num_layers\n",
        "        self.h = h\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n",
        "        self.embedding_dropout = tf.keras.layers.Dropout(0.1)\n",
        "        self.attention = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
        "        self.attention_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
        "\n",
        "        self.attention_norm = [tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6) for _ in range(num_layers)]\n",
        "\n",
        "        self.dense_1 = [tf.keras.layers.Dense(\n",
        "            MODEL_SIZE * 4, activation='relu') for _ in range(num_layers)]\n",
        "        self.dense_2 = [tf.keras.layers.Dense(\n",
        "            model_size) for _ in range(num_layers)]\n",
        "        self.ffn_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
        "        self.ffn_norm = [tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6) for _ in range(num_layers)]\n",
        "\n",
        "    def call(self, sequence, training=True, encoder_mask=None):\n",
        "        \"\"\" Forward pass for the Encoder\n",
        "\n",
        "        Args:\n",
        "            sequence: source input sequences\n",
        "            training: whether training or not (for Dropout)\n",
        "            encoder_mask: padding mask for the Encoder's Multi-Head Attention\n",
        "        \n",
        "        Returns:\n",
        "            The output of the Encoder (batch_size, length, model_size)\n",
        "            The alignment (attention) vectors for all layers\n",
        "        \"\"\"\n",
        "        embed_out = self.embedding(sequence)\n",
        "\n",
        "        embed_out *= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n",
        "        embed_out += pes[:sequence.shape[1], :]\n",
        "        embed_out = self.embedding_dropout(embed_out)\n",
        "\n",
        "        sub_in = embed_out\n",
        "        alignments = []\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            sub_out, alignment = self.attention[i](sub_in, sub_in, encoder_mask)\n",
        "            sub_out = self.attention_dropout[i](sub_out, training=training)\n",
        "            sub_out = sub_in + sub_out\n",
        "            sub_out = self.attention_norm[i](sub_out)\n",
        "            \n",
        "            alignments.append(alignment)\n",
        "            ffn_in = sub_out\n",
        "\n",
        "            ffn_out = self.dense_2[i](self.dense_1[i](ffn_in))\n",
        "            ffn_out = self.ffn_dropout[i](ffn_out, training=training)\n",
        "            ffn_out = ffn_in + ffn_out\n",
        "            ffn_out = self.ffn_norm[i](ffn_out)\n",
        "\n",
        "            sub_in = ffn_out\n",
        "\n",
        "        return ffn_out, alignments\n",
        "\n",
        "\n",
        "H = 8\n",
        "NUM_LAYERS = 4\n",
        "vocab_size = len(fr_tokenizer.word_index) + 1\n",
        "encoder = Encoder(vocab_size, MODEL_SIZE, NUM_LAYERS, H)\n",
        "print(vocab_size)\n",
        "sequence_in = tf.constant([[1, 2, 3, 0, 0]])\n",
        "encoder_output, _ = encoder(sequence_in)\n",
        "encoder_output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDvSTXI6b28T"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    \"\"\" Class for the Decoder\n",
        "\n",
        "    Args:\n",
        "        model_size: d_model in the paper (depth size of the model)\n",
        "        num_layers: number of layers (Multi-Head Attention + FNN)\n",
        "        h: number of attention heads\n",
        "        embedding: Embedding layer\n",
        "        embedding_dropout: Dropout layer for Embedding\n",
        "        attention_bot: array of bottom Multi-Head Attention layers (self attention)\n",
        "        attention_bot_dropout: array of Dropout layers for bottom Multi-Head Attention\n",
        "        attention_bot_norm: array of LayerNorm layers for bottom Multi-Head Attention\n",
        "        attention_mid: array of middle Multi-Head Attention layers\n",
        "        attention_mid_dropout: array of Dropout layers for middle Multi-Head Attention\n",
        "        attention_mid_norm: array of LayerNorm layers for middle Multi-Head Attention\n",
        "        dense_1: array of first Dense layers for FFN\n",
        "        dense_2: array of second Dense layers for FFN\n",
        "        ffn_dropout: array of Dropout layers for FFN\n",
        "        ffn_norm: array of LayerNorm layers for FFN\n",
        "\n",
        "        dense: Dense layer to compute final output\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, model_size, num_layers, h):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.model_size = model_size\n",
        "        self.num_layers = num_layers\n",
        "        self.h = h\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n",
        "        self.embedding_dropout = tf.keras.layers.Dropout(0.1)\n",
        "        self.attention_bot = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
        "        self.attention_bot_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
        "        self.attention_bot_norm = [tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6) for _ in range(num_layers)]\n",
        "        self.attention_mid = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
        "        self.attention_mid_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
        "        self.attention_mid_norm = [tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6) for _ in range(num_layers)]\n",
        "\n",
        "        self.dense_1 = [tf.keras.layers.Dense(\n",
        "            MODEL_SIZE * 4, activation='relu') for _ in range(num_layers)]\n",
        "        self.dense_2 = [tf.keras.layers.Dense(\n",
        "            model_size) for _ in range(num_layers)]\n",
        "        self.ffn_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
        "        self.ffn_norm = [tf.keras.layers.LayerNormalization(\n",
        "            epsilon=1e-6) for _ in range(num_layers)]\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, sequence, encoder_output, training=True, encoder_mask=None):\n",
        "        \"\"\" Forward pass for the Decoder\n",
        "\n",
        "        Args:\n",
        "            sequence: source input sequences\n",
        "            encoder_output: output of the Encoder (for computing middle attention)\n",
        "            training: whether training or not (for Dropout)\n",
        "            encoder_mask: padding mask for the Encoder's Multi-Head Attention\n",
        "        \n",
        "        Returns:\n",
        "            The output of the Encoder (batch_size, length, model_size)\n",
        "            The bottom alignment (attention) vectors for all layers\n",
        "            The middle alignment (attention) vectors for all layers\n",
        "        \"\"\"\n",
        "        # EMBEDDING AND POSITIONAL EMBEDDING\n",
        "        embed_out = self.embedding(sequence)\n",
        "\n",
        "        embed_out *= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n",
        "        embed_out += pes[:sequence.shape[1], :]\n",
        "        embed_out = self.embedding_dropout(embed_out)\n",
        "\n",
        "        bot_sub_in = embed_out\n",
        "        bot_alignments = []\n",
        "        mid_alignments = []\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            # BOTTOM MULTIHEAD SUB LAYER\n",
        "            seq_len = bot_sub_in.shape[1]\n",
        "\n",
        "            if training:\n",
        "                mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "            else:\n",
        "                mask = None\n",
        "            bot_sub_out, bot_alignment = self.attention_bot[i](bot_sub_in, bot_sub_in, mask)\n",
        "            bot_sub_out = self.attention_bot_dropout[i](bot_sub_out, training=training)\n",
        "            bot_sub_out = bot_sub_in + bot_sub_out\n",
        "            bot_sub_out = self.attention_bot_norm[i](bot_sub_out)\n",
        "            \n",
        "            bot_alignments.append(bot_alignment)\n",
        "\n",
        "            # MIDDLE MULTIHEAD SUB LAYER\n",
        "            mid_sub_in = bot_sub_out\n",
        "\n",
        "            mid_sub_out, mid_alignment = self.attention_mid[i](\n",
        "                mid_sub_in, encoder_output, encoder_mask)\n",
        "            mid_sub_out = self.attention_mid_dropout[i](mid_sub_out, training=training)\n",
        "            mid_sub_out = mid_sub_out + mid_sub_in\n",
        "            mid_sub_out = self.attention_mid_norm[i](mid_sub_out)\n",
        "            \n",
        "            mid_alignments.append(mid_alignment)\n",
        "\n",
        "            # FFN\n",
        "            ffn_in = mid_sub_out\n",
        "\n",
        "            ffn_out = self.dense_2[i](self.dense_1[i](ffn_in))\n",
        "            ffn_out = self.ffn_dropout[i](ffn_out, training=training)\n",
        "            ffn_out = ffn_out + ffn_in\n",
        "            ffn_out = self.ffn_norm[i](ffn_out)\n",
        "\n",
        "            bot_sub_in = ffn_out\n",
        "\n",
        "        logits = self.dense(ffn_out)\n",
        "\n",
        "        return logits, bot_alignments, mid_alignments\n",
        "\n",
        "\n",
        "vocab_size = len(twi_tokenizer.word_index) + 1\n",
        "decoder = Decoder(vocab_size, MODEL_SIZE, NUM_LAYERS, H)\n",
        "\n",
        "sequence_in = tf.constant([[14, 24, 36, 0, 0]])\n",
        "decoder_output, _, _ = decoder(sequence_in, encoder_output)\n",
        "decoder_output.shape\n",
        "\n",
        "\n",
        "crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCUjpyo_cgwP"
      },
      "outputs": [],
      "source": [
        "def loss_func(targets, logits):\n",
        "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
        "    mask = tf.cast(mask, dtype=tf.int64)\n",
        "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "class WarmupThenDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    \"\"\" Learning schedule for training the Transformer\n",
        "\n",
        "    Attributes:\n",
        "        model_size: d_model in the paper (depth size of the model)\n",
        "        warmup_steps: number of warmup steps at the beginning\n",
        "    \"\"\"\n",
        "    def __init__(self, model_size, warmup_steps=4000):\n",
        "        super(WarmupThenDecaySchedule, self).__init__()\n",
        "\n",
        "        self.model_size = model_size\n",
        "        self.model_size = tf.cast(self.model_size, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step_term = tf.math.rsqrt(step)\n",
        "        warmup_term = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.model_size) * tf.math.minimum(step_term, warmup_term)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBfgeF35c54I"
      },
      "outputs": [],
      "source": [
        "lr = WarmupThenDecaySchedule(MODEL_SIZE)\n",
        "optimizer = tf.keras.optimizers.Adam(lr,\n",
        "                                     beta_1=0.9,\n",
        "                                     beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhSb1Or_dHhY"
      },
      "outputs": [],
      "source": [
        "def predict(test_source_text=None):\n",
        "    \"\"\" Predict the output sentence for a given input sentence\n",
        "\n",
        "    Args:\n",
        "        test_source_text: input sentence (raw string)\n",
        "    \n",
        "    Returns:\n",
        "        The encoder's attention vectors\n",
        "        The decoder's bottom attention vectors\n",
        "        The decoder's middle attention vectors\n",
        "        The input string array (input sentence split by ' ')\n",
        "        The output string array\n",
        "    \"\"\"\n",
        "    if test_source_text is None:\n",
        "        test_source_text = raw_data_fr[np.random.choice(len(raw_data_fr))]\n",
        "    print(test_source_text)\n",
        "    test_source_seq = fr_tokenizer.texts_to_sequences([test_source_text])\n",
        "    print(test_source_seq)\n",
        "\n",
        "    en_output, en_alignments = encoder(tf.constant(test_source_seq), training=False)\n",
        "\n",
        "    de_input = tf.constant(\n",
        "        [[twi_tokenizer.word_index['<start>']]], dtype=tf.int64)\n",
        "\n",
        "    out_words = []\n",
        "\n",
        "    while True:\n",
        "        de_output, de_bot_alignments, de_mid_alignments = decoder(de_input, en_output, training=False)\n",
        "        new_word = tf.expand_dims(tf.argmax(de_output, -1)[:, -1], axis=1)\n",
        "        out_words.append(twi_tokenizer.index_word[new_word.numpy()[0][0]])\n",
        "\n",
        "        # Transformer doesn't have sequential mechanism (i.e. states)\n",
        "        # so we have to add the last predicted word to create a new input sequence\n",
        "        de_input = tf.concat((de_input, new_word), axis=-1)\n",
        "\n",
        "        # TODO: get a nicer constraint for the sequence length!\n",
        "        if out_words[-1] == '<end>':\n",
        "            break\n",
        "\n",
        "    print(' '.join(out_words))\n",
        "    return en_alignments, de_bot_alignments, de_mid_alignments, test_source_text.split(' '), out_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBQAucJodgZV"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(source_seq, target_seq_in, target_seq_out):\n",
        "    \"\"\" Execute one training step (forward pass + backward pass)\n",
        "\n",
        "    Args:\n",
        "        source_seq: source sequences\n",
        "        target_seq_in: input target sequences (<start> + ...)\n",
        "        target_seq_out: output target sequences (... + <end>)\n",
        "    \n",
        "    Returns:\n",
        "        The loss value of the current pass\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        encoder_mask = 1 - tf.cast(tf.equal(source_seq, 0), dtype=tf.float32)\n",
        "        # encoder_mask has shape (batch_size, source_len)\n",
        "        # we need to add two more dimensions in between\n",
        "        # to make it broadcastable when computing attention heads\n",
        "        encoder_mask = tf.expand_dims(encoder_mask, axis=1)\n",
        "        encoder_mask = tf.expand_dims(encoder_mask, axis=1)\n",
        "        encoder_output, _ = encoder(source_seq, encoder_mask=encoder_mask)\n",
        "\n",
        "        decoder_output, _, _ = decoder(\n",
        "            target_seq_in, encoder_output, encoder_mask=encoder_mask)\n",
        "\n",
        "        loss = loss_func(target_seq_out, decoder_output)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttZLmCYGdxUk",
        "outputId": "7400a8f5-412d-4941-ab4b-8bdf7f544dee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.3983 Elapsed time 9.82s\n",
            "Epoch 1 Batch 100 Loss 2.1213 Elapsed time 6.43s\n",
            "Voulez vous un autre verre de vin ?\n",
            "[[158, 11, 14, 125, 475, 3, 501, 12]]\n",
            "<end>\n",
            "Epoch 2 Batch 0 Loss 1.8448 Elapsed time 7.85s\n",
            "Epoch 2 Batch 100 Loss 1.6875 Elapsed time 6.45s\n",
            "Asamoah n avait aucune idee de l heure a laquelle Abena rentrerait .\n",
            "[[6, 18, 75, 311, 244, 3, 15, 207, 2, 664, 115, 6157, 1]]\n",
            "sɛ sɛ sɛ sɛ no . <end>\n",
            "Epoch 3 Batch 0 Loss 1.5058 Elapsed time 4.18s\n",
            "Epoch 3 Batch 100 Loss 1.3711 Elapsed time 6.44s\n",
            "Ce ne sont pas des idiots .\n",
            "[[19, 13, 42, 9, 32, 2176, 1]]\n",
            "na na me . <end>\n",
            "Epoch 4 Batch 0 Loss 1.4247 Elapsed time 4.09s\n",
            "Epoch 4 Batch 100 Loss 1.2549 Elapsed time 6.49s\n",
            "Attendez ici .\n",
            "[[927, 74, 1]]\n",
            "na na me . <end>\n",
            "Epoch 5 Batch 0 Loss 1.2460 Elapsed time 4.10s\n",
            "Epoch 5 Batch 100 Loss 1.3356 Elapsed time 6.52s\n",
            "Le premier n etait pas le meilleur .\n",
            "[[10, 367, 18, 37, 9, 10, 360, 1]]\n",
            "na na ɛyɛ den sɛ ɛyɛ . <end>\n",
            "Epoch 6 Batch 0 Loss 1.1605 Elapsed time 4.65s\n",
            "Epoch 6 Batch 100 Loss 1.1674 Elapsed time 6.65s\n",
            "Soyez alerte .\n",
            "[[560, 6155, 1]]\n",
            "m ani gye ho . <end>\n",
            "Epoch 7 Batch 0 Loss 0.9628 Elapsed time 4.23s\n",
            "Epoch 7 Batch 100 Loss 1.0847 Elapsed time 6.63s\n",
            "Nous ne les utilisons plus .\n",
            "[[24, 13, 21, 2856, 38, 1]]\n",
            "yɛn ho . <end>\n",
            "Epoch 8 Batch 0 Loss 0.9708 Elapsed time 4.13s\n",
            "Epoch 8 Batch 100 Loss 1.0321 Elapsed time 6.71s\n",
            "Asamoah n avait plus d argent .\n",
            "[[6, 18, 75, 38, 22, 88, 1]]\n",
            "asamoah yɛ obi a na ɔwɔ sika a na ɔwɔ hɔ . <end>\n",
            "Epoch 9 Batch 0 Loss 0.9290 Elapsed time 4.78s\n",
            "Epoch 9 Batch 100 Loss 0.9410 Elapsed time 6.75s\n",
            "As tu bien dormi ?\n",
            "[[129, 29, 84, 773, 12]]\n",
            "so na w ani gye ho ? <end>\n",
            "Epoch 10 Batch 0 Loss 0.8931 Elapsed time 4.47s\n",
            "Epoch 10 Batch 100 Loss 0.8972 Elapsed time 6.80s\n",
            "Les oreilles d un lapin sont plus longues que celles d un renard .\n",
            "[[21, 2055, 22, 14, 4811, 42, 38, 1952, 16, 2674, 22, 14, 1985, 1]]\n",
            "nnipa a ɛyɛ anigye sen biara a ɛyɛ den . <end>\n",
            "Epoch 11 Batch 0 Loss 0.7582 Elapsed time 4.69s\n",
            "Epoch 11 Batch 100 Loss 0.6932 Elapsed time 6.81s\n",
            "Tenez fermement la raquette .\n",
            "[[1318, 2098, 8, 5897, 1]]\n",
            "so hyɛn no mu . <end>\n",
            "Epoch 12 Batch 0 Loss 0.7751 Elapsed time 4.32s\n",
            "Epoch 12 Batch 100 Loss 0.7652 Elapsed time 6.76s\n",
            "Qui a plante ces arbres ?\n",
            "[[49, 2, 906, 184, 1034, 12]]\n",
            "hena na osu bɛtɔ yi ? <end>\n",
            "Epoch 13 Batch 0 Loss 0.5690 Elapsed time 4.35s\n",
            "Epoch 13 Batch 100 Loss 0.7334 Elapsed time 6.73s\n",
            "Le garcon a une montre a la main .\n",
            "[[10, 251, 2, 23, 380, 2, 8, 364, 1]]\n",
            "abarimaa no wɔ ne dan no mu . <end>\n",
            "Epoch 14 Batch 0 Loss 0.6694 Elapsed time 4.48s\n",
            "Epoch 14 Batch 100 Loss 0.6778 Elapsed time 6.72s\n",
            "Depechons nous de retourner a notre hotel .\n",
            "[[3615, 24, 3, 1029, 2, 148, 437, 1]]\n",
            "ma yɛn ani begye ho sɛ yɛn ani begye ho wɔ yɛn sukuu mu . <end>\n",
            "Epoch 15 Batch 0 Loss 0.5655 Elapsed time 5.00s\n",
            "Epoch 15 Batch 100 Loss 0.4932 Elapsed time 6.73s\n",
            "Asamoah s est fait prendre en photo avec Boatemaa .\n",
            "[[6, 33, 5, 50, 232, 26, 238, 47, 101, 1]]\n",
            "asamoah ne boatemaa ho so mfonini . <end>\n",
            "Epoch 16 Batch 0 Loss 0.5338 Elapsed time 4.41s\n",
            "Epoch 16 Batch 100 Loss 0.5735 Elapsed time 6.73s\n",
            "N oubliez pas de me le rappeler .\n",
            "[[18, 1019, 9, 3, 39, 10, 1827, 1]]\n",
            "mma wo werɛ mmfi sɛ mintumi mma no . <end>\n",
            "Epoch 17 Batch 0 Loss 0.4422 Elapsed time 4.59s\n",
            "Epoch 17 Batch 100 Loss 0.4910 Elapsed time 6.75s\n",
            "Pretez lui autant d argent qu il en a besoin .\n",
            "[[1965, 67, 430, 22, 88, 25, 7, 26, 2, 92, 1]]\n",
            "ma no sika biara . <end>\n",
            "Epoch 18 Batch 0 Loss 0.4084 Elapsed time 4.33s\n",
            "Epoch 18 Batch 100 Loss 0.4254 Elapsed time 6.74s\n",
            "Il ne saura pas .\n",
            "[[7, 13, 1841, 9, 1]]\n",
            "ɔnyɛ saa . <end>\n",
            "Epoch 19 Batch 0 Loss 0.2993 Elapsed time 4.17s\n",
            "Epoch 19 Batch 100 Loss 0.3771 Elapsed time 6.75s\n",
            "J aime le sport .\n",
            "[[17, 64, 10, 1052, 1]]\n",
            "m ani gye agumadi ho . <end>\n",
            "Epoch 20 Batch 0 Loss 0.2854 Elapsed time 4.37s\n",
            "Epoch 20 Batch 100 Loss 0.3268 Elapsed time 6.73s\n",
            "Je pensais qu on prendrait le petit dejeuner ensemble .\n",
            "[[4, 186, 25, 139, 2216, 10, 213, 318, 310, 1]]\n",
            "na misusuw sɛ yɛbɛbom adidi . <end>\n",
            "Epoch 21 Batch 0 Loss 0.2921 Elapsed time 4.36s\n",
            "Epoch 21 Batch 100 Loss 0.3584 Elapsed time 6.72s\n",
            "Il a dit qu il m ecrirait mais il ne l a pas fait .\n",
            "[[7, 2, 63, 25, 7, 46, 5741, 102, 7, 13, 15, 2, 9, 50, 1]]\n",
            "ɔka kyerɛɛ me sɛ ɔmpɛ mma wo . <end>\n",
            "Epoch 22 Batch 0 Loss 0.2509 Elapsed time 4.49s\n",
            "Epoch 22 Batch 100 Loss 0.2851 Elapsed time 6.72s\n",
            "Voulez vous que je dise a Asamoah que nous ne le ferons pas ?\n",
            "[[158, 11, 16, 4, 1078, 2, 6, 16, 24, 13, 10, 2908, 9, 12]]\n",
            "so wopɛ sɛ meka kyerɛ asamoah sɛ yɛrenyɛ ? <end>\n",
            "Epoch 23 Batch 0 Loss 0.1699 Elapsed time 4.54s\n",
            "Epoch 23 Batch 100 Loss 0.2629 Elapsed time 6.72s\n",
            "Je ne veux pas retourner en prison .\n",
            "[[4, 13, 58, 9, 1029, 26, 723, 1]]\n",
            "mempɛ sɛ mekɔ afiase bio . <end>\n",
            "Epoch 24 Batch 0 Loss 0.2139 Elapsed time 4.35s\n",
            "Epoch 24 Batch 100 Loss 0.2180 Elapsed time 6.72s\n",
            "Il pleut depuis dimanche dernier .\n",
            "[[7, 667, 162, 525, 374, 1]]\n",
            "efi kwasida a etwa to koraa a ɛsɛ sɛ yɛkɔ so . <end>\n",
            "Epoch 25 Batch 0 Loss 0.1947 Elapsed time 4.77s\n",
            "Epoch 25 Batch 100 Loss 0.2596 Elapsed time 6.72s\n",
            "Ce mec est hors de son rocker !\n",
            "[[19, 2313, 5, 763, 3, 45, 4537, 193]]\n",
            "saa ɔbarima no baa ma ne bag no ! <end>\n",
            "Epoch 26 Batch 0 Loss 0.1417 Elapsed time 4.57s\n",
            "Epoch 26 Batch 100 Loss 0.2028 Elapsed time 6.72s\n",
            "J ai beaucoup nage pendant les vacances d ete .\n",
            "[[17, 20, 73, 800, 173, 21, 640, 22, 59, 1]]\n",
            "sɛ edu ahohuru bere mu nneɛma pii wɔ dapɛn a asamoah no . <end>\n",
            "Epoch 27 Batch 0 Loss 0.1468 Elapsed time 4.87s\n",
            "Epoch 27 Batch 100 Loss 0.1641 Elapsed time 6.72s\n",
            "Il a ete gueri de ses mauvaises habitudes .\n",
            "[[7, 2, 59, 5388, 3, 100, 1130, 2889, 1]]\n",
            "onyaa ayaresa wɔ ne subammɔne no ho . <end>\n",
            "Epoch 28 Batch 0 Loss 0.1233 Elapsed time 4.52s\n",
            "Epoch 28 Batch 100 Loss 0.1633 Elapsed time 6.72s\n",
            "Je me promene avec mon chien .\n",
            "[[4, 39, 2157, 47, 44, 192, 1]]\n",
            "me ne me kraman nam . <end>\n",
            "Epoch 29 Batch 0 Loss 0.1423 Elapsed time 14.32s\n",
            "Epoch 29 Batch 100 Loss 0.1699 Elapsed time 6.69s\n",
            "Ce livre semble interessant .\n",
            "[[19, 110, 285, 562, 1]]\n",
            "ɛte sɛ nea ne anigye . <end>\n",
            "Epoch 30 Batch 0 Loss 0.1145 Elapsed time 4.34s\n",
            "Epoch 30 Batch 100 Loss 0.1394 Elapsed time 6.73s\n",
            "Kwame a l air terrifie .\n",
            "[[175, 2, 15, 225, 2273, 1]]\n",
            "kwame . <end>\n",
            "Epoch 31 Batch 0 Loss 0.0981 Elapsed time 4.10s\n",
            "Epoch 31 Batch 100 Loss 0.1181 Elapsed time 6.79s\n",
            "La plupart des hotels sont ouverts toute l annee .\n",
            "[[8, 589, 32, 2845, 42, 4707, 195, 15, 254, 1]]\n",
            "wɔma ahɔhodan no mu dodow no ara hokwan sɛ wɔn yɔ adwuma afe . <end>\n",
            "Epoch 32 Batch 0 Loss 0.0762 Elapsed time 4.95s\n",
            "Epoch 32 Batch 100 Loss 0.1311 Elapsed time 6.78s\n",
            "Le bus est parti il y a combien de temps ?\n",
            "[[10, 239, 5, 572, 7, 41, 2, 131, 3, 70, 12]]\n",
            "wɔtoo nsa na ɛwɔ bere a na ɛwɔ he ? <end>\n",
            "Epoch 33 Batch 0 Loss 0.0703 Elapsed time 4.66s\n",
            "Epoch 33 Batch 100 Loss 0.1029 Elapsed time 6.73s\n",
            "Je vais le gronder .\n",
            "[[4, 132, 10, 4191, 1]]\n",
            "mɛsan aka n ano atom . <end>\n",
            "Epoch 34 Batch 0 Loss 0.0749 Elapsed time 4.35s\n",
            "Epoch 34 Batch 100 Loss 0.0926 Elapsed time 6.70s\n",
            "Asamoah apprend la programmation .\n",
            "[[6, 1808, 8, 4996, 1]]\n",
            "asamoah ɛsua <end>\n",
            "Epoch 35 Batch 0 Loss 0.0827 Elapsed time 4.05s\n",
            "Epoch 35 Batch 100 Loss 0.0845 Elapsed time 6.69s\n",
            "Le vieil homme a ete renverse et immediatement transporte a l hopital .\n",
            "[[10, 1025, 137, 2, 59, 950, 35, 2134, 2135, 2, 15, 717, 1]]\n",
            "wɔkyeree akwakoraa no kɔɔ ayaresabea ntɛm ara aberɛ a kar fa no so . <end>\n",
            "Epoch 36 Batch 0 Loss 0.0638 Elapsed time 4.89s\n",
            "Epoch 36 Batch 100 Loss 0.0916 Elapsed time 6.69s\n",
            "Je ne peux pas arreter de fumer .\n",
            "[[4, 13, 83, 9, 386, 3, 486, 1]]\n",
            "mintumi nnyae . <end>\n",
            "Epoch 37 Batch 0 Loss 0.0761 Elapsed time 4.12s\n",
            "Epoch 37 Batch 100 Loss 0.1056 Elapsed time 6.71s\n",
            "Je dois etre honnete . J etais un peu nerveux la premiere fois que j ai passe une IRM .\n",
            "[[4, 118, 55, 855, 1, 17, 160, 14, 105, 669, 8, 423, 117, 16, 17, 20, 133, 23, 5405, 1]]\n",
            "nokwasɛm ni bere bi a minyaa mri ho nimdeɛ a minyaa kakra no ehu kaa me kakra . <end>\n",
            "Epoch 38 Batch 0 Loss 0.0459 Elapsed time 5.20s\n",
            "Epoch 38 Batch 100 Loss 0.0798 Elapsed time 6.72s\n",
            "Je veux juste vous poser une question .\n",
            "[[4, 58, 169, 11, 919, 23, 325, 1]]\n",
            "mepɛ emu nea maka asɛm bi fa wo . <end>\n",
            "Epoch 39 Batch 0 Loss 0.0697 Elapsed time 4.56s\n",
            "Epoch 39 Batch 100 Loss 0.0708 Elapsed time 6.72s\n",
            "Pourquoi dois je parler a Asamoah ?\n",
            "[[97, 118, 4, 111, 2, 6, 12]]\n",
            "dɛn nti na me ne asamoah kasa ? <end>\n",
            "Epoch 40 Batch 0 Loss 0.0593 Elapsed time 4.50s\n",
            "Epoch 40 Batch 100 Loss 0.0644 Elapsed time 6.73s\n",
            "Ils sont revenus .\n",
            "[[53, 42, 2113, 1]]\n",
            "wɔasan aba . <end>\n",
            "Epoch 41 Batch 0 Loss 0.0479 Elapsed time 4.14s\n",
            "Epoch 41 Batch 100 Loss 0.0665 Elapsed time 6.71s\n",
            "Tous mes amis parlent francais .\n",
            "[[85, 109, 221, 1146, 144, 1]]\n",
            "me ne mo nyinaa . <end>\n",
            "Epoch 42 Batch 0 Loss 0.0425 Elapsed time 4.29s\n",
            "Epoch 42 Batch 100 Loss 0.0627 Elapsed time 6.70s\n",
            "Nous ne faisons pas confiance au gouvernement .\n",
            "[[24, 13, 644, 9, 506, 40, 2402, 1]]\n",
            "yenni mu . <end>\n",
            "Epoch 43 Batch 0 Loss 0.0479 Elapsed time 4.13s\n",
            "Epoch 43 Batch 100 Loss 0.0490 Elapsed time 6.72s\n",
            "N importe qui peut le faire .\n",
            "[[18, 488, 49, 82, 10, 43, 1]]\n",
            "obiara betumi ayɛ saa . <end>\n",
            "Epoch 44 Batch 0 Loss 0.0570 Elapsed time 4.26s\n",
            "Epoch 44 Batch 100 Loss 0.0579 Elapsed time 6.72s\n",
            "J ai repare la voiture hier .\n",
            "[[17, 20, 1303, 8, 93, 120, 1]]\n",
            "mehyehyɛɛ . <end>\n",
            "Epoch 45 Batch 0 Loss 0.0502 Elapsed time 4.04s\n",
            "Epoch 45 Batch 100 Loss 0.0535 Elapsed time 6.69s\n",
            "Je n ai jamais ete aussi humilie de ma vie .\n",
            "[[4, 18, 20, 86, 59, 164, 2722, 3, 71, 172, 1]]\n",
            "m anim angu ase saa da wɔ m asetram . <end>\n",
            "Epoch 46 Batch 0 Loss 0.0287 Elapsed time 4.60s\n",
            "Epoch 46 Batch 100 Loss 0.0439 Elapsed time 6.70s\n",
            "Je pense a lui tout le temps .\n",
            "[[4, 78, 2, 67, 51, 10, 70, 1]]\n",
            "misusuw . <end>\n",
            "Epoch 47 Batch 0 Loss 0.0316 Elapsed time 4.06s\n",
            "Epoch 47 Batch 100 Loss 0.0346 Elapsed time 6.70s\n",
            "Es tu toujours reveille ?\n",
            "[[119, 29, 95, 749, 12]]\n",
            "so holah ? <end>\n",
            "Epoch 48 Batch 0 Loss 0.0350 Elapsed time 4.13s\n",
            "Epoch 48 Batch 100 Loss 0.0295 Elapsed time 6.71s\n",
            "Je suis devenu son ami .\n",
            "[[4, 36, 454, 45, 206, 1]]\n",
            "mebɛyɛɛ n adamfo . <end>\n",
            "Epoch 49 Batch 0 Loss 0.0282 Elapsed time 4.19s\n",
            "Epoch 49 Batch 100 Loss 0.0504 Elapsed time 6.71s\n",
            "Asamoah m a demande de parler plus lentement .\n",
            "[[6, 46, 2, 121, 3, 111, 38, 897, 1]]\n",
            "asamoah kar no ase da ne ho ban . <end>\n",
            "Epoch 50 Batch 0 Loss 0.0286 Elapsed time 4.55s\n",
            "Epoch 50 Batch 100 Loss 0.0453 Elapsed time 6.71s\n",
            "Il y a du lait dans le refrigerateur .\n",
            "[[7, 41, 2, 34, 513, 31, 10, 1375, 1]]\n",
            "nufusu wɔ . <end>\n",
            "Epoch 51 Batch 0 Loss 0.0296 Elapsed time 4.12s\n",
            "Epoch 51 Batch 100 Loss 0.0422 Elapsed time 6.71s\n",
            "Le navire n est pas equipe de radar .\n",
            "[[10, 955, 18, 5, 9, 1051, 3, 5794, 1]]\n",
            "po so hyɛn no w ani nnye . <end>\n",
            "Epoch 52 Batch 0 Loss 0.0440 Elapsed time 4.47s\n",
            "Epoch 52 Batch 100 Loss 0.0381 Elapsed time 6.71s\n",
            "Je le sais bien .\n",
            "[[4, 10, 94, 84, 1]]\n",
            "minim no yiye . <end>\n",
            "Epoch 53 Batch 0 Loss 0.0409 Elapsed time 4.19s\n",
            "Epoch 53 Batch 100 Loss 0.0272 Elapsed time 6.72s\n",
            "Je pensais que c etait peut etre de la cocaine mais ce n etait que de la farine .\n",
            "[[4, 186, 16, 30, 37, 82, 55, 3, 8, 3544, 102, 19, 18, 37, 16, 3, 8, 1889, 1]]\n",
            "na ɛyɛ te sɛ nea ɛda nsow . <end>\n",
            "Epoch 54 Batch 0 Loss 0.0227 Elapsed time 4.48s\n",
            "Epoch 54 Batch 100 Loss 0.0454 Elapsed time 6.70s\n",
            "Asamoah a utilise tout son argent .\n",
            "[[6, 2, 1147, 51, 45, 88, 1]]\n",
            "sɛ ɔde ne sika nyinaa a mfaso nyinaa so . <end>\n",
            "Epoch 55 Batch 0 Loss 0.0394 Elapsed time 4.59s\n",
            "Epoch 55 Batch 100 Loss 0.0354 Elapsed time 6.70s\n",
            "Toi et Asamoah etiez amis . Qu est il arrive ?\n",
            "[[147, 35, 6, 984, 221, 1, 25, 5, 7, 183, 12]]\n",
            "dɛn ne wo bɛbom ayɛ ? <end>\n",
            "Epoch 56 Batch 0 Loss 0.0278 Elapsed time 4.34s\n",
            "Epoch 56 Batch 100 Loss 0.0384 Elapsed time 6.70s\n",
            "On se demande comment c est possible .\n",
            "[[139, 48, 121, 124, 30, 5, 359, 1]]\n",
            "ɛyɛ sɛ wobɛyɛ saa bere no . <end>\n",
            "Epoch 57 Batch 0 Loss 0.0201 Elapsed time 4.40s\n",
            "Epoch 57 Batch 100 Loss 0.0416 Elapsed time 6.69s\n",
            "La familiarite engendre le mepris et les enfants .\n",
            "[[8, 5155, 5156, 10, 2963, 35, 21, 165, 1]]\n",
            "sɛ menom nso na ɛyɛ atoro . <end>\n",
            "Epoch 58 Batch 0 Loss 0.0179 Elapsed time 4.41s\n",
            "Epoch 58 Batch 100 Loss 0.0304 Elapsed time 6.72s\n",
            "Vous auriez du assister a la reunion d aujourd hui .\n",
            "[[11, 1045, 34, 1321, 2, 8, 414, 22, 140, 141, 1]]\n",
            "anka ɛsɛ sɛ wokɔ ɛnnɛyi nhyiam . <end>\n",
            "Epoch 59 Batch 0 Loss 0.0203 Elapsed time 4.40s\n",
            "Epoch 59 Batch 100 Loss 0.0227 Elapsed time 6.70s\n",
            "Je travaille dans une ecole de langue .\n",
            "[[4, 272, 31, 23, 166, 3, 519, 1]]\n",
            "meyɛ kasa wɔ sukuu bi mu . <end>\n",
            "Epoch 60 Batch 0 Loss 0.0276 Elapsed time 4.42s\n",
            "Epoch 60 Batch 100 Loss 0.0292 Elapsed time 6.72s\n",
            "Je me suis bien amuse a la fete hier soir .\n",
            "[[4, 39, 36, 84, 989, 2, 8, 281, 120, 152, 1]]\n",
            "m ani gyei sɛ besi ɔdasum no anadwo a na etwaam no . <end>\n",
            "Epoch 61 Batch 0 Loss 0.0310 Elapsed time 4.81s\n",
            "Epoch 61 Batch 100 Loss 0.0281 Elapsed time 6.71s\n",
            "Il s est interesse a l histoire .\n",
            "[[7, 33, 5, 677, 2, 15, 429, 1]]\n",
            "n ani gyee ho . <end>\n",
            "Epoch 62 Batch 0 Loss 0.0218 Elapsed time 4.25s\n",
            "Epoch 62 Batch 100 Loss 0.0266 Elapsed time 6.70s\n",
            "Merci pour le merveilleux cadeau .\n",
            "[[361, 28, 10, 1702, 487, 1]]\n",
            "yɛda mo ase wɔ akyɛde nwonwaso yi ho . <end>\n",
            "Epoch 63 Batch 0 Loss 0.0171 Elapsed time 4.53s\n",
            "Epoch 63 Batch 100 Loss 0.0365 Elapsed time 6.70s\n",
            "Je ne sais pas quoi te dire .\n",
            "[[4, 13, 94, 9, 209, 90, 91, 1]]\n",
            "minnim . <end>\n",
            "Epoch 64 Batch 0 Loss 0.0343 Elapsed time 4.05s\n",
            "Epoch 64 Batch 100 Loss 0.0209 Elapsed time 6.69s\n",
            "Je ne pense pas pouvoir traduire ce document sans votre aide .\n",
            "[[4, 13, 78, 9, 465, 1206, 19, 1422, 161, 57, 200, 1]]\n",
            "minsusuw sɛ metumi akyerɛ wo a wo ho mmoa nka ho . <end>\n",
            "Epoch 65 Batch 0 Loss 0.0275 Elapsed time 4.72s\n",
            "Epoch 65 Batch 100 Loss 0.0362 Elapsed time 6.70s\n",
            "Qui a dit que ce serait facile ?\n",
            "[[49, 2, 63, 16, 19, 366, 346, 12]]\n",
            "hena na ɔkae ? <end>\n",
            "Epoch 66 Batch 0 Loss 0.0223 Elapsed time 4.19s\n",
            "Epoch 66 Batch 100 Loss 0.0212 Elapsed time 6.70s\n",
            "Si je connaissais son adresse je lui ecrirais .\n",
            "[[62, 4, 1138, 45, 921, 4, 67, 4592, 1]]\n",
            "sɛ minim ne address no a nanka mekyerɛw no . <end>\n",
            "Epoch 67 Batch 0 Loss 0.0276 Elapsed time 4.61s\n",
            "Epoch 67 Batch 100 Loss 0.0255 Elapsed time 6.70s\n",
            "Je lis le New York Times .\n",
            "[[4, 1604, 10, 747, 884, 5785, 1]]\n",
            "merekenkan new york times . <end>\n",
            "Epoch 68 Batch 0 Loss 0.0184 Elapsed time 4.24s\n",
            "Epoch 68 Batch 100 Loss 0.0255 Elapsed time 6.69s\n",
            "Vous paierez pour cela .\n",
            "[[11, 2710, 28, 98, 1]]\n",
            "wubetua ho . <end>\n",
            "Epoch 69 Batch 0 Loss 0.0224 Elapsed time 4.12s\n",
            "Epoch 69 Batch 100 Loss 0.0168 Elapsed time 6.71s\n",
            "Asamoah a ete vole .\n",
            "[[6, 2, 59, 769, 1]]\n",
            "wɔbɔɔ . <end>\n",
            "Epoch 70 Batch 0 Loss 0.0207 Elapsed time 4.05s\n",
            "Epoch 70 Batch 100 Loss 0.0149 Elapsed time 6.72s\n",
            "Comment se fait il que tu sois toujours en retard a l ecole ?\n",
            "[[124, 48, 50, 7, 16, 29, 1169, 95, 26, 575, 2, 15, 166, 12]]\n",
            "adɛn nti na bere nyinaa woka akyi kɔ sukuu ? <end>\n",
            "Epoch 71 Batch 0 Loss 0.0204 Elapsed time 4.62s\n",
            "Epoch 71 Batch 100 Loss 0.0208 Elapsed time 6.70s\n",
            "Je suis un peu occupe maintenant .\n",
            "[[4, 36, 14, 105, 356, 142, 1]]\n",
            "seesei minni adagyew koraa . <end>\n",
            "Epoch 72 Batch 0 Loss 0.0167 Elapsed time 4.26s\n",
            "Epoch 72 Batch 100 Loss 0.0240 Elapsed time 6.71s\n",
            "Cette annee est une annee importante pour moi .\n",
            "[[65, 254, 5, 23, 254, 1368, 28, 56, 1]]\n",
            "afe adwenem . <end>\n",
            "Epoch 73 Batch 0 Loss 0.0193 Elapsed time 4.12s\n",
            "Epoch 73 Batch 100 Loss 0.0253 Elapsed time 6.70s\n",
            "Je me sens mal .\n",
            "[[4, 39, 306, 177, 1]]\n",
            "ɛhaw me yiye . <end>\n",
            "Epoch 74 Batch 0 Loss 0.0147 Elapsed time 4.21s\n",
            "Epoch 74 Batch 100 Loss 0.0165 Elapsed time 6.70s\n",
            "Tu ne devrais pas te plaindre .\n",
            "[[29, 13, 385, 9, 90, 1663, 1]]\n",
            "ɛnsɛ . <end>\n",
            "Epoch 75 Batch 0 Loss 0.0198 Elapsed time 4.06s\n",
            "Epoch 75 Batch 100 Loss 0.0242 Elapsed time 6.71s\n",
            "Asamoah a du mal a suivre le reste de la classe .\n",
            "[[6, 2, 34, 177, 2, 828, 10, 265, 3, 8, 375, 1]]\n",
            "ɛyɛ den sɛ obi bɛkɔ so no adesuakuw no mufo a aka no abɔ . <end>\n",
            "Epoch 76 Batch 0 Loss 0.0125 Elapsed time 4.95s\n",
            "Epoch 76 Batch 100 Loss 0.0194 Elapsed time 6.70s\n",
            "J ai fait un reve tres etrange la nuit derniere .\n",
            "[[17, 20, 50, 14, 350, 61, 680, 8, 245, 185, 1]]\n",
            "anadwo a mesoo no yɛ dae huhuuhu no . <end>\n",
            "Epoch 77 Batch 0 Loss 0.0151 Elapsed time 4.53s\n",
            "Epoch 77 Batch 100 Loss 0.0219 Elapsed time 6.69s\n",
            "J ai grandi ici a Boston .\n",
            "[[17, 20, 1908, 74, 2, 155, 1]]\n",
            "wɔtetee ha . <end>\n",
            "Epoch 78 Batch 0 Loss 0.0097 Elapsed time 4.11s\n",
            "Epoch 78 Batch 100 Loss 0.0239 Elapsed time 6.70s\n",
            "Asamoah a donne beaucoup d argent a des uvres caritatives .\n",
            "[[6, 2, 167, 73, 22, 88, 2, 32, 5661, 5662, 1]]\n",
            "nana ama adɔe . <end>\n",
            "Epoch 79 Batch 0 Loss 0.0086 Elapsed time 4.19s\n",
            "Epoch 79 Batch 100 Loss 0.0148 Elapsed time 6.70s\n",
            "Le diner est pret .\n",
            "[[10, 344, 5, 379, 1]]\n",
            "aduane aben krado . <end>\n",
            "Epoch 80 Batch 0 Loss 0.0126 Elapsed time 4.19s\n",
            "Epoch 80 Batch 100 Loss 0.0233 Elapsed time 6.69s\n",
            "Elle l a vaincu .\n",
            "[[27, 15, 2, 1867, 1]]\n",
            "odii so nkonim . <end>\n",
            "Epoch 81 Batch 0 Loss 0.0099 Elapsed time 4.19s\n",
            "Epoch 81 Batch 100 Loss 0.0206 Elapsed time 6.70s\n",
            "Asamoah est revenu avec un fusil de chasse .\n",
            "[[6, 5, 770, 47, 14, 2973, 3, 974, 1]]\n",
            "asamoah san baa mma no . <end>\n",
            "Epoch 82 Batch 0 Loss 0.0219 Elapsed time 4.34s\n",
            "Epoch 82 Batch 100 Loss 0.0127 Elapsed time 6.70s\n",
            "Vous etes plus intelligent que vous n en avez l air .\n",
            "[[11, 116, 38, 733, 16, 11, 18, 26, 52, 15, 225, 1]]\n",
            "w biara . <end>\n",
            "Epoch 83 Batch 0 Loss 0.0131 Elapsed time 4.12s\n",
            "Epoch 83 Batch 100 Loss 0.0162 Elapsed time 6.71s\n",
            "Veuillez prendre en charge cette cle .\n",
            "[[235, 232, 26, 1098, 65, 377, 1]]\n",
            "yɛsrɛ ho di saa . <end>\n",
            "Epoch 84 Batch 0 Loss 0.0120 Elapsed time 4.25s\n",
            "Epoch 84 Batch 100 Loss 0.0171 Elapsed time 6.71s\n",
            "Qui est mon professeur ?\n",
            "[[49, 5, 44, 286, 12]]\n",
            "hena ? <end>\n",
            "Epoch 85 Batch 0 Loss 0.0112 Elapsed time 4.06s\n",
            "Epoch 85 Batch 100 Loss 0.0188 Elapsed time 6.71s\n",
            "Voici le bus .\n",
            "[[787, 10, 239, 1]]\n",
            "bɔs . <end>\n",
            "Epoch 86 Batch 0 Loss 0.0082 Elapsed time 4.06s\n",
            "Epoch 86 Batch 100 Loss 0.0129 Elapsed time 6.70s\n",
            "J aimerais vraiment pouvoir etre la avec toi .\n",
            "[[17, 146, 156, 465, 55, 8, 47, 147, 1]]\n",
            "me ne wo ankasa bɛbɔ nkɔmmɔ . <end>\n",
            "Epoch 87 Batch 0 Loss 0.0126 Elapsed time 4.39s\n",
            "Epoch 87 Batch 100 Loss 0.0138 Elapsed time 6.69s\n",
            "Puis il s habilla fourra ses notes de cours dans sa mallette et devala les escaliers .\n",
            "[[229, 7, 33, 6149, 6150, 100, 1714, 3, 824, 31, 69, 2576, 35, 6151, 21, 1429, 1]]\n",
            "afei ne na emu nea enti a ɔde ne ho twe nsɛm ne no si ne bag mu . <end>\n",
            "Epoch 88 Batch 0 Loss 0.0079 Elapsed time 5.23s\n",
            "Epoch 88 Batch 100 Loss 0.0115 Elapsed time 6.68s\n",
            "Un perroquet peut imiter la voix d une personne .\n",
            "[[14, 4412, 82, 2542, 8, 732, 22, 23, 104, 1]]\n",
            "ako . <end>\n",
            "Epoch 89 Batch 0 Loss 0.0182 Elapsed time 4.05s\n",
            "Epoch 89 Batch 100 Loss 0.0212 Elapsed time 6.70s\n",
            "Nous allons nous acheter une voiture .\n",
            "[[24, 442, 24, 211, 23, 93, 1]]\n",
            "yɛkɔtotɔ kar . <end>\n",
            "Epoch 90 Batch 0 Loss 0.0137 Elapsed time 4.11s\n",
            "Epoch 90 Batch 100 Loss 0.0107 Elapsed time 6.69s\n",
            "Comment les pyramides ont elles ete construites ?\n",
            "[[124, 21, 4298, 72, 688, 59, 2734, 12]]\n",
            "ɔkwan bɛn so no wosisii afasu no ? <end>\n",
            "Epoch 91 Batch 0 Loss 0.0210 Elapsed time 4.44s\n",
            "Epoch 91 Batch 100 Loss 0.0105 Elapsed time 6.69s\n",
            "Asamoah connait le pere d Esi .\n",
            "[[6, 499, 10, 107, 22, 145, 1]]\n",
            "sɛnea esi papa no . <end>\n",
            "Epoch 92 Batch 0 Loss 0.0092 Elapsed time 4.24s\n",
            "Epoch 92 Batch 100 Loss 0.0199 Elapsed time 6.70s\n",
            "Tout ce travail m epuisait .\n",
            "[[51, 19, 112, 46, 3474, 1]]\n",
            "adwuma yi nyinaa brɛɛ me ase . <end>\n",
            "Epoch 93 Batch 0 Loss 0.0046 Elapsed time 4.39s\n",
            "Epoch 93 Batch 100 Loss 0.0129 Elapsed time 6.69s\n",
            "Asamoah est mort jeune .\n",
            "[[6, 5, 241, 305, 1]]\n",
            "bere no asamoah wui no . <end>\n",
            "Epoch 94 Batch 0 Loss 0.0092 Elapsed time 4.32s\n",
            "Epoch 94 Batch 100 Loss 0.0122 Elapsed time 6.69s\n",
            "J apporterai une autre serviette .\n",
            "[[17, 4738, 23, 125, 1684, 1]]\n",
            "mɛsan . <end>\n",
            "Epoch 95 Batch 0 Loss 0.0180 Elapsed time 4.05s\n",
            "Epoch 95 Batch 100 Loss 0.0064 Elapsed time 6.69s\n",
            "Quel regime est le meilleur ?\n",
            "[[130, 1235, 5, 10, 360, 12]]\n",
            "aduan he ? <end>\n",
            "Epoch 96 Batch 0 Loss 0.0064 Elapsed time 4.11s\n",
            "Epoch 96 Batch 100 Loss 0.0210 Elapsed time 6.70s\n",
            "Le secteur manufacturier est frenetique face a la nouvelle politique monetaire .\n",
            "[[10, 4159, 4160, 5, 4161, 625, 2, 8, 223, 1247, 4162, 1]]\n",
            "nnwumakuw nhyehyɛe foforo sika ho koraa ama wɔn ho nhyehyɛe nhyehyɛe no so . <end>\n",
            "Epoch 97 Batch 0 Loss 0.0185 Elapsed time 4.86s\n",
            "Epoch 97 Batch 100 Loss 0.0094 Elapsed time 6.69s\n",
            "Les mathematiques sont difficiles pour moi .\n",
            "[[21, 1159, 42, 2428, 28, 56, 1]]\n",
            "akonta ma me <end>\n",
            "Epoch 98 Batch 0 Loss 0.0147 Elapsed time 4.11s\n",
            "Epoch 98 Batch 100 Loss 0.0125 Elapsed time 6.69s\n",
            "A t il un signe distinctif ?\n",
            "[[2, 60, 7, 14, 722, 4340, 12]]\n",
            "so ? <end>\n",
            "Epoch 99 Batch 0 Loss 0.0116 Elapsed time 4.05s\n",
            "Epoch 99 Batch 100 Loss 0.0174 Elapsed time 6.70s\n",
            "Asamoah a bu une tasse de cafe .\n",
            "[[6, 2, 567, 23, 673, 3, 303, 1]]\n",
            "sɛ anadwo kuruwa hwee . <end>\n",
            "Epoch 100 Batch 0 Loss 0.0058 Elapsed time 4.24s\n",
            "Epoch 100 Batch 100 Loss 0.0159 Elapsed time 6.69s\n",
            "Quelqu un peut il me dire l heure ?\n",
            "[[174, 14, 82, 7, 39, 91, 15, 207, 12]]\n",
            "so ? <end>\n"
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 100\n",
        "\n",
        "starttime = time.time()\n",
        "for e in range(NUM_EPOCHS):\n",
        "    for batch, (source_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
        "        loss = train_step(source_seq, target_seq_in,\n",
        "                          target_seq_out)\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f} Elapsed time {:.2f}s'.format(\n",
        "                e + 1, batch, loss.numpy(), time.time() - starttime))\n",
        "            starttime = time.time()\n",
        "\n",
        "    try:\n",
        "        predict()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        continue"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZlAOV/vBgT4+fbRN17u9k",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}