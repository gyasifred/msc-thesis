{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/transformer_twi_fr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is based on the implementation of the paper [Attention is All you Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017). The code are adapted from the Tenssorflow repository at https://www.tensorflow.org/text/tutorials/transformer#build_the_transformer and a big thanks to [[Crater David]](https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:37370749) \n"
      ],
      "metadata": {
        "id": "6qFzQxA29DJp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hblEgiSfLEck"
      },
      "source": [
        "# Install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U tensorflow-text-nightly==2.11.0.dev20220817\n",
        "!pip install -U -q tf-nightly==2.11.0.dev20220817"
      ],
      "metadata": {
        "id": "naWgZqfLsOiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f769092e-425f-4b7e-af86-69eb6391fa29"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 5.9 MB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 582.0 MB 14 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 57.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 439 kB 70.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 51.1 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY7seog2LQOL"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gwubBvId4uGK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W18p7g2lLatA"
      },
      "source": [
        "# preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ovdl-UER8hMX"
      },
      "outputs": [],
      "source": [
        "# This code was adapted from https://github.com/GhanaNLP/kasa/blob/master/Kasa/Preprocessing.py\n",
        "# A subclass of the TFT  for preprocessing data\n",
        "# import required library\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "class Preprocessing:\n",
        "    # dummy initialization method\n",
        "    def __init__(self):\n",
        "        # initialize with some default parameters here later\n",
        "        pass\n",
        "\n",
        "    # read in parallel twi - french dataset\n",
        "    def read_parallel_dataset(self, filepath_twi, filepath_french):\n",
        "\n",
        "        # read french data\n",
        "        french_data = []\n",
        "        with open(filepath_french, encoding='utf-8') as file:\n",
        "            line = file.readline()\n",
        "            cnt = 1\n",
        "            while line:\n",
        "                french_data.append(line.strip())\n",
        "                line = file.readline()\n",
        "                cnt += 1\n",
        "\n",
        "        # read twi data\n",
        "        twi_data = []\n",
        "        with open(filepath_twi, encoding='utf-8') as file:\n",
        "\n",
        "            # twi=file.read()\n",
        "            line = file.readline()\n",
        "            cnt = 1\n",
        "            while line:\n",
        "                twi_data.append(line.strip())\n",
        "                line = file.readline()\n",
        "                cnt += 1\n",
        "\n",
        "        return twi_data, french_data\n",
        "\n",
        "    # Define a helper function to remove string accents\n",
        "\n",
        "    def removeStringAccent(self, s):\n",
        "        return ''.join(\n",
        "            c for c in unicodedata.normalize('NFD', s)\n",
        "            if unicodedata.category(c) != 'Mn'\n",
        "        )\n",
        "\n",
        "    # normalize input twi sentence\n",
        "    def normalize_twi(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.ƆɔɛƐ!?’]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "\n",
        "    # normalize input french sentence\n",
        "    def normalize_fr(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "B4asafNW9HIt"
      },
      "outputs": [],
      "source": [
        "# Create an instance of tft preprocessing class\n",
        "\n",
        "TwiFrPreprocessor = Preprocessing()\n",
        "\n",
        "# Read raw parallel dataset\n",
        "raw_data_twi,raw_data_fr = TwiFrPreprocessor.read_parallel_dataset(\n",
        "        filepath_twi='/content/verified_twi.txt',\n",
        "        filepath_french='/content/verified_french.txt')\n",
        "\n",
        "# Normalize the raw data\n",
        "raw_data_fr = [TwiFrPreprocessor.normalize_fr(data) for data in raw_data_fr]\n",
        "raw_data_twi = [TwiFrPreprocessor.normalize_twi(data) for data in raw_data_twi]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au81TkNOLwnB"
      },
      "source": [
        "# split data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5l08UvTL9mtx"
      },
      "outputs": [],
      "source": [
        "# split the dataset into training and test sets\n",
        "# Keep 20% of the data as test set\n",
        "train_twi,test_twi,train_fr,test_fr = train_test_split(raw_data_twi,raw_data_fr, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VEbWCsNi9wao"
      },
      "outputs": [],
      "source": [
        "# define function to write text to txt file\n",
        "def writeTotxt(destination,data):\n",
        "  with open(destination, 'w') as f:\n",
        "    for line in data:\n",
        "        f.write(f\"{line}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3q4djANP-9g6"
      },
      "outputs": [],
      "source": [
        "# write the preprocess traning and test dataset to a file\n",
        "writeTotxt('train_twi.txt',train_twi)\n",
        "writeTotxt('train_fr.txt',train_fr)\n",
        "writeTotxt('test_twi.txt',test_twi)\n",
        "writeTotxt('test_fr.txt',test_fr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao2l87AzL_kE"
      },
      "source": [
        "# Build tf dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0WZrSEP9_ISJ"
      },
      "outputs": [],
      "source": [
        "# build tf datasets from traning and test sentences in both languages\n",
        "train_dataset_fr = tf.data.TextLineDataset('/content/train_fr.txt')\n",
        "train_dataset_tw = tf.data.TextLineDataset('/content/train_twi.txt')\n",
        "\n",
        "val_dataset_fr = tf.data.TextLineDataset('/content/test_fr.txt')\n",
        "val_dataset_tw = tf.data.TextLineDataset('/content/test_twi.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mmksbF-CAEXg"
      },
      "outputs": [],
      "source": [
        "# combine languages into single dataset\n",
        "trained_combined = tf.data.Dataset.zip((train_dataset_tw, train_dataset_fr))\n",
        "val_combined = tf.data.Dataset.zip((val_dataset_tw, val_dataset_fr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VZJ-hiEUBLDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e92b7bc7-56fb-4c15-f246-fcdc9a6a6142"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Twi:  Dɛn na mefi ase ?\n",
            "French:  Par quoi dois je commencer ?\n",
            "Twi:  Meresua sikasɛm ho ade wɔ suapɔn\n",
            "French:  J etudie l economie au college .\n",
            "Twi:  Saa asubɔnten yi mu nnɔ pii saa bere no .\n",
            "French:  Cette riviere devient peu profonde a cet endroit .\n",
            "Twi:  Asamoah ntumi nhu bere tenten a ɛsɛ sɛ ɔtwɛn Araba .\n",
            "French:  Asamoah se demanda combien de temps il devrait attendre Araba .\n",
            "Twi:  Dodow a me mfe rekɔ anim no dodow no ara na mekae nneɛma a amma saa da .\n",
            "French:  Plus je vieillis plus je me souviens clairement de choses qui ne se sont jamais produites .\n"
          ]
        }
      ],
      "source": [
        "# verify trained_combined dataset is correct\n",
        "for tw, fr in trained_combined.take(5):\n",
        "  print(\"Twi: \", tw.numpy().decode('utf-8'))\n",
        "  print(\"French: \", fr.numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "He8jA0hVBhGr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fe56ad3-b61b-4a1f-dc5f-9595b62e2427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Twi:  Mepɛ sɛ mihu nea enti a woyɛ saa .\n",
            "French:  J aimerais savoir pourquoi tu l as fait .\n",
            "Twi:  Migye di sɛ yɛamma w ani ampa wo so .\n",
            "French:  J espere que nous ne vous avons pas fait attendre .\n",
            "Twi:  Asamoa antwa sɔhwɛ no\n",
            "French:  Asamoah a echoue a l examen .\n",
            "Twi:  So ɛhaw wo ?\n",
            "French:  T ai je blesse ?\n",
            "Twi:  Ma yɛmfa no sɛ serew .\n",
            "French:  Essayons de faire rire Asamoah .\n"
          ]
        }
      ],
      "source": [
        "# verify val_combined dataset is correct\n",
        "for tw, fr in val_combined.take(5):\n",
        "  print(\"Twi: \", tw.numpy().decode('utf-8'))\n",
        "  print(\"French: \", fr.numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaxFg3TZMIK8"
      },
      "source": [
        "# load tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. The tokenizer is which was build from the parallel corpus.\n",
        "2. The code are available on the link https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/build_subword_tokenizer.ipynb\n"
      ],
      "metadata": {
        "id": "Vjb-3AU1BnrK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "aR_vgyQgBo_T"
      },
      "outputs": [],
      "source": [
        "model_named = '/content/drive/MyDrive/translate_fr_twi_converter'\n",
        "# Verify tokenizer model can be reloaded\n",
        "tokenizers = tf.saved_model.load(model_named)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "hdHMl3icEjos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "785300ca-f0a5-4225-993c-aada51913bdb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'je', b'##e', b'##he', b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Verify tokenizer works on french sentence\n",
        "tokens = tokenizers.fr.tokenize(['je suis étudiant'])\n",
        "text_tokens = tokenizers.fr.lookup(tokens)\n",
        "text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aSOMXh_de04Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40322e09-64b7-4aa9-f779-d15ebf8f59c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "je suis etudiant\n"
          ]
        }
      ],
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.fr.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dBNqSqeSfChq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02695545-69f5-47eb-9b54-f72838160265"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'na', b'atwa', b'biara', b'mu',\n",
              "  b'\\xc9\\x94b\\xc9\\x9bk\\xc9\\x94', b'wob\\xc9\\x9by\\xc9\\x9b', b'pii',\n",
              "  b'w\\xc9\\x94', b'a', b'nufusu', b'anaa', b'polisifo', b'h\\xc9\\x94', b'.',\n",
              "  b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Verify tokenizer works on twi sentence\n",
        "tokens = tokenizers.twi.tokenize(['Na mmarimaa ne mmeawa pii wɔ agudibea hɔ .'])\n",
        "text_tokens = tokenizers.twi.lookup(tokens)\n",
        "text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "-lS1xMYQfQR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a89df7d6-6041-4fb5-fd97-4e3c2dfc44ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "na mmarimaa ne mmeawa pii wɔ agudibea hɔ .\n"
          ]
        }
      ],
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.twi.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQVo6G3OMfac"
      },
      "source": [
        "# Check sentence with maximum length"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By checking the maximum length of sentence, you can suitably select the MAX_TOKENS. The MAX_TOKKENS help drops examples longer than the maximum number of tokens (MAX_TOKENS). Without limiting the size of sequences, the performance may be negatively affected.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vvoh1vRgCxSb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7VJxJuNufTCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76ac45d5-bdc0-4573-a003-d39d03c27808"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "......................................................................................................................................................................................................................................................................................................................................................................................................................."
          ]
        }
      ],
      "source": [
        "#The distribution of tokens per example in the dataset is as follows:\n",
        "\n",
        "lengths = []\n",
        "\n",
        "for twi_examples, fr_examples in trained_combined.batch(50):\n",
        "  twi_tokens = tokenizers.twi.tokenize(twi_examples)\n",
        "  lengths.append(twi_tokens.row_lengths())\n",
        "\n",
        "  fr_tokens = tokenizers.fr.tokenize(fr_examples)\n",
        "  lengths.append(fr_tokens.row_lengths())\n",
        "  print('.', end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "ri-qmhyyiBSa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "3a8bbfaa-b04f-4bd0-d40e-999db6127c0e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "all_lengths = np.concatenate(lengths)\n",
        "\n",
        "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
        "plt.ylim(plt.ylim())\n",
        "max_length = max(all_lengths)\n",
        "plt.plot([max_length, max_length], plt.ylim())\n",
        "plt.title(f'Maximum tokens per example: {max_length}');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DYwhdWAMpnW"
      },
      "source": [
        "# Tokenize the training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5piHJYkWjKii"
      },
      "outputs": [],
      "source": [
        "# function to drops examples longer than the maximum number of tokens (MAX_TOKENS).\n",
        "MAX_TOKENS = 40\n",
        "\n",
        "def filter_max_tokens(l1, l2):\n",
        "  num_tokens = tf.maximum(tf.shape(l1)[1],tf.shape(l2)[1])\n",
        "  return num_tokens < MAX_TOKENS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "GC7UlwEeiNTO"
      },
      "outputs": [],
      "source": [
        "# Routine to tokenize sentence pairs; needed to make sentence batches for training\n",
        "def tokenize_pairs(twi,fr):\n",
        "  tw = tokenizers.twi.tokenize(twi)\n",
        "  # Convert from ragged to dense, padding with zeros.\n",
        "  tw = tw.to_tensor()\n",
        "  fr = tokenizers.fr.tokenize(fr)\n",
        "  # Convert from ragged to dense, padding with zeros.\n",
        "  fr = fr.to_tensor()\n",
        "  return tw,fr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EWMySNLzklc_"
      },
      "outputs": [],
      "source": [
        "# create training batches\n",
        "BUFFER_SIZE = len(train_twi)\n",
        "BATCH_SIZE = 64\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .filter(filter_max_tokens)\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "# Create training and validation set batches.\n",
        "train_batches = make_batches(trained_combined)\n",
        "val_batches = make_batches(val_combined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNFMseSdM3wr"
      },
      "source": [
        "# Positional encoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional encodings are added to the embeddings to give the model some information about the relative position of the tokens in the sentence so the model can learn to recognize the word order.\n",
        "\n"
      ],
      "metadata": {
        "id": "5du74jWFF5WK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "  \n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # Apply the sine function to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # Apply the cosine function to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "KXtWVxfeb6px"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a point-wise feed-forward network"
      ],
      "metadata": {
        "id": "mXL_HTJWG44o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def point_wise_feed_forward_network(\n",
        "  d_model, # Input/output dimensionality.\n",
        "  dff # Inner-layer dimensionality.\n",
        "  ):\n",
        "\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # Shape `(batch_size, seq_len, dff)`.\n",
        "      tf.keras.layers.Dense(d_model)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "  ])"
      ],
      "metadata": {
        "id": "db4kXyfGb_l5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Transformer"
      ],
      "metadata": {
        "id": "wSEO7HUZHppR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multihead Attention layer"
      ],
      "metadata": {
        "id": "UtydO3sOIRE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create mask for padding tokens so translator ignores them\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "\n",
        "# Mask future tokens so translator cannot see future\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "\n",
        "# Create final masks\n",
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead)\n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "      q: query shape == (..., seq_len_q, depth)\n",
        "      k: key shape == (..., seq_len_k, depth)\n",
        "      v: value shape == (..., seq_len_v, depth_v)\n",
        "      mask: Float tensor with shape broadcastable\n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "      output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    # (..., seq_len_q, seq_len_k)\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(\n",
        "        scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# Add attention layers to create multi-head attention\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, *, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # (batch_size, num_heads, seq_len_q, depth)\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        # (batch_size, num_heads, seq_len_k, depth)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        # (batch_size, num_heads, seq_len_v, depth)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        # (batch_size, seq_len_q, num_heads, depth)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        # (batch_size, seq_len_q, d_model)\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "        # Build Transformer encoder layer"
      ],
      "metadata": {
        "id": "BSHsspXXIaUR"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "yvNLzFWpJw6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ],
      "metadata": {
        "id": "bCdZS_nJcGVW"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build Transformer encoder from encoder layer\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "              maximum_position_encoding=MAX_TOKENS,rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "metadata": {
        "id": "EBZNB3MacRCF"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "ndsnLCOwRgcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "metadata": {
        "id": "fnXVTpI_cgzR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build Transformer decoder from decoder layer\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "              maximum_position_encoding,rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size,d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ],
      "metadata": {
        "id": "UEuoBDtJcp7Q"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "cjFbGSvJUQY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build full Transformer\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_vocab_size, pe_input=MAX_TOKENS, pe_target=MAX_TOKENS, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.encoder= Encoder(num_layers, d_model, num_heads, dff,input_vocab_size, pe_input, rate)\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff,target_vocab_size, pe_target, rate)\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "  \n",
        "  def call(self, inp, tar, training, enc_padding_mask,look_ahead_mask, dec_padding_mask):\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask) # (batch_size, inp_seq_len, d_model)\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    final_output = self.final_layer(dec_output) # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    return final_output, attention_weights\n"
      ],
      "metadata": {
        "id": "_6sd1ofVcyl4"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer and Metrics"
      ],
      "metadata": {
        "id": "pPhqoBhwWYZ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "y79jHzBnA7wG"
      },
      "outputs": [],
      "source": [
        "# Set learning rate schedule\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set hyperparameters"
      ],
      "metadata": {
        "id": "g11C2K3tXFyP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "-q51YVNusr5_"
      },
      "outputs": [],
      "source": [
        "num_layers = 6\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate a Transformer\n"
      ],
      "metadata": {
        "id": "5fGafl4DXYct"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "9DJxw765BYHi"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=tokenizers.twi.get_vocab_size().numpy(),\n",
        "    target_vocab_size=tokenizers.fr.get_vocab_size().numpy(),\n",
        "    rate=dropout_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "-aGTknsnYDCC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "BF0wsHaaJVKC"
      },
      "outputs": [],
      "source": [
        "# Instantiate learning rate and set optimizer\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "metadata": {
        "id": "HpLaySRfSy89"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "-PHWy8sdGoR8"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = './checkpoints/train'\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# If a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose number of training epochs\n",
        "EPOCHS = 100\n",
        "train_step_signature = [\n",
        "  tf.TensorSpec(shape=(None, None), dtype=tf.int64), \n",
        "  tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp,True,enc_padding_mask,combined_mask,dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ],
      "metadata": {
        "id": "VAj0P_T0ZcBV"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "n7k6GwwEIi_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c7b406d-d80e-49f1-d2f5-18f6f366c987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 7.8339 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 7.2634 Accuracy 0.0516\n",
            "Epoch 1 Batch 100 Loss 6.8858 Accuracy 0.0662\n",
            "Epoch 1 Batch 150 Loss 6.6201 Accuracy 0.0770\n",
            "Epoch 1 Batch 200 Loss 6.3543 Accuracy 0.1007\n",
            "Epoch 1 Batch 250 Loss 6.1307 Accuracy 0.1192\n",
            "Epoch 1 Loss 6.0643 Accuracy 0.1247\n",
            "Time taken for 1 epoch: 76.26 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 5.0605 Accuracy 0.1942\n",
            "Epoch 2 Batch 50 Loss 5.0170 Accuracy 0.2071\n",
            "Epoch 2 Batch 100 Loss 4.9404 Accuracy 0.2155\n",
            "Epoch 2 Batch 150 Loss 4.8648 Accuracy 0.2239\n",
            "Epoch 2 Batch 200 Loss 4.7888 Accuracy 0.2322\n",
            "Epoch 2 Batch 250 Loss 4.7117 Accuracy 0.2393\n",
            "Epoch 2 Loss 4.6837 Accuracy 0.2416\n",
            "Time taken for 1 epoch: 56.85 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4.4179 Accuracy 0.2557\n",
            "Epoch 3 Batch 50 Loss 4.2119 Accuracy 0.2808\n",
            "Epoch 3 Batch 100 Loss 4.1297 Accuracy 0.2887\n",
            "Epoch 3 Batch 150 Loss 4.0770 Accuracy 0.2930\n",
            "Epoch 3 Batch 200 Loss 4.0317 Accuracy 0.2963\n",
            "Epoch 3 Batch 250 Loss 3.9822 Accuracy 0.2997\n",
            "Epoch 3 Loss 3.9685 Accuracy 0.3003\n",
            "Time taken for 1 epoch: 55.78 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 3.5342 Accuracy 0.3380\n",
            "Epoch 4 Batch 50 Loss 3.6210 Accuracy 0.3283\n",
            "Epoch 4 Batch 100 Loss 3.6123 Accuracy 0.3281\n",
            "Epoch 4 Batch 200 Loss 3.5592 Accuracy 0.3336\n",
            "Epoch 4 Batch 250 Loss 3.5311 Accuracy 0.3364\n",
            "Epoch 4 Loss 3.5245 Accuracy 0.3365\n",
            "Time taken for 1 epoch: 55.22 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 3.3193 Accuracy 0.3633\n",
            "Epoch 5 Batch 50 Loss 3.2689 Accuracy 0.3606\n",
            "Epoch 5 Batch 100 Loss 3.2622 Accuracy 0.3621\n",
            "Epoch 5 Batch 150 Loss 3.2470 Accuracy 0.3639\n",
            "Epoch 5 Batch 200 Loss 3.2424 Accuracy 0.3637\n",
            "Epoch 5 Batch 250 Loss 3.2239 Accuracy 0.3647\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
            "Epoch 5 Loss 3.2173 Accuracy 0.3650\n",
            "Time taken for 1 epoch: 59.13 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 3.0642 Accuracy 0.4048\n",
            "Epoch 6 Batch 50 Loss 3.0162 Accuracy 0.3876\n",
            "Epoch 6 Batch 100 Loss 2.9963 Accuracy 0.3910\n",
            "Epoch 6 Batch 150 Loss 2.9828 Accuracy 0.3911\n",
            "Epoch 6 Batch 200 Loss 2.9729 Accuracy 0.3912\n",
            "Epoch 6 Batch 250 Loss 2.9695 Accuracy 0.3911\n",
            "Epoch 6 Loss 2.9696 Accuracy 0.3908\n",
            "Time taken for 1 epoch: 56.16 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.4839 Accuracy 0.4731\n",
            "Epoch 7 Batch 50 Loss 2.7770 Accuracy 0.4124\n",
            "Epoch 7 Batch 100 Loss 2.7762 Accuracy 0.4138\n",
            "Epoch 7 Batch 150 Loss 2.7770 Accuracy 0.4148\n",
            "Epoch 7 Batch 200 Loss 2.7668 Accuracy 0.4166\n",
            "Epoch 7 Batch 250 Loss 2.7665 Accuracy 0.4163\n",
            "Epoch 7 Loss 2.7656 Accuracy 0.4165\n",
            "Time taken for 1 epoch: 55.97 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.5057 Accuracy 0.4774\n",
            "Epoch 8 Batch 50 Loss 2.5739 Accuracy 0.4436\n",
            "Epoch 8 Batch 100 Loss 2.5695 Accuracy 0.4433\n",
            "Epoch 8 Batch 150 Loss 2.5824 Accuracy 0.4419\n",
            "Epoch 8 Batch 200 Loss 2.5861 Accuracy 0.4405\n",
            "Epoch 8 Batch 250 Loss 2.5898 Accuracy 0.4400\n",
            "Epoch 8 Loss 2.5889 Accuracy 0.4402\n",
            "Time taken for 1 epoch: 55.94 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.5029 Accuracy 0.4443\n",
            "Epoch 9 Batch 50 Loss 2.3756 Accuracy 0.4734\n",
            "Epoch 9 Batch 100 Loss 2.4096 Accuracy 0.4673\n",
            "Epoch 9 Batch 150 Loss 2.4266 Accuracy 0.4646\n",
            "Epoch 9 Batch 200 Loss 2.4330 Accuracy 0.4629\n",
            "Epoch 9 Batch 250 Loss 2.4360 Accuracy 0.4628\n",
            "Epoch 9 Loss 2.4361 Accuracy 0.4629\n",
            "Time taken for 1 epoch: 55.94 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 2.3105 Accuracy 0.4923\n",
            "Epoch 10 Batch 50 Loss 2.2744 Accuracy 0.4880\n",
            "Epoch 10 Batch 100 Loss 2.2846 Accuracy 0.4870\n",
            "Epoch 10 Batch 150 Loss 2.3021 Accuracy 0.4835\n",
            "Epoch 10 Batch 200 Loss 2.3081 Accuracy 0.4820\n",
            "Epoch 10 Batch 250 Loss 2.3197 Accuracy 0.4797\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
            "Epoch 10 Loss 2.3257 Accuracy 0.4786\n",
            "Time taken for 1 epoch: 57.98 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 2.3046 Accuracy 0.4726\n",
            "Epoch 11 Batch 50 Loss 2.1471 Accuracy 0.5054\n",
            "Epoch 11 Batch 100 Loss 2.1768 Accuracy 0.4997\n",
            "Epoch 11 Batch 150 Loss 2.1983 Accuracy 0.4976\n",
            "Epoch 11 Batch 200 Loss 2.2120 Accuracy 0.4951\n",
            "Epoch 11 Batch 250 Loss 2.2275 Accuracy 0.4928\n",
            "Epoch 11 Loss 2.2286 Accuracy 0.4927\n",
            "Time taken for 1 epoch: 55.68 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 2.0607 Accuracy 0.5251\n",
            "Epoch 12 Batch 50 Loss 2.0714 Accuracy 0.5213\n",
            "Epoch 12 Batch 100 Loss 2.0903 Accuracy 0.5160\n",
            "Epoch 12 Batch 150 Loss 2.1149 Accuracy 0.5123\n",
            "Epoch 12 Batch 200 Loss 2.1309 Accuracy 0.5091\n",
            "Epoch 12 Batch 250 Loss 2.1484 Accuracy 0.5049\n",
            "Epoch 12 Loss 2.1547 Accuracy 0.5038\n",
            "Time taken for 1 epoch: 56.20 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.9982 Accuracy 0.5073\n",
            "Epoch 13 Batch 50 Loss 2.0167 Accuracy 0.5236\n",
            "Epoch 13 Batch 100 Loss 2.0396 Accuracy 0.5189\n",
            "Epoch 13 Batch 150 Loss 2.0646 Accuracy 0.5159\n",
            "Epoch 13 Batch 200 Loss 2.0831 Accuracy 0.5127\n",
            "Epoch 13 Batch 250 Loss 2.0989 Accuracy 0.5102\n",
            "Epoch 13 Loss 2.1066 Accuracy 0.5092\n",
            "Time taken for 1 epoch: 56.41 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.9311 Accuracy 0.5405\n",
            "Epoch 14 Batch 50 Loss 1.9735 Accuracy 0.5291\n",
            "Epoch 14 Batch 100 Loss 1.9954 Accuracy 0.5255\n",
            "Epoch 14 Batch 150 Loss 2.0280 Accuracy 0.5207\n",
            "Epoch 14 Batch 200 Loss 2.0469 Accuracy 0.5183\n",
            "Epoch 14 Batch 250 Loss 2.0646 Accuracy 0.5157\n",
            "Epoch 14 Loss 2.0702 Accuracy 0.5150\n",
            "Time taken for 1 epoch: 55.03 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.9469 Accuracy 0.5261\n",
            "Epoch 15 Batch 50 Loss 1.9041 Accuracy 0.5419\n",
            "Epoch 15 Batch 100 Loss 1.9542 Accuracy 0.5332\n",
            "Epoch 15 Batch 150 Loss 1.9927 Accuracy 0.5270\n",
            "Epoch 15 Batch 200 Loss 2.0208 Accuracy 0.5221\n",
            "Epoch 15 Batch 250 Loss 2.0390 Accuracy 0.5190\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
            "Epoch 15 Loss 2.0449 Accuracy 0.5180\n",
            "Time taken for 1 epoch: 57.89 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.7356 Accuracy 0.5806\n",
            "Epoch 16 Batch 50 Loss 1.9335 Accuracy 0.5322\n",
            "Epoch 16 Batch 100 Loss 1.9360 Accuracy 0.5328\n",
            "Epoch 16 Batch 150 Loss 1.9635 Accuracy 0.5292\n",
            "Epoch 16 Batch 200 Loss 1.9815 Accuracy 0.5267\n",
            "Epoch 16 Batch 250 Loss 1.9984 Accuracy 0.5249\n",
            "Epoch 16 Loss 2.0013 Accuracy 0.5245\n",
            "Time taken for 1 epoch: 55.34 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.7755 Accuracy 0.5619\n",
            "Epoch 17 Batch 50 Loss 1.8173 Accuracy 0.5573\n",
            "Epoch 17 Batch 100 Loss 1.8404 Accuracy 0.5532\n",
            "Epoch 17 Batch 150 Loss 1.8673 Accuracy 0.5479\n",
            "Epoch 17 Batch 200 Loss 1.8847 Accuracy 0.5451\n",
            "Epoch 17 Batch 250 Loss 1.8982 Accuracy 0.5424\n",
            "Epoch 17 Loss 1.9061 Accuracy 0.5411\n",
            "Time taken for 1 epoch: 55.01 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.7353 Accuracy 0.5769\n",
            "Epoch 18 Batch 50 Loss 1.7414 Accuracy 0.5721\n",
            "Epoch 18 Batch 100 Loss 1.7748 Accuracy 0.5666\n",
            "Epoch 18 Batch 150 Loss 1.7994 Accuracy 0.5619\n",
            "Epoch 18 Batch 200 Loss 1.8125 Accuracy 0.5590\n",
            "Epoch 18 Batch 250 Loss 1.8267 Accuracy 0.5567\n",
            "Epoch 18 Loss 1.8314 Accuracy 0.5558\n",
            "Time taken for 1 epoch: 55.76 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.4991 Accuracy 0.6243\n",
            "Epoch 19 Batch 50 Loss 1.6572 Accuracy 0.5866\n",
            "Epoch 19 Batch 100 Loss 1.6677 Accuracy 0.5842\n",
            "Epoch 19 Batch 150 Loss 1.6954 Accuracy 0.5799\n",
            "Epoch 19 Batch 200 Loss 1.7193 Accuracy 0.5753\n",
            "Epoch 19 Batch 250 Loss 1.7339 Accuracy 0.5731\n",
            "Epoch 19 Loss 1.7374 Accuracy 0.5722\n",
            "Time taken for 1 epoch: 54.80 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.6530 Accuracy 0.5849\n",
            "Epoch 20 Batch 50 Loss 1.5778 Accuracy 0.6000\n",
            "Epoch 20 Batch 100 Loss 1.5901 Accuracy 0.5996\n",
            "Epoch 20 Batch 150 Loss 1.6234 Accuracy 0.5929\n",
            "Epoch 20 Batch 200 Loss 1.6454 Accuracy 0.5888\n",
            "Epoch 20 Batch 250 Loss 1.6597 Accuracy 0.5854\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
            "Epoch 20 Loss 1.6690 Accuracy 0.5840\n",
            "Time taken for 1 epoch: 57.58 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.4580 Accuracy 0.6283\n",
            "Epoch 21 Batch 50 Loss 1.5064 Accuracy 0.6173\n",
            "Epoch 21 Batch 100 Loss 1.5235 Accuracy 0.6124\n",
            "Epoch 21 Batch 150 Loss 1.5489 Accuracy 0.6075\n",
            "Epoch 21 Batch 200 Loss 1.5683 Accuracy 0.6033\n",
            "Epoch 21 Batch 250 Loss 1.5811 Accuracy 0.6011\n",
            "Epoch 21 Loss 1.5870 Accuracy 0.6001\n",
            "Time taken for 1 epoch: 55.59 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 1.5284 Accuracy 0.6149\n",
            "Epoch 22 Batch 50 Loss 1.4295 Accuracy 0.6316\n",
            "Epoch 22 Batch 100 Loss 1.4433 Accuracy 0.6288\n",
            "Epoch 22 Batch 150 Loss 1.4720 Accuracy 0.6232\n",
            "Epoch 22 Batch 200 Loss 1.4898 Accuracy 0.6192\n",
            "Epoch 22 Batch 250 Loss 1.5010 Accuracy 0.6175\n",
            "Epoch 22 Loss 1.5091 Accuracy 0.6158\n",
            "Time taken for 1 epoch: 55.60 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 1.3651 Accuracy 0.6440\n",
            "Epoch 23 Batch 50 Loss 1.3650 Accuracy 0.6460\n",
            "Epoch 23 Batch 100 Loss 1.3886 Accuracy 0.6410\n",
            "Epoch 23 Batch 150 Loss 1.4008 Accuracy 0.6386\n",
            "Epoch 23 Batch 200 Loss 1.4144 Accuracy 0.6354\n",
            "Epoch 23 Batch 250 Loss 1.4345 Accuracy 0.6311\n",
            "Epoch 23 Loss 1.4402 Accuracy 0.6297\n",
            "Time taken for 1 epoch: 55.92 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 1.3266 Accuracy 0.6536\n",
            "Epoch 24 Batch 50 Loss 1.2739 Accuracy 0.6656\n",
            "Epoch 24 Batch 100 Loss 1.2964 Accuracy 0.6606\n",
            "Epoch 24 Batch 150 Loss 1.3154 Accuracy 0.6554\n",
            "Epoch 24 Batch 200 Loss 1.3470 Accuracy 0.6479\n",
            "Epoch 24 Batch 250 Loss 1.3635 Accuracy 0.6441\n",
            "Epoch 24 Loss 1.3689 Accuracy 0.6430\n",
            "Time taken for 1 epoch: 59.49 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 1.0971 Accuracy 0.7014\n",
            "Epoch 25 Batch 50 Loss 1.2055 Accuracy 0.6790\n",
            "Epoch 25 Batch 100 Loss 1.2472 Accuracy 0.6681\n",
            "Epoch 25 Batch 150 Loss 1.2641 Accuracy 0.6650\n",
            "Epoch 25 Batch 200 Loss 1.2857 Accuracy 0.6603\n",
            "Epoch 25 Batch 250 Loss 1.3043 Accuracy 0.6561\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-5\n",
            "Epoch 25 Loss 1.3098 Accuracy 0.6547\n",
            "Time taken for 1 epoch: 61.38 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 1.1821 Accuracy 0.6735\n",
            "Epoch 26 Batch 50 Loss 1.1516 Accuracy 0.6917\n",
            "Epoch 26 Batch 100 Loss 1.1748 Accuracy 0.6851\n",
            "Epoch 26 Batch 150 Loss 1.2010 Accuracy 0.6792\n",
            "Epoch 26 Batch 200 Loss 1.2198 Accuracy 0.6750\n",
            "Epoch 26 Batch 250 Loss 1.2402 Accuracy 0.6698\n",
            "Epoch 26 Loss 1.2447 Accuracy 0.6687\n",
            "Time taken for 1 epoch: 56.56 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 1.1725 Accuracy 0.6775\n",
            "Epoch 27 Batch 50 Loss 1.0996 Accuracy 0.6972\n",
            "Epoch 27 Batch 100 Loss 1.1290 Accuracy 0.6919\n",
            "Epoch 27 Batch 150 Loss 1.1507 Accuracy 0.6879\n",
            "Epoch 27 Batch 200 Loss 1.1705 Accuracy 0.6835\n",
            "Epoch 27 Batch 250 Loss 1.1865 Accuracy 0.6804\n",
            "Epoch 27 Loss 1.1914 Accuracy 0.6792\n",
            "Time taken for 1 epoch: 55.36 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 1.0906 Accuracy 0.7098\n",
            "Epoch 28 Batch 50 Loss 1.0320 Accuracy 0.7183\n",
            "Epoch 28 Batch 100 Loss 1.0652 Accuracy 0.7099\n",
            "Epoch 28 Batch 150 Loss 1.0927 Accuracy 0.7020\n",
            "Epoch 28 Batch 200 Loss 1.1108 Accuracy 0.6976\n",
            "Epoch 28 Batch 250 Loss 1.1300 Accuracy 0.6928\n",
            "Epoch 28 Loss 1.1371 Accuracy 0.6913\n",
            "Time taken for 1 epoch: 56.09 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.9786 Accuracy 0.7256\n",
            "Epoch 29 Batch 50 Loss 1.0031 Accuracy 0.7220\n",
            "Epoch 29 Batch 100 Loss 1.0231 Accuracy 0.7174\n",
            "Epoch 29 Batch 150 Loss 1.0504 Accuracy 0.7106\n",
            "Epoch 29 Batch 200 Loss 1.0670 Accuracy 0.7060\n",
            "Epoch 29 Batch 250 Loss 1.0837 Accuracy 0.7021\n",
            "Epoch 29 Loss 1.0899 Accuracy 0.7008\n",
            "Time taken for 1 epoch: 55.26 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.8607 Accuracy 0.7701\n",
            "Epoch 30 Batch 50 Loss 0.9282 Accuracy 0.7398\n",
            "Epoch 30 Batch 100 Loss 0.9594 Accuracy 0.7326\n",
            "Epoch 30 Batch 150 Loss 0.9858 Accuracy 0.7253\n",
            "Epoch 30 Batch 200 Loss 1.0070 Accuracy 0.7197\n",
            "Epoch 30 Batch 250 Loss 1.0231 Accuracy 0.7153\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-6\n",
            "Epoch 30 Loss 1.0309 Accuracy 0.7133\n",
            "Time taken for 1 epoch: 58.79 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.8594 Accuracy 0.7665\n",
            "Epoch 31 Batch 50 Loss 0.9023 Accuracy 0.7459\n",
            "Epoch 31 Batch 100 Loss 0.9122 Accuracy 0.7430\n",
            "Epoch 31 Batch 150 Loss 0.9394 Accuracy 0.7359\n",
            "Epoch 31 Batch 200 Loss 0.9636 Accuracy 0.7300\n",
            "Epoch 31 Batch 250 Loss 0.9769 Accuracy 0.7265\n",
            "Epoch 31 Loss 0.9839 Accuracy 0.7247\n",
            "Time taken for 1 epoch: 55.55 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.7788 Accuracy 0.7846\n",
            "Epoch 32 Batch 50 Loss 0.8508 Accuracy 0.7589\n",
            "Epoch 32 Batch 100 Loss 0.8640 Accuracy 0.7553\n",
            "Epoch 32 Batch 150 Loss 0.8866 Accuracy 0.7492\n",
            "Epoch 32 Batch 200 Loss 0.9083 Accuracy 0.7430\n",
            "Epoch 32 Batch 250 Loss 0.9255 Accuracy 0.7381\n",
            "Epoch 32 Loss 0.9297 Accuracy 0.7370\n",
            "Time taken for 1 epoch: 55.06 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.7689 Accuracy 0.7769\n",
            "Epoch 33 Batch 50 Loss 0.8088 Accuracy 0.7675\n",
            "Epoch 33 Batch 100 Loss 0.8339 Accuracy 0.7610\n",
            "Epoch 33 Batch 150 Loss 0.8587 Accuracy 0.7547\n",
            "Epoch 33 Batch 200 Loss 0.8785 Accuracy 0.7492\n",
            "Epoch 33 Batch 250 Loss 0.8898 Accuracy 0.7465\n",
            "Epoch 33 Loss 0.8968 Accuracy 0.7450\n",
            "Time taken for 1 epoch: 55.25 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.7159 Accuracy 0.7770\n",
            "Epoch 34 Batch 50 Loss 0.7881 Accuracy 0.7709\n",
            "Epoch 34 Batch 100 Loss 0.7959 Accuracy 0.7690\n",
            "Epoch 34 Batch 150 Loss 0.8156 Accuracy 0.7637\n",
            "Epoch 34 Batch 200 Loss 0.8302 Accuracy 0.7602\n",
            "Epoch 34 Batch 250 Loss 0.8439 Accuracy 0.7569\n",
            "Epoch 34 Loss 0.8516 Accuracy 0.7550\n",
            "Time taken for 1 epoch: 55.56 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.6426 Accuracy 0.8131\n",
            "Epoch 35 Batch 50 Loss 0.7300 Accuracy 0.7877\n",
            "Epoch 35 Batch 100 Loss 0.7534 Accuracy 0.7808\n",
            "Epoch 35 Batch 150 Loss 0.7762 Accuracy 0.7748\n",
            "Epoch 35 Batch 200 Loss 0.7944 Accuracy 0.7699\n",
            "Epoch 35 Batch 250 Loss 0.8098 Accuracy 0.7660\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-7\n",
            "Epoch 35 Loss 0.8138 Accuracy 0.7650\n",
            "Time taken for 1 epoch: 58.28 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.5738 Accuracy 0.8424\n",
            "Epoch 36 Batch 50 Loss 0.7102 Accuracy 0.7939\n",
            "Epoch 36 Batch 100 Loss 0.7236 Accuracy 0.7893\n",
            "Epoch 36 Batch 150 Loss 0.7387 Accuracy 0.7842\n",
            "Epoch 36 Batch 200 Loss 0.7610 Accuracy 0.7782\n",
            "Epoch 36 Batch 250 Loss 0.7776 Accuracy 0.7734\n",
            "Epoch 36 Loss 0.7827 Accuracy 0.7719\n",
            "Time taken for 1 epoch: 55.26 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.5840 Accuracy 0.8344\n",
            "Epoch 37 Batch 50 Loss 0.6646 Accuracy 0.8049\n",
            "Epoch 37 Batch 100 Loss 0.6874 Accuracy 0.7970\n",
            "Epoch 37 Batch 150 Loss 0.7072 Accuracy 0.7919\n",
            "Epoch 37 Batch 200 Loss 0.7211 Accuracy 0.7880\n",
            "Epoch 37 Batch 250 Loss 0.7347 Accuracy 0.7843\n",
            "Epoch 37 Loss 0.7396 Accuracy 0.7827\n",
            "Time taken for 1 epoch: 55.07 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.6524 Accuracy 0.8083\n",
            "Epoch 38 Batch 50 Loss 0.6394 Accuracy 0.8092\n",
            "Epoch 38 Batch 100 Loss 0.6591 Accuracy 0.8044\n",
            "Epoch 38 Batch 150 Loss 0.6736 Accuracy 0.8002\n",
            "Epoch 38 Batch 200 Loss 0.6918 Accuracy 0.7947\n",
            "Epoch 38 Batch 250 Loss 0.7030 Accuracy 0.7914\n",
            "Epoch 38 Loss 0.7083 Accuracy 0.7902\n",
            "Time taken for 1 epoch: 54.74 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.5691 Accuracy 0.8317\n",
            "Epoch 39 Batch 50 Loss 0.5950 Accuracy 0.8231\n",
            "Epoch 39 Batch 100 Loss 0.6166 Accuracy 0.8168\n",
            "Epoch 39 Batch 150 Loss 0.6375 Accuracy 0.8105\n",
            "Epoch 39 Batch 200 Loss 0.6527 Accuracy 0.8061\n",
            "Epoch 39 Batch 250 Loss 0.6679 Accuracy 0.8016\n",
            "Epoch 39 Loss 0.6747 Accuracy 0.8000\n",
            "Time taken for 1 epoch: 54.37 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.4945 Accuracy 0.8554\n",
            "Epoch 40 Batch 50 Loss 0.5846 Accuracy 0.8258\n",
            "Epoch 40 Batch 100 Loss 0.5997 Accuracy 0.8208\n",
            "Epoch 40 Batch 150 Loss 0.6166 Accuracy 0.8153\n",
            "Epoch 40 Batch 200 Loss 0.6310 Accuracy 0.8114\n",
            "Epoch 40 Batch 250 Loss 0.6427 Accuracy 0.8079\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-8\n",
            "Epoch 40 Loss 0.6484 Accuracy 0.8065\n",
            "Time taken for 1 epoch: 58.73 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.5451 Accuracy 0.8321\n",
            "Epoch 41 Batch 50 Loss 0.5612 Accuracy 0.8310\n",
            "Epoch 41 Batch 100 Loss 0.5705 Accuracy 0.8277\n",
            "Epoch 41 Batch 150 Loss 0.5862 Accuracy 0.8235\n",
            "Epoch 41 Batch 200 Loss 0.6022 Accuracy 0.8190\n",
            "Epoch 41 Batch 250 Loss 0.6159 Accuracy 0.8148\n",
            "Epoch 41 Loss 0.6199 Accuracy 0.8137\n",
            "Time taken for 1 epoch: 55.67 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.5614 Accuracy 0.8307\n",
            "Epoch 42 Batch 50 Loss 0.5238 Accuracy 0.8413\n",
            "Epoch 42 Batch 100 Loss 0.5367 Accuracy 0.8368\n",
            "Epoch 42 Batch 150 Loss 0.5539 Accuracy 0.8316\n",
            "Epoch 42 Batch 200 Loss 0.5689 Accuracy 0.8272\n",
            "Epoch 42 Batch 250 Loss 0.5819 Accuracy 0.8234\n",
            "Epoch 42 Loss 0.5863 Accuracy 0.8222\n",
            "Time taken for 1 epoch: 55.33 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.4161 Accuracy 0.8850\n",
            "Epoch 43 Batch 50 Loss 0.4980 Accuracy 0.8497\n",
            "Epoch 43 Batch 100 Loss 0.5194 Accuracy 0.8432\n",
            "Epoch 43 Batch 150 Loss 0.5372 Accuracy 0.8377\n",
            "Epoch 43 Batch 200 Loss 0.5497 Accuracy 0.8332\n",
            "Epoch 43 Batch 250 Loss 0.5632 Accuracy 0.8288\n",
            "Epoch 43 Loss 0.5665 Accuracy 0.8278\n",
            "Time taken for 1 epoch: 54.71 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.4187 Accuracy 0.8708\n",
            "Epoch 44 Batch 50 Loss 0.4796 Accuracy 0.8542\n",
            "Epoch 44 Batch 100 Loss 0.5028 Accuracy 0.8459\n",
            "Epoch 44 Batch 150 Loss 0.5191 Accuracy 0.8414\n",
            "Epoch 44 Batch 200 Loss 0.5314 Accuracy 0.8381\n",
            "Epoch 44 Batch 250 Loss 0.5421 Accuracy 0.8351\n",
            "Epoch 44 Loss 0.5462 Accuracy 0.8338\n",
            "Time taken for 1 epoch: 54.87 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.3902 Accuracy 0.8868\n",
            "Epoch 45 Batch 50 Loss 0.4575 Accuracy 0.8598\n",
            "Epoch 45 Batch 100 Loss 0.4718 Accuracy 0.8550\n",
            "Epoch 45 Batch 150 Loss 0.4898 Accuracy 0.8503\n",
            "Epoch 45 Batch 200 Loss 0.5050 Accuracy 0.8458\n",
            "Epoch 45 Batch 250 Loss 0.5167 Accuracy 0.8423\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/train/ckpt-9\n",
            "Epoch 45 Loss 0.5212 Accuracy 0.8408\n",
            "Time taken for 1 epoch: 58.76 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.5106 Accuracy 0.8298\n",
            "Epoch 46 Batch 50 Loss 0.4472 Accuracy 0.8628\n",
            "Epoch 46 Batch 100 Loss 0.4584 Accuracy 0.8580\n",
            "Epoch 46 Batch 150 Loss 0.4709 Accuracy 0.8544\n",
            "Epoch 46 Batch 200 Loss 0.4843 Accuracy 0.8508\n",
            "Epoch 46 Batch 250 Loss 0.4971 Accuracy 0.8471\n",
            "Epoch 46 Loss 0.5019 Accuracy 0.8458\n",
            "Time taken for 1 epoch: 55.32 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.5205 Accuracy 0.8314\n",
            "Epoch 47 Batch 50 Loss 0.4204 Accuracy 0.8712\n",
            "Epoch 47 Batch 100 Loss 0.4407 Accuracy 0.8655\n",
            "Epoch 47 Batch 150 Loss 0.4551 Accuracy 0.8604\n",
            "Epoch 47 Batch 200 Loss 0.4689 Accuracy 0.8562\n",
            "Epoch 47 Batch 250 Loss 0.4811 Accuracy 0.8525\n",
            "Epoch 47 Loss 0.4854 Accuracy 0.8510\n",
            "Time taken for 1 epoch: 56.71 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.4556 Accuracy 0.8555\n",
            "Epoch 48 Batch 50 Loss 0.4181 Accuracy 0.8689\n",
            "Epoch 48 Batch 100 Loss 0.4275 Accuracy 0.8668\n",
            "Epoch 48 Batch 150 Loss 0.4396 Accuracy 0.8634\n",
            "Epoch 48 Batch 200 Loss 0.4538 Accuracy 0.8591\n",
            "Epoch 48 Batch 250 Loss 0.4645 Accuracy 0.8561\n",
            "Epoch 48 Loss 0.4668 Accuracy 0.8554\n",
            "Time taken for 1 epoch: 55.47 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.3674 Accuracy 0.8837\n",
            "Epoch 49 Batch 50 Loss 0.3981 Accuracy 0.8753\n",
            "Epoch 49 Batch 100 Loss 0.4134 Accuracy 0.8717\n",
            "Epoch 49 Batch 150 Loss 0.4265 Accuracy 0.8676\n",
            "Epoch 49 Batch 200 Loss 0.4365 Accuracy 0.8649\n",
            "Epoch 49 Batch 250 Loss 0.4457 Accuracy 0.8622\n",
            "Epoch 49 Loss 0.4478 Accuracy 0.8616\n",
            "Time taken for 1 epoch: 54.50 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.3078 Accuracy 0.9091\n",
            "Epoch 50 Batch 50 Loss 0.3802 Accuracy 0.8822\n",
            "Epoch 50 Batch 100 Loss 0.3954 Accuracy 0.8773\n",
            "Epoch 50 Batch 150 Loss 0.4120 Accuracy 0.8725\n",
            "Epoch 50 Batch 200 Loss 0.4214 Accuracy 0.8696\n",
            "Epoch 50 Batch 250 Loss 0.4304 Accuracy 0.8668\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-10\n",
            "Epoch 50 Loss 0.4346 Accuracy 0.8655\n",
            "Time taken for 1 epoch: 58.53 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.3031 Accuracy 0.9182\n",
            "Epoch 51 Batch 50 Loss 0.3649 Accuracy 0.8874\n",
            "Epoch 51 Batch 100 Loss 0.3742 Accuracy 0.8834\n",
            "Epoch 51 Batch 150 Loss 0.3908 Accuracy 0.8786\n",
            "Epoch 51 Batch 200 Loss 0.4037 Accuracy 0.8743\n",
            "Epoch 51 Batch 250 Loss 0.4133 Accuracy 0.8719\n",
            "Epoch 51 Loss 0.4165 Accuracy 0.8709\n",
            "Time taken for 1 epoch: 55.63 secs\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.3277 Accuracy 0.8937\n",
            "Epoch 52 Batch 50 Loss 0.3489 Accuracy 0.8915\n",
            "Epoch 52 Batch 100 Loss 0.3660 Accuracy 0.8862\n",
            "Epoch 52 Batch 150 Loss 0.3766 Accuracy 0.8822\n",
            "Epoch 52 Batch 200 Loss 0.3892 Accuracy 0.8782\n",
            "Epoch 52 Batch 250 Loss 0.3995 Accuracy 0.8750\n",
            "Epoch 52 Loss 0.4026 Accuracy 0.8741\n",
            "Time taken for 1 epoch: 55.07 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.3605 Accuracy 0.8899\n",
            "Epoch 53 Batch 50 Loss 0.3549 Accuracy 0.8906\n",
            "Epoch 53 Batch 100 Loss 0.3560 Accuracy 0.8906\n",
            "Epoch 53 Batch 150 Loss 0.3680 Accuracy 0.8862\n",
            "Epoch 53 Batch 200 Loss 0.3769 Accuracy 0.8828\n",
            "Epoch 53 Batch 250 Loss 0.3861 Accuracy 0.8800\n",
            "Epoch 53 Loss 0.3891 Accuracy 0.8792\n",
            "Time taken for 1 epoch: 55.29 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.3439 Accuracy 0.8903\n",
            "Epoch 54 Batch 50 Loss 0.3379 Accuracy 0.8948\n",
            "Epoch 54 Batch 100 Loss 0.3473 Accuracy 0.8911\n",
            "Epoch 54 Batch 150 Loss 0.3580 Accuracy 0.8879\n",
            "Epoch 54 Batch 200 Loss 0.3686 Accuracy 0.8848\n",
            "Epoch 54 Batch 250 Loss 0.3783 Accuracy 0.8820\n",
            "Epoch 54 Loss 0.3808 Accuracy 0.8812\n",
            "Time taken for 1 epoch: 55.79 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.2798 Accuracy 0.9005\n",
            "Epoch 55 Batch 50 Loss 0.3173 Accuracy 0.9020\n",
            "Epoch 55 Batch 100 Loss 0.3315 Accuracy 0.8963\n",
            "Epoch 55 Batch 150 Loss 0.3426 Accuracy 0.8924\n",
            "Epoch 55 Batch 200 Loss 0.3520 Accuracy 0.8896\n",
            "Epoch 55 Batch 250 Loss 0.3602 Accuracy 0.8871\n",
            "Saving checkpoint for epoch 55 at ./checkpoints/train/ckpt-11\n",
            "Epoch 55 Loss 0.3640 Accuracy 0.8858\n",
            "Time taken for 1 epoch: 58.62 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.2916 Accuracy 0.9071\n",
            "Epoch 56 Batch 50 Loss 0.3120 Accuracy 0.9012\n",
            "Epoch 56 Batch 100 Loss 0.3229 Accuracy 0.8985\n",
            "Epoch 56 Batch 150 Loss 0.3339 Accuracy 0.8949\n",
            "Epoch 56 Batch 200 Loss 0.3389 Accuracy 0.8931\n",
            "Epoch 56 Batch 250 Loss 0.3459 Accuracy 0.8908\n",
            "Epoch 56 Loss 0.3495 Accuracy 0.8898\n",
            "Time taken for 1 epoch: 55.99 secs\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.3057 Accuracy 0.9084\n",
            "Epoch 57 Batch 50 Loss 0.2985 Accuracy 0.9064\n",
            "Epoch 57 Batch 100 Loss 0.3081 Accuracy 0.9034\n",
            "Epoch 57 Batch 150 Loss 0.3200 Accuracy 0.8996\n",
            "Epoch 57 Batch 200 Loss 0.3283 Accuracy 0.8971\n",
            "Epoch 57 Batch 250 Loss 0.3374 Accuracy 0.8940\n",
            "Epoch 57 Loss 0.3389 Accuracy 0.8935\n",
            "Time taken for 1 epoch: 55.56 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.2340 Accuracy 0.9251\n",
            "Epoch 58 Batch 50 Loss 0.2838 Accuracy 0.9106\n",
            "Epoch 58 Batch 100 Loss 0.2975 Accuracy 0.9061\n",
            "Epoch 58 Batch 150 Loss 0.3103 Accuracy 0.9023\n",
            "Epoch 58 Batch 200 Loss 0.3180 Accuracy 0.9001\n",
            "Epoch 58 Batch 250 Loss 0.3256 Accuracy 0.8973\n",
            "Epoch 58 Loss 0.3282 Accuracy 0.8966\n",
            "Time taken for 1 epoch: 55.00 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.2614 Accuracy 0.9179\n",
            "Epoch 59 Batch 50 Loss 0.2786 Accuracy 0.9126\n",
            "Epoch 59 Batch 100 Loss 0.2837 Accuracy 0.9106\n",
            "Epoch 59 Batch 150 Loss 0.2985 Accuracy 0.9062\n",
            "Epoch 59 Batch 200 Loss 0.3098 Accuracy 0.9023\n",
            "Epoch 59 Batch 250 Loss 0.3190 Accuracy 0.8998\n",
            "Epoch 59 Loss 0.3210 Accuracy 0.8991\n",
            "Time taken for 1 epoch: 55.06 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.2961 Accuracy 0.8947\n",
            "Epoch 60 Batch 50 Loss 0.2733 Accuracy 0.9138\n",
            "Epoch 60 Batch 100 Loss 0.2841 Accuracy 0.9112\n",
            "Epoch 60 Batch 150 Loss 0.2951 Accuracy 0.9076\n",
            "Epoch 60 Batch 200 Loss 0.3035 Accuracy 0.9046\n",
            "Epoch 60 Batch 250 Loss 0.3112 Accuracy 0.9024\n",
            "Saving checkpoint for epoch 60 at ./checkpoints/train/ckpt-12\n",
            "Epoch 60 Loss 0.3142 Accuracy 0.9015\n",
            "Time taken for 1 epoch: 58.81 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.2782 Accuracy 0.9102\n",
            "Epoch 61 Batch 50 Loss 0.2606 Accuracy 0.9178\n",
            "Epoch 61 Batch 100 Loss 0.2730 Accuracy 0.9133\n",
            "Epoch 61 Batch 150 Loss 0.2824 Accuracy 0.9102\n",
            "Epoch 61 Batch 200 Loss 0.2919 Accuracy 0.9077\n",
            "Epoch 61 Batch 250 Loss 0.2986 Accuracy 0.9054\n",
            "Epoch 61 Loss 0.3019 Accuracy 0.9042\n",
            "Time taken for 1 epoch: 56.20 secs\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.2803 Accuracy 0.9067\n",
            "Epoch 62 Batch 50 Loss 0.2589 Accuracy 0.9186\n",
            "Epoch 62 Batch 100 Loss 0.2718 Accuracy 0.9153\n",
            "Epoch 62 Batch 150 Loss 0.2797 Accuracy 0.9125\n",
            "Epoch 62 Batch 200 Loss 0.2882 Accuracy 0.9097\n",
            "Epoch 62 Batch 250 Loss 0.2953 Accuracy 0.9072\n",
            "Epoch 62 Loss 0.2975 Accuracy 0.9065\n",
            "Time taken for 1 epoch: 55.15 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.2447 Accuracy 0.9217\n",
            "Epoch 63 Batch 50 Loss 0.2630 Accuracy 0.9180\n",
            "Epoch 63 Batch 100 Loss 0.2661 Accuracy 0.9167\n",
            "Epoch 63 Batch 150 Loss 0.2762 Accuracy 0.9133\n",
            "Epoch 63 Batch 200 Loss 0.2820 Accuracy 0.9113\n",
            "Epoch 63 Batch 250 Loss 0.2876 Accuracy 0.9098\n",
            "Epoch 63 Loss 0.2900 Accuracy 0.9091\n",
            "Time taken for 1 epoch: 54.54 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.2360 Accuracy 0.9225\n",
            "Epoch 64 Batch 50 Loss 0.2617 Accuracy 0.9170\n",
            "Epoch 64 Batch 100 Loss 0.2624 Accuracy 0.9170\n",
            "Epoch 64 Batch 150 Loss 0.2713 Accuracy 0.9140\n",
            "Epoch 64 Batch 200 Loss 0.2785 Accuracy 0.9118\n",
            "Epoch 64 Batch 250 Loss 0.2845 Accuracy 0.9104\n",
            "Epoch 64 Loss 0.2866 Accuracy 0.9096\n",
            "Time taken for 1 epoch: 55.36 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.2523 Accuracy 0.9132\n",
            "Epoch 65 Batch 50 Loss 0.2440 Accuracy 0.9234\n",
            "Epoch 65 Batch 100 Loss 0.2515 Accuracy 0.9213\n",
            "Epoch 65 Batch 150 Loss 0.2539 Accuracy 0.9199\n",
            "Epoch 65 Batch 200 Loss 0.2620 Accuracy 0.9174\n",
            "Epoch 65 Batch 250 Loss 0.2676 Accuracy 0.9151\n",
            "Saving checkpoint for epoch 65 at ./checkpoints/train/ckpt-13\n",
            "Epoch 65 Loss 0.2704 Accuracy 0.9147\n",
            "Time taken for 1 epoch: 58.20 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.2212 Accuracy 0.9274\n",
            "Epoch 66 Batch 50 Loss 0.2383 Accuracy 0.9244\n",
            "Epoch 66 Batch 100 Loss 0.2440 Accuracy 0.9225\n",
            "Epoch 66 Batch 150 Loss 0.2527 Accuracy 0.9205\n",
            "Epoch 66 Batch 200 Loss 0.2611 Accuracy 0.9179\n",
            "Epoch 66 Batch 250 Loss 0.2667 Accuracy 0.9155\n",
            "Epoch 66 Loss 0.2678 Accuracy 0.9153\n",
            "Time taken for 1 epoch: 55.34 secs\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.1911 Accuracy 0.9328\n",
            "Epoch 67 Batch 50 Loss 0.2430 Accuracy 0.9254\n",
            "Epoch 67 Batch 100 Loss 0.2434 Accuracy 0.9241\n",
            "Epoch 67 Batch 150 Loss 0.2491 Accuracy 0.9222\n",
            "Epoch 67 Batch 200 Loss 0.2563 Accuracy 0.9200\n",
            "Epoch 67 Batch 250 Loss 0.2614 Accuracy 0.9178\n",
            "Epoch 67 Loss 0.2634 Accuracy 0.9170\n",
            "Time taken for 1 epoch: 55.04 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.2000 Accuracy 0.9432\n",
            "Epoch 68 Batch 50 Loss 0.2258 Accuracy 0.9289\n",
            "Epoch 68 Batch 100 Loss 0.2286 Accuracy 0.9276\n",
            "Epoch 68 Batch 150 Loss 0.2375 Accuracy 0.9246\n",
            "Epoch 68 Batch 200 Loss 0.2445 Accuracy 0.9220\n",
            "Epoch 68 Batch 250 Loss 0.2507 Accuracy 0.9201\n",
            "Epoch 68 Loss 0.2534 Accuracy 0.9194\n",
            "Time taken for 1 epoch: 55.62 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.2081 Accuracy 0.9322\n",
            "Epoch 69 Batch 50 Loss 0.2177 Accuracy 0.9315\n",
            "Epoch 69 Batch 100 Loss 0.2229 Accuracy 0.9299\n",
            "Epoch 69 Batch 150 Loss 0.2321 Accuracy 0.9268\n",
            "Epoch 69 Batch 200 Loss 0.2392 Accuracy 0.9245\n",
            "Epoch 69 Batch 250 Loss 0.2461 Accuracy 0.9226\n",
            "Epoch 69 Loss 0.2483 Accuracy 0.9220\n",
            "Time taken for 1 epoch: 55.09 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.1995 Accuracy 0.9349\n",
            "Epoch 70 Batch 50 Loss 0.2250 Accuracy 0.9294\n",
            "Epoch 70 Batch 100 Loss 0.2266 Accuracy 0.9288\n",
            "Epoch 70 Batch 150 Loss 0.2297 Accuracy 0.9276\n",
            "Epoch 70 Batch 200 Loss 0.2358 Accuracy 0.9258\n",
            "Epoch 70 Batch 250 Loss 0.2413 Accuracy 0.9239\n",
            "Saving checkpoint for epoch 70 at ./checkpoints/train/ckpt-14\n",
            "Epoch 70 Loss 0.2434 Accuracy 0.9232\n",
            "Time taken for 1 epoch: 57.69 secs\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.1647 Accuracy 0.9540\n",
            "Epoch 71 Batch 50 Loss 0.2150 Accuracy 0.9311\n",
            "Epoch 71 Batch 100 Loss 0.2198 Accuracy 0.9293\n",
            "Epoch 71 Batch 150 Loss 0.2248 Accuracy 0.9283\n",
            "Epoch 71 Batch 200 Loss 0.2322 Accuracy 0.9262\n",
            "Epoch 71 Batch 250 Loss 0.2384 Accuracy 0.9244\n",
            "Epoch 71 Loss 0.2404 Accuracy 0.9237\n",
            "Time taken for 1 epoch: 54.84 secs\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.1868 Accuracy 0.9444\n",
            "Epoch 72 Batch 50 Loss 0.2046 Accuracy 0.9353\n",
            "Epoch 72 Batch 100 Loss 0.2105 Accuracy 0.9340\n",
            "Epoch 72 Batch 150 Loss 0.2171 Accuracy 0.9320\n",
            "Epoch 72 Batch 200 Loss 0.2225 Accuracy 0.9302\n",
            "Epoch 72 Batch 250 Loss 0.2270 Accuracy 0.9284\n",
            "Epoch 72 Loss 0.2291 Accuracy 0.9279\n",
            "Time taken for 1 epoch: 55.38 secs\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.1702 Accuracy 0.9411\n",
            "Epoch 73 Batch 50 Loss 0.1960 Accuracy 0.9386\n",
            "Epoch 73 Batch 100 Loss 0.2095 Accuracy 0.9350\n",
            "Epoch 73 Batch 150 Loss 0.2136 Accuracy 0.9333\n",
            "Epoch 73 Batch 200 Loss 0.2182 Accuracy 0.9315\n",
            "Epoch 73 Batch 250 Loss 0.2228 Accuracy 0.9299\n",
            "Epoch 73 Loss 0.2247 Accuracy 0.9294\n",
            "Time taken for 1 epoch: 55.05 secs\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.1952 Accuracy 0.9439\n",
            "Epoch 74 Batch 50 Loss 0.2021 Accuracy 0.9357\n",
            "Epoch 74 Batch 100 Loss 0.2068 Accuracy 0.9345\n",
            "Epoch 74 Batch 150 Loss 0.2089 Accuracy 0.9339\n",
            "Epoch 74 Batch 200 Loss 0.2149 Accuracy 0.9321\n",
            "Epoch 74 Batch 250 Loss 0.2209 Accuracy 0.9302\n",
            "Epoch 74 Loss 0.2226 Accuracy 0.9296\n",
            "Time taken for 1 epoch: 54.64 secs\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.2242 Accuracy 0.9240\n",
            "Epoch 75 Batch 50 Loss 0.1925 Accuracy 0.9409\n",
            "Epoch 75 Batch 100 Loss 0.2002 Accuracy 0.9378\n",
            "Epoch 75 Batch 150 Loss 0.2035 Accuracy 0.9359\n",
            "Epoch 75 Batch 200 Loss 0.2076 Accuracy 0.9342\n",
            "Epoch 75 Batch 250 Loss 0.2131 Accuracy 0.9323\n",
            "Saving checkpoint for epoch 75 at ./checkpoints/train/ckpt-15\n",
            "Epoch 75 Loss 0.2144 Accuracy 0.9319\n",
            "Time taken for 1 epoch: 56.99 secs\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.2024 Accuracy 0.9235\n",
            "Epoch 76 Batch 50 Loss 0.1891 Accuracy 0.9415\n",
            "Epoch 76 Batch 100 Loss 0.1945 Accuracy 0.9388\n",
            "Epoch 76 Batch 150 Loss 0.1995 Accuracy 0.9369\n",
            "Epoch 76 Batch 200 Loss 0.2039 Accuracy 0.9355\n",
            "Epoch 76 Batch 250 Loss 0.2087 Accuracy 0.9341\n",
            "Epoch 76 Loss 0.2112 Accuracy 0.9334\n",
            "Time taken for 1 epoch: 55.63 secs\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.2104 Accuracy 0.9245\n",
            "Epoch 77 Batch 50 Loss 0.1824 Accuracy 0.9424\n",
            "Epoch 77 Batch 100 Loss 0.1885 Accuracy 0.9402\n",
            "Epoch 77 Batch 150 Loss 0.1937 Accuracy 0.9383\n",
            "Epoch 77 Batch 200 Loss 0.2005 Accuracy 0.9361\n",
            "Epoch 77 Batch 250 Loss 0.2071 Accuracy 0.9343\n",
            "Epoch 77 Loss 0.2093 Accuracy 0.9337\n",
            "Time taken for 1 epoch: 55.58 secs\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.1573 Accuracy 0.9465\n",
            "Epoch 78 Batch 50 Loss 0.1802 Accuracy 0.9435\n",
            "Epoch 78 Batch 100 Loss 0.1844 Accuracy 0.9419\n",
            "Epoch 78 Batch 150 Loss 0.1887 Accuracy 0.9403\n",
            "Epoch 78 Batch 200 Loss 0.1935 Accuracy 0.9388\n",
            "Epoch 78 Batch 250 Loss 0.1987 Accuracy 0.9369\n",
            "Epoch 78 Loss 0.2009 Accuracy 0.9362\n",
            "Time taken for 1 epoch: 55.38 secs\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.1684 Accuracy 0.9398\n",
            "Epoch 79 Batch 50 Loss 0.1748 Accuracy 0.9452\n",
            "Epoch 79 Batch 100 Loss 0.1831 Accuracy 0.9425\n",
            "Epoch 79 Batch 150 Loss 0.1896 Accuracy 0.9401\n",
            "Epoch 79 Batch 200 Loss 0.1923 Accuracy 0.9393\n",
            "Epoch 79 Batch 250 Loss 0.1967 Accuracy 0.9376\n",
            "Epoch 79 Loss 0.1978 Accuracy 0.9371\n",
            "Time taken for 1 epoch: 56.03 secs\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.1560 Accuracy 0.9429\n",
            "Epoch 80 Batch 50 Loss 0.1739 Accuracy 0.9460\n",
            "Epoch 80 Batch 100 Loss 0.1834 Accuracy 0.9421\n",
            "Epoch 80 Batch 150 Loss 0.1909 Accuracy 0.9400\n",
            "Epoch 80 Batch 200 Loss 0.1927 Accuracy 0.9394\n",
            "Epoch 80 Batch 250 Loss 0.1960 Accuracy 0.9381\n",
            "Saving checkpoint for epoch 80 at ./checkpoints/train/ckpt-16\n",
            "Epoch 80 Loss 0.1979 Accuracy 0.9374\n",
            "Time taken for 1 epoch: 57.98 secs\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.1270 Accuracy 0.9611\n",
            "Epoch 81 Batch 50 Loss 0.1728 Accuracy 0.9454\n",
            "Epoch 81 Batch 100 Loss 0.1784 Accuracy 0.9440\n",
            "Epoch 81 Batch 150 Loss 0.1843 Accuracy 0.9421\n",
            "Epoch 81 Batch 200 Loss 0.1887 Accuracy 0.9406\n",
            "Epoch 81 Batch 250 Loss 0.1918 Accuracy 0.9396\n",
            "Epoch 81 Loss 0.1933 Accuracy 0.9391\n",
            "Time taken for 1 epoch: 56.03 secs\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.1816 Accuracy 0.9471\n",
            "Epoch 82 Batch 50 Loss 0.1703 Accuracy 0.9476\n",
            "Epoch 82 Batch 100 Loss 0.1737 Accuracy 0.9461\n",
            "Epoch 82 Batch 150 Loss 0.1803 Accuracy 0.9437\n",
            "Epoch 82 Batch 200 Loss 0.1848 Accuracy 0.9422\n",
            "Epoch 82 Batch 250 Loss 0.1876 Accuracy 0.9409\n",
            "Epoch 82 Loss 0.1899 Accuracy 0.9403\n",
            "Time taken for 1 epoch: 55.11 secs\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.1166 Accuracy 0.9594\n",
            "Epoch 83 Batch 50 Loss 0.1685 Accuracy 0.9461\n",
            "Epoch 83 Batch 100 Loss 0.1723 Accuracy 0.9458\n",
            "Epoch 83 Batch 150 Loss 0.1756 Accuracy 0.9446\n",
            "Epoch 83 Batch 200 Loss 0.1784 Accuracy 0.9435\n",
            "Epoch 83 Batch 250 Loss 0.1818 Accuracy 0.9421\n",
            "Epoch 83 Loss 0.1840 Accuracy 0.9415\n",
            "Time taken for 1 epoch: 55.03 secs\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.1549 Accuracy 0.9508\n",
            "Epoch 84 Batch 50 Loss 0.1602 Accuracy 0.9488\n",
            "Epoch 84 Batch 100 Loss 0.1669 Accuracy 0.9466\n",
            "Epoch 84 Batch 150 Loss 0.1727 Accuracy 0.9449\n",
            "Epoch 84 Batch 200 Loss 0.1771 Accuracy 0.9435\n",
            "Epoch 84 Batch 250 Loss 0.1815 Accuracy 0.9422\n",
            "Epoch 84 Loss 0.1828 Accuracy 0.9418\n",
            "Time taken for 1 epoch: 55.12 secs\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.2212 Accuracy 0.9423\n",
            "Epoch 85 Batch 50 Loss 0.1639 Accuracy 0.9477\n",
            "Epoch 85 Batch 100 Loss 0.1645 Accuracy 0.9476\n",
            "Epoch 85 Batch 150 Loss 0.1683 Accuracy 0.9467\n",
            "Epoch 85 Batch 200 Loss 0.1729 Accuracy 0.9453\n",
            "Epoch 85 Batch 250 Loss 0.1758 Accuracy 0.9444\n",
            "Saving checkpoint for epoch 85 at ./checkpoints/train/ckpt-17\n",
            "Epoch 85 Loss 0.1769 Accuracy 0.9440\n",
            "Time taken for 1 epoch: 58.28 secs\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.1441 Accuracy 0.9526\n",
            "Epoch 86 Batch 50 Loss 0.1612 Accuracy 0.9493\n",
            "Epoch 86 Batch 100 Loss 0.1647 Accuracy 0.9481\n",
            "Epoch 86 Batch 150 Loss 0.1667 Accuracy 0.9478\n",
            "Epoch 86 Batch 200 Loss 0.1695 Accuracy 0.9467\n",
            "Epoch 86 Batch 250 Loss 0.1730 Accuracy 0.9456\n",
            "Epoch 86 Loss 0.1736 Accuracy 0.9453\n",
            "Time taken for 1 epoch: 56.20 secs\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.1446 Accuracy 0.9481\n",
            "Epoch 87 Batch 50 Loss 0.1500 Accuracy 0.9532\n",
            "Epoch 87 Batch 100 Loss 0.1567 Accuracy 0.9510\n",
            "Epoch 87 Batch 150 Loss 0.1644 Accuracy 0.9486\n",
            "Epoch 87 Batch 200 Loss 0.1690 Accuracy 0.9472\n",
            "Epoch 87 Batch 250 Loss 0.1721 Accuracy 0.9461\n",
            "Epoch 87 Loss 0.1729 Accuracy 0.9459\n",
            "Time taken for 1 epoch: 54.73 secs\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.1565 Accuracy 0.9394\n",
            "Epoch 88 Batch 50 Loss 0.1527 Accuracy 0.9527\n",
            "Epoch 88 Batch 100 Loss 0.1543 Accuracy 0.9519\n",
            "Epoch 88 Batch 150 Loss 0.1585 Accuracy 0.9511\n",
            "Epoch 88 Batch 200 Loss 0.1624 Accuracy 0.9494\n",
            "Epoch 88 Batch 250 Loss 0.1663 Accuracy 0.9480\n",
            "Epoch 88 Loss 0.1680 Accuracy 0.9475\n",
            "Time taken for 1 epoch: 55.97 secs\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.1850 Accuracy 0.9429\n",
            "Epoch 89 Batch 50 Loss 0.1468 Accuracy 0.9534\n",
            "Epoch 89 Batch 100 Loss 0.1487 Accuracy 0.9530\n",
            "Epoch 89 Batch 150 Loss 0.1542 Accuracy 0.9511\n",
            "Epoch 89 Batch 200 Loss 0.1593 Accuracy 0.9495\n",
            "Epoch 89 Batch 250 Loss 0.1634 Accuracy 0.9481\n",
            "Epoch 89 Loss 0.1642 Accuracy 0.9479\n",
            "Time taken for 1 epoch: 55.00 secs\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.1311 Accuracy 0.9596\n",
            "Epoch 90 Batch 50 Loss 0.1447 Accuracy 0.9542\n",
            "Epoch 90 Batch 100 Loss 0.1505 Accuracy 0.9524\n",
            "Epoch 90 Batch 150 Loss 0.1569 Accuracy 0.9506\n",
            "Epoch 90 Batch 200 Loss 0.1593 Accuracy 0.9497\n",
            "Epoch 90 Batch 250 Loss 0.1633 Accuracy 0.9484\n",
            "Saving checkpoint for epoch 90 at ./checkpoints/train/ckpt-18\n",
            "Epoch 90 Loss 0.1641 Accuracy 0.9483\n",
            "Time taken for 1 epoch: 58.42 secs\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.1841 Accuracy 0.9555\n",
            "Epoch 91 Batch 50 Loss 0.1461 Accuracy 0.9551\n",
            "Epoch 91 Batch 100 Loss 0.1482 Accuracy 0.9539\n",
            "Epoch 91 Batch 150 Loss 0.1521 Accuracy 0.9525\n",
            "Epoch 91 Batch 200 Loss 0.1582 Accuracy 0.9503\n",
            "Epoch 91 Batch 250 Loss 0.1602 Accuracy 0.9496\n",
            "Epoch 91 Loss 0.1615 Accuracy 0.9492\n",
            "Time taken for 1 epoch: 54.75 secs\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.1777 Accuracy 0.9486\n",
            "Epoch 92 Batch 50 Loss 0.1418 Accuracy 0.9543\n",
            "Epoch 92 Batch 100 Loss 0.1481 Accuracy 0.9535\n",
            "Epoch 92 Batch 150 Loss 0.1519 Accuracy 0.9522\n",
            "Epoch 92 Batch 200 Loss 0.1558 Accuracy 0.9508\n",
            "Epoch 92 Batch 250 Loss 0.1588 Accuracy 0.9500\n",
            "Epoch 92 Loss 0.1588 Accuracy 0.9499\n",
            "Time taken for 1 epoch: 54.56 secs\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.1020 Accuracy 0.9651\n",
            "Epoch 93 Batch 50 Loss 0.1380 Accuracy 0.9562\n",
            "Epoch 93 Batch 100 Loss 0.1395 Accuracy 0.9560\n",
            "Epoch 93 Batch 150 Loss 0.1444 Accuracy 0.9541\n",
            "Epoch 93 Batch 200 Loss 0.1495 Accuracy 0.9528\n",
            "Epoch 93 Batch 250 Loss 0.1529 Accuracy 0.9517\n",
            "Epoch 93 Loss 0.1540 Accuracy 0.9513\n",
            "Time taken for 1 epoch: 55.08 secs\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.1267 Accuracy 0.9510\n",
            "Epoch 94 Batch 50 Loss 0.1382 Accuracy 0.9567\n",
            "Epoch 94 Batch 100 Loss 0.1400 Accuracy 0.9566\n",
            "Epoch 94 Batch 150 Loss 0.1427 Accuracy 0.9559\n",
            "Epoch 94 Batch 200 Loss 0.1478 Accuracy 0.9542\n",
            "Epoch 94 Batch 250 Loss 0.1510 Accuracy 0.9532\n",
            "Epoch 94 Loss 0.1529 Accuracy 0.9525\n",
            "Time taken for 1 epoch: 55.59 secs\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.1378 Accuracy 0.9563\n",
            "Epoch 95 Batch 50 Loss 0.1473 Accuracy 0.9553\n",
            "Epoch 95 Batch 100 Loss 0.1454 Accuracy 0.9554\n",
            "Epoch 95 Batch 150 Loss 0.1467 Accuracy 0.9546\n",
            "Epoch 95 Batch 200 Loss 0.1479 Accuracy 0.9541\n",
            "Epoch 95 Batch 250 Loss 0.1491 Accuracy 0.9534\n",
            "Saving checkpoint for epoch 95 at ./checkpoints/train/ckpt-19\n",
            "Epoch 95 Loss 0.1494 Accuracy 0.9533\n",
            "Time taken for 1 epoch: 57.72 secs\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.1366 Accuracy 0.9516\n",
            "Epoch 96 Batch 50 Loss 0.1361 Accuracy 0.9579\n",
            "Epoch 96 Batch 100 Loss 0.1363 Accuracy 0.9573\n",
            "Epoch 96 Batch 150 Loss 0.1398 Accuracy 0.9559\n",
            "Epoch 96 Batch 200 Loss 0.1446 Accuracy 0.9543\n",
            "Epoch 96 Batch 250 Loss 0.1461 Accuracy 0.9539\n",
            "Epoch 96 Loss 0.1469 Accuracy 0.9537\n",
            "Time taken for 1 epoch: 54.84 secs\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.1479 Accuracy 0.9636\n",
            "Epoch 97 Batch 50 Loss 0.1322 Accuracy 0.9580\n",
            "Epoch 97 Batch 100 Loss 0.1334 Accuracy 0.9582\n",
            "Epoch 97 Batch 150 Loss 0.1388 Accuracy 0.9566\n",
            "Epoch 97 Batch 200 Loss 0.1417 Accuracy 0.9556\n",
            "Epoch 97 Batch 250 Loss 0.1453 Accuracy 0.9544\n",
            "Epoch 97 Loss 0.1466 Accuracy 0.9540\n",
            "Time taken for 1 epoch: 54.64 secs\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.1289 Accuracy 0.9662\n",
            "Epoch 98 Batch 50 Loss 0.1307 Accuracy 0.9602\n",
            "Epoch 98 Batch 100 Loss 0.1330 Accuracy 0.9590\n",
            "Epoch 98 Batch 150 Loss 0.1363 Accuracy 0.9576\n",
            "Epoch 98 Batch 200 Loss 0.1389 Accuracy 0.9567\n",
            "Epoch 98 Batch 250 Loss 0.1419 Accuracy 0.9557\n",
            "Epoch 98 Loss 0.1429 Accuracy 0.9555\n",
            "Time taken for 1 epoch: 54.91 secs\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.1497 Accuracy 0.9519\n",
            "Epoch 99 Batch 50 Loss 0.1290 Accuracy 0.9602\n",
            "Epoch 99 Batch 100 Loss 0.1339 Accuracy 0.9583\n",
            "Epoch 99 Batch 150 Loss 0.1380 Accuracy 0.9570\n",
            "Epoch 99 Batch 200 Loss 0.1415 Accuracy 0.9557\n",
            "Epoch 99 Batch 250 Loss 0.1444 Accuracy 0.9545\n",
            "Epoch 99 Loss 0.1450 Accuracy 0.9543\n",
            "Time taken for 1 epoch: 54.92 secs\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.1177 Accuracy 0.9603\n",
            "Epoch 100 Batch 50 Loss 0.1249 Accuracy 0.9616\n",
            "Epoch 100 Batch 100 Loss 0.1292 Accuracy 0.9599\n",
            "Epoch 100 Batch 150 Loss 0.1323 Accuracy 0.9587\n",
            "Epoch 100 Batch 200 Loss 0.1356 Accuracy 0.9577\n",
            "Epoch 100 Batch 250 Loss 0.1393 Accuracy 0.9565\n",
            "Saving checkpoint for epoch 100 at ./checkpoints/train/ckpt-20\n",
            "Epoch 100 Loss 0.1398 Accuracy 0.9564\n",
            "Time taken for 1 epoch: 58.17 secs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run training! Each epoch takes several mins with GPU\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  # inp -> twi, tar -> french\n",
        "  for (batch, (inp,tar)) in enumerate(train_batches):\n",
        "    train_step(inp, tar)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a Translator\n"
      ],
      "metadata": {
        "id": "Aij6BWpIBU52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(tf.Module):\n",
        "    def __init__(self, tokenizers, transformer):\n",
        "        self.tokenizers = tokenizers\n",
        "        self.transformer = transformer\n",
        "\n",
        "    def __call__(self, sentence, max_length=MAX_TOKENS):\n",
        "        # The input sentence is TWI, hence adding the `[START]` and `[END]` tokens.\n",
        "        sentence = tf.convert_to_tensor([sentence])\n",
        "        sentence = self.tokenizers.twi.tokenize(sentence).to_tensor()\n",
        "\n",
        "        encoder_input = sentence\n",
        "\n",
        "        # As the output language is French, initialize the output with the\n",
        "        # English `[START]` token.\n",
        "        start_end = self.tokenizers.fr.tokenize([''])[0]\n",
        "        start = start_end[0][tf.newaxis]\n",
        "        end = start_end[1][tf.newaxis]\n",
        "\n",
        "        # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "        # dynamic-loop can be traced by `tf.function`.\n",
        "        output_array = tf.TensorArray(\n",
        "            dtype=tf.int64, size=0, dynamic_size=True)\n",
        "        output_array = output_array.write(0, start)\n",
        "\n",
        "        for i in tf.range(max_length):\n",
        "            output = tf.transpose(output_array.stack())\n",
        "\n",
        "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "                encoder_input, output)\n",
        "\n",
        "            predictions, attention_weights = self.transformer(\n",
        "                encoder_input, output, False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "\n",
        "            # Select the last token from the `seq_len` dimension.\n",
        "            # Shape `(batch_size, 1, vocab_size)`.\n",
        "            predictions = predictions[:, -1:, :]\n",
        "\n",
        "            predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "            # Concatenate the `predicted_id` to the output which is given to the\n",
        "            # decoder as its input.\n",
        "            output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "            if predicted_id == end:\n",
        "                break\n",
        "\n",
        "        output = tf.transpose(output_array.stack())\n",
        "        # The output shape is `(1, tokens)`.\n",
        "        text = self.tokenizers.fr.detokenize(output)[0]  # Shape: `()`.\n",
        "\n",
        "        tokens = self.tokenizers.fr.lookup(output)[0]\n",
        "\n",
        "        return text, tokens, attention_weights"
      ],
      "metadata": {
        "id": "z0clY8WkaGZZ"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "Tt1OVm3ecSNO"
      },
      "outputs": [],
      "source": [
        "#Create an instance of this Translator class\n",
        "translator = Translator(tokenizers, transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## translate example sentecnes"
      ],
      "metadata": {
        "id": "3dMj9N3fBh1P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "ysGcUmAzc_Yi"
      },
      "outputs": [],
      "source": [
        "def print_translation(sentence, tokens, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "wKmyWMcGduft",
        "outputId": "74b7ae8d-ac4e-4911-cd62-6c9a2570cd8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : Asamoah de hama no bɔɔ asangoli no mu.\n",
            "Prediction     : asamoah a atcal a lave au cerf volantien .\n",
            "Ground truth   : Asamoah a attaché la ficelle au cerf-volant.\n"
          ]
        }
      ],
      "source": [
        "sentence =\"Asamoah de hama no bɔɔ asangoli no mu.\"\n",
        "ground_truth= \"Asamoah a attaché la ficelle au cerf-volant.\"\n",
        "\n",
        "translated_text, translated_tokens, attention_weights = translator(sentence)\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "DYL7f6tu9Y0N",
        "outputId": "88c436d2-4c88-4e93-c9dc-7952065b3165",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : minnim onipa ko a woyɛ\n",
            "Prediction     : je ne sais pas qui tu es .\n",
            "Ground truth   : je ne sais pas qui tu es\n"
          ]
        }
      ],
      "source": [
        "sentence=\"minnim onipa ko a woyɛ\"\n",
        "ground_truth=\"je ne sais pas qui tu es\"\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Attentions"
      ],
      "metadata": {
        "id": "qWa9iysqBrFW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "O9pfpPIb9kUK"
      },
      "outputs": [],
      "source": [
        "def plot_attention_head(in_tokens, translated_tokens, attention):\n",
        "  # The model didn't generate `<START>` in the output. Skip it.\n",
        "  translated_tokens = translated_tokens[1:]\n",
        "\n",
        "  ax = plt.gca()\n",
        "  ax.matshow(attention)\n",
        "  ax.set_xticks(range(len(in_tokens)))\n",
        "  ax.set_yticks(range(len(translated_tokens)))\n",
        "\n",
        "  labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n",
        "  ax.set_xticklabels(\n",
        "      labels, rotation=90)\n",
        "\n",
        "  labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n",
        "  ax.set_yticklabels(labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "head = 0\n",
        "# Shape: `(batch=1, num_attention_heads, seq_len_q, seq_len_k)`.\n",
        "attention_heads = tf.squeeze(\n",
        "  attention_weights['decoder_layer4_block2'], 0)\n",
        "attention = attention_heads[head]\n",
        "attention.shape"
      ],
      "metadata": {
        "id": "4Jp-67xoVB9A",
        "outputId": "4a1b1bb6-327e-478b-e7ee-9501904b5765",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([9, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize last Twi input sentence\n",
        "in_tokens = tf.convert_to_tensor([sentence])\n",
        "in_tokens = tokenizers.twi.tokenize(in_tokens).to_tensor()\n",
        "in_tokens = tokenizers.twi.lookup(in_tokens)[0]\n",
        "np.char.decode(in_tokens.numpy().astype(np.bytes_), 'UTF-8')"
      ],
      "metadata": {
        "id": "qYbkydJST7Mu",
        "outputId": "5dfce61e-fdd3-481d-ed5b-c48820e33ae0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['[START]', 'kɔɔ', 'nsusuwii', 'aku', 'a', '##di', '[END]'],\n",
              "      dtype='<U8')"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View translated tokens\n",
        "translated_tokens"
      ],
      "metadata": {
        "id": "AcL3yO8aVKdg",
        "outputId": "b8864820-705c-49d8-c349-3ebc20569d7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=string, numpy=\n",
              "array([b'[START]', b'je', b'asamoah', b'dire', b'le', b'son', b'dans',\n",
              "       b'trop', b'.', b'[END]'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Transformer attention mechanism operation\n",
        "plot_attention_head(in_tokens, translated_tokens, attention)"
      ],
      "metadata": {
        "id": "CA7BYz3iVbTv",
        "outputId": "e78bedf3-10a5-4741-cc49-6092dc5dd81c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to plot individual attention weights\n",
        "def plot_attention_weights(sentence, translated_tokens, attention_heads):\n",
        "  in_tokens = tf.convert_to_tensor([sentence])\n",
        "  in_tokens = tokenizers.twi.tokenize(in_tokens).to_tensor()\n",
        "  in_tokens = tokenizers.twi.lookup(in_tokens)[0]\n",
        "\n",
        "  fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "  for h, head in enumerate(attention_heads):\n",
        "    ax = fig.add_subplot(2, 4, h+1)\n",
        "\n",
        "    plot_attention_head(in_tokens, translated_tokens, head)\n",
        "\n",
        "    ax.set_xlabel(f'Head {h+1}')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "7ve6yy7VVfd5"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the 8 attention heads in decoder layer 4 for last twi input\n",
        "plot_attention_weights(sentence,\n",
        "                       translated_tokens,\n",
        "                       attention_weights['decoder_layer4_block2'][0])"
      ],
      "metadata": {
        "id": "duWodUGoVo2D",
        "outputId": "1343e433-716f-4778-ab4f-d3e2b151f89b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        }
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x576 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Translator"
      ],
      "metadata": {
        "id": "tBe9E_dXBzne"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "qMpmh77M9ufZ"
      },
      "outputs": [],
      "source": [
        "# class to export translator\n",
        "class ExportTranslator(tf.Module):\n",
        "  def __init__(self, translator):\n",
        "    self.translator = translator\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def __call__(self, sentence):\n",
        "    (result,\n",
        "     tokens,\n",
        "     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "89tEraoTixKG"
      },
      "outputs": [],
      "source": [
        "translator = ExportTranslator(translator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "1ZurB6qDFDzh",
        "outputId": "a5cd2612-ff5f-48e4-c658-ef18fecafdc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'pere se promene tous les jours .'"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ],
      "source": [
        "translator(tf.constant('Nantew yiye da biara da .')).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "module_no_signatures_path = '/content/drive/MyDrive/twi_french_translator'\n",
        "print('Saving model...')\n",
        "tf.saved_model.save(translator, module_no_signatures_path)"
      ],
      "metadata": {
        "id": "YMhMt9_z2ZXH",
        "outputId": "63bdb147-2662-49f7-c25b-05cc2954e965",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn while saving (showing 5 of 332). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded = tf.saved_model.load(module_no_signatures_path)"
      ],
      "metadata": {
        "id": "P0jO3gtH9bih"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded('Saa ice cream no yɛ dɛ dɛn ?').numpy()"
      ],
      "metadata": {
        "id": "adjW9Tug-ZOa",
        "outputId": "3409a9a7-2c9c-4bea-b024-63317478c0c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'quelle est la saveur de cette glace ?'"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}