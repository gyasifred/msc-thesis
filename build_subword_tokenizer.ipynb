{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWqx8bC5esQiSqQwpGWDdR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/build_subword_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "82zBvmJQd-un"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U \"tensorflow-text==2.8.*\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_text as text\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import time\n",
        "# import BERT tool for building tokenizer\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
      ],
      "metadata": {
        "id": "3wkV2wuveOsb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code was adapted from https://github.com/GhanaNLP/kasa/blob/master/Kasa/Preprocessing.py\n",
        "# A subclass of the TFT  for preprocessing data\n",
        "# import required library\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "class Preprocessing:\n",
        "    # dummy initialization method\n",
        "    def __init__(self):\n",
        "        # initialize with some default parameters here later\n",
        "        pass\n",
        "\n",
        "    # read in parallel twi - english dataset\n",
        "    def read_parallel_dataset(self, filepath_twi, filepath_french):\n",
        "\n",
        "        # read french data\n",
        "        french_data = []\n",
        "        with open(filepath_french, encoding='utf-8') as file:\n",
        "            line = file.readline()\n",
        "            cnt = 1\n",
        "            while line:\n",
        "                french_data.append(line.strip())\n",
        "                line = file.readline()\n",
        "                cnt += 1\n",
        "\n",
        "        # read twi data\n",
        "        twi_data = []\n",
        "        with open(filepath_twi, encoding='utf-8') as file:\n",
        "\n",
        "            # twi=file.read()\n",
        "            line = file.readline()\n",
        "            cnt = 1\n",
        "            while line:\n",
        "                twi_data.append(line.strip())\n",
        "                line = file.readline()\n",
        "                cnt += 1\n",
        "\n",
        "        return twi_data, french_data\n",
        "\n",
        "    # Define a helper function to remove string accents\n",
        "\n",
        "    def removeStringAccent(self, s):\n",
        "        return ''.join(\n",
        "            c for c in unicodedata.normalize('NFD', s)\n",
        "            if unicodedata.category(c) != 'Mn'\n",
        "        )\n",
        "\n",
        "    # normalize input twi sentence\n",
        "    def normalize_twi(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.ƆɔɛƐ!?’]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "\n",
        "    # normalize input french sentence\n",
        "    def normalize_fr(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n"
      ],
      "metadata": {
        "id": "yZTktzHBeht0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import preprocessing class \n",
        "\n",
        "#from tft.preprocessing import Preprocessing # note form of library import\n",
        "\n",
        "# Create an instance of tft preprocessing class\n",
        "TwiFrPreprocessor = Preprocessing()\n",
        "\n",
        "# Read raw parallel dataset\n",
        "raw_data_twi,raw_data_fr = TwiFrPreprocessor.read_parallel_dataset(\n",
        "        filepath_twi='/content/verified_twi.txt',\n",
        "        filepath_french='/content/verified_french.txt')\n",
        "\n",
        "# Normalize the raw data\n",
        "raw_data_fr = [TwiFrPreprocessor.normalize_fr(data) for data in raw_data_fr]\n",
        "raw_data_twi = [TwiFrPreprocessor.normalize_twi(data) for data in raw_data_twi]"
      ],
      "metadata": {
        "id": "ZfXSdFbue9fE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write the preprocess dataset to a file\n",
        "with open('training_twi.txt', 'w') as f:\n",
        "    for line in raw_data_twi:\n",
        "        f.write(f\"{line}\\n\")\n",
        "\n",
        "with open('training_fr.txt', 'w') as f:\n",
        "    for line in raw_data_fr:\n",
        "        f.write(f\"{line}\\n\")"
      ],
      "metadata": {
        "id": "SaKbDVcjfrfY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build TF datasets from input sentences in both languages\n",
        "lines_dataset_fr = tf.data.TextLineDataset('/content/training_fr.txt')\n",
        "lines_dataset_tw = tf.data.TextLineDataset('/content/training_twi.txt')"
      ],
      "metadata": {
        "id": "IlB3FrDbgVPH"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine languages into single dataset\n",
        "combined = tf.data.Dataset.zip((lines_dataset_tw, lines_dataset_fr))"
      ],
      "metadata": {
        "id": "_AK-dhmEgra2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set tokenizer parameters and add reserved tokens; input files already lower-cased, but\n",
        "# lower_case option does NFD normalization, which is needed\n",
        "bert_tokenizer_params=dict(lower_case=True)\n",
        "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "# main parameter here that could be tuned is vocab size\n",
        "bert_vocab_args = dict(\n",
        "  # The target vocabulary size\n",
        "  vocab_size = 10000,\n",
        "  # Reserved tokens that must be included in the vocabulary\n",
        "  reserved_tokens=reserved_tokens,\n",
        "  # Arguments for `text.BertTokenizer`\n",
        "  bert_tokenizer_params=bert_tokenizer_params,\n",
        "  # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
        "  learn_params={},\n",
        "  )"
      ],
      "metadata": {
        "id": "MDYCdJN7h74X"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build French vocab file (takes several mins)\n",
        "# this is the bert_vocab module building its vocab file from the raw French sentences\n",
        "%%time\n",
        "fr_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    lines_dataset_fr,\n",
        "    **bert_vocab_args\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1paANH1XiJq-",
        "outputId": "e0ba83c2-3261-49f0-ce89-43b442803ea1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 39 s, sys: 1.24 s, total: 40.3 s\n",
            "Wall time: 43.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm French sub-word vocab built correctly (last line will look strange; this is expected with\n",
        "# bert_vocab)\n",
        "print(fr_vocab[:10])\n",
        "print(fr_vocab[1000:1010])\n",
        "print(fr_vocab[-10:])\n",
        "print(len(fr_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNbtw0PRiLBd",
        "outputId": "5cc0b22d-cbdb-4005-c63a-dfcc882c1616"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '.', '?', 'a', 'b', 'c']\n",
            "['amour', 'arrivera', 'avaient', 'batiment', 'blanc', 'canadien', 'completement', 'court', 'danser', 'debut']\n",
            "['vouloir', 'voulu', 'vus', '##!', '##.', '##?', '##j', '##q', '##v', '##w']\n",
            "2093\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write French vocab to file\n",
        "# this file will be used to build tokenizer\n",
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)"
      ],
      "metadata": {
        "id": "ROj2uzsNn5du"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "write_vocab_file('fr_vocab.txt', fr_vocab)"
      ],
      "metadata": {
        "id": "KvyvZaRLn94H"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build Twi vocab file \n",
        "%%time\n",
        "twi_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "lines_dataset_tw,\n",
        "**bert_vocab_args\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3nwyELtn_uw",
        "outputId": "a54a5bbb-7fc6-4742-b7d6-2dcee54adae2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 27.1 s, sys: 1.3 s, total: 28.4 s\n",
            "Wall time: 21.7 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm TWI sub-word vocab built correctly (last line will look strange; this is expected with\n",
        "# bert_vocab)\n",
        "print(twi_vocab[:10])\n",
        "print(twi_vocab[100:110])\n",
        "print(twi_vocab[1000:1010])\n",
        "print(twi_vocab[-10:])\n",
        "print(len(twi_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmRceX9ToE-t",
        "outputId": "810cbe30-e35d-4f5c-ba5c-0fc11ed6554a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '.', '?', 'a', 'b', 'c']\n",
            "['di', 'paa', 'sen', 'ɔyɛ', 'misusuw', 'bɛn', 'pɛ', '##m', 'kwan', 'ankasa']\n",
            "['gyaade', 'kotoku', 'mabrɛ', 'mansusuw', 'menom', 'nnera', 'nokwasɛm', 'nɔma', 'pefee', 'tia']\n",
            "['##.', '##?', '##b', '##j', '##p', '##q', '##v', '##x', '##z', '##’']\n",
            "1993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write TWI vocab to file and confirm both vocab files now present\n",
        "# this file will be used to build tokenizer\n",
        "write_vocab_file('twi_vocab.txt', twi_vocab)"
      ],
      "metadata": {
        "id": "9lxcrs5IoMSE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build full language-agnostic tokenizer class; this is standard Google code\n",
        "# import BERT tool for building tokenizer\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "\n",
        "class CustomTokenizer(tf.Module):\n",
        "    def __init__(self, reserved_tokens, vocab_path):\n",
        "        self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
        "        self._reserved_tokens = reserved_tokens\n",
        "        self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
        "        vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
        "        self.vocab = tf.Variable(vocab)\n",
        "        # Create the signatures for export:\n",
        "        # Include a tokenize signature for a batch of strings.\n",
        "        self.tokenize.get_concrete_function(\n",
        "            tf.TensorSpec(shape=[None], dtype=tf.string))\n",
        "        # Include `detokenize` and `lookup` signatures for:\n",
        "        # * `Tensors` with shapes [tokens] and [batch, tokens]\n",
        "        # * `RaggedTensors` with shape [batch, tokens]\n",
        "        self.detokenize.get_concrete_function(\n",
        "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "        self.detokenize.get_concrete_function(\n",
        "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "        self.lookup.get_concrete_function(\n",
        "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "        self.lookup.get_concrete_function(\n",
        "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "        # These `get_*` methods take no arguments\n",
        "        self.get_vocab_size.get_concrete_function()\n",
        "        self.get_vocab_path.get_concrete_function()\n",
        "        self.get_reserved_tokens.get_concrete_function()\n",
        "\n",
        "    @tf.function\n",
        "    def add_start_end(self,ragged):\n",
        "        START = tf.argmax(tf.constant(self._reserved_tokens) == \"[START]\")\n",
        "        END = tf.argmax(tf.constant(self._reserved_tokens) == \"[END]\")\n",
        "        count = ragged.bounding_shape()[0]\n",
        "        starts = tf.fill([count, 1], START)\n",
        "        ends = tf.fill([count, 1], END)\n",
        "        return tf.concat([starts, ragged, ends], axis=1)\n",
        "\n",
        "    # Function to remove reserved tokens after detokenization\n",
        "    @tf.function\n",
        "    def cleanup_text(self, reserved_tokens,token_txt):\n",
        "        # Drop the reserved tokens, except for \"[UNK]\".\n",
        "        bad_tokens = [re.escape(tok)\n",
        "                      for tok in reserved_tokens if tok != \"[UNK]\"]\n",
        "        bad_token_re = \"|\".join(bad_tokens)\n",
        "        bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
        "        result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
        "        # Join them into strings.\n",
        "        result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
        "        return result\n",
        "\n",
        "    @tf.function\n",
        "    def tokenize(self, strings):\n",
        "        enc = self.tokenizer.tokenize(strings)\n",
        "        # Merge the `word` and `word-piece` axes.\n",
        "        enc = enc.merge_dims(-2, -1)\n",
        "        enc = self.add_start_end(enc)\n",
        "        return enc\n",
        "\n",
        "    @tf.function\n",
        "    def detokenize(self, tokenized):\n",
        "        words = self.tokenizer.detokenize(tokenized)\n",
        "        return self.cleanup_text(self._reserved_tokens, words)\n",
        "\n",
        "    @tf.function\n",
        "    def lookup(self, token_ids):\n",
        "        return tf.gather(self.vocab, token_ids)\n",
        "\n",
        "    @tf.function\n",
        "    def get_vocab_size(self):\n",
        "        return tf.shape(self.vocab)[0]\n",
        "\n",
        "    @tf.function\n",
        "    def get_vocab_path(self):\n",
        "        return self._vocab_path\n",
        "\n",
        "    @tf.function\n",
        "    def get_reserved_tokens(self):\n",
        "        return tf.constant(self._reserved_tokens)\n"
      ],
      "metadata": {
        "id": "Vu2i_FEdiVMK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate tokenizer class for both TWI and FRENCH\n",
        "tokenizers = tf.Module()\n",
        "tokenizers.fr = CustomTokenizer(reserved_tokens, 'fr_vocab.txt')\n",
        "tokenizers.twi = CustomTokenizer(reserved_tokens, 'twi_vocab.txt')"
      ],
      "metadata": {
        "id": "cSzhIUTVnmJx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokenizer model\n",
        "model_name = '/content/drive/MyDrive/translate_fr_twi_converter'\n",
        "tf.saved_model.save(tokenizers, model_name)"
      ],
      "metadata": {
        "id": "8iGg2UmkqSql"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify tokenizer model can be reloaded\n",
        "tokenizers = tf.saved_model.load(model_name)\n",
        "tokenizers.fr.get_vocab_size().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1PqzgebsG0t",
        "outputId": "c812e60f-8890-4aac-c5af-97ec317dad34"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2093"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify tokenizer works on test sentence\n",
        "tokens = tokenizers.fr.tokenize(['je suis étudiant'])\n",
        "text_tokens = tokenizers.fr.lookup(tokens)\n",
        "text_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpme-11ssrJv",
        "outputId": "6572aa6a-73d1-41dc-88d7-286d484c5262"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'je', b'suis', b'etudiant', b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.fr.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhHAn_mesxjQ",
        "outputId": "cf939eac-28aa-43a6-a85b-6427a1408f8e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "je suis etudiant\n"
          ]
        }
      ]
    }
  ]
}