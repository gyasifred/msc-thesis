{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/build_subword_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook will build a subword tokenizer using the text.[BertTokenizer](https://www.tensorflow.org/text/guide/subwords_tokenizer) for French, English and Akan-Twi."
      ],
      "metadata": {
        "id": "J70FYSdTk5YQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UASjA3duwfmL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "82zBvmJQd-un"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U \"tensorflow-text==2.8.*\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_text as text\n",
        "import tensorflow as tf\n",
        "import time\n",
        "# import BERT tool for building tokenizer\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
      ],
      "metadata": {
        "id": "3wkV2wuveOsb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code was adapted from https://github.com/GhanaNLP/kasa/blob/master/Kasa/Preprocessing.py\n",
        "# A subclass of the kasafrench for preprocessing data\n",
        "# import required library\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "class Preprocessing:\n",
        "    # dummy initialization method\n",
        "    def __init__(self):\n",
        "        # initialize with some default parameters here later\n",
        "        pass\n",
        "\n",
        "    # read in parallel twi - english dataset\n",
        "    def read_parallel_dataset(self, filepath_1, filepath_2, filepath_3=None):\n",
        "        if filepath_3 != None:\n",
        "            # read first language data\n",
        "            lang_1 = []\n",
        "            with open(filepath_1, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_1.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            # read second language data\n",
        "            lang_2 = []\n",
        "            with open(filepath_2, encoding='utf-8') as file:\n",
        "\n",
        "                # twi=file.read()\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_2.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "            # Read third Language data\n",
        "            lang_3 = []\n",
        "            with open(filepath_3, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_3.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            return lang_1, lang_2, lang_3\n",
        "            \n",
        "        else:\n",
        "            # read first language data\n",
        "            lang_1 = []\n",
        "            with open(filepath_1, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_1.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            # read second language data\n",
        "            lang_2 = []\n",
        "            with open(filepath_2, encoding='utf-8') as file:\n",
        "\n",
        "                # twi=file.read()\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_2.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            return lang_1, lang_2\n",
        "\n",
        "    # Define a helper function to remove string accents\n",
        "\n",
        "    def removeStringAccent(self, s):\n",
        "        return ''.join(\n",
        "            c for c in unicodedata.normalize('NFD', s)\n",
        "            if unicodedata.category(c) != 'Mn'\n",
        "        )\n",
        "\n",
        "    # normalize input twi sentence\n",
        "    def normalize_twi(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = s.lower()\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.ƆɔɛƐ!?’]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "\n",
        "    # normalize input french sentence\n",
        "    def normalize_FrEn(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = s.lower()\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "    \n",
        "    def writeTotxt(self,destination,data):\n",
        "        with open(destination, 'w') as f:\n",
        "            for line in data:\n",
        "                 f.write(f\"{line}\\n\")"
      ],
      "metadata": {
        "id": "yZTktzHBeht0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import preprocessing class \n",
        "\n",
        "#from tft.preprocessing import Preprocessing # note form of library import\n",
        "\n",
        "# Create an instance of tft preprocessing class\n",
        "preprocessor = Preprocessing()\n",
        "\n",
        "# Read raw parallel dataset\n",
        "raw_data_twi, raw_data_fr, raw_eng_data = preprocessor.read_parallel_dataset(\n",
        "    filepath_1='/content/drive/MyDrive/verified_twi.txt',\n",
        "    filepath_2='/content/drive/MyDrive/verified_english_french.txt',\n",
        "    filepath_3='/content/drive/MyDrive/verified_english.txt'\n",
        ")\n",
        "\n",
        "# Normalize the raw data\n",
        "raw_data_fr = [preprocessor.normalize_FrEn(data) for data in raw_data_fr]\n",
        "raw_data_twi = [preprocessor.normalize_twi(data) for data in raw_data_twi]\n",
        "raw_data_eng = [preprocessor.normalize_FrEn(data) for data in raw_eng_data ]"
      ],
      "metadata": {
        "id": "ZfXSdFbue9fE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write the preprocess dataset to a file\n",
        "preprocessor.writeTotxt('raw_data_twi.txt',raw_data_twi)\n",
        "preprocessor.writeTotxt('raw_data_fr.txt',raw_data_fr)\n",
        "preprocessor.writeTotxt('raw_data_eng.txt',raw_data_eng)"
      ],
      "metadata": {
        "id": "SaKbDVcjfrfY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Build TF datasets from input sentences in ALL languages"
      ],
      "metadata": {
        "id": "Rbf_ZjsbulNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read the raw datasets\n",
        "lines_dataset_fr = tf.data.TextLineDataset('/content/raw_data_fr.txt')\n",
        "lines_dataset_tw = tf.data.TextLineDataset('/content/raw_data_twi.txt')\n",
        "lines_dataset_eng= tf.data.TextLineDataset('/content/raw_data_eng.txt')"
      ],
      "metadata": {
        "id": "IlB3FrDbgVPH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine languages into single dataset\n",
        "combined = tf.data.Dataset.zip((lines_dataset_tw, lines_dataset_fr,lines_dataset_eng))"
      ],
      "metadata": {
        "id": "_AK-dhmEgra2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set tokenizer parameters and add reserved tokens; input files already lower-cased, but\n",
        "# lower_case option does NFD normalization, which is needed\n",
        "bert_tokenizer_params=dict(lower_case=True)\n",
        "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "# main parameter here that could be tuned is vocab size\n",
        "bert_vocab_args = dict(\n",
        "  # The target vocabulary size\n",
        "  vocab_size = 5000,\n",
        "  # Reserved tokens that must be included in the vocabulary\n",
        "  reserved_tokens=reserved_tokens,\n",
        "  # Arguments for `text.BertTokenizer`\n",
        "  bert_tokenizer_params=bert_tokenizer_params,\n",
        "  # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
        "  learn_params={},\n",
        "  )"
      ],
      "metadata": {
        "id": "MDYCdJN7h74X"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build French vocab file (takes several mins)\n",
        "# this is the bert_vocab module building its vocab file from the raw French sentences\n",
        "%%time\n",
        "fr_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    lines_dataset_fr,\n",
        "    **bert_vocab_args\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1paANH1XiJq-",
        "outputId": "034d5816-9751-4068-88d7-c1a071f3a694"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 54.1 s, sys: 1.39 s, total: 55.4 s\n",
            "Wall time: 55.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm French sub-word vocab built correctly (last line will look strange; this is expected with\n",
        "# bert_vocab)\n",
        "print(fr_vocab[:10])\n",
        "print(fr_vocab[1000:1010])\n",
        "print(fr_vocab[-10:])\n",
        "print(len(fr_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNbtw0PRiLBd",
        "outputId": "d945cb51-ece7-4dfd-c5aa-71f0e49148a3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '.', '?', 'a', 'b', 'c']\n",
            "['##tion', '##tres', '##usement', 'amour', 'arrivera', 'avaient', 'batiment', 'blanc', 'canadien', 'completement']\n",
            "['vouloir', 'voulu', 'vus', '##!', '##.', '##?', '##j', '##q', '##v', '##w']\n",
            "2096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to write the build vocab to file\n",
        "# this file will be used to build tokenizer\n",
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)"
      ],
      "metadata": {
        "id": "ROj2uzsNn5du"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write French Vocabs to file\n",
        "write_vocab_file('fr_vocab.txt', fr_vocab)"
      ],
      "metadata": {
        "id": "KvyvZaRLn94H"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build Twi vocab file \n",
        "%%time\n",
        "twi_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "lines_dataset_tw,\n",
        "**bert_vocab_args\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3nwyELtn_uw",
        "outputId": "aba147e0-b902-4e32-f7c5-174ae57abc53"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 37.3 s, sys: 1.3 s, total: 38.6 s\n",
            "Wall time: 31.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm TWI sub-word vocab built correctly (last line will look strange; this is expected with\n",
        "# bert_vocab)\n",
        "print(twi_vocab[:10])\n",
        "print(twi_vocab[100:110])\n",
        "print(twi_vocab[1000:1010])\n",
        "print(twi_vocab[-10:])\n",
        "print(len(twi_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmRceX9ToE-t",
        "outputId": "bfad4743-5ebd-4a44-f46a-fae18b31df30"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '.', '?', 'a', 'b', 'c']\n",
            "['di', 'paa', 'sen', 'ɔyɛ', 'misusuw', 'bɛn', 'pɛ', '##m', 'kwan', 'ankasa']\n",
            "['gyaade', 'kotoku', 'mabrɛ', 'mansusuw', 'menom', 'nnera', 'nokwasɛm', 'nɔma', 'pefee', 'tia']\n",
            "['##.', '##?', '##b', '##j', '##p', '##q', '##v', '##x', '##z', '##’']\n",
            "1993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write twi vocabs to file\n",
        "write_vocab_file('twi_vocab.txt', twi_vocab)"
      ],
      "metadata": {
        "id": "9lxcrs5IoMSE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build English vocab file\n",
        "%%time\n",
        "eng_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    lines_dataset_eng,\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "metadata": {
        "id": "EKc9rhulyA7t",
        "outputId": "3195be08-93ec-4442-a9e2-349cd2a0f0d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 47.8 s, sys: 1.3 s, total: 49.1 s\n",
            "Wall time: 45 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm TWI sub-word vocab built correctly (last line will look strange; this is expected with\n",
        "# bert_vocab)\n",
        "print(eng_vocab[:10])\n",
        "print(eng_vocab[100:110])\n",
        "print(eng_vocab[1000:1010])\n",
        "print(eng_vocab[-10:])\n",
        "print(len(eng_vocab))"
      ],
      "metadata": {
        "id": "JXrsjAtOyrml",
        "outputId": "4cc72643-b21b-42ee-d3ba-1e58cc85ca00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '.', '?', 'a', 'b', 'c']\n",
            "['out', 'were', 'get', 'doesn', 'who', 'one', 'by', '##d', 'going', 'would']\n",
            "['##te', '##um', 'accept', 'anybody', 'boyfriend', 'case', 'cats', 'chinese', 'concert', 'correct']\n",
            "['workers', 'yellow', '##!', '##.', '##?', '##c', '##j', '##q', '##v', '##z']\n",
            "1905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write english vocabs to file\n",
        "write_vocab_file('eng_vocab.txt', eng_vocab)"
      ],
      "metadata": {
        "id": "jewVuwtszFs4"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build full language-agnostic tokenizer class; this is standard Google code\n",
        "# import BERT tool for building tokenizer\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "\n",
        "class CustomTokenizer(tf.Module):\n",
        "    def __init__(self, reserved_tokens, vocab_path):\n",
        "        self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
        "        self._reserved_tokens = reserved_tokens\n",
        "        self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
        "        vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
        "        self.vocab = tf.Variable(vocab)\n",
        "        # Create the signatures for export:\n",
        "        # Include a tokenize signature for a batch of strings.\n",
        "        self.tokenize.get_concrete_function(\n",
        "            tf.TensorSpec(shape=[None], dtype=tf.string))\n",
        "        # Include `detokenize` and `lookup` signatures for:\n",
        "        # * `Tensors` with shapes [tokens] and [batch, tokens]\n",
        "        # * `RaggedTensors` with shape [batch, tokens]\n",
        "        self.detokenize.get_concrete_function(\n",
        "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "        self.detokenize.get_concrete_function(\n",
        "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "        self.lookup.get_concrete_function(\n",
        "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "        self.lookup.get_concrete_function(\n",
        "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "        # These `get_*` methods take no arguments\n",
        "        self.get_vocab_size.get_concrete_function()\n",
        "        self.get_vocab_path.get_concrete_function()\n",
        "        self.get_reserved_tokens.get_concrete_function()\n",
        "\n",
        "    @tf.function\n",
        "    def add_start_end(self,ragged):\n",
        "        START = tf.argmax(tf.constant(self._reserved_tokens) == \"[START]\")\n",
        "        END = tf.argmax(tf.constant(self._reserved_tokens) == \"[END]\")\n",
        "        count = ragged.bounding_shape()[0]\n",
        "        starts = tf.fill([count, 1], START)\n",
        "        ends = tf.fill([count, 1], END)\n",
        "        return tf.concat([starts, ragged, ends], axis=1)\n",
        "\n",
        "    # Function to remove reserved tokens after detokenization\n",
        "    @tf.function\n",
        "    def cleanup_text(self, reserved_tokens,token_txt):\n",
        "        # Drop the reserved tokens, except for \"[UNK]\".\n",
        "        bad_tokens = [re.escape(tok)\n",
        "                      for tok in reserved_tokens if tok != \"[UNK]\"]\n",
        "        bad_token_re = \"|\".join(bad_tokens)\n",
        "        bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
        "        result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
        "        # Join them into strings.\n",
        "        result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
        "        return result\n",
        "\n",
        "    @tf.function\n",
        "    def tokenize(self, strings):\n",
        "        enc = self.tokenizer.tokenize(strings)\n",
        "        # Merge the `word` and `word-piece` axes.\n",
        "        enc = enc.merge_dims(-2, -1)\n",
        "        enc = self.add_start_end(enc)\n",
        "        return enc\n",
        "\n",
        "    @tf.function\n",
        "    def detokenize(self, tokenized):\n",
        "        words = self.tokenizer.detokenize(tokenized)\n",
        "        return self.cleanup_text(self._reserved_tokens, words)\n",
        "\n",
        "    @tf.function\n",
        "    def lookup(self, token_ids):\n",
        "        return tf.gather(self.vocab, token_ids)\n",
        "\n",
        "    @tf.function\n",
        "    def get_vocab_size(self):\n",
        "        return tf.shape(self.vocab)[0]\n",
        "\n",
        "    @tf.function\n",
        "    def get_vocab_path(self):\n",
        "        return self._vocab_path\n",
        "\n",
        "    @tf.function\n",
        "    def get_reserved_tokens(self):\n",
        "        return tf.constant(self._reserved_tokens)\n"
      ],
      "metadata": {
        "id": "Vu2i_FEdiVMK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate tokenizer class for both TWI and FRENCH\n",
        "tokenizers = tf.Module()\n",
        "tokenizers.fr = CustomTokenizer(reserved_tokens, 'fr_vocab.txt')\n",
        "tokenizers.twi = CustomTokenizer(reserved_tokens, 'twi_vocab.txt')\n",
        "tokenizers.eng = CustomTokenizer(reserved_tokens, 'eng_vocab.txt')"
      ],
      "metadata": {
        "id": "cSzhIUTVnmJx"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokenizer model\n",
        "model_name = '/content/drive/MyDrive/translate_frengtwi_converter'\n",
        "tf.saved_model.save(tokenizers, model_name)"
      ],
      "metadata": {
        "id": "8iGg2UmkqSql"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify tokenizer model can be reloaded\n",
        "tokenizers = tf.saved_model.load(model_name)\n",
        "print(tokenizers.fr.get_vocab_size().numpy())\n",
        "print(tokenizers.twi.get_vocab_size().numpy())\n",
        "print(tokenizers.eng.get_vocab_size().numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1PqzgebsG0t",
        "outputId": "067a122b-c627-4017-d3da-672b0139d68d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2096\n",
            "1993\n",
            "1905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify tokenizer works on test sentence\n",
        "tokens = tokenizers.fr.tokenize(['je suis étudiant'])\n",
        "text_tokens = tokenizers.fr.lookup(tokens)\n",
        "text_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpme-11ssrJv",
        "outputId": "98feb20d-a2a4-4131-dce2-42e757d8ac37"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'je', b'suis', b'etudiant', b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.fr.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhHAn_mesxjQ",
        "outputId": "38846c85-24b1-411f-8624-edf72c9a2172"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "je suis etudiant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify if tokenizer work on test Twi sentence\n",
        "tokens = tokenizers.twi.tokenize(['Obiara ani gyee na akokoduru no ho .'])\n",
        "text_tokens = tokenizers.twi.lookup(tokens)\n",
        "print(text_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZDwPpvQ3BCX",
        "outputId": "5c998746-079e-40b6-d551-3908b55db1f1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.RaggedTensor [[b'[START]', b'obiara', b'ani', b'gyee', b'na', b'akokoduru', b'no',\n",
            "  b'ho', b'.', b'[END]']]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.twi.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTJ7BkPn3CPY",
        "outputId": "935fe233-00d4-460b-bcdd-fce99d324393"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "obiara ani gyee na akokoduru no ho .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify if tokenizer work on test Twi sentence\n",
        "tokens = tokenizers.eng.tokenize(['Patience is key to happiness'])\n",
        "text_tokens = tokenizers.eng.lookup(tokens)\n",
        "print(text_tokens)"
      ],
      "metadata": {
        "id": "4cmshBy51p-o",
        "outputId": "c738e264-dd44-40c1-f3cf-194ab80f57ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.RaggedTensor [[b'[START]', b'patience', b'is', b'key', b'to', b'happiness', b'[END]']]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.eng.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ],
      "metadata": {
        "id": "ENMjitf015rI",
        "outputId": "1ab04d43-afa5-47c6-a392-7d28273913ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patience is key to happiness\n"
          ]
        }
      ]
    }
  ]
}