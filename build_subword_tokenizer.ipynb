{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/build_subword_tokenizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook will build a subword tokenizer using the [text.BertTokenizer](https://www.tensorflow.org/text/guide/subwords_tokenizer) for French, English and Akan-Twi."
      ],
      "metadata": {
        "id": "J70FYSdTk5YQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UASjA3duwfmL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "82zBvmJQd-un",
        "outputId": "402f3ef3-d634-45b6-b76b-8bdeab20f8d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 5.9 MB 4.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 582.0 MB 13 kB/s \n",
            "\u001b[K     |████████████████████████████████| 439 kB 70.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 67.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 38.0 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U tensorflow-text-nightly==2.11.0.dev20220817\n",
        "!pip install -U -q tf-nightly==2.11.0.dev20220817"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_text as text\n",
        "import tensorflow as tf\n",
        "import time\n",
        "# import BERT tool for building tokenizer\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
      ],
      "metadata": {
        "id": "3wkV2wuveOsb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This code was adapted from https://github.com/GhanaNLP/kasa/blob/master/Kasa/Preprocessing.py\n",
        "# A subclass of the kasafrench for preprocessing data\n",
        "# import required library\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "class Preprocessing:\n",
        "    # dummy initialization method\n",
        "    def __init__(self):\n",
        "        # initialize with some default parameters here later\n",
        "        pass\n",
        "\n",
        "    # read in parallel twi - english dataset\n",
        "    def read_parallel_dataset(self, filepath_1, filepath_2, filepath_3=None):\n",
        "        if filepath_3 != None:\n",
        "            # read first language data\n",
        "            lang_1 = []\n",
        "            with open(filepath_1, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_1.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            # read second language data\n",
        "            lang_2 = []\n",
        "            with open(filepath_2, encoding='utf-8') as file:\n",
        "\n",
        "                # twi=file.read()\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_2.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "            # Read third Language data\n",
        "            lang_3 = []\n",
        "            with open(filepath_3, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_3.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            return lang_1, lang_2, lang_3\n",
        "            \n",
        "        else:\n",
        "            # read first language data\n",
        "            lang_1 = []\n",
        "            with open(filepath_1, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_1.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            # read second language data\n",
        "            lang_2 = []\n",
        "            with open(filepath_2, encoding='utf-8') as file:\n",
        "\n",
        "                # twi=file.read()\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_2.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            return lang_1, lang_2\n",
        "\n",
        "    # Define a helper function to remove string accents\n",
        "\n",
        "    def removeStringAccent(self, s):\n",
        "        return ''.join(\n",
        "            c for c in unicodedata.normalize('NFD', s)\n",
        "            if unicodedata.category(c) != 'Mn'\n",
        "        )\n",
        "\n",
        "    # normalize input twi sentence\n",
        "    def normalize_twi(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = s.lower()\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.ƆɔɛƐ!?’]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "\n",
        "    # normalize input french sentence\n",
        "    def normalize_FrEn(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = s.lower()\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "    \n",
        "    def writeTotxt(self,destination,data):\n",
        "        with open(destination, 'w') as f:\n",
        "            for line in data:\n",
        "                 f.write(f\"{line}\\n\")"
      ],
      "metadata": {
        "id": "yZTktzHBeht0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import preprocessing class \n",
        "\n",
        "#from tft.preprocessing import Preprocessing # note form of library import\n",
        "\n",
        "# Create an instance of tft preprocessing class\n",
        "preprocessor = Preprocessing()\n",
        "\n",
        "# Read raw parallel dataset\n",
        "raw_data_twi, raw_data_fr, raw_eng_data = preprocessor.read_parallel_dataset(\n",
        "    filepath_1='/content/drive/MyDrive/verified_twi.txt',\n",
        "    filepath_2='/content/drive/MyDrive/verified_english_french.txt',\n",
        "    filepath_3='/content/drive/MyDrive/verified_english.txt'\n",
        ")\n",
        "\n",
        "# Normalize the raw data\n",
        "raw_data_fr = [preprocessor.normalize_FrEn(data) for data in raw_data_fr]\n",
        "raw_data_twi = [preprocessor.normalize_twi(data) for data in raw_data_twi]\n",
        "raw_data_eng = [preprocessor.normalize_FrEn(data) for data in raw_eng_data ]"
      ],
      "metadata": {
        "id": "ZfXSdFbue9fE"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write the preprocess dataset to a file\n",
        "preprocessor.writeTotxt('raw_data_twi.txt',raw_data_twi)\n",
        "preprocessor.writeTotxt('raw_data_fr.txt',raw_data_fr)\n",
        "preprocessor.writeTotxt('raw_data_eng.txt',raw_data_eng)"
      ],
      "metadata": {
        "id": "SaKbDVcjfrfY"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Build TF datasets from input sentences in ALL languages"
      ],
      "metadata": {
        "id": "Rbf_ZjsbulNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read the raw datasets\n",
        "lines_dataset_fr = tf.data.TextLineDataset('/content/raw_data_fr.txt')\n",
        "lines_dataset_tw = tf.data.TextLineDataset('/content/raw_data_twi.txt')\n",
        "lines_dataset_eng= tf.data.TextLineDataset('/content/raw_data_eng.txt')"
      ],
      "metadata": {
        "id": "IlB3FrDbgVPH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine languages into single dataset\n",
        "combined = tf.data.Dataset.zip((lines_dataset_tw, lines_dataset_fr,lines_dataset_eng))"
      ],
      "metadata": {
        "id": "_AK-dhmEgra2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set tokenizer parameters and add reserved tokens; input files already lower-cased, but\n",
        "# lower_case option does NFD normalization, which is needed\n",
        "bert_tokenizer_params=dict(lower_case=True)\n",
        "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
        "# main parameter here that could be tuned is vocab size\n",
        "bert_vocab_args = dict(\n",
        "  # The target vocabulary size\n",
        "  vocab_size = 5000,\n",
        "  # Reserved tokens that must be included in the vocabulary\n",
        "  reserved_tokens=reserved_tokens,\n",
        "  # Arguments for `text.BertTokenizer`\n",
        "  bert_tokenizer_params=bert_tokenizer_params,\n",
        "  # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
        "  learn_params={},\n",
        "  )"
      ],
      "metadata": {
        "id": "MDYCdJN7h74X"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build French vocab file (takes several mins)\n",
        "# this is the bert_vocab module building its vocab file from the raw French sentences\n",
        "%%time\n",
        "fr_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    lines_dataset_fr,\n",
        "    **bert_vocab_args\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1paANH1XiJq-",
        "outputId": "a48d47cb-4b29-4b77-dffc-c866d71a354a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 53.7 s, sys: 1.5 s, total: 55.2 s\n",
            "Wall time: 45.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm French sub-word vocab built correctly (last line will look strange; this is expected with\n",
        "# bert_vocab)\n",
        "print(fr_vocab[:10])\n",
        "print(fr_vocab[1000:1010])\n",
        "print(fr_vocab[-10:])\n",
        "print(len(fr_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNbtw0PRiLBd",
        "outputId": "0943c36b-be3d-4c1b-ba1e-e5234682fe2a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '.', '?', 'a', 'b', 'c']\n",
            "['##tion', '##tres', '##usement', 'amour', 'arrivera', 'avaient', 'batiment', 'blanc', 'canadien', 'completement']\n",
            "['vouloir', 'voulu', 'vus', '##!', '##.', '##?', '##j', '##q', '##v', '##w']\n",
            "2096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to write the build vocab to file\n",
        "# this file will be used to build tokenizer\n",
        "def write_vocab_file(filepath, vocab):\n",
        "  with open(filepath, 'w') as f:\n",
        "    for token in vocab:\n",
        "      print(token, file=f)"
      ],
      "metadata": {
        "id": "ROj2uzsNn5du"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write French Vocabs to file\n",
        "write_vocab_file('fr_vocab.txt', fr_vocab)"
      ],
      "metadata": {
        "id": "KvyvZaRLn94H"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build Twi vocab file \n",
        "%%time\n",
        "twi_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "lines_dataset_tw,\n",
        "**bert_vocab_args\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3nwyELtn_uw",
        "outputId": "e0305e20-2dbd-41a5-d1e5-f18079094508"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 41.1 s, sys: 1.52 s, total: 42.7 s\n",
            "Wall time: 33.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm TWI sub-word vocab built correctly (last line will look strange; this is expected with\n",
        "# bert_vocab)\n",
        "print(twi_vocab[:10])\n",
        "print(twi_vocab[100:110])\n",
        "print(twi_vocab[1000:1010])\n",
        "print(twi_vocab[-10:])\n",
        "print(len(twi_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmRceX9ToE-t",
        "outputId": "34523199-def1-4e47-f24b-dd702a11672a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '.', '?', 'a', 'b', 'c']\n",
            "['di', 'paa', 'sen', 'ɔyɛ', 'misusuw', 'bɛn', 'pɛ', '##m', 'kwan', 'ankasa']\n",
            "['gyaade', 'kotoku', 'mabrɛ', 'mansusuw', 'menom', 'nnera', 'nokwasɛm', 'nɔma', 'pefee', 'tia']\n",
            "['##.', '##?', '##b', '##j', '##p', '##q', '##v', '##x', '##z', '##’']\n",
            "1993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write twi vocabs to file\n",
        "write_vocab_file('twi_vocab.txt', twi_vocab)"
      ],
      "metadata": {
        "id": "9lxcrs5IoMSE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build English vocab file\n",
        "%%time\n",
        "eng_vocab = bert_vocab.bert_vocab_from_dataset(\n",
        "    lines_dataset_eng,\n",
        "    **bert_vocab_args\n",
        ")"
      ],
      "metadata": {
        "id": "EKc9rhulyA7t",
        "outputId": "71dc3aca-07fe-4b05-ac2c-54095788e8d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 47.3 s, sys: 1.54 s, total: 48.9 s\n",
            "Wall time: 36.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# confirm TWI sub-word vocab built correctly (last line will look strange; this is expected with\n",
        "# bert_vocab)\n",
        "print(eng_vocab[:10])\n",
        "print(eng_vocab[100:110])\n",
        "print(eng_vocab[1000:1010])\n",
        "print(eng_vocab[-10:])\n",
        "print(len(eng_vocab))"
      ],
      "metadata": {
        "id": "JXrsjAtOyrml",
        "outputId": "ca31c65b-e8dc-4f94-864b-5b98a60b9bc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[PAD]', '[UNK]', '[START]', '[END]', '!', '.', '?', 'a', 'b', 'c']\n",
            "['out', 'were', 'get', 'doesn', 'who', 'one', 'by', '##d', 'going', 'would']\n",
            "['##te', '##um', 'accept', 'anybody', 'boyfriend', 'case', 'cats', 'chinese', 'concert', 'correct']\n",
            "['workers', 'yellow', '##!', '##.', '##?', '##c', '##j', '##q', '##v', '##z']\n",
            "1905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write english vocabs to file\n",
        "write_vocab_file('eng_vocab.txt', eng_vocab)"
      ],
      "metadata": {
        "id": "jewVuwtszFs4"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build full language-agnostic tokenizer class; this is standard Google code\n",
        "# import BERT tool for building tokenizer\n",
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as text\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "\n",
        "class CustomTokenizer(tf.Module):\n",
        "    def __init__(self, reserved_tokens, vocab_path):\n",
        "        self.tokenizer = text.BertTokenizer(vocab_path, lower_case=True)\n",
        "        self._reserved_tokens = reserved_tokens\n",
        "        self._vocab_path = tf.saved_model.Asset(vocab_path)\n",
        "        vocab = pathlib.Path(vocab_path).read_text().splitlines()\n",
        "        self.vocab = tf.Variable(vocab)\n",
        "        # Create the signatures for export:\n",
        "        # Include a tokenize signature for a batch of strings.\n",
        "        self.tokenize.get_concrete_function(\n",
        "            tf.TensorSpec(shape=[None], dtype=tf.string))\n",
        "        # Include `detokenize` and `lookup` signatures for:\n",
        "        # * `Tensors` with shapes [tokens] and [batch, tokens]\n",
        "        # * `RaggedTensors` with shape [batch, tokens]\n",
        "        self.detokenize.get_concrete_function(\n",
        "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "        self.detokenize.get_concrete_function(\n",
        "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "        self.lookup.get_concrete_function(\n",
        "            tf.TensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "        self.lookup.get_concrete_function(\n",
        "            tf.RaggedTensorSpec(shape=[None, None], dtype=tf.int64))\n",
        "        # These `get_*` methods take no arguments\n",
        "        self.get_vocab_size.get_concrete_function()\n",
        "        self.get_vocab_path.get_concrete_function()\n",
        "        self.get_reserved_tokens.get_concrete_function()\n",
        "\n",
        "    @tf.function\n",
        "    def add_start_end(self,ragged):\n",
        "        START = tf.argmax(tf.constant(self._reserved_tokens) == \"[START]\")\n",
        "        END = tf.argmax(tf.constant(self._reserved_tokens) == \"[END]\")\n",
        "        count = ragged.bounding_shape()[0]\n",
        "        starts = tf.fill([count, 1], START)\n",
        "        ends = tf.fill([count, 1], END)\n",
        "        return tf.concat([starts, ragged, ends], axis=1)\n",
        "\n",
        "    # Function to remove reserved tokens after detokenization\n",
        "    @tf.function\n",
        "    def cleanup_text(self, reserved_tokens,token_txt):\n",
        "        # Drop the reserved tokens, except for \"[UNK]\".\n",
        "        bad_tokens = [re.escape(tok)\n",
        "                      for tok in reserved_tokens if tok != \"[UNK]\"]\n",
        "        bad_token_re = \"|\".join(bad_tokens)\n",
        "        bad_cells = tf.strings.regex_full_match(token_txt, bad_token_re)\n",
        "        result = tf.ragged.boolean_mask(token_txt, ~bad_cells)\n",
        "        # Join them into strings.\n",
        "        result = tf.strings.reduce_join(result, separator=' ', axis=-1)\n",
        "        return result\n",
        "\n",
        "    @tf.function\n",
        "    def tokenize(self, strings):\n",
        "        enc = self.tokenizer.tokenize(strings)\n",
        "        # Merge the `word` and `word-piece` axes.\n",
        "        enc = enc.merge_dims(-2, -1)\n",
        "        enc = self.add_start_end(enc)\n",
        "        return enc\n",
        "\n",
        "    @tf.function\n",
        "    def detokenize(self, tokenized):\n",
        "        words = self.tokenizer.detokenize(tokenized)\n",
        "        return self.cleanup_text(self._reserved_tokens, words)\n",
        "\n",
        "    @tf.function\n",
        "    def lookup(self, token_ids):\n",
        "        return tf.gather(self.vocab, token_ids)\n",
        "\n",
        "    @tf.function\n",
        "    def get_vocab_size(self):\n",
        "        return tf.shape(self.vocab)[0]\n",
        "\n",
        "    @tf.function\n",
        "    def get_vocab_path(self):\n",
        "        return self._vocab_path\n",
        "\n",
        "    @tf.function\n",
        "    def get_reserved_tokens(self):\n",
        "        return tf.constant(self._reserved_tokens)"
      ],
      "metadata": {
        "id": "Vu2i_FEdiVMK"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate tokenizer class for both TWI and FRENCH\n",
        "tokenizers = tf.Module()\n",
        "tokenizers.fr = CustomTokenizer(reserved_tokens, 'fr_vocab.txt')\n",
        "tokenizers.twi = CustomTokenizer(reserved_tokens, 'twi_vocab.txt')\n",
        "tokenizers.eng = CustomTokenizer(reserved_tokens, 'eng_vocab.txt')"
      ],
      "metadata": {
        "id": "cSzhIUTVnmJx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokenizer model\n",
        "model_name = '/content/drive/MyDrive/translate_frengtwi_converter'\n",
        "tf.saved_model.save(tokenizers, model_name)"
      ],
      "metadata": {
        "id": "8iGg2UmkqSql"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify tokenizer model can be reloaded\n",
        "tokenizers = tf.saved_model.load(model_name)\n",
        "print(tokenizers.fr.get_vocab_size().numpy())\n",
        "print(tokenizers.twi.get_vocab_size().numpy())\n",
        "print(tokenizers.eng.get_vocab_size().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1PqzgebsG0t",
        "outputId": "ed2cd35a-59fd-46a8-ba6c-44bf65c447c9"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2096\n",
            "1993\n",
            "1905\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify tokenizer works on test sentence\n",
        "tokens = tokenizers.fr.tokenize(['je suis étudiant'])\n",
        "text_tokens = tokenizers.fr.lookup(tokens)\n",
        "text_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpme-11ssrJv",
        "outputId": "0506e080-c54b-4fc6-8dbb-7fb0bb064fd7"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'je', b'suis', b'etudiant', b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.fr.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhHAn_mesxjQ",
        "outputId": "287a48bb-64f0-4cd2-827c-fab32101c0b9"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "je suis etudiant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify if tokenizer work on test Twi sentence\n",
        "tokens = tokenizers.twi.tokenize(['Obiara ani gyee na akokoduru no ho .'])\n",
        "text_tokens = tokenizers.twi.lookup(tokens)\n",
        "print(text_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZDwPpvQ3BCX",
        "outputId": "b32d8c55-7b04-4036-ee69-660f93ea711f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.RaggedTensor [[b'[START]', b'obiara', b'ani', b'gyee', b'na', b'akokoduru', b'no',\n",
            "  b'ho', b'.', b'[END]']]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.twi.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTJ7BkPn3CPY",
        "outputId": "bf7b9d7d-d8c7-498e-8538-7db48648c3f1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "obiara ani gyee na akokoduru no ho .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify if tokenizer work on test Twi sentence\n",
        "tokens = tokenizers.eng.tokenize(['Patience is key to happiness'])\n",
        "text_tokens = tokenizers.eng.lookup(tokens)\n",
        "print(text_tokens)"
      ],
      "metadata": {
        "id": "4cmshBy51p-o",
        "outputId": "0fd08c9b-898d-4be1-f673-605cf17583ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.RaggedTensor [[b'[START]', b'patience', b'is', b'key', b'to', b'happiness', b'[END]']]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.eng.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ],
      "metadata": {
        "id": "ENMjitf015rI",
        "outputId": "0917d1bb-c67c-49a5-8fb0-fe5bda7dc153",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "patience is key to happiness\n"
          ]
        }
      ]
    }
  ]
}