{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/transformer__english_twi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qFzQxA29DJp"
      },
      "source": [
        "This notebook is based on the implementation of the paper [Attention is All you Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017). The code are adapted from the Tenssorflow repository at https://www.tensorflow.org/text/tutorials/transformer#build_the_transformer and a big thanks to [[Crater David]](https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:37370749) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hblEgiSfLEck"
      },
      "source": [
        "# Install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naWgZqfLsOiN",
        "outputId": "d981a04e-9e46-4d88-ba9b-fcc18c69bd8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.7/dist-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.7/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2022.6.15)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.0)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2022.9.1)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U tensorflow-text-nightly==2.11.0.dev20220817\n",
        "!pip install -U -q tf-nightly==2.11.0.dev20220817\n",
        "!pip3 install googletrans==3.1.0a0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fofbzpRzPbCO",
        "outputId": "c81144c9-8548-4657-d81d-6633e4837646",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY7seog2LQOL"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gwubBvId4uGK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W18p7g2lLatA"
      },
      "source": [
        "# preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "ovdl-UER8hMX"
      },
      "outputs": [],
      "source": [
        "# This code was adapted from https://github.com/GhanaNLP/kasa/blob/master/Kasa/Preprocessing.py\n",
        "# A subclass of the kasafrench for preprocessing data\n",
        "# import required library\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "class Preprocessing:\n",
        "    # dummy initialization method\n",
        "    def __init__(self):\n",
        "        # initialize with some default parameters here later\n",
        "        pass\n",
        "\n",
        "    # read in parallel twi - english dataset\n",
        "    def read_parallel_dataset(self, filepath_1, filepath_2, filepath_3=None):\n",
        "        if filepath_3 != None:\n",
        "            # read first language data\n",
        "            lang_1 = []\n",
        "            with open(filepath_1, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_1.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            # read second language data\n",
        "            lang_2 = []\n",
        "            with open(filepath_2, encoding='utf-8') as file:\n",
        "\n",
        "                # twi=file.read()\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_2.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "            # Read third Language data\n",
        "            lang_3 = []\n",
        "            with open(filepath_3, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_3.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            return lang_1, lang_2, lang_3\n",
        "            \n",
        "        else:\n",
        "            # read first language data\n",
        "            lang_1 = []\n",
        "            with open(filepath_1, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_1.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            # read second language data\n",
        "            lang_2 = []\n",
        "            with open(filepath_2, encoding='utf-8') as file:\n",
        "\n",
        "                # twi=file.read()\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_2.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            return lang_1, lang_2\n",
        "\n",
        "    # Define a helper function to remove string accents\n",
        "\n",
        "    def removeStringAccent(self, s):\n",
        "        return ''.join(\n",
        "            c for c in unicodedata.normalize('NFD', s)\n",
        "            if unicodedata.category(c) != 'Mn'\n",
        "        )\n",
        "\n",
        "    # normalize input twi sentence\n",
        "    def normalize_twi(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = s.lower()\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.ƆɔɛƐ!?’]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "\n",
        "    # normalize input french sentence\n",
        "    def normalize_FrEn(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = s.lower()\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "    \n",
        "    def writeTotxt(self,destination,data):\n",
        "        with open(destination, 'w') as f:\n",
        "            for line in data:\n",
        "                 f.write(f\"{line}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "B4asafNW9HIt"
      },
      "outputs": [],
      "source": [
        "# import preprocessing class \n",
        "# Create an instance of tft preprocessing class\n",
        "preprocessor = Preprocessing()\n",
        "\n",
        "# Read raw parallel dataset\n",
        "raw_data_twi, raw_data_en = preprocessor.read_parallel_dataset(\n",
        "    filepath_1='/content/drive/MyDrive/verified_twi.txt',\n",
        "    filepath_2='/content/drive/MyDrive/verified_english.txt'\n",
        ")\n",
        "\n",
        "# Normalize the raw data\n",
        "raw_data_en = [preprocessor.normalize_FrEn(data) for data in raw_data_en]\n",
        "raw_data_twi = [preprocessor.normalize_twi(data) for data in raw_data_twi]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au81TkNOLwnB"
      },
      "source": [
        "# split data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5l08UvTL9mtx"
      },
      "outputs": [],
      "source": [
        "# split the dataset into training and test sets\n",
        "# Keep 10% of the data as test set\n",
        "train_twi,test_twi,train_en,test_en = train_test_split(raw_data_twi,raw_data_en, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3q4djANP-9g6"
      },
      "outputs": [],
      "source": [
        "# write the preprocess traning and test dataset to a file\n",
        "preprocessor.writeTotxt('train_twi.txt',train_twi)\n",
        "preprocessor.writeTotxt('train_en.txt',train_en)\n",
        "preprocessor.writeTotxt('test_twi.txt',test_twi)\n",
        "preprocessor.writeTotxt('test_en.txt',test_en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao2l87AzL_kE"
      },
      "source": [
        "# Build tf dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "0WZrSEP9_ISJ"
      },
      "outputs": [],
      "source": [
        "# build tf datasets from traning and test sentences in both languages\n",
        "train_dataset_en = tf.data.TextLineDataset('/content/train_en.txt')\n",
        "train_dataset_tw = tf.data.TextLineDataset('/content/train_twi.txt')\n",
        "\n",
        "val_dataset_en = tf.data.TextLineDataset('/content/test_en.txt')\n",
        "val_dataset_tw = tf.data.TextLineDataset('/content/test_twi.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "mmksbF-CAEXg"
      },
      "outputs": [],
      "source": [
        "# combine languages into single dataset\n",
        "trained_combined = tf.data.Dataset.zip((train_dataset_en, train_dataset_tw))\n",
        "val_combined = tf.data.Dataset.zip((val_dataset_en, val_dataset_tw))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZJ-hiEUBLDu",
        "outputId": "f39707a0-e9ce-4660-cf99-b6c42f40a7ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English:  the first american colonists arrived in the th century .\n",
            "Twi:  amerika atubrafo a wodi kan no duu hɔ wɔ afeha a ɛto so no mu .\n",
            "English:  you need to wash your hands .\n",
            "Twi:  ɛsɛ sɛ wohohoro wo nsa .\n",
            "English:  appiah horrow is payday .\n",
            "Twi:  da a wɔde tua ka no yɛ da koro akatua .\n",
            "English:  i can see why you like asamoah .\n",
            "Twi:  mitumi hu nea enti a w ani gye asamoa ho\n",
            "English:  he had jeans on .\n",
            "Twi:  ɔhyɛ jeans attade\n"
          ]
        }
      ],
      "source": [
        "# verify trained_combined dataset is correct\n",
        "for en,tw in trained_combined.take(5):\n",
        "  print(\"English: \", en.numpy().decode('utf-8'))\n",
        "  print(\"Twi: \", tw.numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaxFg3TZMIK8"
      },
      "source": [
        "# load tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjb-3AU1BnrK"
      },
      "source": [
        "\n",
        "\n",
        "1. The tokenizer is which was build from the parallel corpus.\n",
        "2. The code are available on the link https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/build_subword_tokenizer.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "aR_vgyQgBo_T"
      },
      "outputs": [],
      "source": [
        "model_named = '/content/drive/MyDrive/translate_frengtwi_converter'\n",
        "# Verify tokenizer model can be reloaded\n",
        "tokenizers = tf.saved_model.load(model_named)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdHMl3icEjos",
        "outputId": "7057c0ce-75ad-42b4-8573-0d4703dcf931"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'i', b'love', b'student', b'life', b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "# Verify tokenizer works on french sentence\n",
        "tokens = tokenizers.eng.tokenize(['I LOVE STUDENT LIFE'])\n",
        "text_tokens = tokenizers.eng.lookup(tokens)\n",
        "text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSOMXh_de04Q",
        "outputId": "613cc2f9-d27f-45f1-97a4-ce09155f1d20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i love student life\n"
          ]
        }
      ],
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.eng.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBNqSqeSfChq",
        "outputId": "c94afb15-19d7-456d-df3b-237b02958397"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'na', b'mmarimaa', b'ne', b'mmeawa', b'pii', b'w\\xc9\\x94',\n",
              "  b'agu', b'##di', b'##bea', b'h\\xc9\\x94', b'.', b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "# Verify tokenizer works on twi sentence\n",
        "tokens = tokenizers.twi.tokenize(['Na mmarimaa ne mmeawa pii wɔ agudibea hɔ .'])\n",
        "text_tokens = tokenizers.twi.lookup(tokens)\n",
        "text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lS1xMYQfQR9",
        "outputId": "f471209e-00b3-4cd2-d00f-9f35487495b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "na mmarimaa ne mmeawa pii wɔ agudibea hɔ .\n"
          ]
        }
      ],
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.twi.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQVo6G3OMfac"
      },
      "source": [
        "# Check sentence with maximum length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvoh1vRgCxSb"
      },
      "source": [
        "By checking the maximum length of sentence, you can suitably select the MAX_TOKENS. The MAX_TOKKENS help drops examples longer than the maximum number of tokens (MAX_TOKENS). Without limiting the size of sequences, the performance may be negatively affected.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VJxJuNufTCB",
        "outputId": "ab2f5ec2-f3e1-4519-907f-2f3455adc4a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".........................................................................................................................................................................................................................................................................................................................................................................................................................................................................."
          ]
        }
      ],
      "source": [
        "#The distribution of tokens per example in the dataset is as follows:\n",
        "\n",
        "lengths = []\n",
        "\n",
        "for twi_examples,en_examples in trained_combined.batch(50):\n",
        "  twi_tokens = tokenizers.twi.tokenize(twi_examples)\n",
        "  lengths.append(twi_tokens.row_lengths())\n",
        "\n",
        "  en_tokens = tokenizers.eng.tokenize(en_examples)\n",
        "  lengths.append(en_tokens.row_lengths())\n",
        "  print('.', end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "ri-qmhyyiBSa",
        "outputId": "d1dbfa85-8570-464a-8412-e3e9d3dd50f8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "all_lengths = np.concatenate(lengths)\n",
        "\n",
        "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
        "plt.ylim(plt.ylim())\n",
        "max_length = max(all_lengths)\n",
        "plt.plot([max_length, max_length], plt.ylim())\n",
        "plt.title(f'Maximum tokens per example: {max_length}');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DYwhdWAMpnW"
      },
      "source": [
        "# Tokenize the training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "GC7UlwEeiNTO"
      },
      "outputs": [],
      "source": [
        "MAX_TOKENS = 50\n",
        "\n",
        "def filter_max_tokens(l1, l2):\n",
        "  num_tokens = tf.maximum(tf.shape(l1)[1],tf.shape(l2)[1])\n",
        "  return num_tokens < MAX_TOKENS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Routine to tokenize sentence pairs; needed to make sentence batches for training\n",
        "def tokenize_pairs(en,twi):\n",
        "  en = tokenizers.eng.tokenize(en)\n",
        "  # Convert from ragged to dense, padding with zeros.\n",
        "  en = en.to_tensor()\n",
        "  tw = tokenizers.twi.tokenize(twi)\n",
        "  # Convert from ragged to dense, padding with zeros.\n",
        "  tw = tw.to_tensor()\n",
        "  return en, tw"
      ],
      "metadata": {
        "id": "AORJvDkcOhxC"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "EWMySNLzklc_"
      },
      "outputs": [],
      "source": [
        "# create training batches\n",
        "BUFFER_SIZE = len(train_twi)\n",
        "BATCH_SIZE = 64\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .filter(filter_max_tokens)\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "# Create training and validation set batches.\n",
        "train_batches = make_batches(trained_combined)\n",
        "val_batches = make_batches(val_combined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2MhJY21RqcJ"
      },
      "source": [
        "Write the test batch to file. This will come in handy later if you prefer to translate from txt file and estimate the translator [BLEU](https://aclanthology.org/P02-1040.pdf) score. The translator will run to error for any for any sentence with the shape greater than the MAX_TOKENS used for training. It adivisable to use the trimmed sentences for testting to avoid such occurrance\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "2V_Rh8fFSvPV"
      },
      "outputs": [],
      "source": [
        "en_test= []\n",
        "twi_test = []\n",
        "\n",
        "for en_batches,twi_batches in val_batches:\n",
        "    for en in tokenizers.eng.detokenize(en_batches):\n",
        "      en_test.append(en.numpy().decode(\"utf-8\"))\n",
        "    for tw in tokenizers.twi.detokenize(twi_batches):\n",
        "      twi_test.append(tw.numpy().decode(\"utf-8\"))\n",
        "\n",
        "\n",
        "preprocessor.writeTotxt('/content/drive/MyDrive/twi_testing_set.txt',test_twi)\n",
        "preprocessor.writeTotxt('/content/drive/MyDrive/english_testing_set.txt',en_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNFMseSdM3wr"
      },
      "source": [
        "# Positional encoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5du74jWFF5WK"
      },
      "source": [
        "Positional encodings are added to the embeddings to give the model some information about the relative position of the tokens in the sentence so the model can learn to recognize the word order.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "KXtWVxfeb6px"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "  \n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # Apply the sine function to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # Apply the cosine function to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXL_HTJWG44o"
      },
      "source": [
        "# Create a point-wise feed-forward network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "db4kXyfGb_l5"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(\n",
        "  d_model, # Input/output dimensionality.\n",
        "  dff # Inner-layer dimensionality.\n",
        "  ):\n",
        "\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # Shape `(batch_size, seq_len, dff)`.\n",
        "      tf.keras.layers.Dense(d_model)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "  ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSEO7HUZHppR"
      },
      "source": [
        "# Build Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtydO3sOIRE-"
      },
      "source": [
        "### Multihead Attention layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "BSHsspXXIaUR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create mask for padding tokens so translator ignores them\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "\n",
        "# Mask future tokens so translator cannot see future\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "\n",
        "# Create final masks\n",
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead)\n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "      q: query shape == (..., seq_len_q, depth)\n",
        "      k: key shape == (..., seq_len_k, depth)\n",
        "      v: value shape == (..., seq_len_v, depth_v)\n",
        "      mask: Float tensor with shape broadcastable\n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "      output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    # (..., seq_len_q, seq_len_k)\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(\n",
        "        scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# Add attention layers to create multi-head attention\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, *, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # (batch_size, num_heads, seq_len_q, depth)\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        # (batch_size, num_heads, seq_len_k, depth)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        # (batch_size, num_heads, seq_len_v, depth)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        # (batch_size, seq_len_q, num_heads, depth)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        # (batch_size, seq_len_q, d_model)\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "        # Build Transformer encoder layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvNLzFWpJw6m"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "bCdZS_nJcGVW"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "EBZNB3MacRCF"
      },
      "outputs": [],
      "source": [
        "#Build Transformer encoder from encoder layer\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "              maximum_position_encoding=MAX_TOKENS,rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndsnLCOwRgcY"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "fnXVTpI_cgzR"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "UEuoBDtJcp7Q"
      },
      "outputs": [],
      "source": [
        "#Build Transformer decoder from decoder layer\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "              maximum_position_encoding,rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size,d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjFbGSvJUQY6"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "_6sd1ofVcyl4"
      },
      "outputs": [],
      "source": [
        "# Build full Transformer\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_vocab_size, pe_input=MAX_TOKENS, pe_target=MAX_TOKENS, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.encoder= Encoder(num_layers, d_model, num_heads, dff,input_vocab_size, pe_input, rate)\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff,target_vocab_size, pe_target, rate)\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "  \n",
        "  def call(self, inp, tar, training, enc_padding_mask,look_ahead_mask, dec_padding_mask):\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask) # (batch_size, inp_seq_len, d_model)\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    final_output = self.final_layer(dec_output) # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    return final_output, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPhqoBhwWYZ9"
      },
      "source": [
        "# Optimizer and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "y79jHzBnA7wG"
      },
      "outputs": [],
      "source": [
        "# Set learning rate schedule\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g11C2K3tXFyP"
      },
      "source": [
        "# Set hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "-q51YVNusr5_"
      },
      "outputs": [],
      "source": [
        "num_layers = 6\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fGafl4DXYct"
      },
      "source": [
        "# Instantiate a Transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "9DJxw765BYHi"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=tokenizers.eng.get_vocab_size().numpy(),\n",
        "    target_vocab_size=tokenizers.twi.get_vocab_size().numpy(),\n",
        "    rate=dropout_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aGTknsnYDCC"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "BF0wsHaaJVKC"
      },
      "outputs": [],
      "source": [
        "# Instantiate learning rate and set optimizer\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "HpLaySRfSy89"
      },
      "outputs": [],
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "-PHWy8sdGoR8"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = './checkpoints/train'\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# If a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "VAj0P_T0ZcBV"
      },
      "outputs": [],
      "source": [
        "# Choose number of training epochs\n",
        "EPOCHS = 120\n",
        "train_step_signature = [\n",
        "  tf.TensorSpec(shape=(None, None), dtype=tf.int64), \n",
        "  tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp,True,enc_padding_mask,combined_mask,dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7k6GwwEIi_A",
        "outputId": "ce016aa2-9002-449f-f5e6-63e7a98aa775"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 7.7623 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 7.0496 Accuracy 0.0681\n",
            "Epoch 1 Batch 100 Loss 6.6273 Accuracy 0.0858\n",
            "Epoch 1 Batch 150 Loss 6.3619 Accuracy 0.1017\n",
            "Epoch 1 Batch 200 Loss 6.1123 Accuracy 0.1288\n",
            "Epoch 1 Batch 250 Loss 5.9257 Accuracy 0.1486\n",
            "Epoch 1 Batch 300 Loss 5.7860 Accuracy 0.1631\n",
            "Epoch 1 Batch 350 Loss 5.6695 Accuracy 0.1747\n",
            "Epoch 1 Loss 5.6568 Accuracy 0.1760\n",
            "Time taken for 1 epoch: 83.58 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 4.8979 Accuracy 0.2524\n",
            "Epoch 2 Batch 50 Loss 4.8272 Accuracy 0.2566\n",
            "Epoch 2 Batch 100 Loss 4.8131 Accuracy 0.2577\n",
            "Epoch 2 Batch 150 Loss 4.7570 Accuracy 0.2643\n",
            "Epoch 2 Batch 200 Loss 4.7156 Accuracy 0.2679\n",
            "Epoch 2 Batch 250 Loss 4.6602 Accuracy 0.2736\n",
            "Epoch 2 Batch 300 Loss 4.6147 Accuracy 0.2783\n",
            "Epoch 2 Batch 350 Loss 4.5760 Accuracy 0.2825\n",
            "Epoch 2 Loss 4.5717 Accuracy 0.2829\n",
            "Time taken for 1 epoch: 63.65 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4.0894 Accuracy 0.3238\n",
            "Epoch 3 Batch 50 Loss 4.1766 Accuracy 0.3226\n",
            "Epoch 3 Batch 100 Loss 4.1318 Accuracy 0.3269\n",
            "Epoch 3 Batch 150 Loss 4.0960 Accuracy 0.3307\n",
            "Epoch 3 Batch 200 Loss 4.0601 Accuracy 0.3343\n",
            "Epoch 3 Batch 250 Loss 4.0304 Accuracy 0.3369\n",
            "Epoch 3 Batch 300 Loss 3.9944 Accuracy 0.3403\n",
            "Epoch 3 Batch 350 Loss 3.9670 Accuracy 0.3426\n",
            "Epoch 3 Loss 3.9636 Accuracy 0.3428\n",
            "Time taken for 1 epoch: 63.69 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 3.8302 Accuracy 0.3460\n",
            "Epoch 4 Batch 50 Loss 3.6398 Accuracy 0.3707\n",
            "Epoch 4 Batch 100 Loss 3.6145 Accuracy 0.3728\n",
            "Epoch 4 Batch 150 Loss 3.6012 Accuracy 0.3739\n",
            "Epoch 4 Batch 200 Loss 3.5725 Accuracy 0.3767\n",
            "Epoch 4 Batch 250 Loss 3.5565 Accuracy 0.3781\n",
            "Epoch 4 Batch 300 Loss 3.5443 Accuracy 0.3789\n",
            "Epoch 4 Batch 350 Loss 3.5305 Accuracy 0.3802\n",
            "Epoch 4 Loss 3.5290 Accuracy 0.3804\n",
            "Time taken for 1 epoch: 63.88 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 3.5124 Accuracy 0.3664\n",
            "Epoch 5 Batch 50 Loss 3.2497 Accuracy 0.4082\n",
            "Epoch 5 Batch 100 Loss 3.2366 Accuracy 0.4080\n",
            "Epoch 5 Batch 150 Loss 3.2239 Accuracy 0.4094\n",
            "Epoch 5 Batch 200 Loss 3.2154 Accuracy 0.4102\n",
            "Epoch 5 Batch 250 Loss 3.2115 Accuracy 0.4097\n",
            "Epoch 5 Batch 300 Loss 3.2006 Accuracy 0.4107\n",
            "Epoch 5 Batch 350 Loss 3.1942 Accuracy 0.4115\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
            "Epoch 5 Loss 3.1943 Accuracy 0.4114\n",
            "Time taken for 1 epoch: 67.77 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 3.0656 Accuracy 0.4194\n",
            "Epoch 6 Batch 50 Loss 2.9110 Accuracy 0.4423\n",
            "Epoch 6 Batch 100 Loss 2.9267 Accuracy 0.4391\n",
            "Epoch 6 Batch 150 Loss 2.9269 Accuracy 0.4399\n",
            "Epoch 6 Batch 200 Loss 2.9309 Accuracy 0.4389\n",
            "Epoch 6 Batch 250 Loss 2.9333 Accuracy 0.4383\n",
            "Epoch 6 Batch 300 Loss 2.9291 Accuracy 0.4387\n",
            "Epoch 6 Batch 350 Loss 2.9234 Accuracy 0.4395\n",
            "Epoch 6 Loss 2.9234 Accuracy 0.4393\n",
            "Time taken for 1 epoch: 63.34 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.7391 Accuracy 0.4463\n",
            "Epoch 7 Batch 50 Loss 2.6940 Accuracy 0.4636\n",
            "Epoch 7 Batch 100 Loss 2.6896 Accuracy 0.4639\n",
            "Epoch 7 Batch 150 Loss 2.6957 Accuracy 0.4638\n",
            "Epoch 7 Batch 200 Loss 2.7046 Accuracy 0.4623\n",
            "Epoch 7 Batch 250 Loss 2.7071 Accuracy 0.4618\n",
            "Epoch 7 Batch 300 Loss 2.7066 Accuracy 0.4618\n",
            "Epoch 7 Batch 350 Loss 2.7104 Accuracy 0.4608\n",
            "Epoch 7 Loss 2.7110 Accuracy 0.4607\n",
            "Time taken for 1 epoch: 63.97 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.4677 Accuracy 0.5094\n",
            "Epoch 8 Batch 50 Loss 2.4428 Accuracy 0.4918\n",
            "Epoch 8 Batch 100 Loss 2.4694 Accuracy 0.4895\n",
            "Epoch 8 Batch 150 Loss 2.5004 Accuracy 0.4840\n",
            "Epoch 8 Batch 200 Loss 2.5117 Accuracy 0.4833\n",
            "Epoch 8 Batch 250 Loss 2.5268 Accuracy 0.4809\n",
            "Epoch 8 Batch 300 Loss 2.5346 Accuracy 0.4799\n",
            "Epoch 8 Batch 350 Loss 2.5427 Accuracy 0.4792\n",
            "Epoch 8 Loss 2.5441 Accuracy 0.4789\n",
            "Time taken for 1 epoch: 62.92 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.3836 Accuracy 0.4940\n",
            "Epoch 9 Batch 50 Loss 2.3456 Accuracy 0.5042\n",
            "Epoch 9 Batch 100 Loss 2.3593 Accuracy 0.5021\n",
            "Epoch 9 Batch 150 Loss 2.3805 Accuracy 0.4992\n",
            "Epoch 9 Batch 200 Loss 2.3845 Accuracy 0.4988\n",
            "Epoch 9 Batch 250 Loss 2.3994 Accuracy 0.4968\n",
            "Epoch 9 Batch 300 Loss 2.4087 Accuracy 0.4956\n",
            "Epoch 9 Batch 350 Loss 2.4164 Accuracy 0.4939\n",
            "Epoch 9 Loss 2.4195 Accuracy 0.4936\n",
            "Time taken for 1 epoch: 63.36 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 2.0197 Accuracy 0.5498\n",
            "Epoch 10 Batch 50 Loss 2.2149 Accuracy 0.5153\n",
            "Epoch 10 Batch 100 Loss 2.2353 Accuracy 0.5142\n",
            "Epoch 10 Batch 150 Loss 2.2609 Accuracy 0.5118\n",
            "Epoch 10 Batch 200 Loss 2.2834 Accuracy 0.5098\n",
            "Epoch 10 Batch 250 Loss 2.3051 Accuracy 0.5065\n",
            "Epoch 10 Batch 300 Loss 2.3323 Accuracy 0.5022\n",
            "Epoch 10 Batch 350 Loss 2.3476 Accuracy 0.5002\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
            "Epoch 10 Loss 2.3494 Accuracy 0.4999\n",
            "Time taken for 1 epoch: 66.76 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 2.2467 Accuracy 0.5258\n",
            "Epoch 11 Batch 50 Loss 2.1695 Accuracy 0.5233\n",
            "Epoch 11 Batch 100 Loss 2.1958 Accuracy 0.5186\n",
            "Epoch 11 Batch 150 Loss 2.2084 Accuracy 0.5157\n",
            "Epoch 11 Batch 200 Loss 2.2362 Accuracy 0.5117\n",
            "Epoch 11 Batch 250 Loss 2.2599 Accuracy 0.5079\n",
            "Epoch 11 Batch 300 Loss 2.2801 Accuracy 0.5057\n",
            "Epoch 11 Batch 350 Loss 2.3025 Accuracy 0.5025\n",
            "Epoch 11 Loss 2.3051 Accuracy 0.5021\n",
            "Time taken for 1 epoch: 63.21 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 2.1260 Accuracy 0.5069\n",
            "Epoch 12 Batch 50 Loss 2.1246 Accuracy 0.5280\n",
            "Epoch 12 Batch 100 Loss 2.1851 Accuracy 0.5188\n",
            "Epoch 12 Batch 150 Loss 2.2077 Accuracy 0.5149\n",
            "Epoch 12 Batch 200 Loss 2.2231 Accuracy 0.5124\n",
            "Epoch 12 Batch 250 Loss 2.2364 Accuracy 0.5103\n",
            "Epoch 12 Batch 300 Loss 2.2528 Accuracy 0.5086\n",
            "Epoch 12 Batch 350 Loss 2.2688 Accuracy 0.5060\n",
            "Epoch 12 Loss 2.2690 Accuracy 0.5060\n",
            "Time taken for 1 epoch: 63.55 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.9330 Accuracy 0.5717\n",
            "Epoch 13 Batch 50 Loss 2.0571 Accuracy 0.5357\n",
            "Epoch 13 Batch 100 Loss 2.0672 Accuracy 0.5349\n",
            "Epoch 13 Batch 150 Loss 2.0995 Accuracy 0.5296\n",
            "Epoch 13 Batch 200 Loss 2.1157 Accuracy 0.5271\n",
            "Epoch 13 Batch 250 Loss 2.1267 Accuracy 0.5262\n",
            "Epoch 13 Batch 300 Loss 2.1416 Accuracy 0.5246\n",
            "Epoch 13 Batch 350 Loss 2.1622 Accuracy 0.5213\n",
            "Epoch 13 Loss 2.1630 Accuracy 0.5214\n",
            "Time taken for 1 epoch: 63.47 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.9445 Accuracy 0.5537\n",
            "Epoch 14 Batch 50 Loss 1.9333 Accuracy 0.5554\n",
            "Epoch 14 Batch 100 Loss 1.9687 Accuracy 0.5496\n",
            "Epoch 14 Batch 150 Loss 1.9833 Accuracy 0.5486\n",
            "Epoch 14 Batch 200 Loss 2.0101 Accuracy 0.5443\n",
            "Epoch 14 Batch 250 Loss 2.0234 Accuracy 0.5418\n",
            "Epoch 14 Batch 300 Loss 2.0327 Accuracy 0.5401\n",
            "Epoch 14 Batch 350 Loss 2.0461 Accuracy 0.5383\n",
            "Epoch 14 Loss 2.0461 Accuracy 0.5382\n",
            "Time taken for 1 epoch: 62.73 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.9168 Accuracy 0.5574\n",
            "Epoch 15 Batch 50 Loss 1.8360 Accuracy 0.5702\n",
            "Epoch 15 Batch 100 Loss 1.8687 Accuracy 0.5650\n",
            "Epoch 15 Batch 150 Loss 1.8801 Accuracy 0.5630\n",
            "Epoch 15 Batch 200 Loss 1.8987 Accuracy 0.5606\n",
            "Epoch 15 Batch 250 Loss 1.9173 Accuracy 0.5580\n",
            "Epoch 15 Batch 300 Loss 1.9280 Accuracy 0.5570\n",
            "Epoch 15 Batch 350 Loss 1.9392 Accuracy 0.5551\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
            "Epoch 15 Loss 1.9385 Accuracy 0.5551\n",
            "Time taken for 1 epoch: 66.28 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.5412 Accuracy 0.6403\n",
            "Epoch 16 Batch 50 Loss 1.6840 Accuracy 0.5968\n",
            "Epoch 16 Batch 100 Loss 1.7179 Accuracy 0.5917\n",
            "Epoch 16 Batch 150 Loss 1.7498 Accuracy 0.5851\n",
            "Epoch 16 Batch 200 Loss 1.7687 Accuracy 0.5821\n",
            "Epoch 16 Batch 250 Loss 1.7801 Accuracy 0.5807\n",
            "Epoch 16 Batch 300 Loss 1.7979 Accuracy 0.5776\n",
            "Epoch 16 Batch 350 Loss 1.8152 Accuracy 0.5749\n",
            "Epoch 16 Loss 1.8156 Accuracy 0.5748\n",
            "Time taken for 1 epoch: 63.01 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.7418 Accuracy 0.5988\n",
            "Epoch 17 Batch 50 Loss 1.6351 Accuracy 0.6059\n",
            "Epoch 17 Batch 100 Loss 1.6533 Accuracy 0.6015\n",
            "Epoch 17 Batch 150 Loss 1.6735 Accuracy 0.5969\n",
            "Epoch 17 Batch 200 Loss 1.6770 Accuracy 0.5961\n",
            "Epoch 17 Batch 250 Loss 1.6870 Accuracy 0.5944\n",
            "Epoch 17 Batch 300 Loss 1.7100 Accuracy 0.5904\n",
            "Epoch 17 Batch 350 Loss 1.7249 Accuracy 0.5880\n",
            "Epoch 17 Loss 1.7268 Accuracy 0.5876\n",
            "Time taken for 1 epoch: 63.60 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.4290 Accuracy 0.6220\n",
            "Epoch 18 Batch 50 Loss 1.5008 Accuracy 0.6281\n",
            "Epoch 18 Batch 100 Loss 1.5082 Accuracy 0.6264\n",
            "Epoch 18 Batch 150 Loss 1.5357 Accuracy 0.6222\n",
            "Epoch 18 Batch 200 Loss 1.5554 Accuracy 0.6191\n",
            "Epoch 18 Batch 250 Loss 1.5821 Accuracy 0.6139\n",
            "Epoch 18 Batch 300 Loss 1.6009 Accuracy 0.6103\n",
            "Epoch 18 Batch 350 Loss 1.6101 Accuracy 0.6089\n",
            "Epoch 18 Loss 1.6126 Accuracy 0.6086\n",
            "Time taken for 1 epoch: 63.09 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.4624 Accuracy 0.6321\n",
            "Epoch 19 Batch 50 Loss 1.4333 Accuracy 0.6400\n",
            "Epoch 19 Batch 100 Loss 1.4535 Accuracy 0.6373\n",
            "Epoch 19 Batch 150 Loss 1.4706 Accuracy 0.6336\n",
            "Epoch 19 Batch 200 Loss 1.4899 Accuracy 0.6298\n",
            "Epoch 19 Batch 250 Loss 1.5038 Accuracy 0.6268\n",
            "Epoch 19 Batch 300 Loss 1.5150 Accuracy 0.6242\n",
            "Epoch 19 Batch 350 Loss 1.5268 Accuracy 0.6217\n",
            "Epoch 19 Loss 1.5272 Accuracy 0.6217\n",
            "Time taken for 1 epoch: 63.11 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.3352 Accuracy 0.6643\n",
            "Epoch 20 Batch 50 Loss 1.3045 Accuracy 0.6668\n",
            "Epoch 20 Batch 100 Loss 1.3423 Accuracy 0.6591\n",
            "Epoch 20 Batch 150 Loss 1.3707 Accuracy 0.6524\n",
            "Epoch 20 Batch 200 Loss 1.3766 Accuracy 0.6516\n",
            "Epoch 20 Batch 250 Loss 1.3995 Accuracy 0.6465\n",
            "Epoch 20 Batch 300 Loss 1.4188 Accuracy 0.6423\n",
            "Epoch 20 Batch 350 Loss 1.4320 Accuracy 0.6393\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
            "Epoch 20 Loss 1.4335 Accuracy 0.6390\n",
            "Time taken for 1 epoch: 66.12 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.2918 Accuracy 0.6744\n",
            "Epoch 21 Batch 50 Loss 1.2647 Accuracy 0.6721\n",
            "Epoch 21 Batch 100 Loss 1.2748 Accuracy 0.6695\n",
            "Epoch 21 Batch 150 Loss 1.2950 Accuracy 0.6650\n",
            "Epoch 21 Batch 200 Loss 1.3110 Accuracy 0.6620\n",
            "Epoch 21 Batch 250 Loss 1.3277 Accuracy 0.6588\n",
            "Epoch 21 Batch 300 Loss 1.3409 Accuracy 0.6568\n",
            "Epoch 21 Batch 350 Loss 1.3550 Accuracy 0.6543\n",
            "Epoch 21 Loss 1.3577 Accuracy 0.6538\n",
            "Time taken for 1 epoch: 63.59 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 1.2304 Accuracy 0.6889\n",
            "Epoch 22 Batch 50 Loss 1.1730 Accuracy 0.6931\n",
            "Epoch 22 Batch 100 Loss 1.1967 Accuracy 0.6871\n",
            "Epoch 22 Batch 150 Loss 1.2251 Accuracy 0.6800\n",
            "Epoch 22 Batch 200 Loss 1.2352 Accuracy 0.6774\n",
            "Epoch 22 Batch 250 Loss 1.2504 Accuracy 0.6743\n",
            "Epoch 22 Batch 300 Loss 1.2644 Accuracy 0.6715\n",
            "Epoch 22 Batch 350 Loss 1.2764 Accuracy 0.6693\n",
            "Epoch 22 Loss 1.2760 Accuracy 0.6694\n",
            "Time taken for 1 epoch: 63.63 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 1.0751 Accuracy 0.7192\n",
            "Epoch 23 Batch 50 Loss 1.1103 Accuracy 0.7030\n",
            "Epoch 23 Batch 100 Loss 1.1029 Accuracy 0.7049\n",
            "Epoch 23 Batch 150 Loss 1.1257 Accuracy 0.7006\n",
            "Epoch 23 Batch 200 Loss 1.1488 Accuracy 0.6953\n",
            "Epoch 23 Batch 250 Loss 1.1651 Accuracy 0.6919\n",
            "Epoch 23 Batch 300 Loss 1.1733 Accuracy 0.6896\n",
            "Epoch 23 Batch 350 Loss 1.1874 Accuracy 0.6870\n",
            "Epoch 23 Loss 1.1901 Accuracy 0.6865\n",
            "Time taken for 1 epoch: 63.15 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 1.1220 Accuracy 0.7044\n",
            "Epoch 24 Batch 50 Loss 1.0322 Accuracy 0.7197\n",
            "Epoch 24 Batch 100 Loss 1.0450 Accuracy 0.7178\n",
            "Epoch 24 Batch 150 Loss 1.0762 Accuracy 0.7099\n",
            "Epoch 24 Batch 200 Loss 1.0909 Accuracy 0.7065\n",
            "Epoch 24 Batch 250 Loss 1.1110 Accuracy 0.7020\n",
            "Epoch 24 Batch 300 Loss 1.1272 Accuracy 0.6982\n",
            "Epoch 24 Batch 350 Loss 1.1360 Accuracy 0.6962\n",
            "Epoch 24 Loss 1.1361 Accuracy 0.6961\n",
            "Time taken for 1 epoch: 63.30 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.9453 Accuracy 0.7376\n",
            "Epoch 25 Batch 50 Loss 0.9457 Accuracy 0.7383\n",
            "Epoch 25 Batch 100 Loss 0.9783 Accuracy 0.7319\n",
            "Epoch 25 Batch 150 Loss 1.0145 Accuracy 0.7245\n",
            "Epoch 25 Batch 200 Loss 1.0356 Accuracy 0.7190\n",
            "Epoch 25 Batch 250 Loss 1.0497 Accuracy 0.7155\n",
            "Epoch 25 Batch 300 Loss 1.0622 Accuracy 0.7122\n",
            "Epoch 25 Batch 350 Loss 1.0753 Accuracy 0.7094\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-5\n",
            "Epoch 25 Loss 1.0761 Accuracy 0.7092\n",
            "Time taken for 1 epoch: 66.19 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.9460 Accuracy 0.7584\n",
            "Epoch 26 Batch 50 Loss 0.9001 Accuracy 0.7523\n",
            "Epoch 26 Batch 100 Loss 0.9090 Accuracy 0.7492\n",
            "Epoch 26 Batch 150 Loss 0.9362 Accuracy 0.7417\n",
            "Epoch 26 Batch 200 Loss 0.9597 Accuracy 0.7352\n",
            "Epoch 26 Batch 250 Loss 0.9764 Accuracy 0.7318\n",
            "Epoch 26 Batch 300 Loss 0.9930 Accuracy 0.7284\n",
            "Epoch 26 Batch 350 Loss 1.0085 Accuracy 0.7247\n",
            "Epoch 26 Loss 1.0102 Accuracy 0.7244\n",
            "Time taken for 1 epoch: 63.06 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.7776 Accuracy 0.7827\n",
            "Epoch 27 Batch 50 Loss 0.8754 Accuracy 0.7551\n",
            "Epoch 27 Batch 100 Loss 0.8660 Accuracy 0.7582\n",
            "Epoch 27 Batch 150 Loss 0.8891 Accuracy 0.7522\n",
            "Epoch 27 Batch 200 Loss 0.9122 Accuracy 0.7458\n",
            "Epoch 27 Batch 250 Loss 0.9260 Accuracy 0.7428\n",
            "Epoch 27 Batch 300 Loss 0.9460 Accuracy 0.7378\n",
            "Epoch 27 Batch 350 Loss 0.9613 Accuracy 0.7345\n",
            "Epoch 27 Loss 0.9619 Accuracy 0.7344\n",
            "Time taken for 1 epoch: 63.38 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.8883 Accuracy 0.7492\n",
            "Epoch 28 Batch 50 Loss 0.8115 Accuracy 0.7708\n",
            "Epoch 28 Batch 100 Loss 0.8251 Accuracy 0.7684\n",
            "Epoch 28 Batch 150 Loss 0.8463 Accuracy 0.7628\n",
            "Epoch 28 Batch 200 Loss 0.8633 Accuracy 0.7586\n",
            "Epoch 28 Batch 250 Loss 0.8785 Accuracy 0.7546\n",
            "Epoch 28 Batch 300 Loss 0.8922 Accuracy 0.7508\n",
            "Epoch 28 Batch 350 Loss 0.9065 Accuracy 0.7473\n",
            "Epoch 28 Loss 0.9084 Accuracy 0.7469\n",
            "Time taken for 1 epoch: 62.92 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.7881 Accuracy 0.7817\n",
            "Epoch 29 Batch 50 Loss 0.7551 Accuracy 0.7842\n",
            "Epoch 29 Batch 100 Loss 0.7696 Accuracy 0.7796\n",
            "Epoch 29 Batch 150 Loss 0.7930 Accuracy 0.7728\n",
            "Epoch 29 Batch 200 Loss 0.8084 Accuracy 0.7694\n",
            "Epoch 29 Batch 250 Loss 0.8291 Accuracy 0.7646\n",
            "Epoch 29 Batch 300 Loss 0.8453 Accuracy 0.7601\n",
            "Epoch 29 Batch 350 Loss 0.8565 Accuracy 0.7570\n",
            "Epoch 29 Loss 0.8571 Accuracy 0.7568\n",
            "Time taken for 1 epoch: 62.71 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.7024 Accuracy 0.8118\n",
            "Epoch 30 Batch 50 Loss 0.7426 Accuracy 0.7918\n",
            "Epoch 30 Batch 100 Loss 0.7472 Accuracy 0.7871\n",
            "Epoch 30 Batch 150 Loss 0.7624 Accuracy 0.7825\n",
            "Epoch 30 Batch 200 Loss 0.7790 Accuracy 0.7784\n",
            "Epoch 30 Batch 250 Loss 0.7938 Accuracy 0.7742\n",
            "Epoch 30 Batch 300 Loss 0.8018 Accuracy 0.7717\n",
            "Epoch 30 Batch 350 Loss 0.8141 Accuracy 0.7688\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-6\n",
            "Epoch 30 Loss 0.8152 Accuracy 0.7684\n",
            "Time taken for 1 epoch: 66.08 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.6517 Accuracy 0.8111\n",
            "Epoch 31 Batch 50 Loss 0.6663 Accuracy 0.8061\n",
            "Epoch 31 Batch 100 Loss 0.6907 Accuracy 0.8002\n",
            "Epoch 31 Batch 150 Loss 0.7145 Accuracy 0.7944\n",
            "Epoch 31 Batch 200 Loss 0.7257 Accuracy 0.7917\n",
            "Epoch 31 Batch 250 Loss 0.7400 Accuracy 0.7874\n",
            "Epoch 31 Batch 300 Loss 0.7559 Accuracy 0.7825\n",
            "Epoch 31 Batch 350 Loss 0.7685 Accuracy 0.7794\n",
            "Epoch 31 Loss 0.7696 Accuracy 0.7792\n",
            "Time taken for 1 epoch: 63.17 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.7229 Accuracy 0.7972\n",
            "Epoch 32 Batch 50 Loss 0.6584 Accuracy 0.8082\n",
            "Epoch 32 Batch 100 Loss 0.6586 Accuracy 0.8067\n",
            "Epoch 32 Batch 150 Loss 0.6822 Accuracy 0.8010\n",
            "Epoch 32 Batch 200 Loss 0.7011 Accuracy 0.7965\n",
            "Epoch 32 Batch 250 Loss 0.7118 Accuracy 0.7929\n",
            "Epoch 32 Batch 300 Loss 0.7215 Accuracy 0.7898\n",
            "Epoch 32 Batch 350 Loss 0.7329 Accuracy 0.7870\n",
            "Epoch 32 Loss 0.7348 Accuracy 0.7864\n",
            "Time taken for 1 epoch: 63.27 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.6815 Accuracy 0.8066\n",
            "Epoch 33 Batch 50 Loss 0.6295 Accuracy 0.8173\n",
            "Epoch 33 Batch 100 Loss 0.6319 Accuracy 0.8157\n",
            "Epoch 33 Batch 150 Loss 0.6431 Accuracy 0.8122\n",
            "Epoch 33 Batch 200 Loss 0.6576 Accuracy 0.8082\n",
            "Epoch 33 Batch 250 Loss 0.6745 Accuracy 0.8037\n",
            "Epoch 33 Batch 300 Loss 0.6912 Accuracy 0.7993\n",
            "Epoch 33 Batch 350 Loss 0.7029 Accuracy 0.7962\n",
            "Epoch 33 Loss 0.7038 Accuracy 0.7959\n",
            "Time taken for 1 epoch: 62.88 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.5739 Accuracy 0.8125\n",
            "Epoch 34 Batch 50 Loss 0.6282 Accuracy 0.8168\n",
            "Epoch 34 Batch 100 Loss 0.6450 Accuracy 0.8116\n",
            "Epoch 34 Batch 150 Loss 0.6613 Accuracy 0.8073\n",
            "Epoch 34 Batch 200 Loss 0.6683 Accuracy 0.8051\n",
            "Epoch 34 Batch 250 Loss 0.6758 Accuracy 0.8027\n",
            "Epoch 34 Batch 300 Loss 0.6814 Accuracy 0.8006\n",
            "Epoch 34 Batch 350 Loss 0.6883 Accuracy 0.7987\n",
            "Epoch 34 Loss 0.6883 Accuracy 0.7986\n",
            "Time taken for 1 epoch: 63.37 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.5070 Accuracy 0.8468\n",
            "Epoch 35 Batch 50 Loss 0.5666 Accuracy 0.8335\n",
            "Epoch 35 Batch 100 Loss 0.5879 Accuracy 0.8267\n",
            "Epoch 35 Batch 150 Loss 0.6063 Accuracy 0.8212\n",
            "Epoch 35 Batch 200 Loss 0.6176 Accuracy 0.8181\n",
            "Epoch 35 Batch 250 Loss 0.6285 Accuracy 0.8148\n",
            "Epoch 35 Batch 300 Loss 0.6381 Accuracy 0.8123\n",
            "Epoch 35 Batch 350 Loss 0.6481 Accuracy 0.8093\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-7\n",
            "Epoch 35 Loss 0.6491 Accuracy 0.8091\n",
            "Time taken for 1 epoch: 66.17 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.4243 Accuracy 0.8739\n",
            "Epoch 36 Batch 50 Loss 0.5675 Accuracy 0.8321\n",
            "Epoch 36 Batch 100 Loss 0.5646 Accuracy 0.8320\n",
            "Epoch 36 Batch 150 Loss 0.5829 Accuracy 0.8268\n",
            "Epoch 36 Batch 200 Loss 0.5881 Accuracy 0.8253\n",
            "Epoch 36 Batch 250 Loss 0.6000 Accuracy 0.8220\n",
            "Epoch 36 Batch 300 Loss 0.6110 Accuracy 0.8192\n",
            "Epoch 36 Batch 350 Loss 0.6181 Accuracy 0.8170\n",
            "Epoch 36 Loss 0.6193 Accuracy 0.8166\n",
            "Time taken for 1 epoch: 63.09 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.5681 Accuracy 0.8123\n",
            "Epoch 37 Batch 50 Loss 0.5208 Accuracy 0.8446\n",
            "Epoch 37 Batch 100 Loss 0.5346 Accuracy 0.8410\n",
            "Epoch 37 Batch 150 Loss 0.5443 Accuracy 0.8374\n",
            "Epoch 37 Batch 200 Loss 0.5544 Accuracy 0.8345\n",
            "Epoch 37 Batch 250 Loss 0.5682 Accuracy 0.8303\n",
            "Epoch 37 Batch 300 Loss 0.5765 Accuracy 0.8278\n",
            "Epoch 37 Batch 350 Loss 0.5869 Accuracy 0.8249\n",
            "Epoch 37 Loss 0.5884 Accuracy 0.8246\n",
            "Time taken for 1 epoch: 63.52 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.4711 Accuracy 0.8708\n",
            "Epoch 38 Batch 50 Loss 0.5016 Accuracy 0.8493\n",
            "Epoch 38 Batch 100 Loss 0.5104 Accuracy 0.8470\n",
            "Epoch 38 Batch 150 Loss 0.5248 Accuracy 0.8427\n",
            "Epoch 38 Batch 200 Loss 0.5350 Accuracy 0.8401\n",
            "Epoch 38 Batch 250 Loss 0.5461 Accuracy 0.8369\n",
            "Epoch 38 Batch 300 Loss 0.5571 Accuracy 0.8339\n",
            "Epoch 38 Batch 350 Loss 0.5660 Accuracy 0.8310\n",
            "Epoch 38 Loss 0.5668 Accuracy 0.8308\n",
            "Time taken for 1 epoch: 62.64 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.4863 Accuracy 0.8453\n",
            "Epoch 39 Batch 50 Loss 0.4726 Accuracy 0.8593\n",
            "Epoch 39 Batch 100 Loss 0.4849 Accuracy 0.8549\n",
            "Epoch 39 Batch 150 Loss 0.5006 Accuracy 0.8499\n",
            "Epoch 39 Batch 200 Loss 0.5188 Accuracy 0.8443\n",
            "Epoch 39 Batch 250 Loss 0.5267 Accuracy 0.8422\n",
            "Epoch 39 Batch 300 Loss 0.5342 Accuracy 0.8400\n",
            "Epoch 39 Batch 350 Loss 0.5441 Accuracy 0.8373\n",
            "Epoch 39 Loss 0.5447 Accuracy 0.8370\n",
            "Time taken for 1 epoch: 62.67 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.4519 Accuracy 0.8727\n",
            "Epoch 40 Batch 50 Loss 0.4561 Accuracy 0.8622\n",
            "Epoch 40 Batch 100 Loss 0.4716 Accuracy 0.8573\n",
            "Epoch 40 Batch 150 Loss 0.4820 Accuracy 0.8546\n",
            "Epoch 40 Batch 200 Loss 0.4890 Accuracy 0.8525\n",
            "Epoch 40 Batch 250 Loss 0.5000 Accuracy 0.8492\n",
            "Epoch 40 Batch 300 Loss 0.5068 Accuracy 0.8470\n",
            "Epoch 40 Batch 350 Loss 0.5145 Accuracy 0.8449\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-8\n",
            "Epoch 40 Loss 0.5151 Accuracy 0.8447\n",
            "Time taken for 1 epoch: 65.79 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.4915 Accuracy 0.8573\n",
            "Epoch 41 Batch 50 Loss 0.4304 Accuracy 0.8695\n",
            "Epoch 41 Batch 100 Loss 0.4460 Accuracy 0.8645\n",
            "Epoch 41 Batch 150 Loss 0.4612 Accuracy 0.8599\n",
            "Epoch 41 Batch 200 Loss 0.4714 Accuracy 0.8564\n",
            "Epoch 41 Batch 250 Loss 0.4798 Accuracy 0.8540\n",
            "Epoch 41 Batch 300 Loss 0.4911 Accuracy 0.8508\n",
            "Epoch 41 Batch 350 Loss 0.4991 Accuracy 0.8488\n",
            "Epoch 41 Loss 0.5002 Accuracy 0.8484\n",
            "Time taken for 1 epoch: 62.87 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.3470 Accuracy 0.8967\n",
            "Epoch 42 Batch 50 Loss 0.4319 Accuracy 0.8700\n",
            "Epoch 42 Batch 100 Loss 0.4382 Accuracy 0.8667\n",
            "Epoch 42 Batch 150 Loss 0.4461 Accuracy 0.8648\n",
            "Epoch 42 Batch 200 Loss 0.4539 Accuracy 0.8633\n",
            "Epoch 42 Batch 250 Loss 0.4637 Accuracy 0.8600\n",
            "Epoch 42 Batch 300 Loss 0.4717 Accuracy 0.8575\n",
            "Epoch 42 Batch 350 Loss 0.4811 Accuracy 0.8549\n",
            "Epoch 42 Loss 0.4810 Accuracy 0.8549\n",
            "Time taken for 1 epoch: 62.98 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.4478 Accuracy 0.8674\n",
            "Epoch 43 Batch 50 Loss 0.4204 Accuracy 0.8732\n",
            "Epoch 43 Batch 100 Loss 0.4231 Accuracy 0.8704\n",
            "Epoch 43 Batch 150 Loss 0.4262 Accuracy 0.8698\n",
            "Epoch 43 Batch 200 Loss 0.4358 Accuracy 0.8672\n",
            "Epoch 43 Batch 250 Loss 0.4438 Accuracy 0.8652\n",
            "Epoch 43 Batch 300 Loss 0.4533 Accuracy 0.8625\n",
            "Epoch 43 Batch 350 Loss 0.4617 Accuracy 0.8602\n",
            "Epoch 43 Loss 0.4633 Accuracy 0.8597\n",
            "Time taken for 1 epoch: 62.82 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.4332 Accuracy 0.8610\n",
            "Epoch 44 Batch 50 Loss 0.3978 Accuracy 0.8776\n",
            "Epoch 44 Batch 100 Loss 0.4077 Accuracy 0.8753\n",
            "Epoch 44 Batch 150 Loss 0.4194 Accuracy 0.8725\n",
            "Epoch 44 Batch 200 Loss 0.4289 Accuracy 0.8691\n",
            "Epoch 44 Batch 250 Loss 0.4348 Accuracy 0.8672\n",
            "Epoch 44 Batch 300 Loss 0.4420 Accuracy 0.8652\n",
            "Epoch 44 Batch 350 Loss 0.4490 Accuracy 0.8631\n",
            "Epoch 44 Loss 0.4507 Accuracy 0.8626\n",
            "Time taken for 1 epoch: 62.72 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.3619 Accuracy 0.8926\n",
            "Epoch 45 Batch 50 Loss 0.3910 Accuracy 0.8801\n",
            "Epoch 45 Batch 100 Loss 0.3843 Accuracy 0.8822\n",
            "Epoch 45 Batch 150 Loss 0.3964 Accuracy 0.8786\n",
            "Epoch 45 Batch 200 Loss 0.4065 Accuracy 0.8757\n",
            "Epoch 45 Batch 250 Loss 0.4136 Accuracy 0.8738\n",
            "Epoch 45 Batch 300 Loss 0.4219 Accuracy 0.8713\n",
            "Epoch 45 Batch 350 Loss 0.4298 Accuracy 0.8690\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/train/ckpt-9\n",
            "Epoch 45 Loss 0.4302 Accuracy 0.8688\n",
            "Time taken for 1 epoch: 65.97 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.3406 Accuracy 0.8979\n",
            "Epoch 46 Batch 50 Loss 0.3784 Accuracy 0.8846\n",
            "Epoch 46 Batch 100 Loss 0.3802 Accuracy 0.8842\n",
            "Epoch 46 Batch 150 Loss 0.3843 Accuracy 0.8822\n",
            "Epoch 46 Batch 200 Loss 0.3946 Accuracy 0.8791\n",
            "Epoch 46 Batch 250 Loss 0.4029 Accuracy 0.8768\n",
            "Epoch 46 Batch 300 Loss 0.4107 Accuracy 0.8744\n",
            "Epoch 46 Batch 350 Loss 0.4185 Accuracy 0.8719\n",
            "Epoch 46 Loss 0.4191 Accuracy 0.8716\n",
            "Time taken for 1 epoch: 62.78 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.2976 Accuracy 0.8973\n",
            "Epoch 47 Batch 50 Loss 0.3490 Accuracy 0.8920\n",
            "Epoch 47 Batch 100 Loss 0.3654 Accuracy 0.8882\n",
            "Epoch 47 Batch 150 Loss 0.3668 Accuracy 0.8874\n",
            "Epoch 47 Batch 200 Loss 0.3753 Accuracy 0.8848\n",
            "Epoch 47 Batch 250 Loss 0.3821 Accuracy 0.8825\n",
            "Epoch 47 Batch 300 Loss 0.3870 Accuracy 0.8810\n",
            "Epoch 47 Batch 350 Loss 0.3950 Accuracy 0.8784\n",
            "Epoch 47 Loss 0.3955 Accuracy 0.8784\n",
            "Time taken for 1 epoch: 62.71 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.3243 Accuracy 0.9063\n",
            "Epoch 48 Batch 50 Loss 0.3387 Accuracy 0.8970\n",
            "Epoch 48 Batch 100 Loss 0.3552 Accuracy 0.8916\n",
            "Epoch 48 Batch 150 Loss 0.3609 Accuracy 0.8897\n",
            "Epoch 48 Batch 200 Loss 0.3663 Accuracy 0.8880\n",
            "Epoch 48 Batch 250 Loss 0.3727 Accuracy 0.8863\n",
            "Epoch 48 Batch 300 Loss 0.3799 Accuracy 0.8837\n",
            "Epoch 48 Batch 350 Loss 0.3874 Accuracy 0.8814\n",
            "Epoch 48 Loss 0.3876 Accuracy 0.8814\n",
            "Time taken for 1 epoch: 62.50 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.2587 Accuracy 0.9187\n",
            "Epoch 49 Batch 50 Loss 0.3206 Accuracy 0.9029\n",
            "Epoch 49 Batch 100 Loss 0.3312 Accuracy 0.8989\n",
            "Epoch 49 Batch 150 Loss 0.3380 Accuracy 0.8967\n",
            "Epoch 49 Batch 200 Loss 0.3475 Accuracy 0.8936\n",
            "Epoch 49 Batch 250 Loss 0.3565 Accuracy 0.8907\n",
            "Epoch 49 Batch 300 Loss 0.3650 Accuracy 0.8882\n",
            "Epoch 49 Batch 350 Loss 0.3726 Accuracy 0.8858\n",
            "Epoch 49 Loss 0.3734 Accuracy 0.8855\n",
            "Time taken for 1 epoch: 62.66 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.3102 Accuracy 0.9053\n",
            "Epoch 50 Batch 50 Loss 0.3266 Accuracy 0.9003\n",
            "Epoch 50 Batch 100 Loss 0.3261 Accuracy 0.9006\n",
            "Epoch 50 Batch 150 Loss 0.3356 Accuracy 0.8979\n",
            "Epoch 50 Batch 200 Loss 0.3426 Accuracy 0.8954\n",
            "Epoch 50 Batch 250 Loss 0.3480 Accuracy 0.8933\n",
            "Epoch 50 Batch 300 Loss 0.3528 Accuracy 0.8915\n",
            "Epoch 50 Batch 350 Loss 0.3563 Accuracy 0.8903\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-10\n",
            "Epoch 50 Loss 0.3567 Accuracy 0.8901\n",
            "Time taken for 1 epoch: 65.99 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.3425 Accuracy 0.8894\n",
            "Epoch 51 Batch 50 Loss 0.3151 Accuracy 0.9038\n",
            "Epoch 51 Batch 100 Loss 0.3217 Accuracy 0.9015\n",
            "Epoch 51 Batch 150 Loss 0.3285 Accuracy 0.8991\n",
            "Epoch 51 Batch 200 Loss 0.3360 Accuracy 0.8969\n",
            "Epoch 51 Batch 250 Loss 0.3439 Accuracy 0.8943\n",
            "Epoch 51 Batch 300 Loss 0.3487 Accuracy 0.8928\n",
            "Epoch 51 Batch 350 Loss 0.3524 Accuracy 0.8915\n",
            "Epoch 51 Loss 0.3530 Accuracy 0.8914\n",
            "Time taken for 1 epoch: 62.86 secs\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.2839 Accuracy 0.9255\n",
            "Epoch 52 Batch 50 Loss 0.3134 Accuracy 0.9041\n",
            "Epoch 52 Batch 100 Loss 0.3130 Accuracy 0.9038\n",
            "Epoch 52 Batch 150 Loss 0.3159 Accuracy 0.9027\n",
            "Epoch 52 Batch 200 Loss 0.3197 Accuracy 0.9014\n",
            "Epoch 52 Batch 250 Loss 0.3271 Accuracy 0.8992\n",
            "Epoch 52 Batch 300 Loss 0.3343 Accuracy 0.8970\n",
            "Epoch 52 Batch 350 Loss 0.3395 Accuracy 0.8954\n",
            "Epoch 52 Loss 0.3406 Accuracy 0.8951\n",
            "Time taken for 1 epoch: 62.96 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.2753 Accuracy 0.9162\n",
            "Epoch 53 Batch 50 Loss 0.3009 Accuracy 0.9074\n",
            "Epoch 53 Batch 100 Loss 0.3018 Accuracy 0.9066\n",
            "Epoch 53 Batch 150 Loss 0.3104 Accuracy 0.9038\n",
            "Epoch 53 Batch 200 Loss 0.3189 Accuracy 0.9014\n",
            "Epoch 53 Batch 250 Loss 0.3213 Accuracy 0.9008\n",
            "Epoch 53 Batch 300 Loss 0.3267 Accuracy 0.8995\n",
            "Epoch 53 Batch 350 Loss 0.3311 Accuracy 0.8979\n",
            "Epoch 53 Loss 0.3312 Accuracy 0.8978\n",
            "Time taken for 1 epoch: 63.46 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.2953 Accuracy 0.9175\n",
            "Epoch 54 Batch 50 Loss 0.2929 Accuracy 0.9114\n",
            "Epoch 54 Batch 100 Loss 0.2953 Accuracy 0.9093\n",
            "Epoch 54 Batch 150 Loss 0.3010 Accuracy 0.9072\n",
            "Epoch 54 Batch 200 Loss 0.3042 Accuracy 0.9057\n",
            "Epoch 54 Batch 250 Loss 0.3089 Accuracy 0.9039\n",
            "Epoch 54 Batch 300 Loss 0.3150 Accuracy 0.9019\n",
            "Epoch 54 Batch 350 Loss 0.3199 Accuracy 0.9005\n",
            "Epoch 54 Loss 0.3215 Accuracy 0.9001\n",
            "Time taken for 1 epoch: 62.47 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.3362 Accuracy 0.9115\n",
            "Epoch 55 Batch 50 Loss 0.2897 Accuracy 0.9127\n",
            "Epoch 55 Batch 100 Loss 0.2873 Accuracy 0.9112\n",
            "Epoch 55 Batch 150 Loss 0.2893 Accuracy 0.9102\n",
            "Epoch 55 Batch 200 Loss 0.2989 Accuracy 0.9071\n",
            "Epoch 55 Batch 250 Loss 0.3035 Accuracy 0.9056\n",
            "Epoch 55 Batch 300 Loss 0.3075 Accuracy 0.9044\n",
            "Epoch 55 Batch 350 Loss 0.3119 Accuracy 0.9030\n",
            "Saving checkpoint for epoch 55 at ./checkpoints/train/ckpt-11\n",
            "Epoch 55 Loss 0.3125 Accuracy 0.9029\n",
            "Time taken for 1 epoch: 65.98 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.2366 Accuracy 0.9197\n",
            "Epoch 56 Batch 50 Loss 0.2778 Accuracy 0.9142\n",
            "Epoch 56 Batch 100 Loss 0.2837 Accuracy 0.9121\n",
            "Epoch 56 Batch 150 Loss 0.2890 Accuracy 0.9103\n",
            "Epoch 56 Batch 200 Loss 0.2934 Accuracy 0.9090\n",
            "Epoch 56 Batch 250 Loss 0.2954 Accuracy 0.9082\n",
            "Epoch 56 Batch 300 Loss 0.2982 Accuracy 0.9073\n",
            "Epoch 56 Batch 350 Loss 0.3034 Accuracy 0.9058\n",
            "Epoch 56 Loss 0.3039 Accuracy 0.9057\n",
            "Time taken for 1 epoch: 62.47 secs\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.2456 Accuracy 0.9221\n",
            "Epoch 57 Batch 50 Loss 0.2651 Accuracy 0.9176\n",
            "Epoch 57 Batch 100 Loss 0.2698 Accuracy 0.9162\n",
            "Epoch 57 Batch 150 Loss 0.2752 Accuracy 0.9147\n",
            "Epoch 57 Batch 200 Loss 0.2822 Accuracy 0.9128\n",
            "Epoch 57 Batch 250 Loss 0.2907 Accuracy 0.9103\n",
            "Epoch 57 Batch 300 Loss 0.2943 Accuracy 0.9089\n",
            "Epoch 57 Batch 350 Loss 0.2979 Accuracy 0.9079\n",
            "Epoch 57 Loss 0.2984 Accuracy 0.9077\n",
            "Time taken for 1 epoch: 62.86 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.2685 Accuracy 0.9137\n",
            "Epoch 58 Batch 50 Loss 0.2566 Accuracy 0.9214\n",
            "Epoch 58 Batch 100 Loss 0.2632 Accuracy 0.9187\n",
            "Epoch 58 Batch 150 Loss 0.2675 Accuracy 0.9176\n",
            "Epoch 58 Batch 200 Loss 0.2718 Accuracy 0.9163\n",
            "Epoch 58 Batch 250 Loss 0.2778 Accuracy 0.9141\n",
            "Epoch 58 Batch 300 Loss 0.2801 Accuracy 0.9132\n",
            "Epoch 58 Batch 350 Loss 0.2846 Accuracy 0.9119\n",
            "Epoch 58 Loss 0.2850 Accuracy 0.9116\n",
            "Time taken for 1 epoch: 62.83 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.2249 Accuracy 0.9338\n",
            "Epoch 59 Batch 50 Loss 0.2440 Accuracy 0.9237\n",
            "Epoch 59 Batch 100 Loss 0.2524 Accuracy 0.9210\n",
            "Epoch 59 Batch 150 Loss 0.2553 Accuracy 0.9203\n",
            "Epoch 59 Batch 200 Loss 0.2626 Accuracy 0.9185\n",
            "Epoch 59 Batch 250 Loss 0.2684 Accuracy 0.9164\n",
            "Epoch 59 Batch 300 Loss 0.2724 Accuracy 0.9151\n",
            "Epoch 59 Batch 350 Loss 0.2779 Accuracy 0.9136\n",
            "Epoch 59 Loss 0.2790 Accuracy 0.9132\n",
            "Time taken for 1 epoch: 62.32 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.2258 Accuracy 0.9333\n",
            "Epoch 60 Batch 50 Loss 0.2480 Accuracy 0.9236\n",
            "Epoch 60 Batch 100 Loss 0.2526 Accuracy 0.9217\n",
            "Epoch 60 Batch 150 Loss 0.2549 Accuracy 0.9208\n",
            "Epoch 60 Batch 200 Loss 0.2588 Accuracy 0.9197\n",
            "Epoch 60 Batch 250 Loss 0.2627 Accuracy 0.9184\n",
            "Epoch 60 Batch 300 Loss 0.2672 Accuracy 0.9167\n",
            "Epoch 60 Batch 350 Loss 0.2719 Accuracy 0.9154\n",
            "Saving checkpoint for epoch 60 at ./checkpoints/train/ckpt-12\n",
            "Epoch 60 Loss 0.2723 Accuracy 0.9153\n",
            "Time taken for 1 epoch: 65.86 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.2144 Accuracy 0.9409\n",
            "Epoch 61 Batch 50 Loss 0.2387 Accuracy 0.9256\n",
            "Epoch 61 Batch 100 Loss 0.2432 Accuracy 0.9240\n",
            "Epoch 61 Batch 150 Loss 0.2446 Accuracy 0.9235\n",
            "Epoch 61 Batch 200 Loss 0.2510 Accuracy 0.9217\n",
            "Epoch 61 Batch 250 Loss 0.2560 Accuracy 0.9201\n",
            "Epoch 61 Batch 300 Loss 0.2610 Accuracy 0.9188\n",
            "Epoch 61 Batch 350 Loss 0.2644 Accuracy 0.9177\n",
            "Epoch 61 Loss 0.2647 Accuracy 0.9176\n",
            "Time taken for 1 epoch: 63.29 secs\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.1832 Accuracy 0.9442\n",
            "Epoch 62 Batch 50 Loss 0.2323 Accuracy 0.9288\n",
            "Epoch 62 Batch 100 Loss 0.2356 Accuracy 0.9273\n",
            "Epoch 62 Batch 150 Loss 0.2430 Accuracy 0.9248\n",
            "Epoch 62 Batch 200 Loss 0.2479 Accuracy 0.9231\n",
            "Epoch 62 Batch 250 Loss 0.2530 Accuracy 0.9215\n",
            "Epoch 62 Batch 300 Loss 0.2565 Accuracy 0.9202\n",
            "Epoch 62 Batch 350 Loss 0.2591 Accuracy 0.9193\n",
            "Epoch 62 Loss 0.2595 Accuracy 0.9192\n",
            "Time taken for 1 epoch: 62.61 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.1886 Accuracy 0.9346\n",
            "Epoch 63 Batch 50 Loss 0.2248 Accuracy 0.9302\n",
            "Epoch 63 Batch 100 Loss 0.2286 Accuracy 0.9299\n",
            "Epoch 63 Batch 150 Loss 0.2336 Accuracy 0.9280\n",
            "Epoch 63 Batch 200 Loss 0.2390 Accuracy 0.9260\n",
            "Epoch 63 Batch 250 Loss 0.2468 Accuracy 0.9234\n",
            "Epoch 63 Batch 300 Loss 0.2496 Accuracy 0.9224\n",
            "Epoch 63 Batch 350 Loss 0.2522 Accuracy 0.9216\n",
            "Epoch 63 Loss 0.2524 Accuracy 0.9216\n",
            "Time taken for 1 epoch: 62.77 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.1523 Accuracy 0.9578\n",
            "Epoch 64 Batch 50 Loss 0.2221 Accuracy 0.9317\n",
            "Epoch 64 Batch 100 Loss 0.2282 Accuracy 0.9295\n",
            "Epoch 64 Batch 150 Loss 0.2309 Accuracy 0.9288\n",
            "Epoch 64 Batch 200 Loss 0.2330 Accuracy 0.9281\n",
            "Epoch 64 Batch 250 Loss 0.2377 Accuracy 0.9264\n",
            "Epoch 64 Batch 300 Loss 0.2406 Accuracy 0.9252\n",
            "Epoch 64 Batch 350 Loss 0.2454 Accuracy 0.9237\n",
            "Epoch 64 Loss 0.2455 Accuracy 0.9236\n",
            "Time taken for 1 epoch: 62.51 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.2086 Accuracy 0.9332\n",
            "Epoch 65 Batch 50 Loss 0.2187 Accuracy 0.9316\n",
            "Epoch 65 Batch 100 Loss 0.2214 Accuracy 0.9309\n",
            "Epoch 65 Batch 150 Loss 0.2255 Accuracy 0.9299\n",
            "Epoch 65 Batch 200 Loss 0.2270 Accuracy 0.9295\n",
            "Epoch 65 Batch 250 Loss 0.2313 Accuracy 0.9283\n",
            "Epoch 65 Batch 300 Loss 0.2352 Accuracy 0.9268\n",
            "Epoch 65 Batch 350 Loss 0.2366 Accuracy 0.9266\n",
            "Saving checkpoint for epoch 65 at ./checkpoints/train/ckpt-13\n",
            "Epoch 65 Loss 0.2367 Accuracy 0.9265\n",
            "Time taken for 1 epoch: 65.71 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.2270 Accuracy 0.9448\n",
            "Epoch 66 Batch 50 Loss 0.2107 Accuracy 0.9355\n",
            "Epoch 66 Batch 100 Loss 0.2124 Accuracy 0.9347\n",
            "Epoch 66 Batch 150 Loss 0.2145 Accuracy 0.9340\n",
            "Epoch 66 Batch 200 Loss 0.2187 Accuracy 0.9325\n",
            "Epoch 66 Batch 250 Loss 0.2237 Accuracy 0.9308\n",
            "Epoch 66 Batch 300 Loss 0.2270 Accuracy 0.9299\n",
            "Epoch 66 Batch 350 Loss 0.2313 Accuracy 0.9284\n",
            "Epoch 66 Loss 0.2318 Accuracy 0.9282\n",
            "Time taken for 1 epoch: 62.44 secs\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.2246 Accuracy 0.9325\n",
            "Epoch 67 Batch 50 Loss 0.1971 Accuracy 0.9390\n",
            "Epoch 67 Batch 100 Loss 0.2093 Accuracy 0.9354\n",
            "Epoch 67 Batch 150 Loss 0.2150 Accuracy 0.9336\n",
            "Epoch 67 Batch 200 Loss 0.2197 Accuracy 0.9319\n",
            "Epoch 67 Batch 250 Loss 0.2249 Accuracy 0.9304\n",
            "Epoch 67 Batch 300 Loss 0.2280 Accuracy 0.9298\n",
            "Epoch 67 Batch 350 Loss 0.2300 Accuracy 0.9291\n",
            "Epoch 67 Loss 0.2299 Accuracy 0.9292\n",
            "Time taken for 1 epoch: 62.52 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.1365 Accuracy 0.9522\n",
            "Epoch 68 Batch 50 Loss 0.1938 Accuracy 0.9388\n",
            "Epoch 68 Batch 100 Loss 0.1975 Accuracy 0.9380\n",
            "Epoch 68 Batch 150 Loss 0.2047 Accuracy 0.9356\n",
            "Epoch 68 Batch 200 Loss 0.2119 Accuracy 0.9333\n",
            "Epoch 68 Batch 250 Loss 0.2144 Accuracy 0.9326\n",
            "Epoch 68 Batch 300 Loss 0.2199 Accuracy 0.9311\n",
            "Epoch 68 Batch 350 Loss 0.2218 Accuracy 0.9306\n",
            "Epoch 68 Loss 0.2223 Accuracy 0.9304\n",
            "Time taken for 1 epoch: 63.08 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.1961 Accuracy 0.9440\n",
            "Epoch 69 Batch 50 Loss 0.2019 Accuracy 0.9370\n",
            "Epoch 69 Batch 100 Loss 0.2022 Accuracy 0.9373\n",
            "Epoch 69 Batch 150 Loss 0.2070 Accuracy 0.9358\n",
            "Epoch 69 Batch 200 Loss 0.2102 Accuracy 0.9349\n",
            "Epoch 69 Batch 250 Loss 0.2132 Accuracy 0.9340\n",
            "Epoch 69 Batch 300 Loss 0.2161 Accuracy 0.9330\n",
            "Epoch 69 Batch 350 Loss 0.2192 Accuracy 0.9322\n",
            "Epoch 69 Loss 0.2193 Accuracy 0.9322\n",
            "Time taken for 1 epoch: 62.77 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.1721 Accuracy 0.9476\n",
            "Epoch 70 Batch 50 Loss 0.1942 Accuracy 0.9405\n",
            "Epoch 70 Batch 100 Loss 0.1950 Accuracy 0.9395\n",
            "Epoch 70 Batch 150 Loss 0.2032 Accuracy 0.9367\n",
            "Epoch 70 Batch 200 Loss 0.2097 Accuracy 0.9349\n",
            "Epoch 70 Batch 250 Loss 0.2128 Accuracy 0.9338\n",
            "Epoch 70 Batch 300 Loss 0.2152 Accuracy 0.9331\n",
            "Epoch 70 Batch 350 Loss 0.2170 Accuracy 0.9325\n",
            "Saving checkpoint for epoch 70 at ./checkpoints/train/ckpt-14\n",
            "Epoch 70 Loss 0.2172 Accuracy 0.9324\n",
            "Time taken for 1 epoch: 65.77 secs\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.2216 Accuracy 0.9211\n",
            "Epoch 71 Batch 50 Loss 0.1893 Accuracy 0.9422\n",
            "Epoch 71 Batch 100 Loss 0.1888 Accuracy 0.9429\n",
            "Epoch 71 Batch 150 Loss 0.1925 Accuracy 0.9413\n",
            "Epoch 71 Batch 200 Loss 0.1976 Accuracy 0.9397\n",
            "Epoch 71 Batch 250 Loss 0.2011 Accuracy 0.9384\n",
            "Epoch 71 Batch 300 Loss 0.2043 Accuracy 0.9374\n",
            "Epoch 71 Batch 350 Loss 0.2079 Accuracy 0.9361\n",
            "Epoch 71 Loss 0.2081 Accuracy 0.9360\n",
            "Time taken for 1 epoch: 62.76 secs\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.2072 Accuracy 0.9330\n",
            "Epoch 72 Batch 50 Loss 0.1936 Accuracy 0.9402\n",
            "Epoch 72 Batch 100 Loss 0.1913 Accuracy 0.9406\n",
            "Epoch 72 Batch 150 Loss 0.1931 Accuracy 0.9399\n",
            "Epoch 72 Batch 200 Loss 0.1956 Accuracy 0.9393\n",
            "Epoch 72 Batch 250 Loss 0.1999 Accuracy 0.9379\n",
            "Epoch 72 Batch 300 Loss 0.2035 Accuracy 0.9371\n",
            "Epoch 72 Batch 350 Loss 0.2053 Accuracy 0.9364\n",
            "Epoch 72 Loss 0.2056 Accuracy 0.9362\n",
            "Time taken for 1 epoch: 62.63 secs\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.1695 Accuracy 0.9405\n",
            "Epoch 73 Batch 50 Loss 0.1885 Accuracy 0.9413\n",
            "Epoch 73 Batch 100 Loss 0.1877 Accuracy 0.9412\n",
            "Epoch 73 Batch 150 Loss 0.1904 Accuracy 0.9405\n",
            "Epoch 73 Batch 200 Loss 0.1937 Accuracy 0.9397\n",
            "Epoch 73 Batch 250 Loss 0.1982 Accuracy 0.9381\n",
            "Epoch 73 Batch 300 Loss 0.2010 Accuracy 0.9373\n",
            "Epoch 73 Batch 350 Loss 0.2044 Accuracy 0.9359\n",
            "Epoch 73 Loss 0.2043 Accuracy 0.9360\n",
            "Time taken for 1 epoch: 63.10 secs\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.1994 Accuracy 0.9445\n",
            "Epoch 74 Batch 50 Loss 0.1770 Accuracy 0.9467\n",
            "Epoch 74 Batch 100 Loss 0.1831 Accuracy 0.9442\n",
            "Epoch 74 Batch 150 Loss 0.1880 Accuracy 0.9426\n",
            "Epoch 74 Batch 200 Loss 0.1893 Accuracy 0.9421\n",
            "Epoch 74 Batch 250 Loss 0.1918 Accuracy 0.9412\n",
            "Epoch 74 Batch 300 Loss 0.1960 Accuracy 0.9399\n",
            "Epoch 74 Batch 350 Loss 0.1996 Accuracy 0.9388\n",
            "Epoch 74 Loss 0.2001 Accuracy 0.9386\n",
            "Time taken for 1 epoch: 62.65 secs\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.2293 Accuracy 0.9335\n",
            "Epoch 75 Batch 50 Loss 0.1807 Accuracy 0.9441\n",
            "Epoch 75 Batch 100 Loss 0.1797 Accuracy 0.9446\n",
            "Epoch 75 Batch 150 Loss 0.1845 Accuracy 0.9435\n",
            "Epoch 75 Batch 200 Loss 0.1878 Accuracy 0.9423\n",
            "Epoch 75 Batch 250 Loss 0.1908 Accuracy 0.9413\n",
            "Epoch 75 Batch 300 Loss 0.1941 Accuracy 0.9403\n",
            "Epoch 75 Batch 350 Loss 0.1970 Accuracy 0.9393\n",
            "Saving checkpoint for epoch 75 at ./checkpoints/train/ckpt-15\n",
            "Epoch 75 Loss 0.1971 Accuracy 0.9392\n",
            "Time taken for 1 epoch: 65.69 secs\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.1669 Accuracy 0.9432\n",
            "Epoch 76 Batch 50 Loss 0.1792 Accuracy 0.9455\n",
            "Epoch 76 Batch 100 Loss 0.1810 Accuracy 0.9449\n",
            "Epoch 76 Batch 150 Loss 0.1853 Accuracy 0.9430\n",
            "Epoch 76 Batch 200 Loss 0.1889 Accuracy 0.9418\n",
            "Epoch 76 Batch 250 Loss 0.1904 Accuracy 0.9415\n",
            "Epoch 76 Batch 300 Loss 0.1918 Accuracy 0.9411\n",
            "Epoch 76 Batch 350 Loss 0.1937 Accuracy 0.9404\n",
            "Epoch 76 Loss 0.1940 Accuracy 0.9403\n",
            "Time taken for 1 epoch: 63.06 secs\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.1762 Accuracy 0.9455\n",
            "Epoch 77 Batch 50 Loss 0.1737 Accuracy 0.9482\n",
            "Epoch 77 Batch 100 Loss 0.1801 Accuracy 0.9461\n",
            "Epoch 77 Batch 150 Loss 0.1839 Accuracy 0.9451\n",
            "Epoch 77 Batch 200 Loss 0.1878 Accuracy 0.9433\n",
            "Epoch 77 Batch 250 Loss 0.1871 Accuracy 0.9433\n",
            "Epoch 77 Batch 300 Loss 0.1877 Accuracy 0.9428\n",
            "Epoch 77 Batch 350 Loss 0.1907 Accuracy 0.9418\n",
            "Epoch 77 Loss 0.1908 Accuracy 0.9416\n",
            "Time taken for 1 epoch: 62.73 secs\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.1224 Accuracy 0.9601\n",
            "Epoch 78 Batch 50 Loss 0.1699 Accuracy 0.9473\n",
            "Epoch 78 Batch 100 Loss 0.1727 Accuracy 0.9466\n",
            "Epoch 78 Batch 150 Loss 0.1719 Accuracy 0.9470\n",
            "Epoch 78 Batch 200 Loss 0.1768 Accuracy 0.9455\n",
            "Epoch 78 Batch 250 Loss 0.1799 Accuracy 0.9445\n",
            "Epoch 78 Batch 300 Loss 0.1810 Accuracy 0.9439\n",
            "Epoch 78 Batch 350 Loss 0.1835 Accuracy 0.9432\n",
            "Epoch 78 Loss 0.1836 Accuracy 0.9432\n",
            "Time taken for 1 epoch: 63.14 secs\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.1271 Accuracy 0.9599\n",
            "Epoch 79 Batch 50 Loss 0.1587 Accuracy 0.9520\n",
            "Epoch 79 Batch 100 Loss 0.1638 Accuracy 0.9497\n",
            "Epoch 79 Batch 150 Loss 0.1716 Accuracy 0.9473\n",
            "Epoch 79 Batch 200 Loss 0.1753 Accuracy 0.9458\n",
            "Epoch 79 Batch 250 Loss 0.1758 Accuracy 0.9457\n",
            "Epoch 79 Batch 300 Loss 0.1786 Accuracy 0.9449\n",
            "Epoch 79 Batch 350 Loss 0.1898 Accuracy 0.9426\n",
            "Epoch 79 Loss 0.1900 Accuracy 0.9425\n",
            "Time taken for 1 epoch: 62.45 secs\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.1268 Accuracy 0.9604\n",
            "Epoch 80 Batch 50 Loss 0.1603 Accuracy 0.9514\n",
            "Epoch 80 Batch 100 Loss 0.1649 Accuracy 0.9501\n",
            "Epoch 80 Batch 150 Loss 0.1692 Accuracy 0.9492\n",
            "Epoch 80 Batch 200 Loss 0.1715 Accuracy 0.9484\n",
            "Epoch 80 Batch 250 Loss 0.1755 Accuracy 0.9470\n",
            "Epoch 80 Batch 300 Loss 0.1767 Accuracy 0.9464\n",
            "Epoch 80 Batch 350 Loss 0.1782 Accuracy 0.9457\n",
            "Saving checkpoint for epoch 80 at ./checkpoints/train/ckpt-16\n",
            "Epoch 80 Loss 0.1783 Accuracy 0.9457\n",
            "Time taken for 1 epoch: 65.20 secs\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.1624 Accuracy 0.9626\n",
            "Epoch 81 Batch 50 Loss 0.1644 Accuracy 0.9494\n",
            "Epoch 81 Batch 100 Loss 0.1625 Accuracy 0.9496\n",
            "Epoch 81 Batch 150 Loss 0.1650 Accuracy 0.9492\n",
            "Epoch 81 Batch 200 Loss 0.1680 Accuracy 0.9488\n",
            "Epoch 81 Batch 250 Loss 0.1694 Accuracy 0.9478\n",
            "Epoch 81 Batch 300 Loss 0.1699 Accuracy 0.9474\n",
            "Epoch 81 Batch 350 Loss 0.1731 Accuracy 0.9463\n",
            "Epoch 81 Loss 0.1732 Accuracy 0.9462\n",
            "Time taken for 1 epoch: 63.26 secs\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.1742 Accuracy 0.9416\n",
            "Epoch 82 Batch 50 Loss 0.1532 Accuracy 0.9531\n",
            "Epoch 82 Batch 100 Loss 0.1565 Accuracy 0.9518\n",
            "Epoch 82 Batch 150 Loss 0.1609 Accuracy 0.9510\n",
            "Epoch 82 Batch 200 Loss 0.1645 Accuracy 0.9497\n",
            "Epoch 82 Batch 250 Loss 0.1673 Accuracy 0.9488\n",
            "Epoch 82 Batch 300 Loss 0.1679 Accuracy 0.9483\n",
            "Epoch 82 Batch 350 Loss 0.1694 Accuracy 0.9478\n",
            "Epoch 82 Loss 0.1698 Accuracy 0.9477\n",
            "Time taken for 1 epoch: 62.57 secs\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.2223 Accuracy 0.9405\n",
            "Epoch 83 Batch 50 Loss 0.1592 Accuracy 0.9518\n",
            "Epoch 83 Batch 100 Loss 0.1645 Accuracy 0.9500\n",
            "Epoch 83 Batch 150 Loss 0.1615 Accuracy 0.9506\n",
            "Epoch 83 Batch 200 Loss 0.1617 Accuracy 0.9505\n",
            "Epoch 83 Batch 250 Loss 0.1643 Accuracy 0.9497\n",
            "Epoch 83 Batch 300 Loss 0.1670 Accuracy 0.9488\n",
            "Epoch 83 Batch 350 Loss 0.1683 Accuracy 0.9482\n",
            "Epoch 83 Loss 0.1686 Accuracy 0.9482\n",
            "Time taken for 1 epoch: 63.14 secs\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.0981 Accuracy 0.9713\n",
            "Epoch 84 Batch 50 Loss 0.1479 Accuracy 0.9546\n",
            "Epoch 84 Batch 100 Loss 0.1495 Accuracy 0.9543\n",
            "Epoch 84 Batch 150 Loss 0.1536 Accuracy 0.9526\n",
            "Epoch 84 Batch 200 Loss 0.1565 Accuracy 0.9516\n",
            "Epoch 84 Batch 250 Loss 0.1618 Accuracy 0.9504\n",
            "Epoch 84 Batch 300 Loss 0.1667 Accuracy 0.9490\n",
            "Epoch 84 Batch 350 Loss 0.1689 Accuracy 0.9484\n",
            "Epoch 84 Loss 0.1688 Accuracy 0.9483\n",
            "Time taken for 1 epoch: 62.61 secs\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.1177 Accuracy 0.9712\n",
            "Epoch 85 Batch 50 Loss 0.1516 Accuracy 0.9535\n",
            "Epoch 85 Batch 100 Loss 0.1544 Accuracy 0.9529\n",
            "Epoch 85 Batch 150 Loss 0.1557 Accuracy 0.9522\n",
            "Epoch 85 Batch 200 Loss 0.1571 Accuracy 0.9519\n",
            "Epoch 85 Batch 250 Loss 0.1601 Accuracy 0.9509\n",
            "Epoch 85 Batch 300 Loss 0.1618 Accuracy 0.9505\n",
            "Epoch 85 Batch 350 Loss 0.1640 Accuracy 0.9496\n",
            "Saving checkpoint for epoch 85 at ./checkpoints/train/ckpt-17\n",
            "Epoch 85 Loss 0.1642 Accuracy 0.9496\n",
            "Time taken for 1 epoch: 65.21 secs\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.1736 Accuracy 0.9463\n",
            "Epoch 86 Batch 50 Loss 0.1490 Accuracy 0.9553\n",
            "Epoch 86 Batch 100 Loss 0.1502 Accuracy 0.9541\n",
            "Epoch 86 Batch 150 Loss 0.1538 Accuracy 0.9529\n",
            "Epoch 86 Batch 200 Loss 0.1554 Accuracy 0.9522\n",
            "Epoch 86 Batch 250 Loss 0.1568 Accuracy 0.9516\n",
            "Epoch 86 Batch 300 Loss 0.1585 Accuracy 0.9511\n",
            "Epoch 86 Batch 350 Loss 0.1595 Accuracy 0.9510\n",
            "Epoch 86 Loss 0.1597 Accuracy 0.9509\n",
            "Time taken for 1 epoch: 63.20 secs\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.1246 Accuracy 0.9580\n",
            "Epoch 87 Batch 50 Loss 0.1438 Accuracy 0.9548\n",
            "Epoch 87 Batch 100 Loss 0.1447 Accuracy 0.9553\n",
            "Epoch 87 Batch 150 Loss 0.1485 Accuracy 0.9542\n",
            "Epoch 87 Batch 200 Loss 0.1531 Accuracy 0.9530\n",
            "Epoch 87 Batch 250 Loss 0.1569 Accuracy 0.9519\n",
            "Epoch 87 Batch 300 Loss 0.1593 Accuracy 0.9512\n",
            "Epoch 87 Batch 350 Loss 0.1602 Accuracy 0.9509\n",
            "Epoch 87 Loss 0.1603 Accuracy 0.9510\n",
            "Time taken for 1 epoch: 62.68 secs\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.0969 Accuracy 0.9735\n",
            "Epoch 88 Batch 50 Loss 0.1441 Accuracy 0.9557\n",
            "Epoch 88 Batch 100 Loss 0.1448 Accuracy 0.9547\n",
            "Epoch 88 Batch 150 Loss 0.1451 Accuracy 0.9548\n",
            "Epoch 88 Batch 200 Loss 0.1480 Accuracy 0.9540\n",
            "Epoch 88 Batch 250 Loss 0.1520 Accuracy 0.9532\n",
            "Epoch 88 Batch 300 Loss 0.1570 Accuracy 0.9514\n",
            "Epoch 88 Batch 350 Loss 0.1580 Accuracy 0.9512\n",
            "Epoch 88 Loss 0.1581 Accuracy 0.9512\n",
            "Time taken for 1 epoch: 63.15 secs\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.1316 Accuracy 0.9665\n",
            "Epoch 89 Batch 50 Loss 0.1361 Accuracy 0.9595\n",
            "Epoch 89 Batch 100 Loss 0.1393 Accuracy 0.9576\n",
            "Epoch 89 Batch 150 Loss 0.1406 Accuracy 0.9572\n",
            "Epoch 89 Batch 200 Loss 0.1471 Accuracy 0.9549\n",
            "Epoch 89 Batch 250 Loss 0.1486 Accuracy 0.9543\n",
            "Epoch 89 Batch 300 Loss 0.1499 Accuracy 0.9539\n",
            "Epoch 89 Batch 350 Loss 0.1514 Accuracy 0.9535\n",
            "Epoch 89 Loss 0.1518 Accuracy 0.9533\n",
            "Time taken for 1 epoch: 62.66 secs\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.1059 Accuracy 0.9719\n",
            "Epoch 90 Batch 50 Loss 0.1426 Accuracy 0.9568\n",
            "Epoch 90 Batch 100 Loss 0.1443 Accuracy 0.9561\n",
            "Epoch 90 Batch 150 Loss 0.1436 Accuracy 0.9563\n",
            "Epoch 90 Batch 200 Loss 0.1460 Accuracy 0.9552\n",
            "Epoch 90 Batch 250 Loss 0.1477 Accuracy 0.9547\n",
            "Epoch 90 Batch 300 Loss 0.1487 Accuracy 0.9543\n",
            "Epoch 90 Batch 350 Loss 0.1504 Accuracy 0.9539\n",
            "Saving checkpoint for epoch 90 at ./checkpoints/train/ckpt-18\n",
            "Epoch 90 Loss 0.1503 Accuracy 0.9539\n",
            "Time taken for 1 epoch: 65.33 secs\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.1643 Accuracy 0.9556\n",
            "Epoch 91 Batch 50 Loss 0.1325 Accuracy 0.9605\n",
            "Epoch 91 Batch 100 Loss 0.1340 Accuracy 0.9592\n",
            "Epoch 91 Batch 150 Loss 0.1355 Accuracy 0.9587\n",
            "Epoch 91 Batch 200 Loss 0.1387 Accuracy 0.9575\n",
            "Epoch 91 Batch 250 Loss 0.1412 Accuracy 0.9566\n",
            "Epoch 91 Batch 300 Loss 0.1435 Accuracy 0.9559\n",
            "Epoch 91 Batch 350 Loss 0.1457 Accuracy 0.9552\n",
            "Epoch 91 Loss 0.1460 Accuracy 0.9551\n",
            "Time taken for 1 epoch: 63.52 secs\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.1498 Accuracy 0.9374\n",
            "Epoch 92 Batch 50 Loss 0.1309 Accuracy 0.9586\n",
            "Epoch 92 Batch 100 Loss 0.1342 Accuracy 0.9580\n",
            "Epoch 92 Batch 150 Loss 0.1341 Accuracy 0.9583\n",
            "Epoch 92 Batch 200 Loss 0.1402 Accuracy 0.9565\n",
            "Epoch 92 Batch 250 Loss 0.1430 Accuracy 0.9557\n",
            "Epoch 92 Batch 300 Loss 0.1434 Accuracy 0.9554\n",
            "Epoch 92 Batch 350 Loss 0.1465 Accuracy 0.9547\n",
            "Epoch 92 Loss 0.1467 Accuracy 0.9547\n",
            "Time taken for 1 epoch: 62.19 secs\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.1278 Accuracy 0.9596\n",
            "Epoch 93 Batch 50 Loss 0.1305 Accuracy 0.9600\n",
            "Epoch 93 Batch 100 Loss 0.1283 Accuracy 0.9609\n",
            "Epoch 93 Batch 150 Loss 0.1320 Accuracy 0.9594\n",
            "Epoch 93 Batch 200 Loss 0.1347 Accuracy 0.9586\n",
            "Epoch 93 Batch 250 Loss 0.1362 Accuracy 0.9583\n",
            "Epoch 93 Batch 300 Loss 0.1391 Accuracy 0.9573\n",
            "Epoch 93 Batch 350 Loss 0.1412 Accuracy 0.9567\n",
            "Epoch 93 Loss 0.1414 Accuracy 0.9566\n",
            "Time taken for 1 epoch: 63.19 secs\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.1051 Accuracy 0.9712\n",
            "Epoch 94 Batch 50 Loss 0.1323 Accuracy 0.9597\n",
            "Epoch 94 Batch 100 Loss 0.1292 Accuracy 0.9610\n",
            "Epoch 94 Batch 150 Loss 0.1329 Accuracy 0.9596\n",
            "Epoch 94 Batch 200 Loss 0.1362 Accuracy 0.9584\n",
            "Epoch 94 Batch 250 Loss 0.1375 Accuracy 0.9580\n",
            "Epoch 94 Batch 300 Loss 0.1393 Accuracy 0.9574\n",
            "Epoch 94 Batch 350 Loss 0.1411 Accuracy 0.9568\n",
            "Epoch 94 Loss 0.1411 Accuracy 0.9567\n",
            "Time taken for 1 epoch: 62.52 secs\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.1060 Accuracy 0.9682\n",
            "Epoch 95 Batch 50 Loss 0.1323 Accuracy 0.9590\n",
            "Epoch 95 Batch 100 Loss 0.1295 Accuracy 0.9602\n",
            "Epoch 95 Batch 150 Loss 0.1330 Accuracy 0.9593\n",
            "Epoch 95 Batch 200 Loss 0.1340 Accuracy 0.9589\n",
            "Epoch 95 Batch 250 Loss 0.1359 Accuracy 0.9583\n",
            "Epoch 95 Batch 300 Loss 0.1381 Accuracy 0.9575\n",
            "Epoch 95 Batch 350 Loss 0.1392 Accuracy 0.9571\n",
            "Saving checkpoint for epoch 95 at ./checkpoints/train/ckpt-19\n",
            "Epoch 95 Loss 0.1397 Accuracy 0.9569\n",
            "Time taken for 1 epoch: 65.62 secs\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.1368 Accuracy 0.9592\n",
            "Epoch 96 Batch 50 Loss 0.1304 Accuracy 0.9615\n",
            "Epoch 96 Batch 100 Loss 0.1287 Accuracy 0.9617\n",
            "Epoch 96 Batch 150 Loss 0.1300 Accuracy 0.9613\n",
            "Epoch 96 Batch 200 Loss 0.1333 Accuracy 0.9602\n",
            "Epoch 96 Batch 250 Loss 0.1366 Accuracy 0.9589\n",
            "Epoch 96 Batch 300 Loss 0.1379 Accuracy 0.9582\n",
            "Epoch 96 Batch 350 Loss 0.1377 Accuracy 0.9583\n",
            "Epoch 96 Loss 0.1377 Accuracy 0.9583\n",
            "Time taken for 1 epoch: 63.13 secs\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.1119 Accuracy 0.9650\n",
            "Epoch 97 Batch 50 Loss 0.1275 Accuracy 0.9602\n",
            "Epoch 97 Batch 100 Loss 0.1268 Accuracy 0.9612\n",
            "Epoch 97 Batch 150 Loss 0.1291 Accuracy 0.9608\n",
            "Epoch 97 Batch 200 Loss 0.1304 Accuracy 0.9600\n",
            "Epoch 97 Batch 250 Loss 0.1321 Accuracy 0.9593\n",
            "Epoch 97 Batch 300 Loss 0.1334 Accuracy 0.9588\n",
            "Epoch 97 Batch 350 Loss 0.1349 Accuracy 0.9583\n",
            "Epoch 97 Loss 0.1351 Accuracy 0.9583\n",
            "Time taken for 1 epoch: 62.56 secs\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.1425 Accuracy 0.9627\n",
            "Epoch 98 Batch 50 Loss 0.1195 Accuracy 0.9630\n",
            "Epoch 98 Batch 100 Loss 0.1247 Accuracy 0.9618\n",
            "Epoch 98 Batch 150 Loss 0.1269 Accuracy 0.9610\n",
            "Epoch 98 Batch 200 Loss 0.1284 Accuracy 0.9609\n",
            "Epoch 98 Batch 250 Loss 0.1293 Accuracy 0.9605\n",
            "Epoch 98 Batch 300 Loss 0.1306 Accuracy 0.9602\n",
            "Epoch 98 Batch 350 Loss 0.1338 Accuracy 0.9590\n",
            "Epoch 98 Loss 0.1341 Accuracy 0.9589\n",
            "Time taken for 1 epoch: 62.73 secs\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.0851 Accuracy 0.9724\n",
            "Epoch 99 Batch 50 Loss 0.1180 Accuracy 0.9638\n",
            "Epoch 99 Batch 100 Loss 0.1220 Accuracy 0.9623\n",
            "Epoch 99 Batch 150 Loss 0.1229 Accuracy 0.9621\n",
            "Epoch 99 Batch 200 Loss 0.1235 Accuracy 0.9621\n",
            "Epoch 99 Batch 250 Loss 0.1249 Accuracy 0.9612\n",
            "Epoch 99 Batch 300 Loss 0.1277 Accuracy 0.9604\n",
            "Epoch 99 Batch 350 Loss 0.1303 Accuracy 0.9595\n",
            "Epoch 99 Loss 0.1302 Accuracy 0.9596\n",
            "Time taken for 1 epoch: 62.94 secs\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.1007 Accuracy 0.9622\n",
            "Epoch 100 Batch 50 Loss 0.1194 Accuracy 0.9634\n",
            "Epoch 100 Batch 100 Loss 0.1199 Accuracy 0.9630\n",
            "Epoch 100 Batch 150 Loss 0.1231 Accuracy 0.9620\n",
            "Epoch 100 Batch 200 Loss 0.1250 Accuracy 0.9612\n",
            "Epoch 100 Batch 250 Loss 0.1270 Accuracy 0.9607\n",
            "Epoch 100 Batch 300 Loss 0.1266 Accuracy 0.9609\n",
            "Epoch 100 Batch 350 Loss 0.1283 Accuracy 0.9603\n",
            "Saving checkpoint for epoch 100 at ./checkpoints/train/ckpt-20\n",
            "Epoch 100 Loss 0.1280 Accuracy 0.9603\n",
            "Time taken for 1 epoch: 65.38 secs\n",
            "\n",
            "Epoch 101 Batch 0 Loss 0.1198 Accuracy 0.9579\n",
            "Epoch 101 Batch 50 Loss 0.1143 Accuracy 0.9650\n",
            "Epoch 101 Batch 100 Loss 0.1148 Accuracy 0.9648\n",
            "Epoch 101 Batch 150 Loss 0.1190 Accuracy 0.9636\n",
            "Epoch 101 Batch 200 Loss 0.1225 Accuracy 0.9630\n",
            "Epoch 101 Batch 250 Loss 0.1236 Accuracy 0.9625\n",
            "Epoch 101 Batch 300 Loss 0.1253 Accuracy 0.9619\n",
            "Epoch 101 Batch 350 Loss 0.1257 Accuracy 0.9616\n",
            "Epoch 101 Loss 0.1256 Accuracy 0.9616\n",
            "Time taken for 1 epoch: 63.19 secs\n",
            "\n",
            "Epoch 102 Batch 0 Loss 0.1113 Accuracy 0.9740\n",
            "Epoch 102 Batch 50 Loss 0.1206 Accuracy 0.9627\n",
            "Epoch 102 Batch 100 Loss 0.1199 Accuracy 0.9631\n",
            "Epoch 102 Batch 150 Loss 0.1196 Accuracy 0.9631\n",
            "Epoch 102 Batch 200 Loss 0.1197 Accuracy 0.9634\n",
            "Epoch 102 Batch 250 Loss 0.1215 Accuracy 0.9628\n",
            "Epoch 102 Batch 300 Loss 0.1230 Accuracy 0.9621\n",
            "Epoch 102 Batch 350 Loss 0.1250 Accuracy 0.9613\n",
            "Epoch 102 Loss 0.1250 Accuracy 0.9612\n",
            "Time taken for 1 epoch: 62.45 secs\n",
            "\n",
            "Epoch 103 Batch 0 Loss 0.0920 Accuracy 0.9695\n",
            "Epoch 103 Batch 50 Loss 0.1126 Accuracy 0.9650\n",
            "Epoch 103 Batch 100 Loss 0.1151 Accuracy 0.9643\n",
            "Epoch 103 Batch 150 Loss 0.1190 Accuracy 0.9633\n",
            "Epoch 103 Batch 200 Loss 0.1204 Accuracy 0.9630\n",
            "Epoch 103 Batch 250 Loss 0.1221 Accuracy 0.9625\n",
            "Epoch 103 Batch 300 Loss 0.1236 Accuracy 0.9621\n",
            "Epoch 103 Batch 350 Loss 0.1251 Accuracy 0.9617\n",
            "Epoch 103 Loss 0.1255 Accuracy 0.9616\n",
            "Time taken for 1 epoch: 62.29 secs\n",
            "\n",
            "Epoch 104 Batch 0 Loss 0.1315 Accuracy 0.9545\n",
            "Epoch 104 Batch 50 Loss 0.1164 Accuracy 0.9654\n",
            "Epoch 104 Batch 100 Loss 0.1177 Accuracy 0.9643\n",
            "Epoch 104 Batch 150 Loss 0.1181 Accuracy 0.9643\n",
            "Epoch 104 Batch 200 Loss 0.1190 Accuracy 0.9639\n",
            "Epoch 104 Batch 250 Loss 0.1196 Accuracy 0.9639\n",
            "Epoch 104 Batch 300 Loss 0.1208 Accuracy 0.9633\n",
            "Epoch 104 Batch 350 Loss 0.1217 Accuracy 0.9630\n",
            "Epoch 104 Loss 0.1218 Accuracy 0.9630\n",
            "Time taken for 1 epoch: 63.08 secs\n",
            "\n",
            "Epoch 105 Batch 0 Loss 0.1087 Accuracy 0.9622\n",
            "Epoch 105 Batch 50 Loss 0.1097 Accuracy 0.9663\n",
            "Epoch 105 Batch 100 Loss 0.1137 Accuracy 0.9653\n",
            "Epoch 105 Batch 150 Loss 0.1164 Accuracy 0.9645\n",
            "Epoch 105 Batch 200 Loss 0.1197 Accuracy 0.9632\n",
            "Epoch 105 Batch 250 Loss 0.1196 Accuracy 0.9631\n",
            "Epoch 105 Batch 300 Loss 0.1215 Accuracy 0.9626\n",
            "Epoch 105 Batch 350 Loss 0.1218 Accuracy 0.9627\n",
            "Saving checkpoint for epoch 105 at ./checkpoints/train/ckpt-21\n",
            "Epoch 105 Loss 0.1219 Accuracy 0.9626\n",
            "Time taken for 1 epoch: 65.50 secs\n",
            "\n",
            "Epoch 106 Batch 0 Loss 0.1046 Accuracy 0.9623\n",
            "Epoch 106 Batch 50 Loss 0.1088 Accuracy 0.9663\n",
            "Epoch 106 Batch 100 Loss 0.1115 Accuracy 0.9660\n",
            "Epoch 106 Batch 150 Loss 0.1140 Accuracy 0.9650\n",
            "Epoch 106 Batch 200 Loss 0.1165 Accuracy 0.9641\n",
            "Epoch 106 Batch 250 Loss 0.1159 Accuracy 0.9645\n",
            "Epoch 106 Batch 300 Loss 0.1174 Accuracy 0.9639\n",
            "Epoch 106 Batch 350 Loss 0.1180 Accuracy 0.9636\n",
            "Epoch 106 Loss 0.1180 Accuracy 0.9636\n",
            "Time taken for 1 epoch: 63.43 secs\n",
            "\n",
            "Epoch 107 Batch 0 Loss 0.0801 Accuracy 0.9770\n",
            "Epoch 107 Batch 50 Loss 0.1103 Accuracy 0.9658\n",
            "Epoch 107 Batch 100 Loss 0.1066 Accuracy 0.9671\n",
            "Epoch 107 Batch 150 Loss 0.1101 Accuracy 0.9661\n",
            "Epoch 107 Batch 200 Loss 0.1132 Accuracy 0.9654\n",
            "Epoch 107 Batch 250 Loss 0.1147 Accuracy 0.9649\n",
            "Epoch 107 Batch 300 Loss 0.1160 Accuracy 0.9644\n",
            "Epoch 107 Batch 350 Loss 0.1171 Accuracy 0.9640\n",
            "Epoch 107 Loss 0.1174 Accuracy 0.9639\n",
            "Time taken for 1 epoch: 62.49 secs\n",
            "\n",
            "Epoch 108 Batch 0 Loss 0.1401 Accuracy 0.9657\n",
            "Epoch 108 Batch 50 Loss 0.1065 Accuracy 0.9687\n",
            "Epoch 108 Batch 100 Loss 0.1068 Accuracy 0.9681\n",
            "Epoch 108 Batch 150 Loss 0.1104 Accuracy 0.9668\n",
            "Epoch 108 Batch 200 Loss 0.1127 Accuracy 0.9659\n",
            "Epoch 108 Batch 250 Loss 0.1140 Accuracy 0.9656\n",
            "Epoch 108 Batch 300 Loss 0.1155 Accuracy 0.9652\n",
            "Epoch 108 Batch 350 Loss 0.1162 Accuracy 0.9648\n",
            "Epoch 108 Loss 0.1163 Accuracy 0.9648\n",
            "Time taken for 1 epoch: 62.86 secs\n",
            "\n",
            "Epoch 109 Batch 0 Loss 0.1042 Accuracy 0.9698\n",
            "Epoch 109 Batch 50 Loss 0.1160 Accuracy 0.9651\n",
            "Epoch 109 Batch 100 Loss 0.1146 Accuracy 0.9656\n",
            "Epoch 109 Batch 150 Loss 0.1125 Accuracy 0.9663\n",
            "Epoch 109 Batch 200 Loss 0.1134 Accuracy 0.9660\n",
            "Epoch 109 Batch 250 Loss 0.1136 Accuracy 0.9660\n",
            "Epoch 109 Batch 300 Loss 0.1142 Accuracy 0.9656\n",
            "Epoch 109 Batch 350 Loss 0.1138 Accuracy 0.9656\n",
            "Epoch 109 Loss 0.1140 Accuracy 0.9656\n",
            "Time taken for 1 epoch: 63.02 secs\n",
            "\n",
            "Epoch 110 Batch 0 Loss 0.1147 Accuracy 0.9683\n",
            "Epoch 110 Batch 50 Loss 0.1029 Accuracy 0.9698\n",
            "Epoch 110 Batch 100 Loss 0.1017 Accuracy 0.9699\n",
            "Epoch 110 Batch 150 Loss 0.1057 Accuracy 0.9684\n",
            "Epoch 110 Batch 200 Loss 0.1066 Accuracy 0.9679\n",
            "Epoch 110 Batch 250 Loss 0.1078 Accuracy 0.9672\n",
            "Epoch 110 Batch 300 Loss 0.1101 Accuracy 0.9665\n",
            "Epoch 110 Batch 350 Loss 0.1123 Accuracy 0.9659\n",
            "Saving checkpoint for epoch 110 at ./checkpoints/train/ckpt-22\n",
            "Epoch 110 Loss 0.1124 Accuracy 0.9659\n",
            "Time taken for 1 epoch: 65.37 secs\n",
            "\n",
            "Epoch 111 Batch 0 Loss 0.1324 Accuracy 0.9656\n",
            "Epoch 111 Batch 50 Loss 0.1003 Accuracy 0.9694\n",
            "Epoch 111 Batch 100 Loss 0.1024 Accuracy 0.9691\n",
            "Epoch 111 Batch 150 Loss 0.1040 Accuracy 0.9685\n",
            "Epoch 111 Batch 200 Loss 0.1064 Accuracy 0.9676\n",
            "Epoch 111 Batch 250 Loss 0.1077 Accuracy 0.9671\n",
            "Epoch 111 Batch 300 Loss 0.1084 Accuracy 0.9668\n",
            "Epoch 111 Batch 350 Loss 0.1098 Accuracy 0.9664\n",
            "Epoch 111 Loss 0.1100 Accuracy 0.9663\n",
            "Time taken for 1 epoch: 63.02 secs\n",
            "\n",
            "Epoch 112 Batch 0 Loss 0.0959 Accuracy 0.9638\n",
            "Epoch 112 Batch 50 Loss 0.0989 Accuracy 0.9702\n",
            "Epoch 112 Batch 100 Loss 0.1009 Accuracy 0.9699\n",
            "Epoch 112 Batch 150 Loss 0.1036 Accuracy 0.9689\n",
            "Epoch 112 Batch 200 Loss 0.1065 Accuracy 0.9678\n",
            "Epoch 112 Batch 250 Loss 0.1073 Accuracy 0.9673\n",
            "Epoch 112 Batch 300 Loss 0.1100 Accuracy 0.9666\n",
            "Epoch 112 Batch 350 Loss 0.1105 Accuracy 0.9665\n",
            "Epoch 112 Loss 0.1106 Accuracy 0.9664\n",
            "Time taken for 1 epoch: 63.16 secs\n",
            "\n",
            "Epoch 113 Batch 0 Loss 0.0726 Accuracy 0.9794\n",
            "Epoch 113 Batch 50 Loss 0.1043 Accuracy 0.9682\n",
            "Epoch 113 Batch 100 Loss 0.1088 Accuracy 0.9668\n",
            "Epoch 113 Batch 150 Loss 0.1098 Accuracy 0.9666\n",
            "Epoch 113 Batch 200 Loss 0.1088 Accuracy 0.9669\n",
            "Epoch 113 Batch 250 Loss 0.1096 Accuracy 0.9669\n",
            "Epoch 113 Batch 300 Loss 0.1115 Accuracy 0.9663\n",
            "Epoch 113 Batch 350 Loss 0.1118 Accuracy 0.9661\n",
            "Epoch 113 Loss 0.1118 Accuracy 0.9661\n",
            "Time taken for 1 epoch: 62.42 secs\n",
            "\n",
            "Epoch 114 Batch 0 Loss 0.0857 Accuracy 0.9742\n",
            "Epoch 114 Batch 50 Loss 0.0978 Accuracy 0.9706\n",
            "Epoch 114 Batch 100 Loss 0.0989 Accuracy 0.9697\n",
            "Epoch 114 Batch 150 Loss 0.0995 Accuracy 0.9695\n",
            "Epoch 114 Batch 200 Loss 0.1016 Accuracy 0.9686\n",
            "Epoch 114 Batch 250 Loss 0.1035 Accuracy 0.9682\n",
            "Epoch 114 Batch 300 Loss 0.1040 Accuracy 0.9680\n",
            "Epoch 114 Batch 350 Loss 0.1053 Accuracy 0.9677\n",
            "Epoch 114 Loss 0.1053 Accuracy 0.9677\n",
            "Time taken for 1 epoch: 62.94 secs\n",
            "\n",
            "Epoch 115 Batch 0 Loss 0.1247 Accuracy 0.9615\n",
            "Epoch 115 Batch 50 Loss 0.0969 Accuracy 0.9705\n",
            "Epoch 115 Batch 100 Loss 0.0974 Accuracy 0.9704\n",
            "Epoch 115 Batch 150 Loss 0.0990 Accuracy 0.9697\n",
            "Epoch 115 Batch 200 Loss 0.1023 Accuracy 0.9686\n",
            "Epoch 115 Batch 250 Loss 0.1051 Accuracy 0.9679\n",
            "Epoch 115 Batch 300 Loss 0.1068 Accuracy 0.9672\n",
            "Epoch 115 Batch 350 Loss 0.1085 Accuracy 0.9667\n",
            "Saving checkpoint for epoch 115 at ./checkpoints/train/ckpt-23\n",
            "Epoch 115 Loss 0.1086 Accuracy 0.9667\n",
            "Time taken for 1 epoch: 65.52 secs\n",
            "\n",
            "Epoch 116 Batch 0 Loss 0.0939 Accuracy 0.9671\n",
            "Epoch 116 Batch 50 Loss 0.0965 Accuracy 0.9719\n",
            "Epoch 116 Batch 100 Loss 0.0975 Accuracy 0.9709\n",
            "Epoch 116 Batch 150 Loss 0.0994 Accuracy 0.9700\n",
            "Epoch 116 Batch 200 Loss 0.1006 Accuracy 0.9694\n",
            "Epoch 116 Batch 250 Loss 0.1023 Accuracy 0.9689\n",
            "Epoch 116 Batch 300 Loss 0.1023 Accuracy 0.9690\n",
            "Epoch 116 Batch 350 Loss 0.1034 Accuracy 0.9686\n",
            "Epoch 116 Loss 0.1039 Accuracy 0.9685\n",
            "Time taken for 1 epoch: 62.96 secs\n",
            "\n",
            "Epoch 117 Batch 0 Loss 0.0815 Accuracy 0.9795\n",
            "Epoch 117 Batch 50 Loss 0.0909 Accuracy 0.9729\n",
            "Epoch 117 Batch 100 Loss 0.0988 Accuracy 0.9702\n",
            "Epoch 117 Batch 150 Loss 0.1024 Accuracy 0.9687\n",
            "Epoch 117 Batch 200 Loss 0.1017 Accuracy 0.9691\n",
            "Epoch 117 Batch 250 Loss 0.1019 Accuracy 0.9691\n",
            "Epoch 117 Batch 300 Loss 0.1027 Accuracy 0.9688\n",
            "Epoch 117 Batch 350 Loss 0.1034 Accuracy 0.9686\n",
            "Epoch 117 Loss 0.1033 Accuracy 0.9686\n",
            "Time taken for 1 epoch: 63.18 secs\n",
            "\n",
            "Epoch 118 Batch 0 Loss 0.0660 Accuracy 0.9792\n",
            "Epoch 118 Batch 50 Loss 0.0959 Accuracy 0.9703\n",
            "Epoch 118 Batch 100 Loss 0.0957 Accuracy 0.9705\n",
            "Epoch 118 Batch 150 Loss 0.0967 Accuracy 0.9703\n",
            "Epoch 118 Batch 200 Loss 0.0981 Accuracy 0.9698\n",
            "Epoch 118 Batch 250 Loss 0.0995 Accuracy 0.9695\n",
            "Epoch 118 Batch 300 Loss 0.1009 Accuracy 0.9690\n",
            "Epoch 118 Batch 350 Loss 0.1016 Accuracy 0.9688\n",
            "Epoch 118 Loss 0.1019 Accuracy 0.9686\n",
            "Time taken for 1 epoch: 62.83 secs\n",
            "\n",
            "Epoch 119 Batch 0 Loss 0.0707 Accuracy 0.9794\n",
            "Epoch 119 Batch 50 Loss 0.1008 Accuracy 0.9690\n",
            "Epoch 119 Batch 100 Loss 0.0949 Accuracy 0.9717\n",
            "Epoch 119 Batch 150 Loss 0.0982 Accuracy 0.9706\n",
            "Epoch 119 Batch 200 Loss 0.0981 Accuracy 0.9705\n",
            "Epoch 119 Batch 250 Loss 0.0992 Accuracy 0.9700\n",
            "Epoch 119 Batch 300 Loss 0.1005 Accuracy 0.9696\n",
            "Epoch 119 Batch 350 Loss 0.1010 Accuracy 0.9693\n",
            "Epoch 119 Loss 0.1010 Accuracy 0.9694\n",
            "Time taken for 1 epoch: 63.15 secs\n",
            "\n",
            "Epoch 120 Batch 0 Loss 0.0938 Accuracy 0.9728\n",
            "Epoch 120 Batch 50 Loss 0.0854 Accuracy 0.9743\n",
            "Epoch 120 Batch 100 Loss 0.0911 Accuracy 0.9724\n",
            "Epoch 120 Batch 150 Loss 0.0960 Accuracy 0.9709\n",
            "Epoch 120 Batch 200 Loss 0.0981 Accuracy 0.9702\n",
            "Epoch 120 Batch 250 Loss 0.1000 Accuracy 0.9696\n",
            "Epoch 120 Batch 300 Loss 0.0999 Accuracy 0.9695\n",
            "Epoch 120 Batch 350 Loss 0.1013 Accuracy 0.9690\n",
            "Saving checkpoint for epoch 120 at ./checkpoints/train/ckpt-24\n",
            "Epoch 120 Loss 0.1015 Accuracy 0.9690\n",
            "Time taken for 1 epoch: 65.69 secs\n",
            "\n",
            "CPU times: user 1h 17min 20s, sys: 17min 12s, total: 1h 34min 32s\n",
            "Wall time: 2h 7min 26s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Run training! Each epoch takes several mins with GPU\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  # inp -> twi, tar -> french\n",
        "  for (batch, (inp,tar)) in enumerate(train_batches):\n",
        "    train_step(inp, tar)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aij6BWpIBU52"
      },
      "source": [
        "# Build a Translator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "z0clY8WkaGZZ"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.Module):\n",
        "    def __init__(self, tokenizers, transformer):\n",
        "        self.tokenizers = tokenizers\n",
        "        self.transformer = transformer\n",
        "\n",
        "    def __call__(self, sentence, max_length=MAX_TOKENS):\n",
        "        # The input sentence is English, hence adding the `[START]` and `[END]` tokens.\n",
        "        sentence = tf.convert_to_tensor([sentence])\n",
        "        sentence = self.tokenizers.eng.tokenize(sentence).to_tensor()\n",
        "\n",
        "        encoder_input = sentence\n",
        "\n",
        "        # As the output language is TWI, initialize the output with the\n",
        "        # English `[START]` token.\n",
        "        start_end = self.tokenizers.twi.tokenize([''])[0]\n",
        "        start = start_end[0][tf.newaxis]\n",
        "        end = start_end[1][tf.newaxis]\n",
        "\n",
        "        # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "        # dynamic-loop can be traced by `tf.function`.\n",
        "        output_array = tf.TensorArray(\n",
        "            dtype=tf.int64, size=0, dynamic_size=True)\n",
        "        output_array = output_array.write(0, start)\n",
        "        output = tf.transpose(output_array.stack())\n",
        "\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "                encoder_input, output)\n",
        "\n",
        "        for i in tf.range(max_length):\n",
        "            output = tf.transpose(output_array.stack())\n",
        "\n",
        "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "                encoder_input, output)\n",
        "\n",
        "            predictions, attention_weights = self.transformer(\n",
        "                encoder_input, output, False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "\n",
        "            # Select the last token from the `seq_len` dimension.\n",
        "            # Shape `(batch_size, 1, vocab_size)`.\n",
        "            predictions = predictions[:, -1:, :]\n",
        "\n",
        "            predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "            # Concatenate the `predicted_id` to the output which is given to the\n",
        "            # decoder as its input.\n",
        "            output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "            if predicted_id == end:\n",
        "                break\n",
        "\n",
        "        output = tf.transpose(output_array.stack())\n",
        "        # The output shape is `(1, tokens)`.\n",
        "        text = self.tokenizers.twi.detokenize(output)[0]  # Shape: `()`.\n",
        "\n",
        "        tokens = self.tokenizers.twi.lookup(output)[0]\n",
        "        _, attention_weights = self.transformer(\n",
        "            encoder_input, output[:, :-1], False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "\n",
        "        return text, tokens, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Tt1OVm3ecSNO"
      },
      "outputs": [],
      "source": [
        "#Create an instance of this Translator class\n",
        "translator = Translator(tokenizers, transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dMj9N3fBh1P"
      },
      "source": [
        "## translate example sentecnes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "ysGcUmAzc_Yi"
      },
      "outputs": [],
      "source": [
        "def print_translation(sentence, tokens, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "wKmyWMcGduft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e227b037-bc00-4499-8487-8f9d2ef23ad7"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:         : i ran to your brother on the street .\n",
            "Prediction     : mituu mmirika kɔɔ wo nua no nkyɛn wɔ abɔnten so .\n",
            "Ground truth   : mituu mmirika kɔɔ wo nua no nkyɛn wɔ abɔnten so .\n"
          ]
        }
      ],
      "source": [
        "sentence =\"i ran to your brother on the street .\"\n",
        "ground_truth=\"mituu mmirika kɔɔ wo nua no nkyɛn wɔ abɔnten so .\"\n",
        "translated_text, translated_tokens, attention_weights = translator(sentence)\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "DYL7f6tu9Y0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46c2d568-2163-4f88-d0d2-8ffa0d5540ef"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:         : i want to ask you a question .\n",
            "Prediction     : mepɛ sɛ mibisa wo asɛm bi .\n",
            "Ground truth   : mepɛ sɛ mibisa wo asɛm bi .\n"
          ]
        }
      ],
      "source": [
        "ground_truth=\"mepɛ sɛ mibisa wo asɛm bi .\"\n",
        "sentence=\"i want to ask you a question .\"\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBe9E_dXBzne"
      },
      "source": [
        "# Save Translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "qMpmh77M9ufZ"
      },
      "outputs": [],
      "source": [
        "# class to export translator\n",
        "class ExportTranslator(tf.Module):\n",
        "  def __init__(self, translator):\n",
        "    self.translator = translator\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def __call__(self, sentence):\n",
        "    (result,\n",
        "     tokens,\n",
        "     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "89tEraoTixKG"
      },
      "outputs": [],
      "source": [
        "translator = ExportTranslator(translator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "1ZurB6qDFDzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80a4593c-6ab4-481a-a7c7-1226dd00eed0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "b'mpena y\\xc9\\x9b \\xc9\\x94man bi .'"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "translator(tf.constant(\"mepɛ sɛ minya afoforo anim dom .\")).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "YMhMt9_z2ZXH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dd31493-9757-4a52-95ca-5399b47652c6"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving model...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn while saving (showing 5 of 332). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "module_no_signatures_path = '/content/drive/MyDrive/twi_english_translator'\n",
        "print('Saving model...')\n",
        "tf.saved_model.save(translator, module_no_signatures_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "P0jO3gtH9bih"
      },
      "outputs": [],
      "source": [
        "reloaded = tf.saved_model.load(module_no_signatures_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "adjW9Tug-ZOa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6fee039a-20f6-462d-d8c9-00ba141a57cb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'kɔmputa no wɔ nhomakorabea hɔ .'"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reloaded('the computer is in the library .').numpy().decode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbDLGyb8clxZ"
      },
      "source": [
        "#BLEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "W8k01SazA8Cl"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "\n",
        "class Bleu():\n",
        "    def __init__(self, translator):\n",
        "        self.translator = translator\n",
        "\n",
        "    def get_bleuscore(self, testfile, referencefile, smothingfunction=None):\n",
        "        if type(testfile) == str and type(referencefile) == str:\n",
        "            # Open test file and read lines\n",
        "            f = open(testfile, \"r\")\n",
        "            hypothesis = f.readlines()\n",
        "            f.close()\n",
        "            # open refernce file and read lines\n",
        "            f = open(referencefile, \"r\")\n",
        "            reference = f.readlines()\n",
        "            f.close()\n",
        "        elif type(testfile) == list and type(referencefile) == list:\n",
        "            hypothesis = testfile\n",
        "            reference = referencefile\n",
        "        else:\n",
        "            print(f'File must be txt or python list')\n",
        "\n",
        "        # check the length of our input sentence\n",
        "        length = len(hypothesis)\n",
        "        bleu_total = np.array([0., 0., 0.])\n",
        "        weights = [(1./2, 1./2), (1./3, 1./3, 1./3),\n",
        "                   (1./4, 1./4, 1./4, 1./4)]\n",
        "        for i in range(length):\n",
        "            hypothesis[i] = hypothesis[i]\n",
        "            reference[i] = reference[i]\n",
        "            groundtruth = reference[i].lower().replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            groundtruth = [groundtruth]\n",
        "            translated_text = self.translator(\n",
        "                hypothesis[i]).numpy().decode(\"utf-8\")\n",
        "            # print(\"Translated Text: \", translated_text)\n",
        "            # print(\"Ground Truth: \", reference[i])\n",
        "            candidate = translated_text.replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            bleu = np.array(sentence_bleu(\n",
        "                groundtruth, candidate, weights, smoothing_function=smothingfunction))\n",
        "            bleu_total += bleu\n",
        "\n",
        "        return f'2-GRAMS: {bleu_total[0]/length}',f'3-GRAMS: {bleu_total[1]/length}',f'4-GRAMS: {bleu_total[2]/length}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "TYHXXY10dMh5"
      },
      "outputs": [],
      "source": [
        "#iNSTANTIATE OBJECT OF BLEU CLASS AND IT SMOOTHING FUNCTION\n",
        "smooth= SmoothingFunction()\n",
        "bleu = Bleu(reloaded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "g2y6aHDfdbZK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8def32b0-e602-4355-e26e-8d5ff7868dd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 19min 23s, sys: 2min, total: 21min 23s\n",
            "Wall time: 13min 2s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2-GRAMS: 0.4048235573660457',\n",
              " '3-GRAMS: 0.299394570975767',\n",
              " '4-GRAMS: 0.22097535154160952')"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "# ESTIMATE BLEU SCORE FROM THE TEST DATA\n",
        "# from list\n",
        "%%time\n",
        "bleu.get_bleuscore(en_test,twi_test, smooth.method7)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GOOGLE *API* BLEU\n"
      ],
      "metadata": {
        "id": "VRgT8luKPPuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use google API for bidirectional pivot translation of Twi and French\n",
        "# pivot language = English\n",
        "# import libraries\n",
        "from googletrans import Translator, constants\n",
        "# instantiate a translator object\n",
        "# initiate translator object\n",
        "translator = Translator()\n",
        "# Add Akan to the language supported by this package\n",
        "# Note the googletrans package has not  been updated to capture the new additions by google since May 2022\n",
        "# from https://translate.google.com/?sl=en&tl=ak&op=translate , the key and value for Twi is 'ak':'akan'\n",
        "constants.LANGUAGES['ak'] = 'akan'\n",
        "\n",
        "\n",
        "class GooglePivot:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def evaluate(self, sentences, src_key, dest_key):\n",
        "        eng_text = translator.translate(sentences, src=src_key, dest='en').text\n",
        "        print(eng_text)\n",
        "        output = translator.translate(eng_text, dest=dest_key).text\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class GoogleDirect:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def evaluate(self, sentences, src_key, dest_key):\n",
        "\n",
        "        return translator.translate(sentences, src=src_key, dest=dest_key).text"
      ],
      "metadata": {
        "id": "836KH3HJPNyW"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "\n",
        "class GoogleBleu():\n",
        "    def __init__(self, translator):\n",
        "        self.translator = translator\n",
        "\n",
        "    def get_bleuscore(self, testfile, referencefile,src_lang,dest_lang, smothingfunction=None):\n",
        "        if type(testfile) == str and type(referencefile) == str:\n",
        "            # Open test file and read lines\n",
        "            f = open(testfile, \"r\")\n",
        "            hypothesis = f.readlines()\n",
        "            f.close()\n",
        "            # open refernce file and read lines\n",
        "            f = open(referencefile, \"r\")\n",
        "            reference = f.readlines()\n",
        "            f.close()\n",
        "        elif type(testfile) == list and type(referencefile) == list:\n",
        "            hypothesis = testfile\n",
        "            reference = referencefile\n",
        "        else:\n",
        "            print(f'File must be txt or python list')\n",
        "\n",
        "        # check the length of our input sentence\n",
        "        length = len(hypothesis)\n",
        "        bleu_total = np.array([0., 0., 0.])\n",
        "        weights = [(1./2, 1./2), (1./3, 1./3, 1./3),\n",
        "                   (1./4, 1./4, 1./4, 1./4)]\n",
        "        for i in range(length):\n",
        "            hypothesis[i] = hypothesis[i]\n",
        "            reference[i] = reference[i]\n",
        "            groundtruth = reference[i].lower().replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            groundtruth = [groundtruth]\n",
        "            translated_text = self.translator.evaluate(hypothesis[i],src_key=src_lang, dest_key=dest_lang)\n",
        "            # print(\"Translated Text: \", translated_text)\n",
        "            # print(\"Ground Truth: \", reference[i])\n",
        "            candidate = translated_text.replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            bleu = np.array(sentence_bleu(\n",
        "                groundtruth, candidate, weights, smoothing_function=smothingfunction))\n",
        "            bleu_total += bleu\n",
        "\n",
        "        return f'2-GRAMS: {bleu_total[0]/length}',f'3-GRAMS: {bleu_total[1]/length}',f'4-GRAMS: {bleu_total[2]/length}'"
      ],
      "metadata": {
        "id": "QHT4GJc7Sqpb"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate GoogleDirect class\n",
        "google_translate = GoogleDirect()"
      ],
      "metadata": {
        "id": "DxmDJjRkQLiI"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "google_bleu = GoogleBleu(google_translate)"
      ],
      "metadata": {
        "id": "UHjJ24E4RQ22"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "google_bleu.get_bleuscore(en_test, twi_test,'en','ak',smooth.method7)\n"
      ],
      "metadata": {
        "id": "v8QDG1k8UMhp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f6da459-3350-4cea-a361-b364b14eb570"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 7.17 s, sys: 345 ms, total: 7.52 s\n",
            "Wall time: 5min 7s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2-GRAMS: 0.6066393340660294',\n",
              " '3-GRAMS: 0.48412228524019213',\n",
              " '4-GRAMS: 0.38535013926203615')"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6G85u42AxVHt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}