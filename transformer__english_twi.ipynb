{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/transformer__english_twi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qFzQxA29DJp"
      },
      "source": [
        "This notebook is based on the implementation of the paper [Attention is All you Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017). The code are adapted from the Tenssorflow repository at https://www.tensorflow.org/text/tutorials/transformer#build_the_transformer and a big thanks to [[Crater David]](https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:37370749) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hblEgiSfLEck"
      },
      "source": [
        "# Install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naWgZqfLsOiN",
        "outputId": "299a6d2e-5a4d-49b8-f067-cb97940e8fbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 5.9 MB 4.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 582.0 MB 15 kB/s \n",
            "\u001b[K     |████████████████████████████████| 439 kB 71.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 52.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 55.9 MB/s \n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting hstspreload\n",
            "  Downloading hstspreload-2022.9.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.3 MB/s \n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2022.6.15)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Collecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16367 sha256=55a7d4bf9c42793c24c1d01153d682aedcd4abdbc7ab2f5420cb2aada3f9484d\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/be/fe/93a6a40ffe386e16089e44dad9018ebab9dc4cb9eb7eab65ae\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hyperframe, hpack, sniffio, h2, h11, rfc3986, httpcore, hstspreload, httpx, googletrans\n",
            "Successfully installed googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2022.9.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U tensorflow-text-nightly==2.11.0.dev20220817\n",
        "!pip install -U -q tf-nightly==2.11.0.dev20220817\n",
        "!pip3 install googletrans==3.1.0a0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY7seog2LQOL"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gwubBvId4uGK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W18p7g2lLatA"
      },
      "source": [
        "# preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ovdl-UER8hMX"
      },
      "outputs": [],
      "source": [
        "# This code was adapted from https://github.com/GhanaNLP/kasa/blob/master/Kasa/Preprocessing.py\n",
        "# A subclass of the kasafrench for preprocessing data\n",
        "# import required library\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "class Preprocessing:\n",
        "    # dummy initialization method\n",
        "    def __init__(self):\n",
        "        # initialize with some default parameters here later\n",
        "        pass\n",
        "\n",
        "    # read in parallel twi - english dataset\n",
        "    def read_parallel_dataset(self, filepath_1, filepath_2, filepath_3=None):\n",
        "        if filepath_3 != None:\n",
        "            # read first language data\n",
        "            lang_1 = []\n",
        "            with open(filepath_1, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_1.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            # read second language data\n",
        "            lang_2 = []\n",
        "            with open(filepath_2, encoding='utf-8') as file:\n",
        "\n",
        "                # twi=file.read()\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_2.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "            # Read third Language data\n",
        "            lang_3 = []\n",
        "            with open(filepath_3, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_3.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            return lang_1, lang_2, lang_3\n",
        "            \n",
        "        else:\n",
        "            # read first language data\n",
        "            lang_1 = []\n",
        "            with open(filepath_1, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_1.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            # read second language data\n",
        "            lang_2 = []\n",
        "            with open(filepath_2, encoding='utf-8') as file:\n",
        "\n",
        "                # twi=file.read()\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_2.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            return lang_1, lang_2\n",
        "\n",
        "    # Define a helper function to remove string accents\n",
        "\n",
        "    def removeStringAccent(self, s):\n",
        "        return ''.join(\n",
        "            c for c in unicodedata.normalize('NFD', s)\n",
        "            if unicodedata.category(c) != 'Mn'\n",
        "        )\n",
        "\n",
        "    # normalize input twi sentence\n",
        "    def normalize_twi(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = s.lower()\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.ƆɔɛƐ!?’]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "\n",
        "    # normalize input french sentence\n",
        "    def normalize_FrEn(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = s.lower()\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "    \n",
        "    def writeTotxt(self,destination,data):\n",
        "        with open(destination, 'w') as f:\n",
        "            for line in data:\n",
        "                 f.write(f\"{line}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "B4asafNW9HIt"
      },
      "outputs": [],
      "source": [
        "# import preprocessing class \n",
        "# Create an instance of tft preprocessing class\n",
        "preprocessor = Preprocessing()\n",
        "\n",
        "# Read raw parallel dataset\n",
        "raw_data_twi, raw_data_en = preprocessor.read_parallel_dataset(\n",
        "    filepath_1='/content/drive/MyDrive/verified_twi.txt',\n",
        "    filepath_2='/content/drive/MyDrive/verified_english.txt'\n",
        ")\n",
        "\n",
        "# Normalize the raw data\n",
        "raw_data_en = [preprocessor.normalize_FrEn(data) for data in raw_data_en]\n",
        "raw_data_twi = [preprocessor.normalize_twi(data) for data in raw_data_twi]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au81TkNOLwnB"
      },
      "source": [
        "# split data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5l08UvTL9mtx"
      },
      "outputs": [],
      "source": [
        "# split the dataset into training and test sets\n",
        "# Keep 10% of the data as test set\n",
        "train_twi,test_twi,train_en,test_en = train_test_split(raw_data_twi,raw_data_en, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3q4djANP-9g6"
      },
      "outputs": [],
      "source": [
        "# write the preprocess traning and test dataset to a file\n",
        "preprocessor.writeTotxt('train_twi.txt',train_twi)\n",
        "preprocessor.writeTotxt('train_en.txt',train_en)\n",
        "preprocessor.writeTotxt('test_twi.txt',test_twi)\n",
        "preprocessor.writeTotxt('test_en.txt',test_en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao2l87AzL_kE"
      },
      "source": [
        "# Build tf dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0WZrSEP9_ISJ"
      },
      "outputs": [],
      "source": [
        "# build tf datasets from traning and test sentences in both languages\n",
        "train_dataset_en = tf.data.TextLineDataset('/content/train_en.txt')\n",
        "train_dataset_tw = tf.data.TextLineDataset('/content/train_twi.txt')\n",
        "\n",
        "val_dataset_en = tf.data.TextLineDataset('/content/test_en.txt')\n",
        "val_dataset_tw = tf.data.TextLineDataset('/content/test_twi.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mmksbF-CAEXg"
      },
      "outputs": [],
      "source": [
        "# combine languages into single dataset\n",
        "trained_combined = tf.data.Dataset.zip((train_dataset_en, train_dataset_tw))\n",
        "val_combined = tf.data.Dataset.zip((val_dataset_en, val_dataset_tw))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZJ-hiEUBLDu",
        "outputId": "f8600f12-d048-4722-9a97-1817c9622ddf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English:  the first american colonists arrived in the th century .\n",
            "Twi:  amerika atubrafo a wodi kan no duu hɔ wɔ afeha a ɛto so no mu .\n",
            "English:  you need to wash your hands .\n",
            "Twi:  ɛsɛ sɛ wohohoro wo nsa .\n",
            "English:  appiah horrow is payday .\n",
            "Twi:  da a wɔde tua ka no yɛ da koro akatua .\n",
            "English:  i can see why you like asamoah .\n",
            "Twi:  mitumi hu nea enti a w ani gye asamoa ho\n",
            "English:  he had jeans on .\n",
            "Twi:  ɔhyɛ jeans attade\n"
          ]
        }
      ],
      "source": [
        "# verify trained_combined dataset is correct\n",
        "for en,tw in trained_combined.take(5):\n",
        "  print(\"English: \", en.numpy().decode('utf-8'))\n",
        "  print(\"Twi: \", tw.numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaxFg3TZMIK8"
      },
      "source": [
        "# load tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjb-3AU1BnrK"
      },
      "source": [
        "\n",
        "\n",
        "1. The tokenizer is which was build from the parallel corpus.\n",
        "2. The code are available on the link https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/build_subword_tokenizer.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aR_vgyQgBo_T"
      },
      "outputs": [],
      "source": [
        "model_named = '/content/drive/MyDrive/translate_frengtwi_converter'\n",
        "# Verify tokenizer model can be reloaded\n",
        "tokenizers = tf.saved_model.load(model_named)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdHMl3icEjos",
        "outputId": "78185d9c-d94d-4c6b-81bd-b1bf7efe320f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'i', b'love', b'student', b'life', b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Verify tokenizer works on french sentence\n",
        "tokens = tokenizers.eng.tokenize(['I LOVE STUDENT LIFE'])\n",
        "text_tokens = tokenizers.eng.lookup(tokens)\n",
        "text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSOMXh_de04Q",
        "outputId": "118d6e1a-aef4-476d-f318-53461981065a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i love student life\n"
          ]
        }
      ],
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.eng.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBNqSqeSfChq",
        "outputId": "da0d1f15-677c-4176-a193-46267c3c1a6c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'na', b'mmarimaa', b'ne', b'mmeawa', b'pii', b'w\\xc9\\x94',\n",
              "  b'agu', b'##di', b'##bea', b'h\\xc9\\x94', b'.', b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Verify tokenizer works on twi sentence\n",
        "tokens = tokenizers.twi.tokenize(['Na mmarimaa ne mmeawa pii wɔ agudibea hɔ .'])\n",
        "text_tokens = tokenizers.twi.lookup(tokens)\n",
        "text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lS1xMYQfQR9",
        "outputId": "d8c1afb4-3938-4a8b-b297-85105d421a20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "na mmarimaa ne mmeawa pii wɔ agudibea hɔ .\n"
          ]
        }
      ],
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.twi.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQVo6G3OMfac"
      },
      "source": [
        "# Check sentence with maximum length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvoh1vRgCxSb"
      },
      "source": [
        "By checking the maximum length of sentence, you can suitably select the MAX_TOKENS. The MAX_TOKKENS help drops examples longer than the maximum number of tokens (MAX_TOKENS). Without limiting the size of sequences, the performance may be negatively affected.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VJxJuNufTCB",
        "outputId": "faee79e0-c614-4e50-eee0-7f816e69deda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".........................................................................................................................................................................................................................................................................................................................................................................................................................................................................."
          ]
        }
      ],
      "source": [
        "#The distribution of tokens per example in the dataset is as follows:\n",
        "\n",
        "lengths = []\n",
        "\n",
        "for twi_examples,en_examples in trained_combined.batch(50):\n",
        "  twi_tokens = tokenizers.twi.tokenize(twi_examples)\n",
        "  lengths.append(twi_tokens.row_lengths())\n",
        "\n",
        "  en_tokens = tokenizers.eng.tokenize(en_examples)\n",
        "  lengths.append(en_tokens.row_lengths())\n",
        "  print('.', end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "ri-qmhyyiBSa",
        "outputId": "816305a4-9059-4cb6-b755-4659e98be45b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAaYklEQVR4nO3de7xdZX3n8c+3BBAFCZeImFCDY7xE6jUCjr5aCgrhotDXeIHaEjE27YijtX2NBu2UKcKI05mijLehgoC1IuIFFBQzXNppHcEgCISIHLmYRCCRhIsiavA3f6znpJvjObmcfU5OTs7n/Xrt11nreZ619vPsvbO/ez1r7Z1UFZKkqe23JroDkqSJZxhIkgwDSZJhIEnCMJAkYRhIkjAMNESSnyZ51kT3o19Jzk9y+kT3YypLcm2St010P7R5DINJJMndSX6ZZO8h5TcmqSSz+72Pqtq1qu7sdz9jzTf3qS3JOUluT/LrJG8Zpv7dSe5L8nCS85Ls3FM3O8k1SR5N8v0kr96qnZ8kDIPJ5y7ghMGVJL8DPHniuqPxlI7/TuF7wNuB7w6tSHIEsBg4DHgm8Czgb3qafA64EdgLeD9wSZIZ493hycYX2eTzGeDEnvUFwIW9DZIc3Y4WHk6yIsl/7al7U5K7kjy1rR/ZPlHNaOuV5Nlt+fwkH0/y9TZ99K9Jnp7kw0nWtU9ZL+nZ94Zte7Y/vS0fkmRlkvckWZ3k3iTHJTkqyQ+SrE3yvuEGnGQR8GbgPa0fX23lz29TEQ8mWZbkdSNsv1v7ZHh2e3N9XpIl7T5vT/LGIX3+WJLLkzyS5Lok/67VJclZrf8PJ7klyQEj3Oe1ST6Y5PrW9tIke/bUH5zkW63v30tyyJBtz0jyr8CjdG9uQ/f/jCRfTLKmPZ/vbOV7tsf5tW191yQDSU7cjNfG7PYcntTq1iX5syQvT3Jz6+tHe9q/pb0mPprkofZ6OGy4x6O1f2uS5W2/VyZ55khth6qqj1XVVcBjw1QvAM6tqmVVtQ74APCWdp/PAV4KnFpVP6+qLwK3AP9hc+97yqgqb5PkBtwNvBq4HXg+sAOwku7TUAGzW7tDgN+hC/sXAvcDx/Xs57PA+XSflH4MHNNTV8Cz2/L5wE+AlwFPAq6mOzI5sd336cA1w23bs/3pPX1aD/w1sCPwJ8Aa4B+B3YAXAD8H9h9h7Bv21dZ3BAaA9wE7AYcCjwDP7W3fxnh9Tz+eAqwATgKmAS9pY5zbs90DwIGt/rPARa3uCOAGYDqQ9hzsO0J/rwVWAQe0+/wi8A+tbma7j6Pac/Satj6jZ9sftcdkGrDjkH3/VuvHX7exPwu4Ezii1R8O3Ac8Dfh74JKebQ9hhNcGMLs9h59sz/fhdG++X2n7mgmsBn6vtX9Le07f3Z6PNwEPAXv2jONtbfnY9nw9v43pr4Bv9fTra8Dizfg38C/AW4aUfQ94U8/63m0cewF/ACwf0v6jwP+a6H/P29rNI4PJafDo4DXAcro3nQ2q6tqquqWqfl1VN9MdJv9eT5OT6d48rwW+WlVf28h9fbmqbqiqx4AvA49V1YVV9Tjwebo30831K+CMqvoVcBHdP9qPVNUjVbUMuA140Wbu62BgV+DMqvplVV1N94ZyQk+bZwD/BHyhqv6qlR0D3F1Vn66q9VV1I90b9RuGjPn6qlpPFwYv7un/bsDzgFTV8qq6dyN9/ExV3VpVPwP+C/DGJDsAfwRcUVVXtOdoCbCULhwGnV/dJ9317fHq9XK64Ditjf1Oujf94wGq6pvAF4Cr2j7/dHDDzXhtAHygqh5r+/kZ8LmqWl1Vq4D/yxOf89XAh6vqV1X1eboPKkcP81j8GfDB9pitB/4b8OLBo4OqOqaqztzIY7kxu9KF0KDB5d2GqRus322U97XdMgwmp88Af0j3yezCoZVJDmrTImuSPET3D3HDSeeqepDuzeIA4H9u4r7u71n++TDru25Bvx9oITK47XD739z9PQNYUVW/7im7h+7T66CjgV3oPukOeiZwUJvyeDDJg3RTUE/vaXNfz/Kjg31qgfNR4GPA6nQnNZ+6kT6uGNK3Hemeh2cCbxjSh1cB+46w7VDPBJ4xZPv3Afv0tDmH7vk9v6oeGCzc1Guj2ZLnfFW1j9s943zGCH3+SE9/19IdXc0cpu2W+inQ+zwMLj8yTN1g/SNjcL/bFcNgEqqqe+ima44CvjRMk38ELgP2q6rd6d4MM1iZ5MXAW+k+FZ49hl17lCeezH76SA1HYejP6/4Y2C9PPLn62zzxKOnvgW8AVyR5SitbAfxTVU3vue1aVf9xszpRdXZVvQyYCzwH+M8bab7fkL79im5KagXdUUNvH54y5JPxxn5OeAVw15Dtd6uqowDa0cc5dB8U3t57HodNvDZGYWaS3u1/m+65Ga7Pfzqkz7tU1bf6uO9By3jiEeWLgPtbCC4DnpVktyH1y8bgfrcrhsHktRA4tE1BDLUbsLaqHktyIN1RBABJngT8A90nyZPo/jG/fYz6dBPwh0l2SDKf35x+6Mf9PPFE6nV04fOeJDu2E7CvpZt+6vUOuqmLrybZhW4q6TlJ/rhtt2M7Qfr8TXWgtTsoyY500yePAb/eyCZ/lGRukicDp9HN3T9O9/i/NskR7bF6UroT7LM254GgOwfySJL3Jtml7eOAJC9v9e+jC5O3An8LXNgCAjby2hilpwHvbI/jG+jOCVwxTLtPAqckeQFAkt1b+82SZKf22g2wY3vMBt+/LgQWtsd6Ot35iPMBquoHdK/LU9s2f0B3ruSLoxns9swwmKSq6odVtXSE6rcDpyV5hO4k48U9dR+km175RFX9gm7++vQkc8agW++ie0MenHr5yhjsc9C5wNw2zfCVqvplu68j6T5tfxw4saq+37tRm8JYRHei/VK6T+eH082v/5huSuhDwM5s2lPpjjbW0U2HPED3ZjuSz9C9Kd1Hd0L2na1PK+hOqL6P7iT6CrojjM3699gC5Ri6cxl30Y3/U8DuSV4G/AXdY/F4G1vRXXoJG39tjMZ1wJzWhzOA1/dOS/X0+cutLxcleRi4le65AyDdFWvDXk3WfJNuiurf0x31/Bz43bbvbwD/HbiG7sT7PcCpPdseD8yje97ObH1cM5rBbs/yxOk+SWMhybV0Vw99aqL7Ml7SffnrbVX1qonui/rnkYEkyTCQJDlNJEnCIwNJEt3Xwielvffeu2bPnj3R3ZgcfnJH93fvsbhgSNJkdsMNN/ykqn7jh/ombRjMnj2bpUtHurJST/Dp9usAJ10+sf2QNOGS3DNcudNEkiTDQJJkGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEliEn8DebzNXvxv39a9+8zh/n9vSdp+eGQgSTIMJEmGgSQJw0CShGEgScIwkCRhGEiS2IwwSHJektVJbu0p+9sk309yc5IvJ5neU3dKkoEktyc5oqd8fisbSLK4p3z/JNe18s8n2WksByhJ2rTNOTI4H5g/pGwJcEBVvRD4AXAKQJK5wPHAC9o2H0+yQ5IdgI8BRwJzgRNaW4APAWdV1bOBdcDCvkYkSdpimwyDqvpnYO2Qsm9W1fq2+m1gVls+Frioqn5RVXcBA8CB7TZQVXdW1S+Bi4BjkwQ4FLikbX8BcFyfY5IkbaGxOGfwVuDrbXkmsKKnbmUrG6l8L+DBnmAZLB9WkkVJliZZumbNmjHouiQJ+gyDJO8H1gOfHZvubFxVnVNV86pq3owZM7bGXUrSlDDqH6pL8hbgGOCwqqpWvArYr6fZrFbGCOUPANOTTGtHB73tJUlbyaiODJLMB94DvK6qHu2pugw4PsnOSfYH5gDXA98B5rQrh3aiO8l8WQuRa4DXt+0XAJeObiiSpNHanEtLPwf8P+C5SVYmWQh8FNgNWJLkpiSfBKiqZcDFwG3AN4CTq+rx9qn/HcCVwHLg4tYW4L3AXyQZoDuHcO6YjlCStEmbnCaqqhOGKR7xDbuqzgDOGKb8CuCKYcrvpLvaSJI0QfwGsiTJMJAkGQaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJLEZYZDkvCSrk9zaU7ZnkiVJ7mh/92jlSXJ2koEkNyd5ac82C1r7O5Is6Cl/WZJb2jZnJ8lYD1KStHGbc2RwPjB/SNli4KqqmgNc1dYBjgTmtNsi4BPQhQdwKnAQcCBw6mCAtDZ/0rPd0PuSJI2zaZtqUFX/nGT2kOJjgUPa8gXAtcB7W/mFVVXAt5NMT7Jva7ukqtYCJFkCzE9yLfDUqvp2K78QOA74ej+DGmuzF1++YfnuM4+ewJ5I0vgY7TmDfarq3rZ8H7BPW54JrOhpt7KVbax85TDlw0qyKMnSJEvXrFkzyq5Lkobq+wRyOwqoMejL5tzXOVU1r6rmzZgxY2vcpSRNCaMNg/vb9A/t7+pWvgrYr6fdrFa2sfJZw5RLkrai0YbBZcDgFUELgEt7yk9sVxUdDDzUppOuBA5Pskc7cXw4cGWrezjJwe0qohN79iVJ2ko2eQI5yefoTgDvnWQl3VVBZwIXJ1kI3AO8sTW/AjgKGAAeBU4CqKq1ST4AfKe1O23wZDLwdrorlnahO3G8TZ08lqSpYHOuJjphhKrDhmlbwMkj7Oc84LxhypcCB2yqH5Kk8eM3kCVJhoEkaTOmiaaS3i+XSdJU4pGBJMkwkCQZBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kSfYZBkncnWZbk1iSfS/KkJPsnuS7JQJLPJ9mptd25rQ+0+tk9+zmlld+e5Ij+hiRJ2lKjDoMkM4F3AvOq6gBgB+B44EPAWVX1bGAdsLBtshBY18rPau1IMrdt9wJgPvDxJDuMtl+SpC3X7zTRNGCXJNOAJwP3AocCl7T6C4Dj2vKxbZ1Wf1iStPKLquoXVXUXMAAc2Ge/JElbYNRhUFWrgP8B/IguBB4CbgAerKr1rdlKYGZbngmsaNuub+336i0fZhtJ0lbQzzTRHnSf6vcHngE8hW6aZ9wkWZRkaZKla9asGc+7kqQppZ9polcDd1XVmqr6FfAl4JXA9DZtBDALWNWWVwH7AbT63YEHesuH2eYJquqcqppXVfNmzJjRR9clSb36CYMfAQcneXKb+z8MuA24Bnh9a7MAuLQtX9bWafVXV1W18uPb1Ub7A3OA6/volyRpC03bdJPhVdV1SS4BvgusB24EzgEuBy5KcnorO7dtci7wmSQDwFq6K4ioqmVJLqYLkvXAyVX1+Gj7JUnacqMOA4CqOhU4dUjxnQxzNVBVPQa8YYT9nAGc0U9fJEmj5zeQJUmGgSTJMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgScIwkCRhGEiSMAwkSRgGkiQMA0kShoEkiT7DIMn0JJck+X6S5UlekWTPJEuS3NH+7tHaJsnZSQaS3JzkpT37WdDa35FkQb+DkiRtmX6PDD4CfKOqnge8CFgOLAauqqo5wFVtHeBIYE67LQI+AZBkT+BU4CDgQODUwQCRJG0dow6DJLsDvwucC1BVv6yqB4FjgQtaswuA49ryscCF1fk2MD3JvsARwJKqWltV64AlwPzR9kuStOX6OTLYH1gDfDrJjUk+leQpwD5VdW9rcx+wT1ueCazo2X5lKxup/DckWZRkaZKla9as6aPrkqRe/YTBNOClwCeq6iXAz/i3KSEAqqqA6uM+nqCqzqmqeVU1b8aMGWO1W0ma8voJg5XAyqq6rq1fQhcO97fpH9rf1a1+FbBfz/azWtlI5ZKkrWTUYVBV9wErkjy3FR0G3AZcBgxeEbQAuLQtXwac2K4qOhh4qE0nXQkcnmSPduL48FYmSdpKpvW5/X8CPptkJ+BO4CS6gLk4yULgHuCNre0VwFHAAPBoa0tVrU3yAeA7rd1pVbW2z35JkrZAX2FQVTcB84apOmyYtgWcPMJ+zgPO66cvW8vsxZdvWL77zKMnsCeSNHb8BrIkyTCQJBkGkiQMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRKGgSQJw0CShGEgSWIMwiDJDkluTPK1tr5/kuuSDCT5fJKdWvnObX2g1c/u2ccprfz2JEf02ydJ0pYZiyODdwHLe9Y/BJxVVc8G1gELW/lCYF0rP6u1I8lc4HjgBcB84ONJdhiDfkmSNlNfYZBkFnA08Km2HuBQ4JLW5ALguLZ8bFun1R/W2h8LXFRVv6iqu4AB4MB++iVJ2jL9Hhl8GHgP8Ou2vhfwYFWtb+srgZlteSawAqDVP9TabygfZhtJ0lYw6jBIcgywuqpuGMP+bOo+FyVZmmTpmjVrttbdStJ2r58jg1cCr0tyN3AR3fTQR4DpSaa1NrOAVW15FbAfQKvfHXigt3yYbZ6gqs6pqnlVNW/GjBl9dF2S1GvUYVBVp1TVrKqaTXcC+OqqejNwDfD61mwBcGlbvqyt0+qvrqpq5ce3q432B+YA14+2X5KkLTdt00222HuBi5KcDtwInNvKzwU+k2QAWEsXIFTVsiQXA7cB64GTq+rxceiXJGkEYxIGVXUtcG1bvpNhrgaqqseAN4yw/RnAGWPRF0nSlvMbyJIkw0CSZBhIkjAMJEkYBpIkDANJEoaBJAnDQJKEYSBJwjCQJGEYSJIwDCRJGAaSJAwDSRLj8/8ZTBmzF1++YfnuM4+ewJ5IUn88MpAkGQaSJMNAkoTnDJ4w7y9JU5VHBpIkw0CSZBhIkjAMJEkYBpIk+giDJPsluSbJbUmWJXlXK98zyZIkd7S/e7TyJDk7yUCSm5O8tGdfC1r7O5Is6H9YkqQt0c+RwXrgL6tqLnAwcHKSucBi4KqqmgNc1dYBjgTmtNsi4BPQhQdwKnAQcCBw6mCASJK2jlGHQVXdW1XfbcuPAMuBmcCxwAWt2QXAcW35WODC6nwbmJ5kX+AIYElVra2qdcASYP5o+yVJ2nJjcs4gyWzgJcB1wD5VdW+rug/Ypy3PBFb0bLaylY1UPtz9LEqyNMnSNWvWjEXXJUmMQRgk2RX4IvDnVfVwb11VFVD93kfP/s6pqnlVNW/GjBljtVtJmvL6CoMkO9IFwWer6kut+P42/UP7u7qVrwL269l8VisbqVyStJX0czVRgHOB5VX1dz1VlwGDVwQtAC7tKT+xXVV0MPBQm066Ejg8yR7txPHhrUyStJX080N1rwT+GLglyU2t7H3AmcDFSRYC9wBvbHVXAEcBA8CjwEkAVbU2yQeA77R2p1XV2j76JUnaQqMOg6r6FyAjVB82TPsCTh5hX+cB5422L5Kk/vgNZEmSYSBJMgwkSRgGkiQMA0kShoEkif6+Z6AesxdfvmH57jOPnsCeSNKW88hAkmQYSJIMA0kShoEkCcNAkoRhIEnCMJAkYRhIkjAMJEkYBpIk/DmKceFPU0iabDwykCQZBpIkw0CShGEgScIwkCTh1UTjziuLJE0G28yRQZL5SW5PMpBk8UT3R5Kmkm3iyCDJDsDHgNcAK4HvJLmsqm6b2J6NLY8SJG2rtokwAA4EBqrqToAkFwHHAuMSBr1vyhNlpD4YEpImwrYSBjOBFT3rK4GDhjZKsghY1FZ/muT2Ud7f3sBPRrntuMqHxm3Xe/PWbJNjHkfb7PM8jqbamKfaeKH/MT9zuMJtJQw2S1WdA5zT736SLK2qeWPQpUnDMU8NU23MU228MH5j3lZOIK8C9utZn9XKJElbwbYSBt8B5iTZP8lOwPHAZRPcJ0maMraJaaKqWp/kHcCVwA7AeVW1bBzvsu+ppknIMU8NU23MU228ME5jTlWNx34lSZPItjJNJEmaQIaBJGlqhcH2+pMXSc5LsjrJrT1leyZZkuSO9nePVp4kZ7fH4OYkL524no9ekv2SXJPktiTLkryrlW+3407ypCTXJ/leG/PftPL9k1zXxvb5dhEGSXZu6wOtfvZE9n+0kuyQ5MYkX2vr2/V4AZLcneSWJDclWdrKxvW1PWXCoOcnL44E5gInJJk7sb0aM+cD84eULQauqqo5wFVtHbrxz2m3RcAntlIfx9p64C+rai5wMHByez6353H/Aji0ql4EvBiYn+Rg4EPAWVX1bGAdsLC1Xwisa+VntXaT0buA5T3r2/t4B/1+Vb245zsF4/varqopcQNeAVzZs34KcMpE92sMxzcbuLVn/XZg37a8L3B7W/7fwAnDtZvMN+BSut+2mhLjBp4MfJfum/o/Aaa18g2vc7qr817Rlqe1dpnovm/hOGe1N75Dga8B2Z7H2zPuu4G9h5SN62t7yhwZMPxPXsycoL5sDftU1b1t+T5gn7a83T0ObTrgJcB1bOfjblMmNwGrgSXAD4EHq2p9a9I7rg1jbvUPAXtt3R737cPAe4Bft/W92L7HO6iAbya5of0MD4zza3ub+J6BxldVVZLt8hriJLsCXwT+vKoeTrKhbnscd1U9Drw4yXTgy8DzJrhL4ybJMcDqqrohySET3Z+t7FVVtSrJ04AlSb7fWzker+2pdGQw1X7y4v4k+wK0v6tb+XbzOCTZkS4IPltVX2rF2/24AarqQeAaummS6UkGP9j1jmvDmFv97sADW7mr/Xgl8LokdwMX0U0VfYTtd7wbVNWq9nc1XegfyDi/tqdSGEy1n7y4DFjQlhfQzakPlp/YrkA4GHio59Bz0kh3CHAusLyq/q6narsdd5IZ7YiAJLvQnSNZThcKr2/Nho558LF4PXB1tUnlyaCqTqmqWVU1m+7f69VV9Wa20/EOSvKUJLsNLgOHA7cy3q/tiT5RspVPyhwF/IBunvX9E92fMRzX54B7gV/RzRcupJsrvQq4A/g/wJ6tbeiuqvohcAswb6L7P8oxv4puXvVm4KZ2O2p7HjfwQuDGNuZbgb9u5c8CrgcGgC8AO7fyJ7X1gVb/rIkeQx9jPwT42lQYbxvf99pt2eB71Xi/tv05CknSlJomkiSNwDCQJBkGkiTDQJKEYSBJwjCQJGEYSJKA/w9Mz7u0ojGgugAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "all_lengths = np.concatenate(lengths)\n",
        "\n",
        "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
        "plt.ylim(plt.ylim())\n",
        "max_length = max(all_lengths)\n",
        "plt.plot([max_length, max_length], plt.ylim())\n",
        "plt.title(f'Maximum tokens per example: {max_length}');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DYwhdWAMpnW"
      },
      "source": [
        "# Tokenize the training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GC7UlwEeiNTO"
      },
      "outputs": [],
      "source": [
        "MAX_TOKENS = 50\n",
        "\n",
        "def filter_max_tokens(l1, l2):\n",
        "  num_tokens = tf.maximum(tf.shape(l1)[1],tf.shape(l2)[1])\n",
        "  return num_tokens < MAX_TOKENS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Routine to tokenize sentence pairs; needed to make sentence batches for training\n",
        "def tokenize_pairs(en,twi):\n",
        "  en = tokenizers.eng.tokenize(en)\n",
        "  # Convert from ragged to dense, padding with zeros.\n",
        "  en = en.to_tensor()\n",
        "  tw = tokenizers.twi.tokenize(twi)\n",
        "  # Convert from ragged to dense, padding with zeros.\n",
        "  tw = tw.to_tensor()\n",
        "  return en, tw"
      ],
      "metadata": {
        "id": "AORJvDkcOhxC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EWMySNLzklc_"
      },
      "outputs": [],
      "source": [
        "# create training batches\n",
        "BUFFER_SIZE = len(train_twi)\n",
        "BATCH_SIZE = 64\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .filter(filter_max_tokens)\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "# Create training and validation set batches.\n",
        "train_batches = make_batches(trained_combined)\n",
        "val_batches = make_batches(val_combined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2MhJY21RqcJ"
      },
      "source": [
        "Write the test batch to file. This will come in handy later if you prefer to translate from txt file and estimate the translator [BLEU](https://aclanthology.org/P02-1040.pdf) score. The translator will run to error for any for any sentence with the shape greater than the MAX_TOKENS used for training. It adivisable to use the trimmed sentences for testting to avoid such occurrance\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2V_Rh8fFSvPV"
      },
      "outputs": [],
      "source": [
        "en_test= []\n",
        "twi_test = []\n",
        "\n",
        "for en_batches,twi_batches in val_batches:\n",
        "    for en in tokenizers.eng.detokenize(en_batches):\n",
        "      en_test.append(en.numpy().decode(\"utf-8\"))\n",
        "    for tw in tokenizers.twi.detokenize(twi_batches):\n",
        "      twi_test.append(tw.numpy().decode(\"utf-8\"))\n",
        "\n",
        "\n",
        "preprocessor.writeTotxt('/content/drive/MyDrive/twi_testing_set.txt',test_twi)\n",
        "preprocessor.writeTotxt('/content/drive/MyDrive/english_testing_set.txt',en_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNFMseSdM3wr"
      },
      "source": [
        "# Positional encoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5du74jWFF5WK"
      },
      "source": [
        "Positional encodings are added to the embeddings to give the model some information about the relative position of the tokens in the sentence so the model can learn to recognize the word order.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "KXtWVxfeb6px"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "  \n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # Apply the sine function to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # Apply the cosine function to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXL_HTJWG44o"
      },
      "source": [
        "# Create a point-wise feed-forward network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "db4kXyfGb_l5"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(\n",
        "  d_model, # Input/output dimensionality.\n",
        "  dff # Inner-layer dimensionality.\n",
        "  ):\n",
        "\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # Shape `(batch_size, seq_len, dff)`.\n",
        "      tf.keras.layers.Dense(d_model)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "  ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSEO7HUZHppR"
      },
      "source": [
        "# Build Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtydO3sOIRE-"
      },
      "source": [
        "### Multihead Attention layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BSHsspXXIaUR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create mask for padding tokens so translator ignores them\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "\n",
        "# Mask future tokens so translator cannot see future\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "\n",
        "# Create final masks\n",
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead)\n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "      q: query shape == (..., seq_len_q, depth)\n",
        "      k: key shape == (..., seq_len_k, depth)\n",
        "      v: value shape == (..., seq_len_v, depth_v)\n",
        "      mask: Float tensor with shape broadcastable\n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "      output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    # (..., seq_len_q, seq_len_k)\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(\n",
        "        scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# Add attention layers to create multi-head attention\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, *, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # (batch_size, num_heads, seq_len_q, depth)\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        # (batch_size, num_heads, seq_len_k, depth)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        # (batch_size, num_heads, seq_len_v, depth)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        # (batch_size, seq_len_q, num_heads, depth)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        # (batch_size, seq_len_q, d_model)\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "        # Build Transformer encoder layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvNLzFWpJw6m"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bCdZS_nJcGVW"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EBZNB3MacRCF"
      },
      "outputs": [],
      "source": [
        "#Build Transformer encoder from encoder layer\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "              maximum_position_encoding=MAX_TOKENS,rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndsnLCOwRgcY"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "fnXVTpI_cgzR"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "UEuoBDtJcp7Q"
      },
      "outputs": [],
      "source": [
        "#Build Transformer decoder from decoder layer\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "              maximum_position_encoding,rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size,d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjFbGSvJUQY6"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_6sd1ofVcyl4"
      },
      "outputs": [],
      "source": [
        "# Build full Transformer\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_vocab_size, pe_input=MAX_TOKENS, pe_target=MAX_TOKENS, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.encoder= Encoder(num_layers, d_model, num_heads, dff,input_vocab_size, pe_input, rate)\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff,target_vocab_size, pe_target, rate)\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "  \n",
        "  def call(self, inp, tar, training, enc_padding_mask,look_ahead_mask, dec_padding_mask):\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask) # (batch_size, inp_seq_len, d_model)\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    final_output = self.final_layer(dec_output) # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    return final_output, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPhqoBhwWYZ9"
      },
      "source": [
        "# Optimizer and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "y79jHzBnA7wG"
      },
      "outputs": [],
      "source": [
        "# Set learning rate schedule\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g11C2K3tXFyP"
      },
      "source": [
        "# Set hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-q51YVNusr5_"
      },
      "outputs": [],
      "source": [
        "num_layers = 6\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fGafl4DXYct"
      },
      "source": [
        "# Instantiate a Transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "9DJxw765BYHi"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=tokenizers.eng.get_vocab_size().numpy(),\n",
        "    target_vocab_size=tokenizers.twi.get_vocab_size().numpy(),\n",
        "    rate=dropout_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aGTknsnYDCC"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "BF0wsHaaJVKC"
      },
      "outputs": [],
      "source": [
        "# Instantiate learning rate and set optimizer\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "HpLaySRfSy89"
      },
      "outputs": [],
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "-PHWy8sdGoR8"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = './checkpoints/train'\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# If a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "VAj0P_T0ZcBV"
      },
      "outputs": [],
      "source": [
        "# Choose number of training epochs\n",
        "EPOCHS = 150\n",
        "train_step_signature = [\n",
        "  tf.TensorSpec(shape=(None, None), dtype=tf.int64), \n",
        "  tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp,True,enc_padding_mask,combined_mask,dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7k6GwwEIi_A",
        "outputId": "ba03858f-f246-413f-e9d8-0eb73e49070c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 7.7533 Accuracy 0.0018\n",
            "Epoch 1 Batch 50 Loss 7.0633 Accuracy 0.0666\n",
            "Epoch 1 Batch 100 Loss 6.6385 Accuracy 0.0844\n",
            "Epoch 1 Batch 150 Loss 6.3715 Accuracy 0.1022\n",
            "Epoch 1 Batch 200 Loss 6.1183 Accuracy 0.1308\n",
            "Epoch 1 Batch 250 Loss 5.9270 Accuracy 0.1503\n",
            "Epoch 1 Batch 300 Loss 5.7802 Accuracy 0.1654\n",
            "Epoch 1 Batch 350 Loss 5.6613 Accuracy 0.1772\n",
            "Epoch 1 Loss 5.6492 Accuracy 0.1784\n",
            "Time taken for 1 epoch: 82.52 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 4.7522 Accuracy 0.2825\n",
            "Epoch 2 Batch 50 Loss 4.8227 Accuracy 0.2613\n",
            "Epoch 2 Batch 100 Loss 4.7674 Accuracy 0.2674\n",
            "Epoch 2 Batch 150 Loss 4.7248 Accuracy 0.2712\n",
            "Epoch 2 Batch 200 Loss 4.6831 Accuracy 0.2748\n",
            "Epoch 2 Batch 250 Loss 4.6361 Accuracy 0.2789\n",
            "Epoch 2 Batch 300 Loss 4.5929 Accuracy 0.2830\n",
            "Epoch 2 Batch 350 Loss 4.5456 Accuracy 0.2881\n",
            "Epoch 2 Loss 4.5421 Accuracy 0.2884\n",
            "Time taken for 1 epoch: 61.54 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4.3317 Accuracy 0.2959\n",
            "Epoch 3 Batch 50 Loss 4.1418 Accuracy 0.3254\n",
            "Epoch 3 Batch 100 Loss 4.1069 Accuracy 0.3298\n",
            "Epoch 3 Batch 150 Loss 4.0751 Accuracy 0.3337\n",
            "Epoch 3 Batch 200 Loss 4.0513 Accuracy 0.3362\n",
            "Epoch 3 Batch 250 Loss 4.0165 Accuracy 0.3395\n",
            "Epoch 3 Batch 300 Loss 3.9883 Accuracy 0.3419\n",
            "Epoch 3 Batch 350 Loss 3.9594 Accuracy 0.3444\n",
            "Epoch 3 Loss 3.9556 Accuracy 0.3446\n",
            "Time taken for 1 epoch: 62.60 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 3.6960 Accuracy 0.3473\n",
            "Epoch 4 Batch 50 Loss 3.6212 Accuracy 0.3722\n",
            "Epoch 4 Batch 100 Loss 3.6023 Accuracy 0.3736\n",
            "Epoch 4 Batch 150 Loss 3.5913 Accuracy 0.3751\n",
            "Epoch 4 Batch 200 Loss 3.5672 Accuracy 0.3778\n",
            "Epoch 4 Batch 250 Loss 3.5480 Accuracy 0.3795\n",
            "Epoch 4 Batch 300 Loss 3.5300 Accuracy 0.3810\n",
            "Epoch 4 Batch 350 Loss 3.5145 Accuracy 0.3822\n",
            "Epoch 4 Loss 3.5121 Accuracy 0.3826\n",
            "Time taken for 1 epoch: 62.19 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 3.2315 Accuracy 0.4049\n",
            "Epoch 5 Batch 50 Loss 3.2408 Accuracy 0.4067\n",
            "Epoch 5 Batch 100 Loss 3.2245 Accuracy 0.4087\n",
            "Epoch 5 Batch 150 Loss 3.2165 Accuracy 0.4090\n",
            "Epoch 5 Batch 200 Loss 3.2148 Accuracy 0.4090\n",
            "Epoch 5 Batch 250 Loss 3.2082 Accuracy 0.4100\n",
            "Epoch 5 Batch 300 Loss 3.1970 Accuracy 0.4114\n",
            "Epoch 5 Batch 350 Loss 3.1879 Accuracy 0.4122\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
            "Epoch 5 Loss 3.1871 Accuracy 0.4121\n",
            "Time taken for 1 epoch: 65.39 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 2.8698 Accuracy 0.4408\n",
            "Epoch 6 Batch 50 Loss 2.9277 Accuracy 0.4363\n",
            "Epoch 6 Batch 100 Loss 2.9269 Accuracy 0.4386\n",
            "Epoch 6 Batch 150 Loss 2.9211 Accuracy 0.4393\n",
            "Epoch 6 Batch 200 Loss 2.9280 Accuracy 0.4386\n",
            "Epoch 6 Batch 250 Loss 2.9326 Accuracy 0.4381\n",
            "Epoch 6 Batch 300 Loss 2.9246 Accuracy 0.4390\n",
            "Epoch 6 Batch 350 Loss 2.9208 Accuracy 0.4394\n",
            "Epoch 6 Loss 2.9212 Accuracy 0.4390\n",
            "Time taken for 1 epoch: 61.91 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.6335 Accuracy 0.4859\n",
            "Epoch 7 Batch 50 Loss 2.6514 Accuracy 0.4683\n",
            "Epoch 7 Batch 100 Loss 2.6644 Accuracy 0.4674\n",
            "Epoch 7 Batch 150 Loss 2.6820 Accuracy 0.4646\n",
            "Epoch 7 Batch 200 Loss 2.6872 Accuracy 0.4643\n",
            "Epoch 7 Batch 250 Loss 2.6976 Accuracy 0.4631\n",
            "Epoch 7 Batch 300 Loss 2.7034 Accuracy 0.4621\n",
            "Epoch 7 Batch 350 Loss 2.7051 Accuracy 0.4621\n",
            "Epoch 7 Loss 2.7049 Accuracy 0.4621\n",
            "Time taken for 1 epoch: 61.81 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.3400 Accuracy 0.5035\n",
            "Epoch 8 Batch 50 Loss 2.5020 Accuracy 0.4845\n",
            "Epoch 8 Batch 100 Loss 2.4780 Accuracy 0.4898\n",
            "Epoch 8 Batch 150 Loss 2.4990 Accuracy 0.4862\n",
            "Epoch 8 Batch 200 Loss 2.5057 Accuracy 0.4851\n",
            "Epoch 8 Batch 250 Loss 2.5127 Accuracy 0.4838\n",
            "Epoch 8 Batch 300 Loss 2.5249 Accuracy 0.4819\n",
            "Epoch 8 Batch 350 Loss 2.5343 Accuracy 0.4808\n",
            "Epoch 8 Loss 2.5356 Accuracy 0.4804\n",
            "Time taken for 1 epoch: 61.50 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.4006 Accuracy 0.4865\n",
            "Epoch 9 Batch 50 Loss 2.3088 Accuracy 0.5065\n",
            "Epoch 9 Batch 100 Loss 2.3287 Accuracy 0.5033\n",
            "Epoch 9 Batch 150 Loss 2.3406 Accuracy 0.5020\n",
            "Epoch 9 Batch 200 Loss 2.3636 Accuracy 0.4987\n",
            "Epoch 9 Batch 250 Loss 2.3892 Accuracy 0.4956\n",
            "Epoch 9 Batch 300 Loss 2.4040 Accuracy 0.4937\n",
            "Epoch 9 Batch 350 Loss 2.4162 Accuracy 0.4921\n",
            "Epoch 9 Loss 2.4176 Accuracy 0.4921\n",
            "Time taken for 1 epoch: 61.99 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 2.3218 Accuracy 0.4975\n",
            "Epoch 10 Batch 50 Loss 2.2321 Accuracy 0.5157\n",
            "Epoch 10 Batch 100 Loss 2.2493 Accuracy 0.5144\n",
            "Epoch 10 Batch 150 Loss 2.2612 Accuracy 0.5125\n",
            "Epoch 10 Batch 200 Loss 2.2816 Accuracy 0.5093\n",
            "Epoch 10 Batch 250 Loss 2.2995 Accuracy 0.5065\n",
            "Epoch 10 Batch 300 Loss 2.3174 Accuracy 0.5039\n",
            "Epoch 10 Batch 350 Loss 2.3357 Accuracy 0.5010\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
            "Epoch 10 Loss 2.3374 Accuracy 0.5008\n",
            "Time taken for 1 epoch: 64.77 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 2.1043 Accuracy 0.5399\n",
            "Epoch 11 Batch 50 Loss 2.1563 Accuracy 0.5271\n",
            "Epoch 11 Batch 100 Loss 2.1909 Accuracy 0.5188\n",
            "Epoch 11 Batch 150 Loss 2.2230 Accuracy 0.5146\n",
            "Epoch 11 Batch 200 Loss 2.2442 Accuracy 0.5115\n",
            "Epoch 11 Batch 250 Loss 2.2536 Accuracy 0.5103\n",
            "Epoch 11 Batch 300 Loss 2.2751 Accuracy 0.5070\n",
            "Epoch 11 Batch 350 Loss 2.2939 Accuracy 0.5045\n",
            "Epoch 11 Loss 2.2967 Accuracy 0.5042\n",
            "Time taken for 1 epoch: 61.68 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 2.2947 Accuracy 0.4792\n",
            "Epoch 12 Batch 50 Loss 2.1151 Accuracy 0.5276\n",
            "Epoch 12 Batch 100 Loss 2.1473 Accuracy 0.5230\n",
            "Epoch 12 Batch 150 Loss 2.1823 Accuracy 0.5178\n",
            "Epoch 12 Batch 200 Loss 2.2067 Accuracy 0.5137\n",
            "Epoch 12 Batch 250 Loss 2.2318 Accuracy 0.5104\n",
            "Epoch 12 Batch 300 Loss 2.2526 Accuracy 0.5075\n",
            "Epoch 12 Batch 350 Loss 2.2618 Accuracy 0.5064\n",
            "Epoch 12 Loss 2.2624 Accuracy 0.5063\n",
            "Time taken for 1 epoch: 61.82 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.8645 Accuracy 0.5823\n",
            "Epoch 13 Batch 50 Loss 2.0500 Accuracy 0.5353\n",
            "Epoch 13 Batch 100 Loss 2.0808 Accuracy 0.5328\n",
            "Epoch 13 Batch 150 Loss 2.0928 Accuracy 0.5322\n",
            "Epoch 13 Batch 200 Loss 2.1076 Accuracy 0.5298\n",
            "Epoch 13 Batch 250 Loss 2.1161 Accuracy 0.5290\n",
            "Epoch 13 Batch 300 Loss 2.1279 Accuracy 0.5274\n",
            "Epoch 13 Batch 350 Loss 2.1415 Accuracy 0.5254\n",
            "Epoch 13 Loss 2.1433 Accuracy 0.5251\n",
            "Time taken for 1 epoch: 61.24 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.8435 Accuracy 0.5590\n",
            "Epoch 14 Batch 50 Loss 1.9167 Accuracy 0.5573\n",
            "Epoch 14 Batch 100 Loss 1.9260 Accuracy 0.5543\n",
            "Epoch 14 Batch 150 Loss 1.9624 Accuracy 0.5503\n",
            "Epoch 14 Batch 200 Loss 1.9883 Accuracy 0.5455\n",
            "Epoch 14 Batch 250 Loss 2.0023 Accuracy 0.5441\n",
            "Epoch 14 Batch 300 Loss 2.0177 Accuracy 0.5413\n",
            "Epoch 14 Batch 350 Loss 2.0308 Accuracy 0.5400\n",
            "Epoch 14 Loss 2.0310 Accuracy 0.5399\n",
            "Time taken for 1 epoch: 61.32 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.7696 Accuracy 0.5701\n",
            "Epoch 15 Batch 50 Loss 1.8040 Accuracy 0.5766\n",
            "Epoch 15 Batch 100 Loss 1.8287 Accuracy 0.5711\n",
            "Epoch 15 Batch 150 Loss 1.8444 Accuracy 0.5687\n",
            "Epoch 15 Batch 200 Loss 1.8715 Accuracy 0.5644\n",
            "Epoch 15 Batch 250 Loss 1.9002 Accuracy 0.5602\n",
            "Epoch 15 Batch 300 Loss 1.9091 Accuracy 0.5595\n",
            "Epoch 15 Batch 350 Loss 1.9146 Accuracy 0.5585\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
            "Epoch 15 Loss 1.9152 Accuracy 0.5584\n",
            "Time taken for 1 epoch: 64.07 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.7768 Accuracy 0.6036\n",
            "Epoch 16 Batch 50 Loss 1.6793 Accuracy 0.5943\n",
            "Epoch 16 Batch 100 Loss 1.6821 Accuracy 0.5960\n",
            "Epoch 16 Batch 150 Loss 1.7098 Accuracy 0.5905\n",
            "Epoch 16 Batch 200 Loss 1.7340 Accuracy 0.5865\n",
            "Epoch 16 Batch 250 Loss 1.7538 Accuracy 0.5833\n",
            "Epoch 16 Batch 300 Loss 1.7715 Accuracy 0.5803\n",
            "Epoch 16 Batch 350 Loss 1.7920 Accuracy 0.5770\n",
            "Epoch 16 Loss 1.7950 Accuracy 0.5766\n",
            "Time taken for 1 epoch: 62.08 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.5948 Accuracy 0.6127\n",
            "Epoch 17 Batch 50 Loss 1.5644 Accuracy 0.6184\n",
            "Epoch 17 Batch 100 Loss 1.5924 Accuracy 0.6138\n",
            "Epoch 17 Batch 150 Loss 1.6080 Accuracy 0.6096\n",
            "Epoch 17 Batch 200 Loss 1.6431 Accuracy 0.6030\n",
            "Epoch 17 Batch 250 Loss 1.6622 Accuracy 0.6003\n",
            "Epoch 17 Batch 300 Loss 1.6745 Accuracy 0.5984\n",
            "Epoch 17 Batch 350 Loss 1.6873 Accuracy 0.5958\n",
            "Epoch 17 Loss 1.6897 Accuracy 0.5954\n",
            "Time taken for 1 epoch: 61.97 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.5680 Accuracy 0.5822\n",
            "Epoch 18 Batch 50 Loss 1.4934 Accuracy 0.6266\n",
            "Epoch 18 Batch 100 Loss 1.5127 Accuracy 0.6252\n",
            "Epoch 18 Batch 150 Loss 1.5316 Accuracy 0.6232\n",
            "Epoch 18 Batch 200 Loss 1.5524 Accuracy 0.6180\n",
            "Epoch 18 Batch 250 Loss 1.5704 Accuracy 0.6149\n",
            "Epoch 18 Batch 300 Loss 1.5805 Accuracy 0.6132\n",
            "Epoch 18 Batch 350 Loss 1.5922 Accuracy 0.6109\n",
            "Epoch 18 Loss 1.5929 Accuracy 0.6108\n",
            "Time taken for 1 epoch: 60.79 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.2468 Accuracy 0.6724\n",
            "Epoch 19 Batch 50 Loss 1.3912 Accuracy 0.6524\n",
            "Epoch 19 Batch 100 Loss 1.4140 Accuracy 0.6458\n",
            "Epoch 19 Batch 150 Loss 1.4360 Accuracy 0.6414\n",
            "Epoch 19 Batch 200 Loss 1.4504 Accuracy 0.6381\n",
            "Epoch 19 Batch 250 Loss 1.4683 Accuracy 0.6345\n",
            "Epoch 19 Batch 300 Loss 1.4907 Accuracy 0.6298\n",
            "Epoch 19 Batch 350 Loss 1.5059 Accuracy 0.6266\n",
            "Epoch 19 Loss 1.5080 Accuracy 0.6262\n",
            "Time taken for 1 epoch: 61.38 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.2904 Accuracy 0.7024\n",
            "Epoch 20 Batch 50 Loss 1.2721 Accuracy 0.6704\n",
            "Epoch 20 Batch 100 Loss 1.3055 Accuracy 0.6638\n",
            "Epoch 20 Batch 150 Loss 1.3320 Accuracy 0.6581\n",
            "Epoch 20 Batch 200 Loss 1.3473 Accuracy 0.6554\n",
            "Epoch 20 Batch 250 Loss 1.3726 Accuracy 0.6506\n",
            "Epoch 20 Batch 300 Loss 1.3935 Accuracy 0.6472\n",
            "Epoch 20 Batch 350 Loss 1.4092 Accuracy 0.6448\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
            "Epoch 20 Loss 1.4115 Accuracy 0.6442\n",
            "Time taken for 1 epoch: 63.84 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.3202 Accuracy 0.6730\n",
            "Epoch 21 Batch 50 Loss 1.2301 Accuracy 0.6814\n",
            "Epoch 21 Batch 100 Loss 1.2353 Accuracy 0.6786\n",
            "Epoch 21 Batch 150 Loss 1.2555 Accuracy 0.6746\n",
            "Epoch 21 Batch 200 Loss 1.2767 Accuracy 0.6698\n",
            "Epoch 21 Batch 250 Loss 1.2970 Accuracy 0.6654\n",
            "Epoch 21 Batch 300 Loss 1.3134 Accuracy 0.6620\n",
            "Epoch 21 Batch 350 Loss 1.3271 Accuracy 0.6597\n",
            "Epoch 21 Loss 1.3271 Accuracy 0.6596\n",
            "Time taken for 1 epoch: 61.53 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 1.1099 Accuracy 0.7127\n",
            "Epoch 22 Batch 50 Loss 1.1459 Accuracy 0.6992\n",
            "Epoch 22 Batch 100 Loss 1.1721 Accuracy 0.6942\n",
            "Epoch 22 Batch 150 Loss 1.1928 Accuracy 0.6883\n",
            "Epoch 22 Batch 200 Loss 1.2032 Accuracy 0.6855\n",
            "Epoch 22 Batch 250 Loss 1.2217 Accuracy 0.6809\n",
            "Epoch 22 Batch 300 Loss 1.2319 Accuracy 0.6790\n",
            "Epoch 22 Batch 350 Loss 1.2474 Accuracy 0.6756\n",
            "Epoch 22 Loss 1.2483 Accuracy 0.6754\n",
            "Time taken for 1 epoch: 60.98 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 1.0144 Accuracy 0.7277\n",
            "Epoch 23 Batch 50 Loss 1.0877 Accuracy 0.7074\n",
            "Epoch 23 Batch 100 Loss 1.0972 Accuracy 0.7055\n",
            "Epoch 23 Batch 150 Loss 1.1226 Accuracy 0.6998\n",
            "Epoch 23 Batch 200 Loss 1.1351 Accuracy 0.6968\n",
            "Epoch 23 Batch 250 Loss 1.1494 Accuracy 0.6945\n",
            "Epoch 23 Batch 300 Loss 1.1630 Accuracy 0.6916\n",
            "Epoch 23 Batch 350 Loss 1.1766 Accuracy 0.6886\n",
            "Epoch 23 Loss 1.1789 Accuracy 0.6880\n",
            "Time taken for 1 epoch: 61.16 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.8418 Accuracy 0.7741\n",
            "Epoch 24 Batch 50 Loss 0.9824 Accuracy 0.7350\n",
            "Epoch 24 Batch 100 Loss 1.0067 Accuracy 0.7291\n",
            "Epoch 24 Batch 150 Loss 1.0389 Accuracy 0.7215\n",
            "Epoch 24 Batch 200 Loss 1.0581 Accuracy 0.7165\n",
            "Epoch 24 Batch 250 Loss 1.0761 Accuracy 0.7119\n",
            "Epoch 24 Batch 300 Loss 1.0920 Accuracy 0.7078\n",
            "Epoch 24 Batch 350 Loss 1.1047 Accuracy 0.7046\n",
            "Epoch 24 Loss 1.1066 Accuracy 0.7041\n",
            "Time taken for 1 epoch: 61.79 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.8322 Accuracy 0.7537\n",
            "Epoch 25 Batch 50 Loss 0.9638 Accuracy 0.7360\n",
            "Epoch 25 Batch 100 Loss 0.9689 Accuracy 0.7349\n",
            "Epoch 25 Batch 150 Loss 0.9861 Accuracy 0.7307\n",
            "Epoch 25 Batch 200 Loss 1.0016 Accuracy 0.7267\n",
            "Epoch 25 Batch 250 Loss 1.0142 Accuracy 0.7239\n",
            "Epoch 25 Batch 300 Loss 1.0278 Accuracy 0.7210\n",
            "Epoch 25 Batch 350 Loss 1.0396 Accuracy 0.7181\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-5\n",
            "Epoch 25 Loss 1.0402 Accuracy 0.7179\n",
            "Time taken for 1 epoch: 64.29 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.9983 Accuracy 0.7215\n",
            "Epoch 26 Batch 50 Loss 0.8761 Accuracy 0.7582\n",
            "Epoch 26 Batch 100 Loss 0.9077 Accuracy 0.7498\n",
            "Epoch 26 Batch 150 Loss 0.9338 Accuracy 0.7426\n",
            "Epoch 26 Batch 200 Loss 0.9476 Accuracy 0.7387\n",
            "Epoch 26 Batch 250 Loss 0.9635 Accuracy 0.7349\n",
            "Epoch 26 Batch 300 Loss 0.9764 Accuracy 0.7315\n",
            "Epoch 26 Batch 350 Loss 0.9864 Accuracy 0.7290\n",
            "Epoch 26 Loss 0.9894 Accuracy 0.7283\n",
            "Time taken for 1 epoch: 61.42 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.8742 Accuracy 0.7547\n",
            "Epoch 27 Batch 50 Loss 0.8286 Accuracy 0.7681\n",
            "Epoch 27 Batch 100 Loss 0.8424 Accuracy 0.7639\n",
            "Epoch 27 Batch 150 Loss 0.8598 Accuracy 0.7593\n",
            "Epoch 27 Batch 200 Loss 0.8778 Accuracy 0.7544\n",
            "Epoch 27 Batch 250 Loss 0.8960 Accuracy 0.7499\n",
            "Epoch 27 Batch 300 Loss 0.9085 Accuracy 0.7466\n",
            "Epoch 27 Batch 350 Loss 0.9249 Accuracy 0.7426\n",
            "Epoch 27 Loss 0.9264 Accuracy 0.7422\n",
            "Time taken for 1 epoch: 61.07 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.8261 Accuracy 0.7625\n",
            "Epoch 28 Batch 50 Loss 0.7841 Accuracy 0.7793\n",
            "Epoch 28 Batch 100 Loss 0.7988 Accuracy 0.7746\n",
            "Epoch 28 Batch 150 Loss 0.8030 Accuracy 0.7734\n",
            "Epoch 28 Batch 200 Loss 0.8278 Accuracy 0.7671\n",
            "Epoch 28 Batch 250 Loss 0.8436 Accuracy 0.7629\n",
            "Epoch 28 Batch 300 Loss 0.8598 Accuracy 0.7584\n",
            "Epoch 28 Batch 350 Loss 0.8709 Accuracy 0.7561\n",
            "Epoch 28 Loss 0.8727 Accuracy 0.7555\n",
            "Time taken for 1 epoch: 61.16 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.7771 Accuracy 0.7915\n",
            "Epoch 29 Batch 50 Loss 0.7450 Accuracy 0.7875\n",
            "Epoch 29 Batch 100 Loss 0.7602 Accuracy 0.7832\n",
            "Epoch 29 Batch 150 Loss 0.7746 Accuracy 0.7791\n",
            "Epoch 29 Batch 200 Loss 0.7898 Accuracy 0.7752\n",
            "Epoch 29 Batch 250 Loss 0.8024 Accuracy 0.7720\n",
            "Epoch 29 Batch 300 Loss 0.8168 Accuracy 0.7685\n",
            "Epoch 29 Batch 350 Loss 0.8302 Accuracy 0.7652\n",
            "Epoch 29 Loss 0.8322 Accuracy 0.7648\n",
            "Time taken for 1 epoch: 61.35 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.7040 Accuracy 0.7993\n",
            "Epoch 30 Batch 50 Loss 0.7103 Accuracy 0.7969\n",
            "Epoch 30 Batch 100 Loss 0.7058 Accuracy 0.7972\n",
            "Epoch 30 Batch 150 Loss 0.7261 Accuracy 0.7917\n",
            "Epoch 30 Batch 200 Loss 0.7432 Accuracy 0.7867\n",
            "Epoch 30 Batch 250 Loss 0.7588 Accuracy 0.7829\n",
            "Epoch 30 Batch 300 Loss 0.7743 Accuracy 0.7786\n",
            "Epoch 30 Batch 350 Loss 0.7851 Accuracy 0.7761\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-6\n",
            "Epoch 30 Loss 0.7865 Accuracy 0.7757\n",
            "Time taken for 1 epoch: 64.19 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.7310 Accuracy 0.8096\n",
            "Epoch 31 Batch 50 Loss 0.6722 Accuracy 0.8063\n",
            "Epoch 31 Batch 100 Loss 0.6810 Accuracy 0.8032\n",
            "Epoch 31 Batch 150 Loss 0.6984 Accuracy 0.7987\n",
            "Epoch 31 Batch 200 Loss 0.7108 Accuracy 0.7949\n",
            "Epoch 31 Batch 250 Loss 0.7252 Accuracy 0.7910\n",
            "Epoch 31 Batch 300 Loss 0.7416 Accuracy 0.7867\n",
            "Epoch 31 Batch 350 Loss 0.7506 Accuracy 0.7841\n",
            "Epoch 31 Loss 0.7515 Accuracy 0.7838\n",
            "Time taken for 1 epoch: 61.50 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.7243 Accuracy 0.8031\n",
            "Epoch 32 Batch 50 Loss 0.6392 Accuracy 0.8127\n",
            "Epoch 32 Batch 100 Loss 0.6564 Accuracy 0.8089\n",
            "Epoch 32 Batch 150 Loss 0.6654 Accuracy 0.8061\n",
            "Epoch 32 Batch 200 Loss 0.6754 Accuracy 0.8035\n",
            "Epoch 32 Batch 250 Loss 0.6893 Accuracy 0.7997\n",
            "Epoch 32 Batch 300 Loss 0.7007 Accuracy 0.7969\n",
            "Epoch 32 Batch 350 Loss 0.7125 Accuracy 0.7940\n",
            "Epoch 32 Loss 0.7149 Accuracy 0.7933\n",
            "Time taken for 1 epoch: 61.67 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.5640 Accuracy 0.8475\n",
            "Epoch 33 Batch 50 Loss 0.6076 Accuracy 0.8225\n",
            "Epoch 33 Batch 100 Loss 0.6158 Accuracy 0.8212\n",
            "Epoch 33 Batch 150 Loss 0.6304 Accuracy 0.8165\n",
            "Epoch 33 Batch 200 Loss 0.6401 Accuracy 0.8136\n",
            "Epoch 33 Batch 250 Loss 0.6530 Accuracy 0.8099\n",
            "Epoch 33 Batch 300 Loss 0.6613 Accuracy 0.8072\n",
            "Epoch 33 Batch 350 Loss 0.6727 Accuracy 0.8038\n",
            "Epoch 33 Loss 0.6731 Accuracy 0.8036\n",
            "Time taken for 1 epoch: 61.32 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.5730 Accuracy 0.8424\n",
            "Epoch 34 Batch 50 Loss 0.5607 Accuracy 0.8359\n",
            "Epoch 34 Batch 100 Loss 0.5866 Accuracy 0.8277\n",
            "Epoch 34 Batch 150 Loss 0.6029 Accuracy 0.8225\n",
            "Epoch 34 Batch 200 Loss 0.6195 Accuracy 0.8178\n",
            "Epoch 34 Batch 250 Loss 0.6303 Accuracy 0.8144\n",
            "Epoch 34 Batch 300 Loss 0.6413 Accuracy 0.8114\n",
            "Epoch 34 Batch 350 Loss 0.6513 Accuracy 0.8086\n",
            "Epoch 34 Loss 0.6528 Accuracy 0.8081\n",
            "Time taken for 1 epoch: 60.77 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.5029 Accuracy 0.8459\n",
            "Epoch 35 Batch 50 Loss 0.5374 Accuracy 0.8401\n",
            "Epoch 35 Batch 100 Loss 0.5590 Accuracy 0.8345\n",
            "Epoch 35 Batch 150 Loss 0.5697 Accuracy 0.8319\n",
            "Epoch 35 Batch 200 Loss 0.5756 Accuracy 0.8295\n",
            "Epoch 35 Batch 250 Loss 0.5883 Accuracy 0.8258\n",
            "Epoch 35 Batch 300 Loss 0.6018 Accuracy 0.8219\n",
            "Epoch 35 Batch 350 Loss 0.6113 Accuracy 0.8190\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-7\n",
            "Epoch 35 Loss 0.6124 Accuracy 0.8187\n",
            "Time taken for 1 epoch: 63.86 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.6145 Accuracy 0.8165\n",
            "Epoch 36 Batch 50 Loss 0.5302 Accuracy 0.8409\n",
            "Epoch 36 Batch 100 Loss 0.5408 Accuracy 0.8388\n",
            "Epoch 36 Batch 150 Loss 0.5456 Accuracy 0.8367\n",
            "Epoch 36 Batch 200 Loss 0.5605 Accuracy 0.8323\n",
            "Epoch 36 Batch 250 Loss 0.5723 Accuracy 0.8293\n",
            "Epoch 36 Batch 300 Loss 0.5837 Accuracy 0.8260\n",
            "Epoch 36 Batch 350 Loss 0.5910 Accuracy 0.8240\n",
            "Epoch 36 Loss 0.5919 Accuracy 0.8237\n",
            "Time taken for 1 epoch: 61.58 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.4729 Accuracy 0.8535\n",
            "Epoch 37 Batch 50 Loss 0.5089 Accuracy 0.8498\n",
            "Epoch 37 Batch 100 Loss 0.5145 Accuracy 0.8480\n",
            "Epoch 37 Batch 150 Loss 0.5225 Accuracy 0.8445\n",
            "Epoch 37 Batch 200 Loss 0.5344 Accuracy 0.8401\n",
            "Epoch 37 Batch 250 Loss 0.5434 Accuracy 0.8375\n",
            "Epoch 37 Batch 300 Loss 0.5540 Accuracy 0.8344\n",
            "Epoch 37 Batch 350 Loss 0.5643 Accuracy 0.8315\n",
            "Epoch 37 Loss 0.5654 Accuracy 0.8312\n",
            "Time taken for 1 epoch: 61.02 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.5214 Accuracy 0.8507\n",
            "Epoch 38 Batch 50 Loss 0.4832 Accuracy 0.8540\n",
            "Epoch 38 Batch 100 Loss 0.4879 Accuracy 0.8529\n",
            "Epoch 38 Batch 150 Loss 0.5024 Accuracy 0.8487\n",
            "Epoch 38 Batch 200 Loss 0.5084 Accuracy 0.8465\n",
            "Epoch 38 Batch 250 Loss 0.5194 Accuracy 0.8433\n",
            "Epoch 38 Batch 300 Loss 0.5281 Accuracy 0.8407\n",
            "Epoch 38 Batch 350 Loss 0.5383 Accuracy 0.8378\n",
            "Epoch 38 Loss 0.5386 Accuracy 0.8379\n",
            "Time taken for 1 epoch: 61.06 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.4387 Accuracy 0.8591\n",
            "Epoch 39 Batch 50 Loss 0.4543 Accuracy 0.8633\n",
            "Epoch 39 Batch 100 Loss 0.4675 Accuracy 0.8587\n",
            "Epoch 39 Batch 150 Loss 0.4805 Accuracy 0.8552\n",
            "Epoch 39 Batch 200 Loss 0.4893 Accuracy 0.8524\n",
            "Epoch 39 Batch 250 Loss 0.4992 Accuracy 0.8494\n",
            "Epoch 39 Batch 300 Loss 0.5072 Accuracy 0.8467\n",
            "Epoch 39 Batch 350 Loss 0.5152 Accuracy 0.8446\n",
            "Epoch 39 Loss 0.5161 Accuracy 0.8443\n",
            "Time taken for 1 epoch: 61.00 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.4927 Accuracy 0.8601\n",
            "Epoch 40 Batch 50 Loss 0.4373 Accuracy 0.8682\n",
            "Epoch 40 Batch 100 Loss 0.4511 Accuracy 0.8642\n",
            "Epoch 40 Batch 150 Loss 0.4643 Accuracy 0.8603\n",
            "Epoch 40 Batch 200 Loss 0.4734 Accuracy 0.8573\n",
            "Epoch 40 Batch 250 Loss 0.4811 Accuracy 0.8549\n",
            "Epoch 40 Batch 300 Loss 0.4886 Accuracy 0.8527\n",
            "Epoch 40 Batch 350 Loss 0.4980 Accuracy 0.8499\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-8\n",
            "Epoch 40 Loss 0.4983 Accuracy 0.8498\n",
            "Time taken for 1 epoch: 63.92 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.4112 Accuracy 0.8694\n",
            "Epoch 41 Batch 50 Loss 0.4248 Accuracy 0.8723\n",
            "Epoch 41 Batch 100 Loss 0.4259 Accuracy 0.8714\n",
            "Epoch 41 Batch 150 Loss 0.4402 Accuracy 0.8666\n",
            "Epoch 41 Batch 200 Loss 0.4477 Accuracy 0.8646\n",
            "Epoch 41 Batch 250 Loss 0.4573 Accuracy 0.8621\n",
            "Epoch 41 Batch 300 Loss 0.4681 Accuracy 0.8586\n",
            "Epoch 41 Batch 350 Loss 0.4748 Accuracy 0.8565\n",
            "Epoch 41 Loss 0.4763 Accuracy 0.8560\n",
            "Time taken for 1 epoch: 61.47 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.3637 Accuracy 0.8887\n",
            "Epoch 42 Batch 50 Loss 0.3855 Accuracy 0.8838\n",
            "Epoch 42 Batch 100 Loss 0.3988 Accuracy 0.8800\n",
            "Epoch 42 Batch 150 Loss 0.4085 Accuracy 0.8758\n",
            "Epoch 42 Batch 200 Loss 0.4216 Accuracy 0.8717\n",
            "Epoch 42 Batch 250 Loss 0.4344 Accuracy 0.8682\n",
            "Epoch 42 Batch 300 Loss 0.4424 Accuracy 0.8659\n",
            "Epoch 42 Batch 350 Loss 0.4523 Accuracy 0.8629\n",
            "Epoch 42 Loss 0.4532 Accuracy 0.8626\n",
            "Time taken for 1 epoch: 61.44 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.3600 Accuracy 0.8967\n",
            "Epoch 43 Batch 50 Loss 0.3825 Accuracy 0.8858\n",
            "Epoch 43 Batch 100 Loss 0.3885 Accuracy 0.8827\n",
            "Epoch 43 Batch 150 Loss 0.4030 Accuracy 0.8783\n",
            "Epoch 43 Batch 200 Loss 0.4117 Accuracy 0.8752\n",
            "Epoch 43 Batch 250 Loss 0.4192 Accuracy 0.8729\n",
            "Epoch 43 Batch 300 Loss 0.4283 Accuracy 0.8699\n",
            "Epoch 43 Batch 350 Loss 0.4366 Accuracy 0.8674\n",
            "Epoch 43 Loss 0.4368 Accuracy 0.8673\n",
            "Time taken for 1 epoch: 61.57 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.4505 Accuracy 0.8576\n",
            "Epoch 44 Batch 50 Loss 0.3711 Accuracy 0.8876\n",
            "Epoch 44 Batch 100 Loss 0.3727 Accuracy 0.8865\n",
            "Epoch 44 Batch 150 Loss 0.3848 Accuracy 0.8829\n",
            "Epoch 44 Batch 200 Loss 0.3968 Accuracy 0.8791\n",
            "Epoch 44 Batch 250 Loss 0.4046 Accuracy 0.8772\n",
            "Epoch 44 Batch 300 Loss 0.4134 Accuracy 0.8744\n",
            "Epoch 44 Batch 350 Loss 0.4203 Accuracy 0.8722\n",
            "Epoch 44 Loss 0.4208 Accuracy 0.8721\n",
            "Time taken for 1 epoch: 61.29 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.3131 Accuracy 0.9105\n",
            "Epoch 45 Batch 50 Loss 0.3526 Accuracy 0.8921\n",
            "Epoch 45 Batch 100 Loss 0.3687 Accuracy 0.8873\n",
            "Epoch 45 Batch 150 Loss 0.3820 Accuracy 0.8834\n",
            "Epoch 45 Batch 200 Loss 0.3972 Accuracy 0.8787\n",
            "Epoch 45 Batch 250 Loss 0.4008 Accuracy 0.8774\n",
            "Epoch 45 Batch 300 Loss 0.4053 Accuracy 0.8758\n",
            "Epoch 45 Batch 350 Loss 0.4114 Accuracy 0.8739\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/train/ckpt-9\n",
            "Epoch 45 Loss 0.4125 Accuracy 0.8737\n",
            "Time taken for 1 epoch: 63.88 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.4412 Accuracy 0.8725\n",
            "Epoch 46 Batch 50 Loss 0.3625 Accuracy 0.8918\n",
            "Epoch 46 Batch 100 Loss 0.3641 Accuracy 0.8899\n",
            "Epoch 46 Batch 150 Loss 0.3677 Accuracy 0.8883\n",
            "Epoch 46 Batch 200 Loss 0.3714 Accuracy 0.8870\n",
            "Epoch 46 Batch 250 Loss 0.3793 Accuracy 0.8843\n",
            "Epoch 46 Batch 300 Loss 0.3859 Accuracy 0.8818\n",
            "Epoch 46 Batch 350 Loss 0.3941 Accuracy 0.8793\n",
            "Epoch 46 Loss 0.3950 Accuracy 0.8790\n",
            "Time taken for 1 epoch: 61.09 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.3423 Accuracy 0.8879\n",
            "Epoch 47 Batch 50 Loss 0.3549 Accuracy 0.8909\n",
            "Epoch 47 Batch 100 Loss 0.3543 Accuracy 0.8918\n",
            "Epoch 47 Batch 150 Loss 0.3616 Accuracy 0.8892\n",
            "Epoch 47 Batch 200 Loss 0.3675 Accuracy 0.8874\n",
            "Epoch 47 Batch 250 Loss 0.3754 Accuracy 0.8846\n",
            "Epoch 47 Batch 300 Loss 0.3811 Accuracy 0.8831\n",
            "Epoch 47 Batch 350 Loss 0.3855 Accuracy 0.8820\n",
            "Epoch 47 Loss 0.3859 Accuracy 0.8819\n",
            "Time taken for 1 epoch: 61.32 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.2723 Accuracy 0.9048\n",
            "Epoch 48 Batch 50 Loss 0.3290 Accuracy 0.8997\n",
            "Epoch 48 Batch 100 Loss 0.3429 Accuracy 0.8951\n",
            "Epoch 48 Batch 150 Loss 0.3471 Accuracy 0.8938\n",
            "Epoch 48 Batch 200 Loss 0.3533 Accuracy 0.8924\n",
            "Epoch 48 Batch 250 Loss 0.3575 Accuracy 0.8912\n",
            "Epoch 48 Batch 300 Loss 0.3634 Accuracy 0.8890\n",
            "Epoch 48 Batch 350 Loss 0.3688 Accuracy 0.8871\n",
            "Epoch 48 Loss 0.3692 Accuracy 0.8869\n",
            "Time taken for 1 epoch: 61.24 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.3372 Accuracy 0.8887\n",
            "Epoch 49 Batch 50 Loss 0.3216 Accuracy 0.9004\n",
            "Epoch 49 Batch 100 Loss 0.3242 Accuracy 0.9000\n",
            "Epoch 49 Batch 150 Loss 0.3272 Accuracy 0.8983\n",
            "Epoch 49 Batch 200 Loss 0.3345 Accuracy 0.8965\n",
            "Epoch 49 Batch 250 Loss 0.3422 Accuracy 0.8944\n",
            "Epoch 49 Batch 300 Loss 0.3511 Accuracy 0.8917\n",
            "Epoch 49 Batch 350 Loss 0.3563 Accuracy 0.8901\n",
            "Epoch 49 Loss 0.3574 Accuracy 0.8897\n",
            "Time taken for 1 epoch: 61.34 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.2596 Accuracy 0.9209\n",
            "Epoch 50 Batch 50 Loss 0.3026 Accuracy 0.9070\n",
            "Epoch 50 Batch 100 Loss 0.3061 Accuracy 0.9064\n",
            "Epoch 50 Batch 150 Loss 0.3163 Accuracy 0.9031\n",
            "Epoch 50 Batch 200 Loss 0.3229 Accuracy 0.9009\n",
            "Epoch 50 Batch 250 Loss 0.3318 Accuracy 0.8977\n",
            "Epoch 50 Batch 300 Loss 0.3371 Accuracy 0.8959\n",
            "Epoch 50 Batch 350 Loss 0.3430 Accuracy 0.8943\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-10\n",
            "Epoch 50 Loss 0.3434 Accuracy 0.8941\n",
            "Time taken for 1 epoch: 64.04 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.2675 Accuracy 0.9175\n",
            "Epoch 51 Batch 50 Loss 0.3108 Accuracy 0.9046\n",
            "Epoch 51 Batch 100 Loss 0.3118 Accuracy 0.9033\n",
            "Epoch 51 Batch 150 Loss 0.3143 Accuracy 0.9028\n",
            "Epoch 51 Batch 200 Loss 0.3192 Accuracy 0.9015\n",
            "Epoch 51 Batch 250 Loss 0.3240 Accuracy 0.9001\n",
            "Epoch 51 Batch 300 Loss 0.3307 Accuracy 0.8982\n",
            "Epoch 51 Batch 350 Loss 0.3355 Accuracy 0.8969\n",
            "Epoch 51 Loss 0.3366 Accuracy 0.8967\n",
            "Time taken for 1 epoch: 60.88 secs\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.2902 Accuracy 0.9222\n",
            "Epoch 52 Batch 50 Loss 0.2955 Accuracy 0.9079\n",
            "Epoch 52 Batch 100 Loss 0.3019 Accuracy 0.9065\n",
            "Epoch 52 Batch 150 Loss 0.3055 Accuracy 0.9057\n",
            "Epoch 52 Batch 200 Loss 0.3097 Accuracy 0.9049\n",
            "Epoch 52 Batch 250 Loss 0.3184 Accuracy 0.9023\n",
            "Epoch 52 Batch 300 Loss 0.3211 Accuracy 0.9013\n",
            "Epoch 52 Batch 350 Loss 0.3258 Accuracy 0.8998\n",
            "Epoch 52 Loss 0.3260 Accuracy 0.8996\n",
            "Time taken for 1 epoch: 61.06 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.2991 Accuracy 0.9097\n",
            "Epoch 53 Batch 50 Loss 0.2860 Accuracy 0.9116\n",
            "Epoch 53 Batch 100 Loss 0.2884 Accuracy 0.9104\n",
            "Epoch 53 Batch 150 Loss 0.2906 Accuracy 0.9102\n",
            "Epoch 53 Batch 200 Loss 0.3000 Accuracy 0.9072\n",
            "Epoch 53 Batch 250 Loss 0.3059 Accuracy 0.9052\n",
            "Epoch 53 Batch 300 Loss 0.3110 Accuracy 0.9038\n",
            "Epoch 53 Batch 350 Loss 0.3162 Accuracy 0.9023\n",
            "Epoch 53 Loss 0.3166 Accuracy 0.9022\n",
            "Time taken for 1 epoch: 60.89 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.2487 Accuracy 0.9288\n",
            "Epoch 54 Batch 50 Loss 0.2832 Accuracy 0.9124\n",
            "Epoch 54 Batch 100 Loss 0.2760 Accuracy 0.9149\n",
            "Epoch 54 Batch 150 Loss 0.2818 Accuracy 0.9127\n",
            "Epoch 54 Batch 200 Loss 0.2896 Accuracy 0.9102\n",
            "Epoch 54 Batch 250 Loss 0.2985 Accuracy 0.9079\n",
            "Epoch 54 Batch 300 Loss 0.3048 Accuracy 0.9060\n",
            "Epoch 54 Batch 350 Loss 0.3077 Accuracy 0.9050\n",
            "Epoch 54 Loss 0.3081 Accuracy 0.9049\n",
            "Time taken for 1 epoch: 61.15 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.2295 Accuracy 0.9187\n",
            "Epoch 55 Batch 50 Loss 0.2699 Accuracy 0.9192\n",
            "Epoch 55 Batch 100 Loss 0.2766 Accuracy 0.9158\n",
            "Epoch 55 Batch 150 Loss 0.2824 Accuracy 0.9144\n",
            "Epoch 55 Batch 200 Loss 0.2875 Accuracy 0.9122\n",
            "Epoch 55 Batch 250 Loss 0.2933 Accuracy 0.9105\n",
            "Epoch 55 Batch 300 Loss 0.2974 Accuracy 0.9089\n",
            "Epoch 55 Batch 350 Loss 0.3016 Accuracy 0.9075\n",
            "Saving checkpoint for epoch 55 at ./checkpoints/train/ckpt-11\n",
            "Epoch 55 Loss 0.3019 Accuracy 0.9073\n",
            "Time taken for 1 epoch: 64.03 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.2524 Accuracy 0.9163\n",
            "Epoch 56 Batch 50 Loss 0.2537 Accuracy 0.9241\n",
            "Epoch 56 Batch 100 Loss 0.2579 Accuracy 0.9214\n",
            "Epoch 56 Batch 150 Loss 0.2641 Accuracy 0.9186\n",
            "Epoch 56 Batch 200 Loss 0.2693 Accuracy 0.9166\n",
            "Epoch 56 Batch 250 Loss 0.2775 Accuracy 0.9141\n",
            "Epoch 56 Batch 300 Loss 0.2823 Accuracy 0.9128\n",
            "Epoch 56 Batch 350 Loss 0.2851 Accuracy 0.9119\n",
            "Epoch 56 Loss 0.2856 Accuracy 0.9118\n",
            "Time taken for 1 epoch: 61.73 secs\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.3014 Accuracy 0.9076\n",
            "Epoch 57 Batch 50 Loss 0.2510 Accuracy 0.9219\n",
            "Epoch 57 Batch 100 Loss 0.2610 Accuracy 0.9192\n",
            "Epoch 57 Batch 150 Loss 0.2641 Accuracy 0.9183\n",
            "Epoch 57 Batch 200 Loss 0.2677 Accuracy 0.9168\n",
            "Epoch 57 Batch 250 Loss 0.2748 Accuracy 0.9151\n",
            "Epoch 57 Batch 300 Loss 0.2788 Accuracy 0.9137\n",
            "Epoch 57 Batch 350 Loss 0.2817 Accuracy 0.9127\n",
            "Epoch 57 Loss 0.2822 Accuracy 0.9127\n",
            "Time taken for 1 epoch: 61.29 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.2280 Accuracy 0.9237\n",
            "Epoch 58 Batch 50 Loss 0.2449 Accuracy 0.9259\n",
            "Epoch 58 Batch 100 Loss 0.2474 Accuracy 0.9238\n",
            "Epoch 58 Batch 150 Loss 0.2489 Accuracy 0.9233\n",
            "Epoch 58 Batch 200 Loss 0.2543 Accuracy 0.9217\n",
            "Epoch 58 Batch 250 Loss 0.2597 Accuracy 0.9198\n",
            "Epoch 58 Batch 300 Loss 0.2632 Accuracy 0.9188\n",
            "Epoch 58 Batch 350 Loss 0.2670 Accuracy 0.9174\n",
            "Epoch 58 Loss 0.2675 Accuracy 0.9173\n",
            "Time taken for 1 epoch: 60.84 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.2227 Accuracy 0.9311\n",
            "Epoch 59 Batch 50 Loss 0.2272 Accuracy 0.9309\n",
            "Epoch 59 Batch 100 Loss 0.2456 Accuracy 0.9245\n",
            "Epoch 59 Batch 150 Loss 0.2519 Accuracy 0.9230\n",
            "Epoch 59 Batch 200 Loss 0.2555 Accuracy 0.9217\n",
            "Epoch 59 Batch 250 Loss 0.2616 Accuracy 0.9196\n",
            "Epoch 59 Batch 300 Loss 0.2664 Accuracy 0.9180\n",
            "Epoch 59 Batch 350 Loss 0.2697 Accuracy 0.9170\n",
            "Epoch 59 Loss 0.2701 Accuracy 0.9168\n",
            "Time taken for 1 epoch: 61.04 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.2310 Accuracy 0.9407\n",
            "Epoch 60 Batch 50 Loss 0.2318 Accuracy 0.9305\n",
            "Epoch 60 Batch 100 Loss 0.2389 Accuracy 0.9279\n",
            "Epoch 60 Batch 150 Loss 0.2469 Accuracy 0.9252\n",
            "Epoch 60 Batch 200 Loss 0.2503 Accuracy 0.9238\n",
            "Epoch 60 Batch 250 Loss 0.2533 Accuracy 0.9225\n",
            "Epoch 60 Batch 300 Loss 0.2574 Accuracy 0.9213\n",
            "Epoch 60 Batch 350 Loss 0.2588 Accuracy 0.9206\n",
            "Saving checkpoint for epoch 60 at ./checkpoints/train/ckpt-12\n",
            "Epoch 60 Loss 0.2590 Accuracy 0.9205\n",
            "Time taken for 1 epoch: 63.92 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.1997 Accuracy 0.9288\n",
            "Epoch 61 Batch 50 Loss 0.2152 Accuracy 0.9335\n",
            "Epoch 61 Batch 100 Loss 0.2208 Accuracy 0.9318\n",
            "Epoch 61 Batch 150 Loss 0.2303 Accuracy 0.9286\n",
            "Epoch 61 Batch 200 Loss 0.2403 Accuracy 0.9251\n",
            "Epoch 61 Batch 250 Loss 0.2424 Accuracy 0.9241\n",
            "Epoch 61 Batch 300 Loss 0.2465 Accuracy 0.9230\n",
            "Epoch 61 Batch 350 Loss 0.2505 Accuracy 0.9218\n",
            "Epoch 61 Loss 0.2509 Accuracy 0.9217\n",
            "Time taken for 1 epoch: 61.31 secs\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.2193 Accuracy 0.9343\n",
            "Epoch 62 Batch 50 Loss 0.2202 Accuracy 0.9329\n",
            "Epoch 62 Batch 100 Loss 0.2212 Accuracy 0.9316\n",
            "Epoch 62 Batch 150 Loss 0.2286 Accuracy 0.9293\n",
            "Epoch 62 Batch 200 Loss 0.2366 Accuracy 0.9267\n",
            "Epoch 62 Batch 250 Loss 0.2416 Accuracy 0.9254\n",
            "Epoch 62 Batch 300 Loss 0.2459 Accuracy 0.9237\n",
            "Epoch 62 Batch 350 Loss 0.2503 Accuracy 0.9223\n",
            "Epoch 62 Loss 0.2503 Accuracy 0.9223\n",
            "Time taken for 1 epoch: 61.03 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.2109 Accuracy 0.9119\n",
            "Epoch 63 Batch 50 Loss 0.2167 Accuracy 0.9322\n",
            "Epoch 63 Batch 100 Loss 0.2205 Accuracy 0.9310\n",
            "Epoch 63 Batch 150 Loss 0.2273 Accuracy 0.9288\n",
            "Epoch 63 Batch 200 Loss 0.2312 Accuracy 0.9275\n",
            "Epoch 63 Batch 250 Loss 0.2363 Accuracy 0.9265\n",
            "Epoch 63 Batch 300 Loss 0.2405 Accuracy 0.9252\n",
            "Epoch 63 Batch 350 Loss 0.2441 Accuracy 0.9239\n",
            "Epoch 63 Loss 0.2446 Accuracy 0.9237\n",
            "Time taken for 1 epoch: 61.32 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.1629 Accuracy 0.9436\n",
            "Epoch 64 Batch 50 Loss 0.2082 Accuracy 0.9363\n",
            "Epoch 64 Batch 100 Loss 0.2131 Accuracy 0.9345\n",
            "Epoch 64 Batch 150 Loss 0.2182 Accuracy 0.9327\n",
            "Epoch 64 Batch 200 Loss 0.2227 Accuracy 0.9311\n",
            "Epoch 64 Batch 250 Loss 0.2267 Accuracy 0.9300\n",
            "Epoch 64 Batch 300 Loss 0.2314 Accuracy 0.9285\n",
            "Epoch 64 Batch 350 Loss 0.2342 Accuracy 0.9276\n",
            "Epoch 64 Loss 0.2352 Accuracy 0.9273\n",
            "Time taken for 1 epoch: 61.32 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.2479 Accuracy 0.9210\n",
            "Epoch 65 Batch 50 Loss 0.2006 Accuracy 0.9369\n",
            "Epoch 65 Batch 100 Loss 0.2090 Accuracy 0.9349\n",
            "Epoch 65 Batch 150 Loss 0.2121 Accuracy 0.9344\n",
            "Epoch 65 Batch 200 Loss 0.2155 Accuracy 0.9332\n",
            "Epoch 65 Batch 250 Loss 0.2173 Accuracy 0.9325\n",
            "Epoch 65 Batch 300 Loss 0.2227 Accuracy 0.9309\n",
            "Epoch 65 Batch 350 Loss 0.2285 Accuracy 0.9292\n",
            "Saving checkpoint for epoch 65 at ./checkpoints/train/ckpt-13\n",
            "Epoch 65 Loss 0.2286 Accuracy 0.9292\n",
            "Time taken for 1 epoch: 63.63 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.2196 Accuracy 0.9304\n",
            "Epoch 66 Batch 50 Loss 0.2055 Accuracy 0.9355\n",
            "Epoch 66 Batch 100 Loss 0.2050 Accuracy 0.9365\n",
            "Epoch 66 Batch 150 Loss 0.2108 Accuracy 0.9350\n",
            "Epoch 66 Batch 200 Loss 0.2167 Accuracy 0.9331\n",
            "Epoch 66 Batch 250 Loss 0.2181 Accuracy 0.9325\n",
            "Epoch 66 Batch 300 Loss 0.2217 Accuracy 0.9313\n",
            "Epoch 66 Batch 350 Loss 0.2238 Accuracy 0.9306\n",
            "Epoch 66 Loss 0.2245 Accuracy 0.9304\n",
            "Time taken for 1 epoch: 60.97 secs\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.2287 Accuracy 0.9307\n",
            "Epoch 67 Batch 50 Loss 0.1974 Accuracy 0.9391\n",
            "Epoch 67 Batch 100 Loss 0.1988 Accuracy 0.9379\n",
            "Epoch 67 Batch 150 Loss 0.2015 Accuracy 0.9377\n",
            "Epoch 67 Batch 200 Loss 0.2073 Accuracy 0.9356\n",
            "Epoch 67 Batch 250 Loss 0.2103 Accuracy 0.9347\n",
            "Epoch 67 Batch 300 Loss 0.2141 Accuracy 0.9335\n",
            "Epoch 67 Batch 350 Loss 0.2172 Accuracy 0.9324\n",
            "Epoch 67 Loss 0.2174 Accuracy 0.9323\n",
            "Time taken for 1 epoch: 60.88 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.1855 Accuracy 0.9358\n",
            "Epoch 68 Batch 50 Loss 0.2014 Accuracy 0.9383\n",
            "Epoch 68 Batch 100 Loss 0.1997 Accuracy 0.9391\n",
            "Epoch 68 Batch 150 Loss 0.2064 Accuracy 0.9376\n",
            "Epoch 68 Batch 200 Loss 0.2083 Accuracy 0.9370\n",
            "Epoch 68 Batch 250 Loss 0.2122 Accuracy 0.9355\n",
            "Epoch 68 Batch 300 Loss 0.2156 Accuracy 0.9345\n",
            "Epoch 68 Batch 350 Loss 0.2184 Accuracy 0.9333\n",
            "Epoch 68 Loss 0.2184 Accuracy 0.9333\n",
            "Time taken for 1 epoch: 61.34 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.2163 Accuracy 0.9330\n",
            "Epoch 69 Batch 50 Loss 0.1822 Accuracy 0.9442\n",
            "Epoch 69 Batch 100 Loss 0.1901 Accuracy 0.9419\n",
            "Epoch 69 Batch 150 Loss 0.1933 Accuracy 0.9405\n",
            "Epoch 69 Batch 200 Loss 0.1984 Accuracy 0.9391\n",
            "Epoch 69 Batch 250 Loss 0.2028 Accuracy 0.9380\n",
            "Epoch 69 Batch 300 Loss 0.2060 Accuracy 0.9369\n",
            "Epoch 69 Batch 350 Loss 0.2094 Accuracy 0.9357\n",
            "Epoch 69 Loss 0.2096 Accuracy 0.9357\n",
            "Time taken for 1 epoch: 60.62 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.1646 Accuracy 0.9563\n",
            "Epoch 70 Batch 50 Loss 0.1900 Accuracy 0.9427\n",
            "Epoch 70 Batch 100 Loss 0.1904 Accuracy 0.9416\n",
            "Epoch 70 Batch 150 Loss 0.1907 Accuracy 0.9414\n",
            "Epoch 70 Batch 200 Loss 0.1955 Accuracy 0.9398\n",
            "Epoch 70 Batch 250 Loss 0.1991 Accuracy 0.9386\n",
            "Epoch 70 Batch 300 Loss 0.1998 Accuracy 0.9384\n",
            "Epoch 70 Batch 350 Loss 0.2038 Accuracy 0.9373\n",
            "Saving checkpoint for epoch 70 at ./checkpoints/train/ckpt-14\n",
            "Epoch 70 Loss 0.2044 Accuracy 0.9371\n",
            "Time taken for 1 epoch: 64.33 secs\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.1443 Accuracy 0.9604\n",
            "Epoch 71 Batch 50 Loss 0.1759 Accuracy 0.9471\n",
            "Epoch 71 Batch 100 Loss 0.1802 Accuracy 0.9454\n",
            "Epoch 71 Batch 150 Loss 0.1868 Accuracy 0.9430\n",
            "Epoch 71 Batch 200 Loss 0.1906 Accuracy 0.9421\n",
            "Epoch 71 Batch 250 Loss 0.1949 Accuracy 0.9405\n",
            "Epoch 71 Batch 300 Loss 0.1966 Accuracy 0.9398\n",
            "Epoch 71 Batch 350 Loss 0.1990 Accuracy 0.9390\n",
            "Epoch 71 Loss 0.1993 Accuracy 0.9390\n",
            "Time taken for 1 epoch: 61.01 secs\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.1753 Accuracy 0.9401\n",
            "Epoch 72 Batch 50 Loss 0.1723 Accuracy 0.9462\n",
            "Epoch 72 Batch 100 Loss 0.1753 Accuracy 0.9454\n",
            "Epoch 72 Batch 150 Loss 0.1833 Accuracy 0.9434\n",
            "Epoch 72 Batch 200 Loss 0.1850 Accuracy 0.9426\n",
            "Epoch 72 Batch 250 Loss 0.1882 Accuracy 0.9416\n",
            "Epoch 72 Batch 300 Loss 0.1921 Accuracy 0.9405\n",
            "Epoch 72 Batch 350 Loss 0.1952 Accuracy 0.9396\n",
            "Epoch 72 Loss 0.1956 Accuracy 0.9394\n",
            "Time taken for 1 epoch: 60.98 secs\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.1489 Accuracy 0.9430\n",
            "Epoch 73 Batch 50 Loss 0.1722 Accuracy 0.9484\n",
            "Epoch 73 Batch 100 Loss 0.1761 Accuracy 0.9466\n",
            "Epoch 73 Batch 150 Loss 0.1778 Accuracy 0.9453\n",
            "Epoch 73 Batch 200 Loss 0.1823 Accuracy 0.9440\n",
            "Epoch 73 Batch 250 Loss 0.1847 Accuracy 0.9430\n",
            "Epoch 73 Batch 300 Loss 0.1874 Accuracy 0.9421\n",
            "Epoch 73 Batch 350 Loss 0.1900 Accuracy 0.9413\n",
            "Epoch 73 Loss 0.1901 Accuracy 0.9412\n",
            "Time taken for 1 epoch: 61.45 secs\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.1843 Accuracy 0.9416\n",
            "Epoch 74 Batch 50 Loss 0.1797 Accuracy 0.9450\n",
            "Epoch 74 Batch 100 Loss 0.1818 Accuracy 0.9445\n",
            "Epoch 74 Batch 150 Loss 0.1832 Accuracy 0.9439\n",
            "Epoch 74 Batch 200 Loss 0.1834 Accuracy 0.9434\n",
            "Epoch 74 Batch 250 Loss 0.1845 Accuracy 0.9434\n",
            "Epoch 74 Batch 300 Loss 0.1864 Accuracy 0.9428\n",
            "Epoch 74 Batch 350 Loss 0.1882 Accuracy 0.9423\n",
            "Epoch 74 Loss 0.1882 Accuracy 0.9422\n",
            "Time taken for 1 epoch: 60.86 secs\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.1455 Accuracy 0.9545\n",
            "Epoch 75 Batch 50 Loss 0.1752 Accuracy 0.9466\n",
            "Epoch 75 Batch 100 Loss 0.1756 Accuracy 0.9462\n",
            "Epoch 75 Batch 150 Loss 0.1778 Accuracy 0.9456\n",
            "Epoch 75 Batch 200 Loss 0.1802 Accuracy 0.9446\n",
            "Epoch 75 Batch 250 Loss 0.1823 Accuracy 0.9438\n",
            "Epoch 75 Batch 300 Loss 0.1838 Accuracy 0.9433\n",
            "Epoch 75 Batch 350 Loss 0.1852 Accuracy 0.9429\n",
            "Saving checkpoint for epoch 75 at ./checkpoints/train/ckpt-15\n",
            "Epoch 75 Loss 0.1856 Accuracy 0.9428\n",
            "Time taken for 1 epoch: 63.68 secs\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.1411 Accuracy 0.9591\n",
            "Epoch 76 Batch 50 Loss 0.1726 Accuracy 0.9474\n",
            "Epoch 76 Batch 100 Loss 0.1741 Accuracy 0.9467\n",
            "Epoch 76 Batch 150 Loss 0.1767 Accuracy 0.9458\n",
            "Epoch 76 Batch 200 Loss 0.1792 Accuracy 0.9447\n",
            "Epoch 76 Batch 250 Loss 0.1813 Accuracy 0.9440\n",
            "Epoch 76 Batch 300 Loss 0.1831 Accuracy 0.9436\n",
            "Epoch 76 Batch 350 Loss 0.1846 Accuracy 0.9429\n",
            "Epoch 76 Loss 0.1847 Accuracy 0.9429\n",
            "Time taken for 1 epoch: 61.22 secs\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.1762 Accuracy 0.9486\n",
            "Epoch 77 Batch 50 Loss 0.1594 Accuracy 0.9505\n",
            "Epoch 77 Batch 100 Loss 0.1649 Accuracy 0.9485\n",
            "Epoch 77 Batch 150 Loss 0.1687 Accuracy 0.9477\n",
            "Epoch 77 Batch 200 Loss 0.1708 Accuracy 0.9472\n",
            "Epoch 77 Batch 250 Loss 0.1751 Accuracy 0.9460\n",
            "Epoch 77 Batch 300 Loss 0.1773 Accuracy 0.9451\n",
            "Epoch 77 Batch 350 Loss 0.1807 Accuracy 0.9439\n",
            "Epoch 77 Loss 0.1809 Accuracy 0.9437\n",
            "Time taken for 1 epoch: 61.24 secs\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.1726 Accuracy 0.9511\n",
            "Epoch 78 Batch 50 Loss 0.1631 Accuracy 0.9492\n",
            "Epoch 78 Batch 100 Loss 0.1643 Accuracy 0.9492\n",
            "Epoch 78 Batch 150 Loss 0.1658 Accuracy 0.9486\n",
            "Epoch 78 Batch 200 Loss 0.1668 Accuracy 0.9485\n",
            "Epoch 78 Batch 250 Loss 0.1689 Accuracy 0.9479\n",
            "Epoch 78 Batch 300 Loss 0.1724 Accuracy 0.9470\n",
            "Epoch 78 Batch 350 Loss 0.1749 Accuracy 0.9463\n",
            "Epoch 78 Loss 0.1749 Accuracy 0.9462\n",
            "Time taken for 1 epoch: 60.76 secs\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.1307 Accuracy 0.9674\n",
            "Epoch 79 Batch 50 Loss 0.1616 Accuracy 0.9513\n",
            "Epoch 79 Batch 100 Loss 0.1591 Accuracy 0.9519\n",
            "Epoch 79 Batch 150 Loss 0.1643 Accuracy 0.9501\n",
            "Epoch 79 Batch 200 Loss 0.1641 Accuracy 0.9501\n",
            "Epoch 79 Batch 250 Loss 0.1665 Accuracy 0.9493\n",
            "Epoch 79 Batch 300 Loss 0.1686 Accuracy 0.9484\n",
            "Epoch 79 Batch 350 Loss 0.1714 Accuracy 0.9474\n",
            "Epoch 79 Loss 0.1720 Accuracy 0.9471\n",
            "Time taken for 1 epoch: 60.89 secs\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.0946 Accuracy 0.9673\n",
            "Epoch 80 Batch 50 Loss 0.1456 Accuracy 0.9556\n",
            "Epoch 80 Batch 100 Loss 0.1515 Accuracy 0.9534\n",
            "Epoch 80 Batch 150 Loss 0.1534 Accuracy 0.9528\n",
            "Epoch 80 Batch 200 Loss 0.1575 Accuracy 0.9515\n",
            "Epoch 80 Batch 250 Loss 0.1610 Accuracy 0.9506\n",
            "Epoch 80 Batch 300 Loss 0.1645 Accuracy 0.9495\n",
            "Epoch 80 Batch 350 Loss 0.1657 Accuracy 0.9492\n",
            "Saving checkpoint for epoch 80 at ./checkpoints/train/ckpt-16\n",
            "Epoch 80 Loss 0.1657 Accuracy 0.9493\n",
            "Time taken for 1 epoch: 63.88 secs\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.1778 Accuracy 0.9502\n",
            "Epoch 81 Batch 50 Loss 0.1506 Accuracy 0.9543\n",
            "Epoch 81 Batch 100 Loss 0.1589 Accuracy 0.9518\n",
            "Epoch 81 Batch 150 Loss 0.1586 Accuracy 0.9516\n",
            "Epoch 81 Batch 200 Loss 0.1609 Accuracy 0.9510\n",
            "Epoch 81 Batch 250 Loss 0.1611 Accuracy 0.9506\n",
            "Epoch 81 Batch 300 Loss 0.1633 Accuracy 0.9499\n",
            "Epoch 81 Batch 350 Loss 0.1663 Accuracy 0.9490\n",
            "Epoch 81 Loss 0.1668 Accuracy 0.9489\n",
            "Time taken for 1 epoch: 60.84 secs\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.1478 Accuracy 0.9614\n",
            "Epoch 82 Batch 50 Loss 0.1487 Accuracy 0.9551\n",
            "Epoch 82 Batch 100 Loss 0.1510 Accuracy 0.9543\n",
            "Epoch 82 Batch 150 Loss 0.1537 Accuracy 0.9532\n",
            "Epoch 82 Batch 200 Loss 0.1554 Accuracy 0.9523\n",
            "Epoch 82 Batch 250 Loss 0.1573 Accuracy 0.9516\n",
            "Epoch 82 Batch 300 Loss 0.1595 Accuracy 0.9511\n",
            "Epoch 82 Batch 350 Loss 0.1619 Accuracy 0.9503\n",
            "Epoch 82 Loss 0.1627 Accuracy 0.9501\n",
            "Time taken for 1 epoch: 60.98 secs\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.1493 Accuracy 0.9599\n",
            "Epoch 83 Batch 50 Loss 0.1451 Accuracy 0.9554\n",
            "Epoch 83 Batch 100 Loss 0.1470 Accuracy 0.9544\n",
            "Epoch 83 Batch 150 Loss 0.1496 Accuracy 0.9536\n",
            "Epoch 83 Batch 200 Loss 0.1543 Accuracy 0.9520\n",
            "Epoch 83 Batch 250 Loss 0.1562 Accuracy 0.9514\n",
            "Epoch 83 Batch 300 Loss 0.1589 Accuracy 0.9505\n",
            "Epoch 83 Batch 350 Loss 0.1609 Accuracy 0.9500\n",
            "Epoch 83 Loss 0.1611 Accuracy 0.9499\n",
            "Time taken for 1 epoch: 61.38 secs\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.1561 Accuracy 0.9645\n",
            "Epoch 84 Batch 50 Loss 0.1450 Accuracy 0.9562\n",
            "Epoch 84 Batch 100 Loss 0.1465 Accuracy 0.9551\n",
            "Epoch 84 Batch 150 Loss 0.1485 Accuracy 0.9546\n",
            "Epoch 84 Batch 200 Loss 0.1511 Accuracy 0.9539\n",
            "Epoch 84 Batch 250 Loss 0.1529 Accuracy 0.9532\n",
            "Epoch 84 Batch 300 Loss 0.1545 Accuracy 0.9528\n",
            "Epoch 84 Batch 350 Loss 0.1554 Accuracy 0.9524\n",
            "Epoch 84 Loss 0.1555 Accuracy 0.9523\n",
            "Time taken for 1 epoch: 61.15 secs\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.0942 Accuracy 0.9674\n",
            "Epoch 85 Batch 50 Loss 0.1360 Accuracy 0.9576\n",
            "Epoch 85 Batch 100 Loss 0.1416 Accuracy 0.9559\n",
            "Epoch 85 Batch 150 Loss 0.1439 Accuracy 0.9555\n",
            "Epoch 85 Batch 200 Loss 0.1456 Accuracy 0.9552\n",
            "Epoch 85 Batch 250 Loss 0.1486 Accuracy 0.9542\n",
            "Epoch 85 Batch 300 Loss 0.1506 Accuracy 0.9536\n",
            "Epoch 85 Batch 350 Loss 0.1535 Accuracy 0.9525\n",
            "Saving checkpoint for epoch 85 at ./checkpoints/train/ckpt-17\n",
            "Epoch 85 Loss 0.1537 Accuracy 0.9525\n",
            "Time taken for 1 epoch: 64.17 secs\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.1064 Accuracy 0.9695\n",
            "Epoch 86 Batch 50 Loss 0.1471 Accuracy 0.9554\n",
            "Epoch 86 Batch 100 Loss 0.1397 Accuracy 0.9576\n",
            "Epoch 86 Batch 150 Loss 0.1398 Accuracy 0.9574\n",
            "Epoch 86 Batch 200 Loss 0.1432 Accuracy 0.9561\n",
            "Epoch 86 Batch 250 Loss 0.1443 Accuracy 0.9557\n",
            "Epoch 86 Batch 300 Loss 0.1461 Accuracy 0.9551\n",
            "Epoch 86 Batch 350 Loss 0.1496 Accuracy 0.9541\n",
            "Epoch 86 Loss 0.1500 Accuracy 0.9540\n",
            "Time taken for 1 epoch: 61.16 secs\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.1192 Accuracy 0.9624\n",
            "Epoch 87 Batch 50 Loss 0.1491 Accuracy 0.9544\n",
            "Epoch 87 Batch 100 Loss 0.1447 Accuracy 0.9555\n",
            "Epoch 87 Batch 150 Loss 0.1443 Accuracy 0.9555\n",
            "Epoch 87 Batch 200 Loss 0.1441 Accuracy 0.9557\n",
            "Epoch 87 Batch 250 Loss 0.1464 Accuracy 0.9549\n",
            "Epoch 87 Batch 300 Loss 0.1483 Accuracy 0.9545\n",
            "Epoch 87 Batch 350 Loss 0.1488 Accuracy 0.9543\n",
            "Epoch 87 Loss 0.1491 Accuracy 0.9542\n",
            "Time taken for 1 epoch: 60.82 secs\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.1254 Accuracy 0.9584\n",
            "Epoch 88 Batch 50 Loss 0.1290 Accuracy 0.9615\n",
            "Epoch 88 Batch 100 Loss 0.1343 Accuracy 0.9588\n",
            "Epoch 88 Batch 150 Loss 0.1389 Accuracy 0.9573\n",
            "Epoch 88 Batch 200 Loss 0.1425 Accuracy 0.9565\n",
            "Epoch 88 Batch 250 Loss 0.1431 Accuracy 0.9563\n",
            "Epoch 88 Batch 300 Loss 0.1440 Accuracy 0.9559\n",
            "Epoch 88 Batch 350 Loss 0.1462 Accuracy 0.9552\n",
            "Epoch 88 Loss 0.1462 Accuracy 0.9551\n",
            "Time taken for 1 epoch: 60.82 secs\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.1577 Accuracy 0.9507\n",
            "Epoch 89 Batch 50 Loss 0.1384 Accuracy 0.9590\n",
            "Epoch 89 Batch 100 Loss 0.1382 Accuracy 0.9577\n",
            "Epoch 89 Batch 150 Loss 0.1396 Accuracy 0.9573\n",
            "Epoch 89 Batch 200 Loss 0.1429 Accuracy 0.9560\n",
            "Epoch 89 Batch 250 Loss 0.1430 Accuracy 0.9561\n",
            "Epoch 89 Batch 300 Loss 0.1434 Accuracy 0.9560\n",
            "Epoch 89 Batch 350 Loss 0.1455 Accuracy 0.9555\n",
            "Epoch 89 Loss 0.1457 Accuracy 0.9555\n",
            "Time taken for 1 epoch: 60.93 secs\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.1328 Accuracy 0.9542\n",
            "Epoch 90 Batch 50 Loss 0.1265 Accuracy 0.9619\n",
            "Epoch 90 Batch 100 Loss 0.1308 Accuracy 0.9599\n",
            "Epoch 90 Batch 150 Loss 0.1333 Accuracy 0.9593\n",
            "Epoch 90 Batch 200 Loss 0.1369 Accuracy 0.9582\n",
            "Epoch 90 Batch 250 Loss 0.1380 Accuracy 0.9578\n",
            "Epoch 90 Batch 300 Loss 0.1397 Accuracy 0.9571\n",
            "Epoch 90 Batch 350 Loss 0.1416 Accuracy 0.9565\n",
            "Saving checkpoint for epoch 90 at ./checkpoints/train/ckpt-18\n",
            "Epoch 90 Loss 0.1418 Accuracy 0.9565\n",
            "Time taken for 1 epoch: 64.51 secs\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.1094 Accuracy 0.9608\n",
            "Epoch 91 Batch 50 Loss 0.1328 Accuracy 0.9602\n",
            "Epoch 91 Batch 100 Loss 0.1320 Accuracy 0.9607\n",
            "Epoch 91 Batch 150 Loss 0.1357 Accuracy 0.9593\n",
            "Epoch 91 Batch 200 Loss 0.1366 Accuracy 0.9589\n",
            "Epoch 91 Batch 250 Loss 0.1371 Accuracy 0.9587\n",
            "Epoch 91 Batch 300 Loss 0.1404 Accuracy 0.9575\n",
            "Epoch 91 Batch 350 Loss 0.1435 Accuracy 0.9564\n",
            "Epoch 91 Loss 0.1435 Accuracy 0.9564\n",
            "Time taken for 1 epoch: 61.35 secs\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.1241 Accuracy 0.9551\n",
            "Epoch 92 Batch 50 Loss 0.1313 Accuracy 0.9595\n",
            "Epoch 92 Batch 100 Loss 0.1302 Accuracy 0.9597\n",
            "Epoch 92 Batch 150 Loss 0.1304 Accuracy 0.9596\n",
            "Epoch 92 Batch 200 Loss 0.1325 Accuracy 0.9591\n",
            "Epoch 92 Batch 250 Loss 0.1339 Accuracy 0.9589\n",
            "Epoch 92 Batch 300 Loss 0.1361 Accuracy 0.9582\n",
            "Epoch 92 Batch 350 Loss 0.1375 Accuracy 0.9576\n",
            "Epoch 92 Loss 0.1379 Accuracy 0.9574\n",
            "Time taken for 1 epoch: 61.22 secs\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.0974 Accuracy 0.9772\n",
            "Epoch 93 Batch 50 Loss 0.1233 Accuracy 0.9636\n",
            "Epoch 93 Batch 100 Loss 0.1260 Accuracy 0.9622\n",
            "Epoch 93 Batch 150 Loss 0.1291 Accuracy 0.9614\n",
            "Epoch 93 Batch 200 Loss 0.1327 Accuracy 0.9603\n",
            "Epoch 93 Batch 250 Loss 0.1346 Accuracy 0.9592\n",
            "Epoch 93 Batch 300 Loss 0.1358 Accuracy 0.9588\n",
            "Epoch 93 Batch 350 Loss 0.1375 Accuracy 0.9581\n",
            "Epoch 93 Loss 0.1374 Accuracy 0.9581\n",
            "Time taken for 1 epoch: 60.82 secs\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.1015 Accuracy 0.9657\n",
            "Epoch 94 Batch 50 Loss 0.1229 Accuracy 0.9632\n",
            "Epoch 94 Batch 100 Loss 0.1261 Accuracy 0.9616\n",
            "Epoch 94 Batch 150 Loss 0.1286 Accuracy 0.9603\n",
            "Epoch 94 Batch 200 Loss 0.1301 Accuracy 0.9600\n",
            "Epoch 94 Batch 250 Loss 0.1313 Accuracy 0.9597\n",
            "Epoch 94 Batch 300 Loss 0.1313 Accuracy 0.9599\n",
            "Epoch 94 Batch 350 Loss 0.1339 Accuracy 0.9590\n",
            "Epoch 94 Loss 0.1342 Accuracy 0.9590\n",
            "Time taken for 1 epoch: 60.79 secs\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.0888 Accuracy 0.9745\n",
            "Epoch 95 Batch 50 Loss 0.1274 Accuracy 0.9610\n",
            "Epoch 95 Batch 100 Loss 0.1259 Accuracy 0.9610\n",
            "Epoch 95 Batch 150 Loss 0.1266 Accuracy 0.9611\n",
            "Epoch 95 Batch 200 Loss 0.1279 Accuracy 0.9610\n",
            "Epoch 95 Batch 250 Loss 0.1306 Accuracy 0.9602\n",
            "Epoch 95 Batch 300 Loss 0.1332 Accuracy 0.9593\n",
            "Epoch 95 Batch 350 Loss 0.1345 Accuracy 0.9588\n",
            "Saving checkpoint for epoch 95 at ./checkpoints/train/ckpt-19\n",
            "Epoch 95 Loss 0.1348 Accuracy 0.9588\n",
            "Time taken for 1 epoch: 63.64 secs\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.1207 Accuracy 0.9684\n",
            "Epoch 96 Batch 50 Loss 0.1223 Accuracy 0.9635\n",
            "Epoch 96 Batch 100 Loss 0.1234 Accuracy 0.9630\n",
            "Epoch 96 Batch 150 Loss 0.1237 Accuracy 0.9627\n",
            "Epoch 96 Batch 200 Loss 0.1259 Accuracy 0.9621\n",
            "Epoch 96 Batch 250 Loss 0.1266 Accuracy 0.9617\n",
            "Epoch 96 Batch 300 Loss 0.1291 Accuracy 0.9609\n",
            "Epoch 96 Batch 350 Loss 0.1306 Accuracy 0.9602\n",
            "Epoch 96 Loss 0.1304 Accuracy 0.9603\n",
            "Time taken for 1 epoch: 61.28 secs\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.0902 Accuracy 0.9747\n",
            "Epoch 97 Batch 50 Loss 0.1158 Accuracy 0.9638\n",
            "Epoch 97 Batch 100 Loss 0.1186 Accuracy 0.9633\n",
            "Epoch 97 Batch 150 Loss 0.1210 Accuracy 0.9631\n",
            "Epoch 97 Batch 200 Loss 0.1226 Accuracy 0.9626\n",
            "Epoch 97 Batch 250 Loss 0.1240 Accuracy 0.9622\n",
            "Epoch 97 Batch 300 Loss 0.1249 Accuracy 0.9620\n",
            "Epoch 97 Batch 350 Loss 0.1265 Accuracy 0.9614\n",
            "Epoch 97 Loss 0.1266 Accuracy 0.9614\n",
            "Time taken for 1 epoch: 61.45 secs\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.1709 Accuracy 0.9560\n",
            "Epoch 98 Batch 50 Loss 0.1156 Accuracy 0.9651\n",
            "Epoch 98 Batch 100 Loss 0.1196 Accuracy 0.9637\n",
            "Epoch 98 Batch 150 Loss 0.1217 Accuracy 0.9627\n",
            "Epoch 98 Batch 200 Loss 0.1260 Accuracy 0.9612\n",
            "Epoch 98 Batch 250 Loss 0.1282 Accuracy 0.9605\n",
            "Epoch 98 Batch 300 Loss 0.1289 Accuracy 0.9604\n",
            "Epoch 98 Batch 350 Loss 0.1292 Accuracy 0.9602\n",
            "Epoch 98 Loss 0.1292 Accuracy 0.9601\n",
            "Time taken for 1 epoch: 60.87 secs\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.1165 Accuracy 0.9601\n",
            "Epoch 99 Batch 50 Loss 0.1127 Accuracy 0.9663\n",
            "Epoch 99 Batch 100 Loss 0.1184 Accuracy 0.9644\n",
            "Epoch 99 Batch 150 Loss 0.1183 Accuracy 0.9642\n",
            "Epoch 99 Batch 200 Loss 0.1209 Accuracy 0.9633\n",
            "Epoch 99 Batch 250 Loss 0.1228 Accuracy 0.9627\n",
            "Epoch 99 Batch 300 Loss 0.1252 Accuracy 0.9620\n",
            "Epoch 99 Batch 350 Loss 0.1261 Accuracy 0.9615\n",
            "Epoch 99 Loss 0.1263 Accuracy 0.9615\n",
            "Time taken for 1 epoch: 60.94 secs\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.1244 Accuracy 0.9538\n",
            "Epoch 100 Batch 50 Loss 0.1093 Accuracy 0.9661\n",
            "Epoch 100 Batch 100 Loss 0.1088 Accuracy 0.9655\n",
            "Epoch 100 Batch 150 Loss 0.1125 Accuracy 0.9646\n",
            "Epoch 100 Batch 200 Loss 0.1141 Accuracy 0.9647\n",
            "Epoch 100 Batch 250 Loss 0.1183 Accuracy 0.9635\n",
            "Epoch 100 Batch 300 Loss 0.1196 Accuracy 0.9631\n",
            "Epoch 100 Batch 350 Loss 0.1214 Accuracy 0.9627\n",
            "Saving checkpoint for epoch 100 at ./checkpoints/train/ckpt-20\n",
            "Epoch 100 Loss 0.1214 Accuracy 0.9626\n",
            "Time taken for 1 epoch: 63.79 secs\n",
            "\n",
            "Epoch 101 Batch 0 Loss 0.1306 Accuracy 0.9622\n",
            "Epoch 101 Batch 50 Loss 0.1081 Accuracy 0.9668\n",
            "Epoch 101 Batch 100 Loss 0.1109 Accuracy 0.9657\n",
            "Epoch 101 Batch 150 Loss 0.1145 Accuracy 0.9646\n",
            "Epoch 101 Batch 200 Loss 0.1187 Accuracy 0.9633\n",
            "Epoch 101 Batch 250 Loss 0.1211 Accuracy 0.9626\n",
            "Epoch 101 Batch 300 Loss 0.1244 Accuracy 0.9616\n",
            "Epoch 101 Batch 350 Loss 0.1251 Accuracy 0.9615\n",
            "Epoch 101 Loss 0.1251 Accuracy 0.9615\n",
            "Time taken for 1 epoch: 61.19 secs\n",
            "\n",
            "Epoch 102 Batch 0 Loss 0.0962 Accuracy 0.9627\n",
            "Epoch 102 Batch 50 Loss 0.1089 Accuracy 0.9670\n",
            "Epoch 102 Batch 100 Loss 0.1133 Accuracy 0.9654\n",
            "Epoch 102 Batch 150 Loss 0.1137 Accuracy 0.9651\n",
            "Epoch 102 Batch 200 Loss 0.1170 Accuracy 0.9639\n",
            "Epoch 102 Batch 250 Loss 0.1182 Accuracy 0.9637\n",
            "Epoch 102 Batch 300 Loss 0.1195 Accuracy 0.9634\n",
            "Epoch 102 Batch 350 Loss 0.1211 Accuracy 0.9629\n",
            "Epoch 102 Loss 0.1215 Accuracy 0.9628\n",
            "Time taken for 1 epoch: 60.96 secs\n",
            "\n",
            "Epoch 103 Batch 0 Loss 0.0711 Accuracy 0.9756\n",
            "Epoch 103 Batch 50 Loss 0.1100 Accuracy 0.9675\n",
            "Epoch 103 Batch 100 Loss 0.1093 Accuracy 0.9668\n",
            "Epoch 103 Batch 150 Loss 0.1115 Accuracy 0.9661\n",
            "Epoch 103 Batch 200 Loss 0.1131 Accuracy 0.9655\n",
            "Epoch 103 Batch 250 Loss 0.1145 Accuracy 0.9651\n",
            "Epoch 103 Batch 300 Loss 0.1162 Accuracy 0.9644\n",
            "Epoch 103 Batch 350 Loss 0.1183 Accuracy 0.9638\n",
            "Epoch 103 Loss 0.1182 Accuracy 0.9637\n",
            "Time taken for 1 epoch: 61.30 secs\n",
            "\n",
            "Epoch 104 Batch 0 Loss 0.1123 Accuracy 0.9690\n",
            "Epoch 104 Batch 50 Loss 0.1097 Accuracy 0.9671\n",
            "Epoch 104 Batch 100 Loss 0.1108 Accuracy 0.9666\n",
            "Epoch 104 Batch 150 Loss 0.1111 Accuracy 0.9667\n",
            "Epoch 104 Batch 200 Loss 0.1116 Accuracy 0.9662\n",
            "Epoch 104 Batch 250 Loss 0.1130 Accuracy 0.9656\n",
            "Epoch 104 Batch 300 Loss 0.1140 Accuracy 0.9653\n",
            "Epoch 104 Batch 350 Loss 0.1154 Accuracy 0.9648\n",
            "Epoch 104 Loss 0.1157 Accuracy 0.9647\n",
            "Time taken for 1 epoch: 61.58 secs\n",
            "\n",
            "Epoch 105 Batch 0 Loss 0.0907 Accuracy 0.9721\n",
            "Epoch 105 Batch 50 Loss 0.1053 Accuracy 0.9695\n",
            "Epoch 105 Batch 100 Loss 0.1079 Accuracy 0.9679\n",
            "Epoch 105 Batch 150 Loss 0.1088 Accuracy 0.9672\n",
            "Epoch 105 Batch 200 Loss 0.1114 Accuracy 0.9664\n",
            "Epoch 105 Batch 250 Loss 0.1126 Accuracy 0.9660\n",
            "Epoch 105 Batch 300 Loss 0.1143 Accuracy 0.9655\n",
            "Epoch 105 Batch 350 Loss 0.1152 Accuracy 0.9651\n",
            "Saving checkpoint for epoch 105 at ./checkpoints/train/ckpt-21\n",
            "Epoch 105 Loss 0.1152 Accuracy 0.9651\n",
            "Time taken for 1 epoch: 63.68 secs\n",
            "\n",
            "Epoch 106 Batch 0 Loss 0.0943 Accuracy 0.9656\n",
            "Epoch 106 Batch 50 Loss 0.1013 Accuracy 0.9694\n",
            "Epoch 106 Batch 100 Loss 0.1023 Accuracy 0.9691\n",
            "Epoch 106 Batch 150 Loss 0.1058 Accuracy 0.9679\n",
            "Epoch 106 Batch 200 Loss 0.1090 Accuracy 0.9669\n",
            "Epoch 106 Batch 250 Loss 0.1094 Accuracy 0.9667\n",
            "Epoch 106 Batch 300 Loss 0.1095 Accuracy 0.9667\n",
            "Epoch 106 Batch 350 Loss 0.1104 Accuracy 0.9663\n",
            "Epoch 106 Loss 0.1105 Accuracy 0.9663\n",
            "Time taken for 1 epoch: 61.18 secs\n",
            "\n",
            "Epoch 107 Batch 0 Loss 0.1222 Accuracy 0.9704\n",
            "Epoch 107 Batch 50 Loss 0.1103 Accuracy 0.9679\n",
            "Epoch 107 Batch 100 Loss 0.1072 Accuracy 0.9682\n",
            "Epoch 107 Batch 150 Loss 0.1077 Accuracy 0.9673\n",
            "Epoch 107 Batch 200 Loss 0.1083 Accuracy 0.9672\n",
            "Epoch 107 Batch 250 Loss 0.1081 Accuracy 0.9672\n",
            "Epoch 107 Batch 300 Loss 0.1099 Accuracy 0.9666\n",
            "Epoch 107 Batch 350 Loss 0.1099 Accuracy 0.9664\n",
            "Epoch 107 Loss 0.1103 Accuracy 0.9663\n",
            "Time taken for 1 epoch: 60.69 secs\n",
            "\n",
            "Epoch 108 Batch 0 Loss 0.1163 Accuracy 0.9600\n",
            "Epoch 108 Batch 50 Loss 0.1031 Accuracy 0.9696\n",
            "Epoch 108 Batch 100 Loss 0.1028 Accuracy 0.9693\n",
            "Epoch 108 Batch 150 Loss 0.1055 Accuracy 0.9682\n",
            "Epoch 108 Batch 200 Loss 0.1071 Accuracy 0.9677\n",
            "Epoch 108 Batch 250 Loss 0.1081 Accuracy 0.9674\n",
            "Epoch 108 Batch 300 Loss 0.1093 Accuracy 0.9671\n",
            "Epoch 108 Batch 350 Loss 0.1105 Accuracy 0.9666\n",
            "Epoch 108 Loss 0.1109 Accuracy 0.9664\n",
            "Time taken for 1 epoch: 60.51 secs\n",
            "\n",
            "Epoch 109 Batch 0 Loss 0.0668 Accuracy 0.9816\n",
            "Epoch 109 Batch 50 Loss 0.1048 Accuracy 0.9683\n",
            "Epoch 109 Batch 100 Loss 0.1034 Accuracy 0.9688\n",
            "Epoch 109 Batch 150 Loss 0.1046 Accuracy 0.9687\n",
            "Epoch 109 Batch 200 Loss 0.1042 Accuracy 0.9688\n",
            "Epoch 109 Batch 250 Loss 0.1057 Accuracy 0.9681\n",
            "Epoch 109 Batch 300 Loss 0.1063 Accuracy 0.9680\n",
            "Epoch 109 Batch 350 Loss 0.1072 Accuracy 0.9678\n",
            "Epoch 109 Loss 0.1073 Accuracy 0.9677\n",
            "Time taken for 1 epoch: 60.93 secs\n",
            "\n",
            "Epoch 110 Batch 0 Loss 0.0896 Accuracy 0.9717\n",
            "Epoch 110 Batch 50 Loss 0.1013 Accuracy 0.9700\n",
            "Epoch 110 Batch 100 Loss 0.0991 Accuracy 0.9702\n",
            "Epoch 110 Batch 150 Loss 0.1006 Accuracy 0.9695\n",
            "Epoch 110 Batch 200 Loss 0.1040 Accuracy 0.9687\n",
            "Epoch 110 Batch 250 Loss 0.1056 Accuracy 0.9681\n",
            "Epoch 110 Batch 300 Loss 0.1059 Accuracy 0.9681\n",
            "Epoch 110 Batch 350 Loss 0.1074 Accuracy 0.9675\n",
            "Saving checkpoint for epoch 110 at ./checkpoints/train/ckpt-22\n",
            "Epoch 110 Loss 0.1078 Accuracy 0.9674\n",
            "Time taken for 1 epoch: 63.77 secs\n",
            "\n",
            "Epoch 111 Batch 0 Loss 0.0890 Accuracy 0.9753\n",
            "Epoch 111 Batch 50 Loss 0.1014 Accuracy 0.9681\n",
            "Epoch 111 Batch 100 Loss 0.1059 Accuracy 0.9675\n",
            "Epoch 111 Batch 150 Loss 0.1057 Accuracy 0.9676\n",
            "Epoch 111 Batch 200 Loss 0.1070 Accuracy 0.9672\n",
            "Epoch 111 Batch 250 Loss 0.1094 Accuracy 0.9666\n",
            "Epoch 111 Batch 300 Loss 0.1104 Accuracy 0.9662\n",
            "Epoch 111 Batch 350 Loss 0.1108 Accuracy 0.9660\n",
            "Epoch 111 Loss 0.1110 Accuracy 0.9660\n",
            "Time taken for 1 epoch: 61.68 secs\n",
            "\n",
            "Epoch 112 Batch 0 Loss 0.0777 Accuracy 0.9742\n",
            "Epoch 112 Batch 50 Loss 0.0910 Accuracy 0.9721\n",
            "Epoch 112 Batch 100 Loss 0.0937 Accuracy 0.9717\n",
            "Epoch 112 Batch 150 Loss 0.0977 Accuracy 0.9699\n",
            "Epoch 112 Batch 200 Loss 0.0997 Accuracy 0.9693\n",
            "Epoch 112 Batch 250 Loss 0.1021 Accuracy 0.9685\n",
            "Epoch 112 Batch 300 Loss 0.1027 Accuracy 0.9686\n",
            "Epoch 112 Batch 350 Loss 0.1041 Accuracy 0.9681\n",
            "Epoch 112 Loss 0.1045 Accuracy 0.9680\n",
            "Time taken for 1 epoch: 60.87 secs\n",
            "\n",
            "Epoch 113 Batch 0 Loss 0.1025 Accuracy 0.9694\n",
            "Epoch 113 Batch 50 Loss 0.1069 Accuracy 0.9686\n",
            "Epoch 113 Batch 100 Loss 0.1032 Accuracy 0.9696\n",
            "Epoch 113 Batch 150 Loss 0.1035 Accuracy 0.9690\n",
            "Epoch 113 Batch 200 Loss 0.1048 Accuracy 0.9684\n",
            "Epoch 113 Batch 250 Loss 0.1048 Accuracy 0.9684\n",
            "Epoch 113 Batch 300 Loss 0.1058 Accuracy 0.9680\n",
            "Epoch 113 Batch 350 Loss 0.1065 Accuracy 0.9677\n",
            "Epoch 113 Loss 0.1065 Accuracy 0.9677\n",
            "Time taken for 1 epoch: 60.72 secs\n",
            "\n",
            "Epoch 114 Batch 0 Loss 0.0944 Accuracy 0.9672\n",
            "Epoch 114 Batch 50 Loss 0.0879 Accuracy 0.9736\n",
            "Epoch 114 Batch 100 Loss 0.0928 Accuracy 0.9722\n",
            "Epoch 114 Batch 150 Loss 0.0959 Accuracy 0.9711\n",
            "Epoch 114 Batch 200 Loss 0.0988 Accuracy 0.9702\n",
            "Epoch 114 Batch 250 Loss 0.1006 Accuracy 0.9695\n",
            "Epoch 114 Batch 300 Loss 0.1020 Accuracy 0.9689\n",
            "Epoch 114 Batch 350 Loss 0.1038 Accuracy 0.9685\n",
            "Epoch 114 Loss 0.1038 Accuracy 0.9684\n",
            "Time taken for 1 epoch: 61.00 secs\n",
            "\n",
            "Epoch 115 Batch 0 Loss 0.0797 Accuracy 0.9735\n",
            "Epoch 115 Batch 50 Loss 0.0974 Accuracy 0.9704\n",
            "Epoch 115 Batch 100 Loss 0.0972 Accuracy 0.9706\n",
            "Epoch 115 Batch 150 Loss 0.0995 Accuracy 0.9700\n",
            "Epoch 115 Batch 200 Loss 0.1004 Accuracy 0.9700\n",
            "Epoch 115 Batch 250 Loss 0.1015 Accuracy 0.9696\n",
            "Epoch 115 Batch 300 Loss 0.1030 Accuracy 0.9690\n",
            "Epoch 115 Batch 350 Loss 0.1036 Accuracy 0.9688\n",
            "Saving checkpoint for epoch 115 at ./checkpoints/train/ckpt-23\n",
            "Epoch 115 Loss 0.1034 Accuracy 0.9688\n",
            "Time taken for 1 epoch: 63.82 secs\n",
            "\n",
            "Epoch 116 Batch 0 Loss 0.1148 Accuracy 0.9657\n",
            "Epoch 116 Batch 50 Loss 0.0908 Accuracy 0.9719\n",
            "Epoch 116 Batch 100 Loss 0.0938 Accuracy 0.9715\n",
            "Epoch 116 Batch 150 Loss 0.0931 Accuracy 0.9720\n",
            "Epoch 116 Batch 200 Loss 0.0953 Accuracy 0.9714\n",
            "Epoch 116 Batch 250 Loss 0.0977 Accuracy 0.9707\n",
            "Epoch 116 Batch 300 Loss 0.0997 Accuracy 0.9700\n",
            "Epoch 116 Batch 350 Loss 0.1008 Accuracy 0.9695\n",
            "Epoch 116 Loss 0.1006 Accuracy 0.9696\n",
            "Time taken for 1 epoch: 60.58 secs\n",
            "\n",
            "Epoch 117 Batch 0 Loss 0.1903 Accuracy 0.9435\n",
            "Epoch 117 Batch 50 Loss 0.1008 Accuracy 0.9699\n",
            "Epoch 117 Batch 100 Loss 0.0995 Accuracy 0.9699\n",
            "Epoch 117 Batch 150 Loss 0.1001 Accuracy 0.9698\n",
            "Epoch 117 Batch 200 Loss 0.1001 Accuracy 0.9696\n",
            "Epoch 117 Batch 250 Loss 0.1010 Accuracy 0.9691\n",
            "Epoch 117 Batch 300 Loss 0.1006 Accuracy 0.9692\n",
            "Epoch 117 Batch 350 Loss 0.1014 Accuracy 0.9689\n",
            "Epoch 117 Loss 0.1017 Accuracy 0.9689\n",
            "Time taken for 1 epoch: 60.77 secs\n",
            "\n",
            "Epoch 118 Batch 0 Loss 0.0814 Accuracy 0.9688\n",
            "Epoch 118 Batch 50 Loss 0.0893 Accuracy 0.9724\n",
            "Epoch 118 Batch 100 Loss 0.0924 Accuracy 0.9716\n",
            "Epoch 118 Batch 150 Loss 0.0930 Accuracy 0.9715\n",
            "Epoch 118 Batch 200 Loss 0.0958 Accuracy 0.9708\n",
            "Epoch 118 Batch 250 Loss 0.0973 Accuracy 0.9706\n",
            "Epoch 118 Batch 300 Loss 0.0983 Accuracy 0.9704\n",
            "Epoch 118 Batch 350 Loss 0.1001 Accuracy 0.9696\n",
            "Epoch 118 Loss 0.1001 Accuracy 0.9697\n",
            "Time taken for 1 epoch: 61.19 secs\n",
            "\n",
            "Epoch 119 Batch 0 Loss 0.0737 Accuracy 0.9708\n",
            "Epoch 119 Batch 50 Loss 0.0864 Accuracy 0.9732\n",
            "Epoch 119 Batch 100 Loss 0.0866 Accuracy 0.9740\n",
            "Epoch 119 Batch 150 Loss 0.0907 Accuracy 0.9729\n",
            "Epoch 119 Batch 200 Loss 0.0926 Accuracy 0.9721\n",
            "Epoch 119 Batch 250 Loss 0.0944 Accuracy 0.9714\n",
            "Epoch 119 Batch 300 Loss 0.0952 Accuracy 0.9711\n",
            "Epoch 119 Batch 350 Loss 0.0958 Accuracy 0.9708\n",
            "Epoch 119 Loss 0.0959 Accuracy 0.9708\n",
            "Time taken for 1 epoch: 60.89 secs\n",
            "\n",
            "Epoch 120 Batch 0 Loss 0.0803 Accuracy 0.9758\n",
            "Epoch 120 Batch 50 Loss 0.0877 Accuracy 0.9737\n",
            "Epoch 120 Batch 100 Loss 0.0866 Accuracy 0.9739\n",
            "Epoch 120 Batch 150 Loss 0.0885 Accuracy 0.9735\n",
            "Epoch 120 Batch 200 Loss 0.0916 Accuracy 0.9727\n",
            "Epoch 120 Batch 250 Loss 0.0934 Accuracy 0.9720\n",
            "Epoch 120 Batch 300 Loss 0.0945 Accuracy 0.9717\n",
            "Epoch 120 Batch 350 Loss 0.0957 Accuracy 0.9714\n",
            "Saving checkpoint for epoch 120 at ./checkpoints/train/ckpt-24\n",
            "Epoch 120 Loss 0.0958 Accuracy 0.9713\n",
            "Time taken for 1 epoch: 63.85 secs\n",
            "\n",
            "Epoch 121 Batch 0 Loss 0.0640 Accuracy 0.9733\n",
            "Epoch 121 Batch 50 Loss 0.0908 Accuracy 0.9733\n",
            "Epoch 121 Batch 100 Loss 0.0899 Accuracy 0.9732\n",
            "Epoch 121 Batch 150 Loss 0.0892 Accuracy 0.9733\n",
            "Epoch 121 Batch 200 Loss 0.0916 Accuracy 0.9726\n",
            "Epoch 121 Batch 250 Loss 0.0926 Accuracy 0.9721\n",
            "Epoch 121 Batch 300 Loss 0.0936 Accuracy 0.9717\n",
            "Epoch 121 Batch 350 Loss 0.0944 Accuracy 0.9716\n",
            "Epoch 121 Loss 0.0945 Accuracy 0.9715\n",
            "Time taken for 1 epoch: 60.82 secs\n",
            "\n",
            "Epoch 122 Batch 0 Loss 0.0871 Accuracy 0.9789\n",
            "Epoch 122 Batch 50 Loss 0.0880 Accuracy 0.9741\n",
            "Epoch 122 Batch 100 Loss 0.0910 Accuracy 0.9728\n",
            "Epoch 122 Batch 150 Loss 0.0893 Accuracy 0.9730\n",
            "Epoch 122 Batch 200 Loss 0.0898 Accuracy 0.9731\n",
            "Epoch 122 Batch 250 Loss 0.0917 Accuracy 0.9724\n",
            "Epoch 122 Batch 300 Loss 0.0933 Accuracy 0.9718\n",
            "Epoch 122 Batch 350 Loss 0.0936 Accuracy 0.9717\n",
            "Epoch 122 Loss 0.0937 Accuracy 0.9717\n",
            "Time taken for 1 epoch: 61.02 secs\n",
            "\n",
            "Epoch 123 Batch 0 Loss 0.1099 Accuracy 0.9625\n",
            "Epoch 123 Batch 50 Loss 0.0830 Accuracy 0.9752\n",
            "Epoch 123 Batch 100 Loss 0.0834 Accuracy 0.9747\n",
            "Epoch 123 Batch 150 Loss 0.0866 Accuracy 0.9737\n",
            "Epoch 123 Batch 200 Loss 0.0883 Accuracy 0.9734\n",
            "Epoch 123 Batch 250 Loss 0.0895 Accuracy 0.9728\n",
            "Epoch 123 Batch 300 Loss 0.0913 Accuracy 0.9721\n",
            "Epoch 123 Batch 350 Loss 0.0926 Accuracy 0.9719\n",
            "Epoch 123 Loss 0.0924 Accuracy 0.9720\n",
            "Time taken for 1 epoch: 60.89 secs\n",
            "\n",
            "Epoch 124 Batch 0 Loss 0.0790 Accuracy 0.9783\n",
            "Epoch 124 Batch 50 Loss 0.0936 Accuracy 0.9725\n",
            "Epoch 124 Batch 100 Loss 0.0912 Accuracy 0.9725\n",
            "Epoch 124 Batch 150 Loss 0.0918 Accuracy 0.9722\n",
            "Epoch 124 Batch 200 Loss 0.0922 Accuracy 0.9721\n",
            "Epoch 124 Batch 250 Loss 0.0923 Accuracy 0.9723\n",
            "Epoch 124 Batch 300 Loss 0.0926 Accuracy 0.9720\n",
            "Epoch 124 Batch 350 Loss 0.0934 Accuracy 0.9718\n",
            "Epoch 124 Loss 0.0934 Accuracy 0.9718\n",
            "Time taken for 1 epoch: 61.06 secs\n",
            "\n",
            "Epoch 125 Batch 0 Loss 0.0991 Accuracy 0.9762\n",
            "Epoch 125 Batch 50 Loss 0.0808 Accuracy 0.9772\n",
            "Epoch 125 Batch 100 Loss 0.0851 Accuracy 0.9751\n",
            "Epoch 125 Batch 150 Loss 0.0869 Accuracy 0.9745\n",
            "Epoch 125 Batch 200 Loss 0.0880 Accuracy 0.9739\n",
            "Epoch 125 Batch 250 Loss 0.0892 Accuracy 0.9733\n",
            "Epoch 125 Batch 300 Loss 0.0908 Accuracy 0.9727\n",
            "Epoch 125 Batch 350 Loss 0.0919 Accuracy 0.9722\n",
            "Saving checkpoint for epoch 125 at ./checkpoints/train/ckpt-25\n",
            "Epoch 125 Loss 0.0917 Accuracy 0.9723\n",
            "Time taken for 1 epoch: 64.25 secs\n",
            "\n",
            "Epoch 126 Batch 0 Loss 0.0497 Accuracy 0.9841\n",
            "Epoch 126 Batch 50 Loss 0.0823 Accuracy 0.9758\n",
            "Epoch 126 Batch 100 Loss 0.0869 Accuracy 0.9738\n",
            "Epoch 126 Batch 150 Loss 0.0873 Accuracy 0.9736\n",
            "Epoch 126 Batch 200 Loss 0.0884 Accuracy 0.9734\n",
            "Epoch 126 Batch 250 Loss 0.0899 Accuracy 0.9729\n",
            "Epoch 126 Batch 300 Loss 0.0906 Accuracy 0.9727\n",
            "Epoch 126 Batch 350 Loss 0.0911 Accuracy 0.9726\n",
            "Epoch 126 Loss 0.0909 Accuracy 0.9726\n",
            "Time taken for 1 epoch: 61.12 secs\n",
            "\n",
            "Epoch 127 Batch 0 Loss 0.0922 Accuracy 0.9696\n",
            "Epoch 127 Batch 50 Loss 0.0761 Accuracy 0.9770\n",
            "Epoch 127 Batch 100 Loss 0.0820 Accuracy 0.9755\n",
            "Epoch 127 Batch 150 Loss 0.0836 Accuracy 0.9751\n",
            "Epoch 127 Batch 200 Loss 0.0865 Accuracy 0.9743\n",
            "Epoch 127 Batch 250 Loss 0.0875 Accuracy 0.9738\n",
            "Epoch 127 Batch 300 Loss 0.0887 Accuracy 0.9733\n",
            "Epoch 127 Batch 350 Loss 0.0899 Accuracy 0.9729\n",
            "Epoch 127 Loss 0.0900 Accuracy 0.9729\n",
            "Time taken for 1 epoch: 60.90 secs\n",
            "\n",
            "Epoch 128 Batch 0 Loss 0.0719 Accuracy 0.9752\n",
            "Epoch 128 Batch 50 Loss 0.0864 Accuracy 0.9752\n",
            "Epoch 128 Batch 100 Loss 0.0859 Accuracy 0.9749\n",
            "Epoch 128 Batch 150 Loss 0.0864 Accuracy 0.9743\n",
            "Epoch 128 Batch 200 Loss 0.0864 Accuracy 0.9742\n",
            "Epoch 128 Batch 250 Loss 0.0862 Accuracy 0.9741\n",
            "Epoch 128 Batch 300 Loss 0.0868 Accuracy 0.9738\n",
            "Epoch 128 Batch 350 Loss 0.0887 Accuracy 0.9732\n",
            "Epoch 128 Loss 0.0888 Accuracy 0.9731\n",
            "Time taken for 1 epoch: 60.75 secs\n",
            "\n",
            "Epoch 129 Batch 0 Loss 0.0741 Accuracy 0.9781\n",
            "Epoch 129 Batch 50 Loss 0.0844 Accuracy 0.9749\n",
            "Epoch 129 Batch 100 Loss 0.0854 Accuracy 0.9740\n",
            "Epoch 129 Batch 150 Loss 0.0876 Accuracy 0.9738\n",
            "Epoch 129 Batch 200 Loss 0.0866 Accuracy 0.9741\n",
            "Epoch 129 Batch 250 Loss 0.0873 Accuracy 0.9738\n",
            "Epoch 129 Batch 300 Loss 0.0871 Accuracy 0.9738\n",
            "Epoch 129 Batch 350 Loss 0.0869 Accuracy 0.9738\n",
            "Epoch 129 Loss 0.0873 Accuracy 0.9736\n",
            "Time taken for 1 epoch: 60.91 secs\n",
            "\n",
            "Epoch 130 Batch 0 Loss 0.1015 Accuracy 0.9681\n",
            "Epoch 130 Batch 50 Loss 0.0833 Accuracy 0.9756\n",
            "Epoch 130 Batch 100 Loss 0.0829 Accuracy 0.9761\n",
            "Epoch 130 Batch 150 Loss 0.0830 Accuracy 0.9759\n",
            "Epoch 130 Batch 200 Loss 0.0830 Accuracy 0.9753\n",
            "Epoch 130 Batch 250 Loss 0.0846 Accuracy 0.9748\n",
            "Epoch 130 Batch 300 Loss 0.0857 Accuracy 0.9743\n",
            "Epoch 130 Batch 350 Loss 0.0865 Accuracy 0.9742\n",
            "Saving checkpoint for epoch 130 at ./checkpoints/train/ckpt-26\n",
            "Epoch 130 Loss 0.0865 Accuracy 0.9741\n",
            "Time taken for 1 epoch: 63.45 secs\n",
            "\n",
            "Epoch 131 Batch 0 Loss 0.0644 Accuracy 0.9827\n",
            "Epoch 131 Batch 50 Loss 0.0814 Accuracy 0.9763\n",
            "Epoch 131 Batch 100 Loss 0.0818 Accuracy 0.9756\n",
            "Epoch 131 Batch 150 Loss 0.0811 Accuracy 0.9758\n",
            "Epoch 131 Batch 200 Loss 0.0821 Accuracy 0.9754\n",
            "Epoch 131 Batch 250 Loss 0.0833 Accuracy 0.9751\n",
            "Epoch 131 Batch 300 Loss 0.0854 Accuracy 0.9743\n",
            "Epoch 131 Batch 350 Loss 0.0862 Accuracy 0.9741\n",
            "Epoch 131 Loss 0.0861 Accuracy 0.9741\n",
            "Time taken for 1 epoch: 61.01 secs\n",
            "\n",
            "Epoch 132 Batch 0 Loss 0.0847 Accuracy 0.9732\n",
            "Epoch 132 Batch 50 Loss 0.0789 Accuracy 0.9769\n",
            "Epoch 132 Batch 100 Loss 0.0792 Accuracy 0.9764\n",
            "Epoch 132 Batch 150 Loss 0.0792 Accuracy 0.9764\n",
            "Epoch 132 Batch 200 Loss 0.0806 Accuracy 0.9760\n",
            "Epoch 132 Batch 250 Loss 0.0814 Accuracy 0.9755\n",
            "Epoch 132 Batch 300 Loss 0.0822 Accuracy 0.9751\n",
            "Epoch 132 Batch 350 Loss 0.0834 Accuracy 0.9747\n",
            "Epoch 132 Loss 0.0835 Accuracy 0.9747\n",
            "Time taken for 1 epoch: 60.78 secs\n",
            "\n",
            "Epoch 133 Batch 0 Loss 0.0890 Accuracy 0.9777\n",
            "Epoch 133 Batch 50 Loss 0.0808 Accuracy 0.9760\n",
            "Epoch 133 Batch 100 Loss 0.0795 Accuracy 0.9762\n",
            "Epoch 133 Batch 150 Loss 0.0800 Accuracy 0.9760\n",
            "Epoch 133 Batch 200 Loss 0.0823 Accuracy 0.9752\n",
            "Epoch 133 Batch 250 Loss 0.0826 Accuracy 0.9749\n",
            "Epoch 133 Batch 300 Loss 0.0827 Accuracy 0.9749\n",
            "Epoch 133 Batch 350 Loss 0.0845 Accuracy 0.9745\n",
            "Epoch 133 Loss 0.0846 Accuracy 0.9745\n",
            "Time taken for 1 epoch: 61.22 secs\n",
            "\n",
            "Epoch 134 Batch 0 Loss 0.0562 Accuracy 0.9843\n",
            "Epoch 134 Batch 50 Loss 0.0783 Accuracy 0.9766\n",
            "Epoch 134 Batch 100 Loss 0.0776 Accuracy 0.9766\n",
            "Epoch 134 Batch 150 Loss 0.0800 Accuracy 0.9757\n",
            "Epoch 134 Batch 200 Loss 0.0803 Accuracy 0.9756\n",
            "Epoch 134 Batch 250 Loss 0.0821 Accuracy 0.9751\n",
            "Epoch 134 Batch 300 Loss 0.0840 Accuracy 0.9746\n",
            "Epoch 134 Batch 350 Loss 0.0850 Accuracy 0.9743\n",
            "Epoch 134 Loss 0.0854 Accuracy 0.9742\n",
            "Time taken for 1 epoch: 61.32 secs\n",
            "\n",
            "Epoch 135 Batch 0 Loss 0.0791 Accuracy 0.9717\n",
            "Epoch 135 Batch 50 Loss 0.0814 Accuracy 0.9749\n",
            "Epoch 135 Batch 100 Loss 0.0823 Accuracy 0.9750\n",
            "Epoch 135 Batch 150 Loss 0.0809 Accuracy 0.9755\n",
            "Epoch 135 Batch 200 Loss 0.0816 Accuracy 0.9752\n",
            "Epoch 135 Batch 250 Loss 0.0821 Accuracy 0.9751\n",
            "Epoch 135 Batch 300 Loss 0.0836 Accuracy 0.9747\n",
            "Epoch 135 Batch 350 Loss 0.0856 Accuracy 0.9741\n",
            "Saving checkpoint for epoch 135 at ./checkpoints/train/ckpt-27\n",
            "Epoch 135 Loss 0.0859 Accuracy 0.9740\n",
            "Time taken for 1 epoch: 64.07 secs\n",
            "\n",
            "Epoch 136 Batch 0 Loss 0.0756 Accuracy 0.9782\n",
            "Epoch 136 Batch 50 Loss 0.0783 Accuracy 0.9771\n",
            "Epoch 136 Batch 100 Loss 0.0766 Accuracy 0.9773\n",
            "Epoch 136 Batch 150 Loss 0.0779 Accuracy 0.9770\n",
            "Epoch 136 Batch 200 Loss 0.0805 Accuracy 0.9759\n",
            "Epoch 136 Batch 250 Loss 0.0814 Accuracy 0.9756\n",
            "Epoch 136 Batch 300 Loss 0.0816 Accuracy 0.9756\n",
            "Epoch 136 Batch 350 Loss 0.0828 Accuracy 0.9751\n",
            "Epoch 136 Loss 0.0827 Accuracy 0.9751\n",
            "Time taken for 1 epoch: 60.90 secs\n",
            "\n",
            "Epoch 137 Batch 0 Loss 0.0737 Accuracy 0.9785\n",
            "Epoch 137 Batch 50 Loss 0.0712 Accuracy 0.9791\n",
            "Epoch 137 Batch 100 Loss 0.0708 Accuracy 0.9787\n",
            "Epoch 137 Batch 150 Loss 0.0744 Accuracy 0.9780\n",
            "Epoch 137 Batch 200 Loss 0.0760 Accuracy 0.9774\n",
            "Epoch 137 Batch 250 Loss 0.0782 Accuracy 0.9768\n",
            "Epoch 137 Batch 300 Loss 0.0788 Accuracy 0.9765\n",
            "Epoch 137 Batch 350 Loss 0.0798 Accuracy 0.9762\n",
            "Epoch 137 Loss 0.0800 Accuracy 0.9762\n",
            "Time taken for 1 epoch: 61.12 secs\n",
            "\n",
            "Epoch 138 Batch 0 Loss 0.0517 Accuracy 0.9859\n",
            "Epoch 138 Batch 50 Loss 0.0740 Accuracy 0.9782\n",
            "Epoch 138 Batch 100 Loss 0.0737 Accuracy 0.9784\n",
            "Epoch 138 Batch 150 Loss 0.0750 Accuracy 0.9776\n",
            "Epoch 138 Batch 200 Loss 0.0767 Accuracy 0.9772\n",
            "Epoch 138 Batch 250 Loss 0.0779 Accuracy 0.9769\n",
            "Epoch 138 Batch 300 Loss 0.0791 Accuracy 0.9764\n",
            "Epoch 138 Batch 350 Loss 0.0798 Accuracy 0.9760\n",
            "Epoch 138 Loss 0.0798 Accuracy 0.9760\n",
            "Time taken for 1 epoch: 61.04 secs\n",
            "\n",
            "Epoch 139 Batch 0 Loss 0.1207 Accuracy 0.9611\n",
            "Epoch 139 Batch 50 Loss 0.0714 Accuracy 0.9791\n",
            "Epoch 139 Batch 100 Loss 0.0712 Accuracy 0.9787\n",
            "Epoch 139 Batch 150 Loss 0.0727 Accuracy 0.9784\n",
            "Epoch 139 Batch 200 Loss 0.0756 Accuracy 0.9774\n",
            "Epoch 139 Batch 250 Loss 0.0772 Accuracy 0.9768\n",
            "Epoch 139 Batch 300 Loss 0.0782 Accuracy 0.9765\n",
            "Epoch 139 Batch 350 Loss 0.0795 Accuracy 0.9760\n",
            "Epoch 139 Loss 0.0795 Accuracy 0.9760\n",
            "Time taken for 1 epoch: 60.70 secs\n",
            "\n",
            "Epoch 140 Batch 0 Loss 0.0874 Accuracy 0.9725\n",
            "Epoch 140 Batch 50 Loss 0.0783 Accuracy 0.9763\n",
            "Epoch 140 Batch 100 Loss 0.0814 Accuracy 0.9752\n",
            "Epoch 140 Batch 150 Loss 0.0787 Accuracy 0.9763\n",
            "Epoch 140 Batch 200 Loss 0.0772 Accuracy 0.9770\n",
            "Epoch 140 Batch 250 Loss 0.0775 Accuracy 0.9769\n",
            "Epoch 140 Batch 300 Loss 0.0783 Accuracy 0.9764\n",
            "Epoch 140 Batch 350 Loss 0.0788 Accuracy 0.9762\n",
            "Saving checkpoint for epoch 140 at ./checkpoints/train/ckpt-28\n",
            "Epoch 140 Loss 0.0789 Accuracy 0.9761\n",
            "Time taken for 1 epoch: 64.32 secs\n",
            "\n",
            "Epoch 141 Batch 0 Loss 0.0637 Accuracy 0.9772\n",
            "Epoch 141 Batch 50 Loss 0.0696 Accuracy 0.9791\n",
            "Epoch 141 Batch 100 Loss 0.0705 Accuracy 0.9788\n",
            "Epoch 141 Batch 150 Loss 0.0735 Accuracy 0.9781\n",
            "Epoch 141 Batch 200 Loss 0.0748 Accuracy 0.9776\n",
            "Epoch 141 Batch 250 Loss 0.0762 Accuracy 0.9772\n",
            "Epoch 141 Batch 300 Loss 0.0770 Accuracy 0.9770\n",
            "Epoch 141 Batch 350 Loss 0.0778 Accuracy 0.9766\n",
            "Epoch 141 Loss 0.0778 Accuracy 0.9766\n",
            "Time taken for 1 epoch: 61.82 secs\n",
            "\n",
            "Epoch 142 Batch 0 Loss 0.1100 Accuracy 0.9644\n",
            "Epoch 142 Batch 50 Loss 0.0748 Accuracy 0.9776\n",
            "Epoch 142 Batch 100 Loss 0.0734 Accuracy 0.9781\n",
            "Epoch 142 Batch 150 Loss 0.0739 Accuracy 0.9778\n",
            "Epoch 142 Batch 200 Loss 0.0757 Accuracy 0.9773\n",
            "Epoch 142 Batch 250 Loss 0.0769 Accuracy 0.9770\n",
            "Epoch 142 Batch 300 Loss 0.0784 Accuracy 0.9765\n",
            "Epoch 142 Batch 350 Loss 0.0792 Accuracy 0.9761\n",
            "Epoch 142 Loss 0.0793 Accuracy 0.9761\n",
            "Time taken for 1 epoch: 61.04 secs\n",
            "\n",
            "Epoch 143 Batch 0 Loss 0.0806 Accuracy 0.9714\n",
            "Epoch 143 Batch 50 Loss 0.0662 Accuracy 0.9800\n",
            "Epoch 143 Batch 100 Loss 0.0693 Accuracy 0.9791\n",
            "Epoch 143 Batch 150 Loss 0.0709 Accuracy 0.9784\n",
            "Epoch 143 Batch 200 Loss 0.0714 Accuracy 0.9783\n",
            "Epoch 143 Batch 250 Loss 0.0734 Accuracy 0.9777\n",
            "Epoch 143 Batch 300 Loss 0.0750 Accuracy 0.9772\n",
            "Epoch 143 Batch 350 Loss 0.0756 Accuracy 0.9771\n",
            "Epoch 143 Loss 0.0754 Accuracy 0.9771\n",
            "Time taken for 1 epoch: 61.14 secs\n",
            "\n",
            "Epoch 144 Batch 0 Loss 0.0540 Accuracy 0.9845\n",
            "Epoch 144 Batch 50 Loss 0.0715 Accuracy 0.9786\n",
            "Epoch 144 Batch 100 Loss 0.0709 Accuracy 0.9787\n",
            "Epoch 144 Batch 150 Loss 0.0723 Accuracy 0.9779\n",
            "Epoch 144 Batch 200 Loss 0.0730 Accuracy 0.9777\n",
            "Epoch 144 Batch 250 Loss 0.0741 Accuracy 0.9775\n",
            "Epoch 144 Batch 300 Loss 0.0753 Accuracy 0.9773\n",
            "Epoch 144 Batch 350 Loss 0.0761 Accuracy 0.9770\n",
            "Epoch 144 Loss 0.0760 Accuracy 0.9771\n",
            "Time taken for 1 epoch: 60.88 secs\n",
            "\n",
            "Epoch 145 Batch 0 Loss 0.0717 Accuracy 0.9762\n",
            "Epoch 145 Batch 50 Loss 0.0720 Accuracy 0.9793\n",
            "Epoch 145 Batch 100 Loss 0.0700 Accuracy 0.9795\n",
            "Epoch 145 Batch 150 Loss 0.0717 Accuracy 0.9788\n",
            "Epoch 145 Batch 200 Loss 0.0735 Accuracy 0.9781\n",
            "Epoch 145 Batch 250 Loss 0.0738 Accuracy 0.9779\n",
            "Epoch 145 Batch 300 Loss 0.0746 Accuracy 0.9777\n",
            "Epoch 145 Batch 350 Loss 0.0757 Accuracy 0.9773\n",
            "Saving checkpoint for epoch 145 at ./checkpoints/train/ckpt-29\n",
            "Epoch 145 Loss 0.0758 Accuracy 0.9772\n",
            "Time taken for 1 epoch: 63.85 secs\n",
            "\n",
            "Epoch 146 Batch 0 Loss 0.0805 Accuracy 0.9714\n",
            "Epoch 146 Batch 50 Loss 0.0693 Accuracy 0.9788\n",
            "Epoch 146 Batch 100 Loss 0.0709 Accuracy 0.9787\n",
            "Epoch 146 Batch 150 Loss 0.0714 Accuracy 0.9787\n",
            "Epoch 146 Batch 200 Loss 0.0733 Accuracy 0.9780\n",
            "Epoch 146 Batch 250 Loss 0.0744 Accuracy 0.9778\n",
            "Epoch 146 Batch 300 Loss 0.0748 Accuracy 0.9777\n",
            "Epoch 146 Batch 350 Loss 0.0751 Accuracy 0.9775\n",
            "Epoch 146 Loss 0.0752 Accuracy 0.9775\n",
            "Time taken for 1 epoch: 60.51 secs\n",
            "\n",
            "Epoch 147 Batch 0 Loss 0.0799 Accuracy 0.9779\n",
            "Epoch 147 Batch 50 Loss 0.0693 Accuracy 0.9798\n",
            "Epoch 147 Batch 100 Loss 0.0715 Accuracy 0.9788\n",
            "Epoch 147 Batch 150 Loss 0.0725 Accuracy 0.9784\n",
            "Epoch 147 Batch 200 Loss 0.0730 Accuracy 0.9783\n",
            "Epoch 147 Batch 250 Loss 0.0737 Accuracy 0.9781\n",
            "Epoch 147 Batch 300 Loss 0.0748 Accuracy 0.9776\n",
            "Epoch 147 Batch 350 Loss 0.0748 Accuracy 0.9775\n",
            "Epoch 147 Loss 0.0747 Accuracy 0.9775\n",
            "Time taken for 1 epoch: 60.73 secs\n",
            "\n",
            "Epoch 148 Batch 0 Loss 0.0590 Accuracy 0.9825\n",
            "Epoch 148 Batch 50 Loss 0.0777 Accuracy 0.9777\n",
            "Epoch 148 Batch 100 Loss 0.0741 Accuracy 0.9784\n",
            "Epoch 148 Batch 150 Loss 0.0728 Accuracy 0.9790\n",
            "Epoch 148 Batch 200 Loss 0.0731 Accuracy 0.9788\n",
            "Epoch 148 Batch 250 Loss 0.0736 Accuracy 0.9785\n",
            "Epoch 148 Batch 300 Loss 0.0741 Accuracy 0.9784\n",
            "Epoch 148 Batch 350 Loss 0.0738 Accuracy 0.9783\n",
            "Epoch 148 Loss 0.0737 Accuracy 0.9783\n",
            "Time taken for 1 epoch: 60.82 secs\n",
            "\n",
            "Epoch 149 Batch 0 Loss 0.0604 Accuracy 0.9828\n",
            "Epoch 149 Batch 50 Loss 0.0641 Accuracy 0.9799\n",
            "Epoch 149 Batch 100 Loss 0.0676 Accuracy 0.9793\n",
            "Epoch 149 Batch 150 Loss 0.0693 Accuracy 0.9787\n",
            "Epoch 149 Batch 200 Loss 0.0690 Accuracy 0.9791\n",
            "Epoch 149 Batch 250 Loss 0.0717 Accuracy 0.9781\n",
            "Epoch 149 Batch 300 Loss 0.0715 Accuracy 0.9783\n",
            "Epoch 149 Batch 350 Loss 0.0720 Accuracy 0.9782\n",
            "Epoch 149 Loss 0.0722 Accuracy 0.9782\n",
            "Time taken for 1 epoch: 60.49 secs\n",
            "\n",
            "Epoch 150 Batch 0 Loss 0.0590 Accuracy 0.9864\n",
            "Epoch 150 Batch 50 Loss 0.0632 Accuracy 0.9820\n",
            "Epoch 150 Batch 100 Loss 0.0664 Accuracy 0.9810\n",
            "Epoch 150 Batch 150 Loss 0.0674 Accuracy 0.9804\n",
            "Epoch 150 Batch 200 Loss 0.0685 Accuracy 0.9800\n",
            "Epoch 150 Batch 250 Loss 0.0692 Accuracy 0.9799\n",
            "Epoch 150 Batch 300 Loss 0.0705 Accuracy 0.9793\n",
            "Epoch 150 Batch 350 Loss 0.0711 Accuracy 0.9790\n",
            "Saving checkpoint for epoch 150 at ./checkpoints/train/ckpt-30\n",
            "Epoch 150 Loss 0.0712 Accuracy 0.9790\n",
            "Time taken for 1 epoch: 63.63 secs\n",
            "\n",
            "CPU times: user 1h 30min 54s, sys: 22min 34s, total: 1h 53min 28s\n",
            "Wall time: 2h 34min 41s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Run training! Each epoch takes several mins with GPU\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  # inp -> twi, tar -> french\n",
        "  for (batch, (inp,tar)) in enumerate(train_batches):\n",
        "    train_step(inp, tar)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aij6BWpIBU52"
      },
      "source": [
        "# Build a Translator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "z0clY8WkaGZZ"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.Module):\n",
        "    def __init__(self, tokenizers, transformer):\n",
        "        self.tokenizers = tokenizers\n",
        "        self.transformer = transformer\n",
        "\n",
        "    def __call__(self, sentence, max_length=MAX_TOKENS):\n",
        "        # The input sentence is English, hence adding the `[START]` and `[END]` tokens.\n",
        "        sentence = tf.convert_to_tensor([sentence])\n",
        "        sentence = self.tokenizers.eng.tokenize(sentence).to_tensor()\n",
        "\n",
        "        encoder_input = sentence\n",
        "\n",
        "        # As the output language is TWI, initialize the output with the\n",
        "        # English `[START]` token.\n",
        "        start_end = self.tokenizers.twi.tokenize([''])[0]\n",
        "        start = start_end[0][tf.newaxis]\n",
        "        end = start_end[1][tf.newaxis]\n",
        "\n",
        "        # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "        # dynamic-loop can be traced by `tf.function`.\n",
        "        output_array = tf.TensorArray(\n",
        "            dtype=tf.int64, size=0, dynamic_size=True)\n",
        "        output_array = output_array.write(0, start)\n",
        "        output = tf.transpose(output_array.stack())\n",
        "\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "                encoder_input, output)\n",
        "\n",
        "        for i in tf.range(max_length):\n",
        "            output = tf.transpose(output_array.stack())\n",
        "\n",
        "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "                encoder_input, output)\n",
        "\n",
        "            predictions, attention_weights = self.transformer(\n",
        "                encoder_input, output, False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "\n",
        "            # Select the last token from the `seq_len` dimension.\n",
        "            # Shape `(batch_size, 1, vocab_size)`.\n",
        "            predictions = predictions[:, -1:, :]\n",
        "\n",
        "            predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "            # Concatenate the `predicted_id` to the output which is given to the\n",
        "            # decoder as its input.\n",
        "            output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "            if predicted_id == end:\n",
        "                break\n",
        "\n",
        "        output = tf.transpose(output_array.stack())\n",
        "        # The output shape is `(1, tokens)`.\n",
        "        text = self.tokenizers.twi.detokenize(output)[0]  # Shape: `()`.\n",
        "\n",
        "        tokens = self.tokenizers.twi.lookup(output)[0]\n",
        "        _, attention_weights = self.transformer(\n",
        "            encoder_input, output[:, :-1], False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "\n",
        "        return text, tokens, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Tt1OVm3ecSNO"
      },
      "outputs": [],
      "source": [
        "#Create an instance of this Translator class\n",
        "translator = Translator(tokenizers, transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dMj9N3fBh1P"
      },
      "source": [
        "## translate example sentecnes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ysGcUmAzc_Yi"
      },
      "outputs": [],
      "source": [
        "def print_translation(sentence, tokens, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "wKmyWMcGduft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "731a2d04-25b8-4bad-ffcd-d76bf7d9843b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : i ran to your brother on the street .\n",
            "Prediction     : mituu mmirika kɔɔ wo nua no nkyɛn wɔ abɔnten so .\n",
            "Ground truth   : mituu mmirika kɔɔ wo nua no nkyɛn wɔ abɔnten so .\n"
          ]
        }
      ],
      "source": [
        "sentence =\"i ran to your brother on the street .\"\n",
        "ground_truth=\"mituu mmirika kɔɔ wo nua no nkyɛn wɔ abɔnten so .\"\n",
        "translated_text, translated_tokens, attention_weights = translator(sentence)\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "DYL7f6tu9Y0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56c16ead-26ec-40fb-8d5d-69bc522e3bc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : i want to ask you a question .\n",
            "Prediction     : mepɛ sɛ mibisa wo asɛm bi .\n",
            "Ground truth   : mepɛ sɛ mibisa wo asɛm bi .\n"
          ]
        }
      ],
      "source": [
        "ground_truth=\"mepɛ sɛ mibisa wo asɛm bi .\"\n",
        "sentence=\"i want to ask you a question .\"\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBe9E_dXBzne"
      },
      "source": [
        "# Save Translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "qMpmh77M9ufZ"
      },
      "outputs": [],
      "source": [
        "# class to export translator\n",
        "class ExportTranslator(tf.Module):\n",
        "  def __init__(self, translator):\n",
        "    self.translator = translator\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def __call__(self, sentence):\n",
        "    (result,\n",
        "     tokens,\n",
        "     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "89tEraoTixKG"
      },
      "outputs": [],
      "source": [
        "translator = ExportTranslator(translator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "1ZurB6qDFDzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e489cca-4d73-468d-cc77-5a7cd70cbc60"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'savn ma samillemillem'"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "translator(tf.constant(\"mepɛ sɛ minya afoforo anim dom .\")).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "YMhMt9_z2ZXH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bf8fc72-2bd4-45a7-9284-3788f6d26e91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn while saving (showing 5 of 332). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "module_no_signatures_path = '/content/drive/MyDrive/twi_english_translator'\n",
        "print('Saving model...')\n",
        "tf.saved_model.save(translator, module_no_signatures_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "P0jO3gtH9bih"
      },
      "outputs": [],
      "source": [
        "reloaded = tf.saved_model.load(module_no_signatures_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "adjW9Tug-ZOa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "33e928aa-c7d4-4dfd-f11f-e5b4a1c698ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'kɔmputa no wɔ nhomakorabea hɔ .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "reloaded('the computer is in the library .').numpy().decode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbDLGyb8clxZ"
      },
      "source": [
        "#BLEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "W8k01SazA8Cl"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "\n",
        "class Bleu():\n",
        "    def __init__(self, translator):\n",
        "        self.translator = translator\n",
        "\n",
        "    def get_bleuscore(self, testfile, referencefile, smothingfunction=None):\n",
        "        if type(testfile) == str and type(referencefile) == str:\n",
        "            # Open test file and read lines\n",
        "            f = open(testfile, \"r\")\n",
        "            hypothesis = f.readlines()\n",
        "            f.close()\n",
        "            # open refernce file and read lines\n",
        "            f = open(referencefile, \"r\")\n",
        "            reference = f.readlines()\n",
        "            f.close()\n",
        "        elif type(testfile) == list and type(referencefile) == list:\n",
        "            hypothesis = testfile\n",
        "            reference = referencefile\n",
        "        else:\n",
        "            print(f'File must be txt or python list')\n",
        "\n",
        "        # check the length of our input sentence\n",
        "        length = len(hypothesis)\n",
        "        bleu_total = 0\n",
        "        weights = (0.58,0,0,0)\n",
        "        for i in range(length):\n",
        "            hypothesis[i] = hypothesis[i]\n",
        "            reference[i] = reference[i]\n",
        "            groundtruth = reference[i].lower().replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            groundtruth = [groundtruth]\n",
        "            translated_text = self.translator(\n",
        "                hypothesis[i]).numpy().decode(\"utf-8\")\n",
        "            # print(\"Translated Text: \", translated_text)\n",
        "            # print(\"Ground Truth: \", reference[i])\n",
        "            candidate = translated_text.replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            bleu = sentence_bleu(\n",
        "                groundtruth, candidate, weights, smoothing_function=smothingfunction,auto_reweigh=True)\n",
        "            bleu_total += bleu\n",
        "\n",
        "        return f'BLEU SCORE: {bleu_total/length}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "TYHXXY10dMh5"
      },
      "outputs": [],
      "source": [
        "#iNSTANTIATE OBJECT OF BLEU CLASS AND IT SMOOTHING FUNCTION\n",
        "smooth= SmoothingFunction()\n",
        "bleu = Bleu(reloaded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "g2y6aHDfdbZK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "d591abc5-4c11-4b79-aa79-9520897ddb45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 18min 6s, sys: 2min 8s, total: 20min 15s\n",
            "Wall time: 12min 35s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BLEU SCORE: 0.6117860834737133'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "# ESTIMATE BLEU SCORE FROM THE TEST DATA\n",
        "# from list\n",
        "%%time\n",
        "bleu.get_bleuscore(en_test,twi_test, smooth.method7)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GOOGLE *API* BLEU\n"
      ],
      "metadata": {
        "id": "VRgT8luKPPuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use google API for bidirectional pivot translation of Twi and French\n",
        "# pivot language = English\n",
        "# import libraries\n",
        "from googletrans import Translator, constants\n",
        "# instantiate a translator object\n",
        "# initiate translator object\n",
        "translator = Translator()\n",
        "# Add Akan to the language supported by this package\n",
        "# Note the googletrans package has not  been updated to capture the new additions by google since May 2022\n",
        "# from https://translate.google.com/?sl=en&tl=ak&op=translate , the key and value for Twi is 'ak':'akan'\n",
        "constants.LANGUAGES['ak'] = 'akan'\n",
        "\n",
        "\n",
        "class GooglePivot:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def evaluate(self, sentences, src_key, dest_key):\n",
        "        eng_text = translator.translate(sentences, src=src_key, dest='en').text\n",
        "        print(eng_text)\n",
        "        output = translator.translate(eng_text, dest=dest_key).text\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class GoogleDirect:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def evaluate(self, sentences, src_key, dest_key):\n",
        "\n",
        "        return translator.translate(sentences, src=src_key, dest=dest_key).text"
      ],
      "metadata": {
        "id": "836KH3HJPNyW"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "\n",
        "class GoogleBleu():\n",
        "    def __init__(self, translator):\n",
        "        self.translator = translator\n",
        "\n",
        "    def get_bleuscore(self, testfile, referencefile,src_lang,dest_lang, smothingfunction=None):\n",
        "        if type(testfile) == str and type(referencefile) == str:\n",
        "            # Open test file and read lines\n",
        "            f = open(testfile, \"r\")\n",
        "            hypothesis = f.readlines()\n",
        "            f.close()\n",
        "            # open refernce file and read lines\n",
        "            f = open(referencefile, \"r\")\n",
        "            reference = f.readlines()\n",
        "            f.close()\n",
        "        elif type(testfile) == list and type(referencefile) == list:\n",
        "            hypothesis = testfile\n",
        "            reference = referencefile\n",
        "        else:\n",
        "            print(f'File must be txt or python list')\n",
        "\n",
        "        # check the length of our input sentence\n",
        "        length = len(hypothesis)\n",
        "        bleu_total = 0\n",
        "        weights = (0.58,0,0,0)\n",
        "        for i in range(length):\n",
        "            hypothesis[i] = hypothesis[i]\n",
        "            reference[i] = reference[i]\n",
        "            groundtruth = reference[i].lower().replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            groundtruth = [groundtruth]\n",
        "            translated_text = self.translator.evaluate(hypothesis[i],src_key=src_lang, dest_key=dest_lang)\n",
        "            # print(\"Translated Text: \", translated_text)\n",
        "            # print(\"Ground Truth: \", reference[i])\n",
        "            candidate = translated_text.replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            bleu = sentence_bleu(\n",
        "                groundtruth, candidate, weights, smoothing_function=smothingfunction,auto_reweigh=True)\n",
        "            bleu_total += bleu\n",
        "\n",
        "        return f'BLEU SCORE: {bleu_total/length}'"
      ],
      "metadata": {
        "id": "QHT4GJc7Sqpb"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate GoogleDirect class\n",
        "google_translate = GoogleDirect()"
      ],
      "metadata": {
        "id": "DxmDJjRkQLiI"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "google_bleu = GoogleBleu(google_translate)"
      ],
      "metadata": {
        "id": "UHjJ24E4RQ22"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "google_bleu.get_bleuscore(en_test, twi_test,'en','ak',smooth.method7)\n"
      ],
      "metadata": {
        "id": "v8QDG1k8UMhp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "224229c4-36f2-4d2f-e295-3191c061681e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 14 s, sys: 1.33 s, total: 15.3 s\n",
            "Wall time: 5min 29s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BLEU SCORE: 0.7998710387704359'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6G85u42AxVHt"
      },
      "execution_count": 56,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}