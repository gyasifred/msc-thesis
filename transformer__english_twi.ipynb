{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/transformer__english_twi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qFzQxA29DJp"
      },
      "source": [
        "This notebook is based on the implementation of the paper [Attention is All you Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017). The code are adapted from the Tenssorflow repository at https://www.tensorflow.org/text/tutorials/transformer#build_the_transformer and a big thanks to [[Crater David]](https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:37370749) \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "-D6K6UBeEiHv",
        "outputId": "f9d9b039-d08f-4754-e1f2-b9b1e8629c6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hblEgiSfLEck"
      },
      "source": [
        "# Install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naWgZqfLsOiN",
        "outputId": "cb7f853c-8cd4-4404-bde7-406215f68a63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 5.9 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 582.0 MB 14 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 58.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 439 kB 73.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 59.4 MB/s \n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2022.6.15)\n",
            "Collecting hstspreload\n",
            "  Downloading hstspreload-2022.9.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 9.6 MB/s \n",
            "\u001b[?25hCollecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.7 MB/s \n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Collecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16367 sha256=50be15f734a8cf8b7437b3697abb62997340e375a651ffa76af9a2f7a23a2f17\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/be/fe/93a6a40ffe386e16089e44dad9018ebab9dc4cb9eb7eab65ae\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hyperframe, hpack, sniffio, h2, h11, rfc3986, httpcore, hstspreload, httpx, googletrans\n",
            "Successfully installed googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2022.9.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U tensorflow-text-nightly==2.11.0.dev20220817\n",
        "!pip install -U -q tf-nightly==2.11.0.dev20220817\n",
        "!pip3 install googletrans==3.1.0a0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY7seog2LQOL"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "gwubBvId4uGK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W18p7g2lLatA"
      },
      "source": [
        "# preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ovdl-UER8hMX"
      },
      "outputs": [],
      "source": [
        "# This code was adapted from https://github.com/GhanaNLP/kasa/blob/master/Kasa/Preprocessing.py\n",
        "# A subclass of the kasafrench for preprocessing data\n",
        "# import required library\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "class Preprocessing:\n",
        "    # dummy initialization method\n",
        "    def __init__(self):\n",
        "        # initialize with some default parameters here later\n",
        "        pass\n",
        "\n",
        "    # read in parallel twi - english dataset\n",
        "    def read_parallel_dataset(self, filepath_1, filepath_2, filepath_3=None):\n",
        "        if filepath_3 != None:\n",
        "            # read first language data\n",
        "            lang_1 = []\n",
        "            with open(filepath_1, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_1.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            # read second language data\n",
        "            lang_2 = []\n",
        "            with open(filepath_2, encoding='utf-8') as file:\n",
        "\n",
        "                # twi=file.read()\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_2.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "            # Read third Language data\n",
        "            lang_3 = []\n",
        "            with open(filepath_3, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_3.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            return lang_1, lang_2, lang_3\n",
        "            \n",
        "        else:\n",
        "            # read first language data\n",
        "            lang_1 = []\n",
        "            with open(filepath_1, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_1.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            # read second language data\n",
        "            lang_2 = []\n",
        "            with open(filepath_2, encoding='utf-8') as file:\n",
        "\n",
        "                # twi=file.read()\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_2.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            return lang_1, lang_2\n",
        "\n",
        "    # Define a helper function to remove string accents\n",
        "\n",
        "    def removeStringAccent(self, s):\n",
        "        return ''.join(\n",
        "            c for c in unicodedata.normalize('NFD', s)\n",
        "            if unicodedata.category(c) != 'Mn'\n",
        "        )\n",
        "\n",
        "    # normalize input twi sentence\n",
        "    def normalize_twi(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = s.lower()\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.ƆɔɛƐ!?’]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "\n",
        "    # normalize input french sentence\n",
        "    def normalize_FrEn(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = s.lower()\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "    \n",
        "    def writeTotxt(self,destination,data):\n",
        "        with open(destination, 'w') as f:\n",
        "            for line in data:\n",
        "                 f.write(f\"{line}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "B4asafNW9HIt"
      },
      "outputs": [],
      "source": [
        "# import preprocessing class \n",
        "# Create an instance of tft preprocessing class\n",
        "preprocessor = Preprocessing()\n",
        "\n",
        "# Read raw parallel dataset\n",
        "raw_data_twi, raw_data_en = preprocessor.read_parallel_dataset(\n",
        "    filepath_1='/content/drive/MyDrive/verified_twi.txt',\n",
        "    filepath_2='/content/drive/MyDrive/verified_english.txt'\n",
        ")\n",
        "\n",
        "# Normalize the raw data\n",
        "raw_data_en = [preprocessor.normalize_FrEn(data) for data in raw_data_en]\n",
        "raw_data_twi = [preprocessor.normalize_twi(data) for data in raw_data_twi]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au81TkNOLwnB"
      },
      "source": [
        "# split data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5l08UvTL9mtx"
      },
      "outputs": [],
      "source": [
        "# split the dataset into training and test sets\n",
        "# Keep 10% of the data as test set\n",
        "train_twi,test_twi,train_en,test_en = train_test_split(raw_data_twi,raw_data_en, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3q4djANP-9g6"
      },
      "outputs": [],
      "source": [
        "# write the preprocess traning and test dataset to a file\n",
        "preprocessor.writeTotxt('train_twi.txt',train_twi)\n",
        "preprocessor.writeTotxt('train_en.txt',train_en)\n",
        "preprocessor.writeTotxt('test_twi.txt',test_twi)\n",
        "preprocessor.writeTotxt('test_en.txt',test_en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao2l87AzL_kE"
      },
      "source": [
        "# Build tf dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0WZrSEP9_ISJ"
      },
      "outputs": [],
      "source": [
        "# build tf datasets from traning and test sentences in both languages\n",
        "train_dataset_en = tf.data.TextLineDataset('/content/train_en.txt')\n",
        "train_dataset_tw = tf.data.TextLineDataset('/content/train_twi.txt')\n",
        "\n",
        "val_dataset_en = tf.data.TextLineDataset('/content/test_en.txt')\n",
        "val_dataset_tw = tf.data.TextLineDataset('/content/test_twi.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "mmksbF-CAEXg"
      },
      "outputs": [],
      "source": [
        "# combine languages into single dataset\n",
        "trained_combined = tf.data.Dataset.zip((train_dataset_en, train_dataset_tw))\n",
        "val_combined = tf.data.Dataset.zip((val_dataset_en, val_dataset_tw))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZJ-hiEUBLDu",
        "outputId": "c085daf4-2beb-4cd7-f361-6059a3d4bfcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English:  the first american colonists arrived in the th century .\n",
            "Twi:  amerika atubrafo a wodi kan no duu hɔ wɔ afeha a ɛto so no mu .\n",
            "English:  you need to wash your hands .\n",
            "Twi:  ɛsɛ sɛ wohohoro wo nsa .\n",
            "English:  appiah horrow is payday .\n",
            "Twi:  da a wɔde tua ka no yɛ da koro akatua .\n",
            "English:  i can see why you like asamoah .\n",
            "Twi:  mitumi hu nea enti a w ani gye asamoa ho\n",
            "English:  he had jeans on .\n",
            "Twi:  ɔhyɛ jeans attade\n"
          ]
        }
      ],
      "source": [
        "# verify trained_combined dataset is correct\n",
        "for en,tw in trained_combined.take(5):\n",
        "  print(\"English: \", en.numpy().decode('utf-8'))\n",
        "  print(\"Twi: \", tw.numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaxFg3TZMIK8"
      },
      "source": [
        "# load tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjb-3AU1BnrK"
      },
      "source": [
        "\n",
        "\n",
        "1. The tokenizer is which was build from the parallel corpus.\n",
        "2. The code are available on the link https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/build_subword_tokenizer.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aR_vgyQgBo_T"
      },
      "outputs": [],
      "source": [
        "model_named = '/content/drive/MyDrive/translate_frengtwi_converter'\n",
        "# Verify tokenizer model can be reloaded\n",
        "tokenizers = tf.saved_model.load(model_named)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdHMl3icEjos",
        "outputId": "c1a4cc05-0e63-479f-998a-a5685943d386"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'i', b'love', b'student', b'life', b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Verify tokenizer works on french sentence\n",
        "tokens = tokenizers.eng.tokenize(['I LOVE STUDENT LIFE'])\n",
        "text_tokens = tokenizers.eng.lookup(tokens)\n",
        "text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSOMXh_de04Q",
        "outputId": "6eb9e3ba-34fc-4a93-f17d-2784739c3429"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i love student life\n"
          ]
        }
      ],
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.eng.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBNqSqeSfChq",
        "outputId": "d333abc4-c819-413b-f97a-280cbb9a8126"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'na', b'mmarimaa', b'ne', b'mmeawa', b'pii', b'w\\xc9\\x94',\n",
              "  b'agu', b'##di', b'##bea', b'h\\xc9\\x94', b'.', b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Verify tokenizer works on twi sentence\n",
        "tokens = tokenizers.twi.tokenize(['Na mmarimaa ne mmeawa pii wɔ agudibea hɔ .'])\n",
        "text_tokens = tokenizers.twi.lookup(tokens)\n",
        "text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lS1xMYQfQR9",
        "outputId": "5e6ef512-965f-42c8-a2d9-1608962e45bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "na mmarimaa ne mmeawa pii wɔ agudibea hɔ .\n"
          ]
        }
      ],
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.twi.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQVo6G3OMfac"
      },
      "source": [
        "# Check sentence with maximum length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvoh1vRgCxSb"
      },
      "source": [
        "By checking the maximum length of sentence, you can suitably select the MAX_TOKENS. The MAX_TOKKENS help drops examples longer than the maximum number of tokens (MAX_TOKENS). Without limiting the size of sequences, the performance may be negatively affected.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VJxJuNufTCB",
        "outputId": "9c3caef2-297a-4f60-8cb1-8c21fc1125fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".........................................................................................................................................................................................................................................................................................................................................................................................................................................................................."
          ]
        }
      ],
      "source": [
        "#The distribution of tokens per example in the dataset is as follows:\n",
        "\n",
        "lengths = []\n",
        "\n",
        "for twi_examples,en_examples in trained_combined.batch(50):\n",
        "  twi_tokens = tokenizers.twi.tokenize(twi_examples)\n",
        "  lengths.append(twi_tokens.row_lengths())\n",
        "\n",
        "  en_tokens = tokenizers.eng.tokenize(en_examples)\n",
        "  lengths.append(en_tokens.row_lengths())\n",
        "  print('.', end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "ri-qmhyyiBSa",
        "outputId": "51bc483b-e4ed-41b2-809e-be7321f927a0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "all_lengths = np.concatenate(lengths)\n",
        "\n",
        "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
        "plt.ylim(plt.ylim())\n",
        "max_length = max(all_lengths)\n",
        "plt.plot([max_length, max_length], plt.ylim())\n",
        "plt.title(f'Maximum tokens per example: {max_length}');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DYwhdWAMpnW"
      },
      "source": [
        "# Tokenize the training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GC7UlwEeiNTO"
      },
      "outputs": [],
      "source": [
        "MAX_TOKENS = 50\n",
        "\n",
        "def filter_max_tokens(l1, l2):\n",
        "  num_tokens = tf.maximum(tf.shape(l1)[1],tf.shape(l2)[1])\n",
        "  return num_tokens < MAX_TOKENS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Routine to tokenize sentence pairs; needed to make sentence batches for training\n",
        "def tokenize_pairs(en,twi):\n",
        "  en = tokenizers.eng.tokenize(en)\n",
        "  # Convert from ragged to dense, padding with zeros.\n",
        "  en = en.to_tensor()\n",
        "  tw = tokenizers.twi.tokenize(twi)\n",
        "  # Convert from ragged to dense, padding with zeros.\n",
        "  tw = tw.to_tensor()\n",
        "  return en, tw"
      ],
      "metadata": {
        "id": "AORJvDkcOhxC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "EWMySNLzklc_"
      },
      "outputs": [],
      "source": [
        "# create training batches\n",
        "BUFFER_SIZE = len(train_twi)\n",
        "BATCH_SIZE = 64\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .filter(filter_max_tokens)\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "# Create training and validation set batches.\n",
        "train_batches = make_batches(trained_combined)\n",
        "val_batches = make_batches(val_combined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2MhJY21RqcJ"
      },
      "source": [
        "Write the test batch to file. This will come in handy later if you prefer to translate from txt file and estimate the translator [BLEU](https://aclanthology.org/P02-1040.pdf) score. The translator will run to error for any for any sentence with the shape greater than the MAX_TOKENS used for training. It adivisable to use the trimmed sentences for testting to avoid such occurrance\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2V_Rh8fFSvPV"
      },
      "outputs": [],
      "source": [
        "en_test= []\n",
        "twi_test = []\n",
        "\n",
        "for en_batches,twi_batches in val_batches:\n",
        "    for en in tokenizers.eng.detokenize(en_batches):\n",
        "      en_test.append(en.numpy().decode(\"utf-8\"))\n",
        "    for tw in tokenizers.twi.detokenize(twi_batches):\n",
        "      twi_test.append(tw.numpy().decode(\"utf-8\"))\n",
        "\n",
        "\n",
        "preprocessor.writeTotxt('/content/drive/MyDrive/twi_testing_set.txt',test_twi)\n",
        "preprocessor.writeTotxt('/content/drive/MyDrive/english_testing_set.txt',en_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNFMseSdM3wr"
      },
      "source": [
        "# Positional encoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5du74jWFF5WK"
      },
      "source": [
        "Positional encodings are added to the embeddings to give the model some information about the relative position of the tokens in the sentence so the model can learn to recognize the word order.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "KXtWVxfeb6px"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "  \n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # Apply the sine function to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # Apply the cosine function to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXL_HTJWG44o"
      },
      "source": [
        "# Create a point-wise feed-forward network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "db4kXyfGb_l5"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(\n",
        "  d_model, # Input/output dimensionality.\n",
        "  dff # Inner-layer dimensionality.\n",
        "  ):\n",
        "\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # Shape `(batch_size, seq_len, dff)`.\n",
        "      tf.keras.layers.Dense(d_model)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "  ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSEO7HUZHppR"
      },
      "source": [
        "# Build Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtydO3sOIRE-"
      },
      "source": [
        "### Multihead Attention layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "BSHsspXXIaUR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create mask for padding tokens so translator ignores them\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "\n",
        "# Mask future tokens so translator cannot see future\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "\n",
        "# Create final masks\n",
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead)\n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "      q: query shape == (..., seq_len_q, depth)\n",
        "      k: key shape == (..., seq_len_k, depth)\n",
        "      v: value shape == (..., seq_len_v, depth_v)\n",
        "      mask: Float tensor with shape broadcastable\n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "      output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    # (..., seq_len_q, seq_len_k)\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(\n",
        "        scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# Add attention layers to create multi-head attention\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, *, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # (batch_size, num_heads, seq_len_q, depth)\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        # (batch_size, num_heads, seq_len_k, depth)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        # (batch_size, num_heads, seq_len_v, depth)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        # (batch_size, seq_len_q, num_heads, depth)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        # (batch_size, seq_len_q, d_model)\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "        # Build Transformer encoder layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvNLzFWpJw6m"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "bCdZS_nJcGVW"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "EBZNB3MacRCF"
      },
      "outputs": [],
      "source": [
        "#Build Transformer encoder from encoder layer\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "              maximum_position_encoding=MAX_TOKENS,rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndsnLCOwRgcY"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "fnXVTpI_cgzR"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "UEuoBDtJcp7Q"
      },
      "outputs": [],
      "source": [
        "#Build Transformer decoder from decoder layer\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "              maximum_position_encoding,rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size,d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjFbGSvJUQY6"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "_6sd1ofVcyl4"
      },
      "outputs": [],
      "source": [
        "# Build full Transformer\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_vocab_size, pe_input=MAX_TOKENS, pe_target=MAX_TOKENS, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.encoder= Encoder(num_layers, d_model, num_heads, dff,input_vocab_size, pe_input, rate)\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff,target_vocab_size, pe_target, rate)\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "  \n",
        "  def call(self, inp, tar, training, enc_padding_mask,look_ahead_mask, dec_padding_mask):\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask) # (batch_size, inp_seq_len, d_model)\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    final_output = self.final_layer(dec_output) # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    return final_output, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPhqoBhwWYZ9"
      },
      "source": [
        "# Optimizer and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "y79jHzBnA7wG"
      },
      "outputs": [],
      "source": [
        "# Set learning rate schedule\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g11C2K3tXFyP"
      },
      "source": [
        "# Set hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "-q51YVNusr5_"
      },
      "outputs": [],
      "source": [
        "num_layers = 6\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fGafl4DXYct"
      },
      "source": [
        "# Instantiate a Transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "9DJxw765BYHi"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=tokenizers.eng.get_vocab_size().numpy(),\n",
        "    target_vocab_size=tokenizers.twi.get_vocab_size().numpy(),\n",
        "    rate=dropout_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aGTknsnYDCC"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "BF0wsHaaJVKC"
      },
      "outputs": [],
      "source": [
        "# Instantiate learning rate and set optimizer\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "HpLaySRfSy89"
      },
      "outputs": [],
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "-PHWy8sdGoR8"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = './checkpoints/train'\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# If a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "VAj0P_T0ZcBV"
      },
      "outputs": [],
      "source": [
        "# Choose number of training epochs\n",
        "EPOCHS = 150\n",
        "train_step_signature = [\n",
        "  tf.TensorSpec(shape=(None, None), dtype=tf.int64), \n",
        "  tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp,True,enc_padding_mask,combined_mask,dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7k6GwwEIi_A",
        "outputId": "25c863e5-967c-4112-841f-c31e008c9660"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 7.9133 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 7.1091 Accuracy 0.0628\n",
            "Epoch 1 Batch 100 Loss 6.6485 Accuracy 0.0825\n",
            "Epoch 1 Batch 150 Loss 6.3749 Accuracy 0.0991\n",
            "Epoch 1 Batch 200 Loss 6.1189 Accuracy 0.1278\n",
            "Epoch 1 Batch 250 Loss 5.9298 Accuracy 0.1476\n",
            "Epoch 1 Batch 300 Loss 5.7872 Accuracy 0.1625\n",
            "Epoch 1 Batch 350 Loss 5.6699 Accuracy 0.1743\n",
            "Epoch 1 Loss 5.6577 Accuracy 0.1757\n",
            "Time taken for 1 epoch: 85.00 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 5.0189 Accuracy 0.2329\n",
            "Epoch 2 Batch 50 Loss 4.8403 Accuracy 0.2591\n",
            "Epoch 2 Batch 100 Loss 4.7966 Accuracy 0.2625\n",
            "Epoch 2 Batch 150 Loss 4.7420 Accuracy 0.2674\n",
            "Epoch 2 Batch 200 Loss 4.6968 Accuracy 0.2714\n",
            "Epoch 2 Batch 250 Loss 4.6514 Accuracy 0.2756\n",
            "Epoch 2 Batch 300 Loss 4.6109 Accuracy 0.2795\n",
            "Epoch 2 Batch 350 Loss 4.5637 Accuracy 0.2845\n",
            "Epoch 2 Loss 4.5597 Accuracy 0.2848\n",
            "Time taken for 1 epoch: 64.56 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4.1360 Accuracy 0.3622\n",
            "Epoch 3 Batch 50 Loss 4.1579 Accuracy 0.3255\n",
            "Epoch 3 Batch 100 Loss 4.1250 Accuracy 0.3276\n",
            "Epoch 3 Batch 150 Loss 4.1009 Accuracy 0.3299\n",
            "Epoch 3 Batch 200 Loss 4.0703 Accuracy 0.3322\n",
            "Epoch 3 Batch 250 Loss 4.0374 Accuracy 0.3352\n",
            "Epoch 3 Batch 300 Loss 4.0109 Accuracy 0.3376\n",
            "Epoch 3 Batch 350 Loss 3.9823 Accuracy 0.3407\n",
            "Epoch 3 Loss 3.9780 Accuracy 0.3411\n",
            "Time taken for 1 epoch: 64.55 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 3.6754 Accuracy 0.3643\n",
            "Epoch 4 Batch 50 Loss 3.6288 Accuracy 0.3708\n",
            "Epoch 4 Batch 100 Loss 3.6114 Accuracy 0.3737\n",
            "Epoch 4 Batch 150 Loss 3.6019 Accuracy 0.3752\n",
            "Epoch 4 Batch 200 Loss 3.5901 Accuracy 0.3762\n",
            "Epoch 4 Batch 250 Loss 3.5742 Accuracy 0.3773\n",
            "Epoch 4 Batch 300 Loss 3.5557 Accuracy 0.3790\n",
            "Epoch 4 Batch 350 Loss 3.5393 Accuracy 0.3801\n",
            "Epoch 4 Loss 3.5365 Accuracy 0.3803\n",
            "Time taken for 1 epoch: 63.91 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 3.3236 Accuracy 0.4023\n",
            "Epoch 5 Batch 50 Loss 3.2673 Accuracy 0.4037\n",
            "Epoch 5 Batch 100 Loss 3.2389 Accuracy 0.4074\n",
            "Epoch 5 Batch 150 Loss 3.2271 Accuracy 0.4090\n",
            "Epoch 5 Batch 200 Loss 3.2355 Accuracy 0.4072\n",
            "Epoch 5 Batch 250 Loss 3.2284 Accuracy 0.4075\n",
            "Epoch 5 Batch 300 Loss 3.2222 Accuracy 0.4078\n",
            "Epoch 5 Batch 350 Loss 3.2077 Accuracy 0.4090\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
            "Epoch 5 Loss 3.2053 Accuracy 0.4090\n",
            "Time taken for 1 epoch: 66.89 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 2.7672 Accuracy 0.4581\n",
            "Epoch 6 Batch 50 Loss 2.9466 Accuracy 0.4376\n",
            "Epoch 6 Batch 100 Loss 2.9404 Accuracy 0.4371\n",
            "Epoch 6 Batch 150 Loss 2.9410 Accuracy 0.4363\n",
            "Epoch 6 Batch 200 Loss 2.9484 Accuracy 0.4356\n",
            "Epoch 6 Batch 250 Loss 2.9446 Accuracy 0.4358\n",
            "Epoch 6 Batch 300 Loss 2.9465 Accuracy 0.4350\n",
            "Epoch 6 Batch 350 Loss 2.9403 Accuracy 0.4359\n",
            "Epoch 6 Loss 2.9385 Accuracy 0.4360\n",
            "Time taken for 1 epoch: 64.03 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.7778 Accuracy 0.4565\n",
            "Epoch 7 Batch 50 Loss 2.6997 Accuracy 0.4632\n",
            "Epoch 7 Batch 100 Loss 2.7000 Accuracy 0.4634\n",
            "Epoch 7 Batch 150 Loss 2.7038 Accuracy 0.4621\n",
            "Epoch 7 Batch 200 Loss 2.7089 Accuracy 0.4615\n",
            "Epoch 7 Batch 250 Loss 2.7129 Accuracy 0.4611\n",
            "Epoch 7 Batch 300 Loss 2.7216 Accuracy 0.4598\n",
            "Epoch 7 Batch 350 Loss 2.7198 Accuracy 0.4602\n",
            "Epoch 7 Loss 2.7234 Accuracy 0.4597\n",
            "Time taken for 1 epoch: 63.52 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.7146 Accuracy 0.4389\n",
            "Epoch 8 Batch 50 Loss 2.4626 Accuracy 0.4889\n",
            "Epoch 8 Batch 100 Loss 2.4707 Accuracy 0.4880\n",
            "Epoch 8 Batch 150 Loss 2.5049 Accuracy 0.4833\n",
            "Epoch 8 Batch 200 Loss 2.5264 Accuracy 0.4803\n",
            "Epoch 8 Batch 250 Loss 2.5349 Accuracy 0.4799\n",
            "Epoch 8 Batch 300 Loss 2.5475 Accuracy 0.4780\n",
            "Epoch 8 Batch 350 Loss 2.5573 Accuracy 0.4766\n",
            "Epoch 8 Loss 2.5592 Accuracy 0.4764\n",
            "Time taken for 1 epoch: 64.13 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.5165 Accuracy 0.4645\n",
            "Epoch 9 Batch 50 Loss 2.3531 Accuracy 0.5007\n",
            "Epoch 9 Batch 100 Loss 2.3558 Accuracy 0.5011\n",
            "Epoch 9 Batch 150 Loss 2.3784 Accuracy 0.4979\n",
            "Epoch 9 Batch 200 Loss 2.3957 Accuracy 0.4954\n",
            "Epoch 9 Batch 250 Loss 2.4155 Accuracy 0.4930\n",
            "Epoch 9 Batch 300 Loss 2.4275 Accuracy 0.4915\n",
            "Epoch 9 Batch 350 Loss 2.4384 Accuracy 0.4901\n",
            "Epoch 9 Loss 2.4418 Accuracy 0.4895\n",
            "Time taken for 1 epoch: 63.74 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 2.3346 Accuracy 0.5143\n",
            "Epoch 10 Batch 50 Loss 2.2644 Accuracy 0.5103\n",
            "Epoch 10 Batch 100 Loss 2.2637 Accuracy 0.5131\n",
            "Epoch 10 Batch 150 Loss 2.2792 Accuracy 0.5107\n",
            "Epoch 10 Batch 200 Loss 2.3049 Accuracy 0.5069\n",
            "Epoch 10 Batch 250 Loss 2.3223 Accuracy 0.5041\n",
            "Epoch 10 Batch 300 Loss 2.3442 Accuracy 0.5013\n",
            "Epoch 10 Batch 350 Loss 2.3653 Accuracy 0.4976\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
            "Epoch 10 Loss 2.3674 Accuracy 0.4973\n",
            "Time taken for 1 epoch: 66.14 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 2.1403 Accuracy 0.5339\n",
            "Epoch 11 Batch 50 Loss 2.2063 Accuracy 0.5172\n",
            "Epoch 11 Batch 100 Loss 2.2168 Accuracy 0.5164\n",
            "Epoch 11 Batch 150 Loss 2.2523 Accuracy 0.5136\n",
            "Epoch 11 Batch 200 Loss 2.2778 Accuracy 0.5089\n",
            "Epoch 11 Batch 250 Loss 2.2935 Accuracy 0.5061\n",
            "Epoch 11 Batch 300 Loss 2.3146 Accuracy 0.5028\n",
            "Epoch 11 Batch 350 Loss 2.3253 Accuracy 0.5010\n",
            "Epoch 11 Loss 2.3284 Accuracy 0.5006\n",
            "Time taken for 1 epoch: 63.64 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 2.2046 Accuracy 0.5056\n",
            "Epoch 12 Batch 50 Loss 2.1533 Accuracy 0.5237\n",
            "Epoch 12 Batch 100 Loss 2.2071 Accuracy 0.5148\n",
            "Epoch 12 Batch 150 Loss 2.2216 Accuracy 0.5123\n",
            "Epoch 12 Batch 200 Loss 2.2443 Accuracy 0.5090\n",
            "Epoch 12 Batch 250 Loss 2.2691 Accuracy 0.5055\n",
            "Epoch 12 Batch 300 Loss 2.2872 Accuracy 0.5032\n",
            "Epoch 12 Batch 350 Loss 2.2958 Accuracy 0.5025\n",
            "Epoch 12 Loss 2.2986 Accuracy 0.5020\n",
            "Time taken for 1 epoch: 63.38 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 2.1053 Accuracy 0.5239\n",
            "Epoch 13 Batch 50 Loss 2.0979 Accuracy 0.5282\n",
            "Epoch 13 Batch 100 Loss 2.1107 Accuracy 0.5276\n",
            "Epoch 13 Batch 150 Loss 2.1339 Accuracy 0.5246\n",
            "Epoch 13 Batch 200 Loss 2.1473 Accuracy 0.5234\n",
            "Epoch 13 Batch 250 Loss 2.1699 Accuracy 0.5207\n",
            "Epoch 13 Batch 300 Loss 2.1801 Accuracy 0.5192\n",
            "Epoch 13 Batch 350 Loss 2.1947 Accuracy 0.5174\n",
            "Epoch 13 Loss 2.1976 Accuracy 0.5169\n",
            "Time taken for 1 epoch: 63.09 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.9082 Accuracy 0.5577\n",
            "Epoch 14 Batch 50 Loss 1.9857 Accuracy 0.5479\n",
            "Epoch 14 Batch 100 Loss 2.0176 Accuracy 0.5421\n",
            "Epoch 14 Batch 150 Loss 2.0413 Accuracy 0.5390\n",
            "Epoch 14 Batch 200 Loss 2.0537 Accuracy 0.5371\n",
            "Epoch 14 Batch 250 Loss 2.0662 Accuracy 0.5357\n",
            "Epoch 14 Batch 300 Loss 2.0741 Accuracy 0.5344\n",
            "Epoch 14 Batch 350 Loss 2.0839 Accuracy 0.5334\n",
            "Epoch 14 Loss 2.0848 Accuracy 0.5334\n",
            "Time taken for 1 epoch: 63.67 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.9904 Accuracy 0.5300\n",
            "Epoch 15 Batch 50 Loss 1.8903 Accuracy 0.5612\n",
            "Epoch 15 Batch 100 Loss 1.8962 Accuracy 0.5610\n",
            "Epoch 15 Batch 150 Loss 1.9088 Accuracy 0.5595\n",
            "Epoch 15 Batch 200 Loss 1.9255 Accuracy 0.5573\n",
            "Epoch 15 Batch 250 Loss 1.9402 Accuracy 0.5551\n",
            "Epoch 15 Batch 300 Loss 1.9577 Accuracy 0.5518\n",
            "Epoch 15 Batch 350 Loss 1.9701 Accuracy 0.5498\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
            "Epoch 15 Loss 1.9733 Accuracy 0.5492\n",
            "Time taken for 1 epoch: 66.09 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.7431 Accuracy 0.5972\n",
            "Epoch 16 Batch 50 Loss 1.7963 Accuracy 0.5758\n",
            "Epoch 16 Batch 100 Loss 1.7981 Accuracy 0.5766\n",
            "Epoch 16 Batch 150 Loss 1.8045 Accuracy 0.5762\n",
            "Epoch 16 Batch 200 Loss 1.8164 Accuracy 0.5739\n",
            "Epoch 16 Batch 250 Loss 1.8374 Accuracy 0.5702\n",
            "Epoch 16 Batch 300 Loss 1.8537 Accuracy 0.5676\n",
            "Epoch 16 Batch 350 Loss 1.8653 Accuracy 0.5657\n",
            "Epoch 16 Loss 1.8686 Accuracy 0.5651\n",
            "Time taken for 1 epoch: 63.43 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.7373 Accuracy 0.5832\n",
            "Epoch 17 Batch 50 Loss 1.6948 Accuracy 0.5934\n",
            "Epoch 17 Batch 100 Loss 1.6965 Accuracy 0.5942\n",
            "Epoch 17 Batch 150 Loss 1.7129 Accuracy 0.5911\n",
            "Epoch 17 Batch 200 Loss 1.7165 Accuracy 0.5904\n",
            "Epoch 17 Batch 250 Loss 1.7410 Accuracy 0.5868\n",
            "Epoch 17 Batch 300 Loss 1.7509 Accuracy 0.5850\n",
            "Epoch 17 Batch 350 Loss 1.7644 Accuracy 0.5827\n",
            "Epoch 17 Loss 1.7647 Accuracy 0.5825\n",
            "Time taken for 1 epoch: 63.34 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.6496 Accuracy 0.5883\n",
            "Epoch 18 Batch 50 Loss 1.5802 Accuracy 0.6193\n",
            "Epoch 18 Batch 100 Loss 1.5780 Accuracy 0.6158\n",
            "Epoch 18 Batch 150 Loss 1.6048 Accuracy 0.6093\n",
            "Epoch 18 Batch 200 Loss 1.6278 Accuracy 0.6059\n",
            "Epoch 18 Batch 250 Loss 1.6532 Accuracy 0.6011\n",
            "Epoch 18 Batch 300 Loss 1.6693 Accuracy 0.5983\n",
            "Epoch 18 Batch 350 Loss 1.6835 Accuracy 0.5959\n",
            "Epoch 18 Loss 1.6854 Accuracy 0.5957\n",
            "Time taken for 1 epoch: 63.11 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.5211 Accuracy 0.6228\n",
            "Epoch 19 Batch 50 Loss 1.4927 Accuracy 0.6304\n",
            "Epoch 19 Batch 100 Loss 1.5080 Accuracy 0.6256\n",
            "Epoch 19 Batch 150 Loss 1.5192 Accuracy 0.6237\n",
            "Epoch 19 Batch 200 Loss 1.5333 Accuracy 0.6219\n",
            "Epoch 19 Batch 250 Loss 1.5537 Accuracy 0.6184\n",
            "Epoch 19 Batch 300 Loss 1.5628 Accuracy 0.6164\n",
            "Epoch 19 Batch 350 Loss 1.5764 Accuracy 0.6135\n",
            "Epoch 19 Loss 1.5771 Accuracy 0.6135\n",
            "Time taken for 1 epoch: 63.32 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.2212 Accuracy 0.6978\n",
            "Epoch 20 Batch 50 Loss 1.3728 Accuracy 0.6511\n",
            "Epoch 20 Batch 100 Loss 1.4056 Accuracy 0.6438\n",
            "Epoch 20 Batch 150 Loss 1.4189 Accuracy 0.6416\n",
            "Epoch 20 Batch 200 Loss 1.4422 Accuracy 0.6377\n",
            "Epoch 20 Batch 250 Loss 1.4704 Accuracy 0.6326\n",
            "Epoch 20 Batch 300 Loss 1.4855 Accuracy 0.6299\n",
            "Epoch 20 Batch 350 Loss 1.5028 Accuracy 0.6265\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
            "Epoch 20 Loss 1.5040 Accuracy 0.6263\n",
            "Time taken for 1 epoch: 66.30 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.4746 Accuracy 0.6377\n",
            "Epoch 21 Batch 50 Loss 1.3044 Accuracy 0.6635\n",
            "Epoch 21 Batch 100 Loss 1.3159 Accuracy 0.6618\n",
            "Epoch 21 Batch 150 Loss 1.3441 Accuracy 0.6556\n",
            "Epoch 21 Batch 200 Loss 1.3719 Accuracy 0.6497\n",
            "Epoch 21 Batch 250 Loss 1.3905 Accuracy 0.6461\n",
            "Epoch 21 Batch 300 Loss 1.4105 Accuracy 0.6427\n",
            "Epoch 21 Batch 350 Loss 1.4218 Accuracy 0.6406\n",
            "Epoch 21 Loss 1.4236 Accuracy 0.6403\n",
            "Time taken for 1 epoch: 63.46 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 1.2398 Accuracy 0.6655\n",
            "Epoch 22 Batch 50 Loss 1.2335 Accuracy 0.6802\n",
            "Epoch 22 Batch 100 Loss 1.2621 Accuracy 0.6723\n",
            "Epoch 22 Batch 150 Loss 1.2854 Accuracy 0.6670\n",
            "Epoch 22 Batch 200 Loss 1.3044 Accuracy 0.6629\n",
            "Epoch 22 Batch 250 Loss 1.3195 Accuracy 0.6597\n",
            "Epoch 22 Batch 300 Loss 1.3333 Accuracy 0.6573\n",
            "Epoch 22 Batch 350 Loss 1.3498 Accuracy 0.6541\n",
            "Epoch 22 Loss 1.3504 Accuracy 0.6539\n",
            "Time taken for 1 epoch: 63.50 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.9849 Accuracy 0.7118\n",
            "Epoch 23 Batch 50 Loss 1.1469 Accuracy 0.6978\n",
            "Epoch 23 Batch 100 Loss 1.1756 Accuracy 0.6915\n",
            "Epoch 23 Batch 150 Loss 1.2140 Accuracy 0.6830\n",
            "Epoch 23 Batch 200 Loss 1.2316 Accuracy 0.6788\n",
            "Epoch 23 Batch 250 Loss 1.2374 Accuracy 0.6770\n",
            "Epoch 23 Batch 300 Loss 1.2492 Accuracy 0.6740\n",
            "Epoch 23 Batch 350 Loss 1.2636 Accuracy 0.6712\n",
            "Epoch 23 Loss 1.2657 Accuracy 0.6707\n",
            "Time taken for 1 epoch: 62.95 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 1.0168 Accuracy 0.7427\n",
            "Epoch 24 Batch 50 Loss 1.1080 Accuracy 0.7045\n",
            "Epoch 24 Batch 100 Loss 1.1192 Accuracy 0.7019\n",
            "Epoch 24 Batch 150 Loss 1.1253 Accuracy 0.7000\n",
            "Epoch 24 Batch 200 Loss 1.1452 Accuracy 0.6955\n",
            "Epoch 24 Batch 250 Loss 1.1641 Accuracy 0.6917\n",
            "Epoch 24 Batch 300 Loss 1.1803 Accuracy 0.6881\n",
            "Epoch 24 Batch 350 Loss 1.1969 Accuracy 0.6844\n",
            "Epoch 24 Loss 1.1987 Accuracy 0.6839\n",
            "Time taken for 1 epoch: 62.73 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.9886 Accuracy 0.7318\n",
            "Epoch 25 Batch 50 Loss 1.0381 Accuracy 0.7194\n",
            "Epoch 25 Batch 100 Loss 1.0487 Accuracy 0.7174\n",
            "Epoch 25 Batch 150 Loss 1.0763 Accuracy 0.7103\n",
            "Epoch 25 Batch 200 Loss 1.0935 Accuracy 0.7064\n",
            "Epoch 25 Batch 250 Loss 1.1087 Accuracy 0.7033\n",
            "Epoch 25 Batch 300 Loss 1.1254 Accuracy 0.6990\n",
            "Epoch 25 Batch 350 Loss 1.1388 Accuracy 0.6955\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-5\n",
            "Epoch 25 Loss 1.1397 Accuracy 0.6952\n",
            "Time taken for 1 epoch: 66.39 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.9144 Accuracy 0.7414\n",
            "Epoch 26 Batch 50 Loss 0.9701 Accuracy 0.7361\n",
            "Epoch 26 Batch 100 Loss 0.9890 Accuracy 0.7311\n",
            "Epoch 26 Batch 150 Loss 1.0100 Accuracy 0.7257\n",
            "Epoch 26 Batch 200 Loss 1.0334 Accuracy 0.7200\n",
            "Epoch 26 Batch 250 Loss 1.0497 Accuracy 0.7163\n",
            "Epoch 26 Batch 300 Loss 1.0655 Accuracy 0.7123\n",
            "Epoch 26 Batch 350 Loss 1.0801 Accuracy 0.7088\n",
            "Epoch 26 Loss 1.0819 Accuracy 0.7086\n",
            "Time taken for 1 epoch: 63.03 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.8225 Accuracy 0.7496\n",
            "Epoch 27 Batch 50 Loss 0.9316 Accuracy 0.7430\n",
            "Epoch 27 Batch 100 Loss 0.9546 Accuracy 0.7382\n",
            "Epoch 27 Batch 150 Loss 0.9804 Accuracy 0.7322\n",
            "Epoch 27 Batch 200 Loss 0.9905 Accuracy 0.7293\n",
            "Epoch 27 Batch 250 Loss 1.0094 Accuracy 0.7243\n",
            "Epoch 27 Batch 300 Loss 1.0277 Accuracy 0.7197\n",
            "Epoch 27 Batch 350 Loss 1.0439 Accuracy 0.7160\n",
            "Epoch 27 Loss 1.0448 Accuracy 0.7158\n",
            "Time taken for 1 epoch: 63.56 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.6760 Accuracy 0.8015\n",
            "Epoch 28 Batch 50 Loss 0.8724 Accuracy 0.7584\n",
            "Epoch 28 Batch 100 Loss 0.8918 Accuracy 0.7515\n",
            "Epoch 28 Batch 150 Loss 0.9047 Accuracy 0.7475\n",
            "Epoch 28 Batch 200 Loss 0.9264 Accuracy 0.7419\n",
            "Epoch 28 Batch 250 Loss 0.9466 Accuracy 0.7374\n",
            "Epoch 28 Batch 300 Loss 0.9596 Accuracy 0.7340\n",
            "Epoch 28 Batch 350 Loss 0.9742 Accuracy 0.7306\n",
            "Epoch 28 Loss 0.9755 Accuracy 0.7303\n",
            "Time taken for 1 epoch: 63.25 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.8557 Accuracy 0.7589\n",
            "Epoch 29 Batch 50 Loss 0.8393 Accuracy 0.7637\n",
            "Epoch 29 Batch 100 Loss 0.8427 Accuracy 0.7633\n",
            "Epoch 29 Batch 150 Loss 0.8581 Accuracy 0.7598\n",
            "Epoch 29 Batch 200 Loss 0.8738 Accuracy 0.7561\n",
            "Epoch 29 Batch 250 Loss 0.8903 Accuracy 0.7513\n",
            "Epoch 29 Batch 300 Loss 0.9036 Accuracy 0.7476\n",
            "Epoch 29 Batch 350 Loss 0.9189 Accuracy 0.7433\n",
            "Epoch 29 Loss 0.9186 Accuracy 0.7433\n",
            "Time taken for 1 epoch: 63.21 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.7833 Accuracy 0.7913\n",
            "Epoch 30 Batch 50 Loss 0.7817 Accuracy 0.7810\n",
            "Epoch 30 Batch 100 Loss 0.7967 Accuracy 0.7759\n",
            "Epoch 30 Batch 150 Loss 0.8149 Accuracy 0.7708\n",
            "Epoch 30 Batch 200 Loss 0.8312 Accuracy 0.7664\n",
            "Epoch 30 Batch 250 Loss 0.8497 Accuracy 0.7611\n",
            "Epoch 30 Batch 300 Loss 0.8630 Accuracy 0.7575\n",
            "Epoch 30 Batch 350 Loss 0.8731 Accuracy 0.7548\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-6\n",
            "Epoch 30 Loss 0.8742 Accuracy 0.7544\n",
            "Time taken for 1 epoch: 66.43 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.7968 Accuracy 0.7659\n",
            "Epoch 31 Batch 50 Loss 0.7504 Accuracy 0.7866\n",
            "Epoch 31 Batch 100 Loss 0.7658 Accuracy 0.7818\n",
            "Epoch 31 Batch 150 Loss 0.7804 Accuracy 0.7774\n",
            "Epoch 31 Batch 200 Loss 0.7974 Accuracy 0.7731\n",
            "Epoch 31 Batch 250 Loss 0.8121 Accuracy 0.7693\n",
            "Epoch 31 Batch 300 Loss 0.8219 Accuracy 0.7667\n",
            "Epoch 31 Batch 350 Loss 0.8349 Accuracy 0.7632\n",
            "Epoch 31 Loss 0.8352 Accuracy 0.7631\n",
            "Time taken for 1 epoch: 63.06 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.8034 Accuracy 0.7752\n",
            "Epoch 32 Batch 50 Loss 0.7049 Accuracy 0.7974\n",
            "Epoch 32 Batch 100 Loss 0.7391 Accuracy 0.7893\n",
            "Epoch 32 Batch 150 Loss 0.7565 Accuracy 0.7850\n",
            "Epoch 32 Batch 200 Loss 0.7670 Accuracy 0.7811\n",
            "Epoch 32 Batch 250 Loss 0.7745 Accuracy 0.7785\n",
            "Epoch 32 Batch 300 Loss 0.7876 Accuracy 0.7748\n",
            "Epoch 32 Batch 350 Loss 0.7975 Accuracy 0.7721\n",
            "Epoch 32 Loss 0.7988 Accuracy 0.7717\n",
            "Time taken for 1 epoch: 63.31 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.7049 Accuracy 0.7916\n",
            "Epoch 33 Batch 50 Loss 0.6864 Accuracy 0.8003\n",
            "Epoch 33 Batch 100 Loss 0.6919 Accuracy 0.7985\n",
            "Epoch 33 Batch 150 Loss 0.7067 Accuracy 0.7942\n",
            "Epoch 33 Batch 200 Loss 0.7210 Accuracy 0.7904\n",
            "Epoch 33 Batch 250 Loss 0.7388 Accuracy 0.7866\n",
            "Epoch 33 Batch 300 Loss 0.7520 Accuracy 0.7832\n",
            "Epoch 33 Batch 350 Loss 0.7609 Accuracy 0.7806\n",
            "Epoch 33 Loss 0.7629 Accuracy 0.7801\n",
            "Time taken for 1 epoch: 63.80 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.5756 Accuracy 0.8387\n",
            "Epoch 34 Batch 50 Loss 0.6582 Accuracy 0.8089\n",
            "Epoch 34 Batch 100 Loss 0.6658 Accuracy 0.8058\n",
            "Epoch 34 Batch 150 Loss 0.6829 Accuracy 0.8005\n",
            "Epoch 34 Batch 200 Loss 0.6949 Accuracy 0.7969\n",
            "Epoch 34 Batch 250 Loss 0.7084 Accuracy 0.7931\n",
            "Epoch 34 Batch 300 Loss 0.7200 Accuracy 0.7899\n",
            "Epoch 34 Batch 350 Loss 0.7269 Accuracy 0.7882\n",
            "Epoch 34 Loss 0.7267 Accuracy 0.7884\n",
            "Time taken for 1 epoch: 63.66 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.6288 Accuracy 0.8068\n",
            "Epoch 35 Batch 50 Loss 0.5953 Accuracy 0.8250\n",
            "Epoch 35 Batch 100 Loss 0.6065 Accuracy 0.8223\n",
            "Epoch 35 Batch 150 Loss 0.6270 Accuracy 0.8167\n",
            "Epoch 35 Batch 200 Loss 0.6485 Accuracy 0.8104\n",
            "Epoch 35 Batch 250 Loss 0.6613 Accuracy 0.8068\n",
            "Epoch 35 Batch 300 Loss 0.6762 Accuracy 0.8029\n",
            "Epoch 35 Batch 350 Loss 0.6864 Accuracy 0.8000\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-7\n",
            "Epoch 35 Loss 0.6881 Accuracy 0.7996\n",
            "Time taken for 1 epoch: 67.16 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.6182 Accuracy 0.8200\n",
            "Epoch 36 Batch 50 Loss 0.6016 Accuracy 0.8245\n",
            "Epoch 36 Batch 100 Loss 0.6127 Accuracy 0.8210\n",
            "Epoch 36 Batch 150 Loss 0.6195 Accuracy 0.8185\n",
            "Epoch 36 Batch 200 Loss 0.6281 Accuracy 0.8161\n",
            "Epoch 36 Batch 250 Loss 0.6399 Accuracy 0.8123\n",
            "Epoch 36 Batch 300 Loss 0.6517 Accuracy 0.8088\n",
            "Epoch 36 Batch 350 Loss 0.6647 Accuracy 0.8054\n",
            "Epoch 36 Loss 0.6661 Accuracy 0.8050\n",
            "Time taken for 1 epoch: 63.91 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.7350 Accuracy 0.7849\n",
            "Epoch 37 Batch 50 Loss 0.5697 Accuracy 0.8292\n",
            "Epoch 37 Batch 100 Loss 0.5779 Accuracy 0.8276\n",
            "Epoch 37 Batch 150 Loss 0.5947 Accuracy 0.8231\n",
            "Epoch 37 Batch 200 Loss 0.6064 Accuracy 0.8197\n",
            "Epoch 37 Batch 250 Loss 0.6222 Accuracy 0.8156\n",
            "Epoch 37 Batch 300 Loss 0.6342 Accuracy 0.8126\n",
            "Epoch 37 Batch 350 Loss 0.6403 Accuracy 0.8105\n",
            "Epoch 37 Loss 0.6415 Accuracy 0.8103\n",
            "Time taken for 1 epoch: 63.52 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.5761 Accuracy 0.8114\n",
            "Epoch 38 Batch 50 Loss 0.5571 Accuracy 0.8328\n",
            "Epoch 38 Batch 100 Loss 0.5664 Accuracy 0.8315\n",
            "Epoch 38 Batch 150 Loss 0.5762 Accuracy 0.8288\n",
            "Epoch 38 Batch 200 Loss 0.5839 Accuracy 0.8266\n",
            "Epoch 38 Batch 250 Loss 0.5973 Accuracy 0.8232\n",
            "Epoch 38 Batch 300 Loss 0.6078 Accuracy 0.8204\n",
            "Epoch 38 Batch 350 Loss 0.6176 Accuracy 0.8172\n",
            "Epoch 38 Loss 0.6183 Accuracy 0.8170\n",
            "Time taken for 1 epoch: 64.04 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.4343 Accuracy 0.8892\n",
            "Epoch 39 Batch 50 Loss 0.5334 Accuracy 0.8422\n",
            "Epoch 39 Batch 100 Loss 0.5426 Accuracy 0.8385\n",
            "Epoch 39 Batch 150 Loss 0.5554 Accuracy 0.8346\n",
            "Epoch 39 Batch 200 Loss 0.5673 Accuracy 0.8313\n",
            "Epoch 39 Batch 250 Loss 0.5753 Accuracy 0.8286\n",
            "Epoch 39 Batch 300 Loss 0.5885 Accuracy 0.8249\n",
            "Epoch 39 Batch 350 Loss 0.5964 Accuracy 0.8231\n",
            "Epoch 39 Loss 0.5974 Accuracy 0.8226\n",
            "Time taken for 1 epoch: 63.54 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.5293 Accuracy 0.8412\n",
            "Epoch 40 Batch 50 Loss 0.5069 Accuracy 0.8479\n",
            "Epoch 40 Batch 100 Loss 0.5120 Accuracy 0.8462\n",
            "Epoch 40 Batch 150 Loss 0.5241 Accuracy 0.8431\n",
            "Epoch 40 Batch 200 Loss 0.5335 Accuracy 0.8407\n",
            "Epoch 40 Batch 250 Loss 0.5461 Accuracy 0.8374\n",
            "Epoch 40 Batch 300 Loss 0.5574 Accuracy 0.8340\n",
            "Epoch 40 Batch 350 Loss 0.5665 Accuracy 0.8312\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-8\n",
            "Epoch 40 Loss 0.5671 Accuracy 0.8310\n",
            "Time taken for 1 epoch: 66.36 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.4712 Accuracy 0.8559\n",
            "Epoch 41 Batch 50 Loss 0.4990 Accuracy 0.8526\n",
            "Epoch 41 Batch 100 Loss 0.4954 Accuracy 0.8518\n",
            "Epoch 41 Batch 150 Loss 0.5019 Accuracy 0.8496\n",
            "Epoch 41 Batch 200 Loss 0.5092 Accuracy 0.8475\n",
            "Epoch 41 Batch 250 Loss 0.5218 Accuracy 0.8430\n",
            "Epoch 41 Batch 300 Loss 0.5299 Accuracy 0.8406\n",
            "Epoch 41 Batch 350 Loss 0.5413 Accuracy 0.8371\n",
            "Epoch 41 Loss 0.5419 Accuracy 0.8369\n",
            "Time taken for 1 epoch: 63.76 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.5726 Accuracy 0.8365\n",
            "Epoch 42 Batch 50 Loss 0.4792 Accuracy 0.8575\n",
            "Epoch 42 Batch 100 Loss 0.4738 Accuracy 0.8565\n",
            "Epoch 42 Batch 150 Loss 0.4814 Accuracy 0.8544\n",
            "Epoch 42 Batch 200 Loss 0.4941 Accuracy 0.8505\n",
            "Epoch 42 Batch 250 Loss 0.5070 Accuracy 0.8468\n",
            "Epoch 42 Batch 300 Loss 0.5120 Accuracy 0.8449\n",
            "Epoch 42 Batch 350 Loss 0.5193 Accuracy 0.8431\n",
            "Epoch 42 Loss 0.5203 Accuracy 0.8428\n",
            "Time taken for 1 epoch: 63.47 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.4073 Accuracy 0.8817\n",
            "Epoch 43 Batch 50 Loss 0.4490 Accuracy 0.8642\n",
            "Epoch 43 Batch 100 Loss 0.4579 Accuracy 0.8622\n",
            "Epoch 43 Batch 150 Loss 0.4752 Accuracy 0.8569\n",
            "Epoch 43 Batch 200 Loss 0.4789 Accuracy 0.8553\n",
            "Epoch 43 Batch 250 Loss 0.4897 Accuracy 0.8522\n",
            "Epoch 43 Batch 300 Loss 0.4971 Accuracy 0.8497\n",
            "Epoch 43 Batch 350 Loss 0.5039 Accuracy 0.8475\n",
            "Epoch 43 Loss 0.5053 Accuracy 0.8470\n",
            "Time taken for 1 epoch: 63.77 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.4443 Accuracy 0.8654\n",
            "Epoch 44 Batch 50 Loss 0.4171 Accuracy 0.8722\n",
            "Epoch 44 Batch 100 Loss 0.4297 Accuracy 0.8693\n",
            "Epoch 44 Batch 150 Loss 0.4403 Accuracy 0.8654\n",
            "Epoch 44 Batch 200 Loss 0.4611 Accuracy 0.8593\n",
            "Epoch 44 Batch 250 Loss 0.4719 Accuracy 0.8568\n",
            "Epoch 44 Batch 300 Loss 0.4885 Accuracy 0.8520\n",
            "Epoch 44 Batch 350 Loss 0.5003 Accuracy 0.8485\n",
            "Epoch 44 Loss 0.5007 Accuracy 0.8484\n",
            "Time taken for 1 epoch: 63.14 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.3454 Accuracy 0.8995\n",
            "Epoch 45 Batch 50 Loss 0.4286 Accuracy 0.8681\n",
            "Epoch 45 Batch 100 Loss 0.4374 Accuracy 0.8654\n",
            "Epoch 45 Batch 150 Loss 0.4447 Accuracy 0.8636\n",
            "Epoch 45 Batch 200 Loss 0.4532 Accuracy 0.8611\n",
            "Epoch 45 Batch 250 Loss 0.4621 Accuracy 0.8588\n",
            "Epoch 45 Batch 300 Loss 0.4688 Accuracy 0.8568\n",
            "Epoch 45 Batch 350 Loss 0.4765 Accuracy 0.8544\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/train/ckpt-9\n",
            "Epoch 45 Loss 0.4771 Accuracy 0.8543\n",
            "Time taken for 1 epoch: 65.98 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.3824 Accuracy 0.8762\n",
            "Epoch 46 Batch 50 Loss 0.4066 Accuracy 0.8785\n",
            "Epoch 46 Batch 100 Loss 0.4185 Accuracy 0.8751\n",
            "Epoch 46 Batch 150 Loss 0.4352 Accuracy 0.8699\n",
            "Epoch 46 Batch 200 Loss 0.4446 Accuracy 0.8672\n",
            "Epoch 46 Batch 250 Loss 0.4540 Accuracy 0.8640\n",
            "Epoch 46 Batch 300 Loss 0.4573 Accuracy 0.8625\n",
            "Epoch 46 Batch 350 Loss 0.4612 Accuracy 0.8611\n",
            "Epoch 46 Loss 0.4619 Accuracy 0.8609\n",
            "Time taken for 1 epoch: 63.35 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.3027 Accuracy 0.9096\n",
            "Epoch 47 Batch 50 Loss 0.3833 Accuracy 0.8841\n",
            "Epoch 47 Batch 100 Loss 0.3952 Accuracy 0.8805\n",
            "Epoch 47 Batch 150 Loss 0.4134 Accuracy 0.8748\n",
            "Epoch 47 Batch 200 Loss 0.4218 Accuracy 0.8727\n",
            "Epoch 47 Batch 250 Loss 0.4276 Accuracy 0.8711\n",
            "Epoch 47 Batch 300 Loss 0.4382 Accuracy 0.8676\n",
            "Epoch 47 Batch 350 Loss 0.4449 Accuracy 0.8652\n",
            "Epoch 47 Loss 0.4458 Accuracy 0.8650\n",
            "Time taken for 1 epoch: 63.30 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.3702 Accuracy 0.8813\n",
            "Epoch 48 Batch 50 Loss 0.3961 Accuracy 0.8774\n",
            "Epoch 48 Batch 100 Loss 0.3985 Accuracy 0.8775\n",
            "Epoch 48 Batch 150 Loss 0.4056 Accuracy 0.8759\n",
            "Epoch 48 Batch 200 Loss 0.4115 Accuracy 0.8739\n",
            "Epoch 48 Batch 250 Loss 0.4192 Accuracy 0.8721\n",
            "Epoch 48 Batch 300 Loss 0.4266 Accuracy 0.8698\n",
            "Epoch 48 Batch 350 Loss 0.4321 Accuracy 0.8682\n",
            "Epoch 48 Loss 0.4325 Accuracy 0.8680\n",
            "Time taken for 1 epoch: 63.12 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.2274 Accuracy 0.9369\n",
            "Epoch 49 Batch 50 Loss 0.3724 Accuracy 0.8870\n",
            "Epoch 49 Batch 100 Loss 0.3778 Accuracy 0.8847\n",
            "Epoch 49 Batch 150 Loss 0.3865 Accuracy 0.8818\n",
            "Epoch 49 Batch 200 Loss 0.3915 Accuracy 0.8804\n",
            "Epoch 49 Batch 250 Loss 0.4015 Accuracy 0.8774\n",
            "Epoch 49 Batch 300 Loss 0.4125 Accuracy 0.8744\n",
            "Epoch 49 Batch 350 Loss 0.4185 Accuracy 0.8723\n",
            "Epoch 49 Loss 0.4193 Accuracy 0.8721\n",
            "Time taken for 1 epoch: 63.58 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.3616 Accuracy 0.8837\n",
            "Epoch 50 Batch 50 Loss 0.3770 Accuracy 0.8848\n",
            "Epoch 50 Batch 100 Loss 0.3796 Accuracy 0.8835\n",
            "Epoch 50 Batch 150 Loss 0.3863 Accuracy 0.8821\n",
            "Epoch 50 Batch 200 Loss 0.3903 Accuracy 0.8807\n",
            "Epoch 50 Batch 250 Loss 0.3943 Accuracy 0.8793\n",
            "Epoch 50 Batch 300 Loss 0.4017 Accuracy 0.8771\n",
            "Epoch 50 Batch 350 Loss 0.4069 Accuracy 0.8754\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-10\n",
            "Epoch 50 Loss 0.4076 Accuracy 0.8752\n",
            "Time taken for 1 epoch: 65.90 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.3831 Accuracy 0.8759\n",
            "Epoch 51 Batch 50 Loss 0.3443 Accuracy 0.8951\n",
            "Epoch 51 Batch 100 Loss 0.3573 Accuracy 0.8903\n",
            "Epoch 51 Batch 150 Loss 0.3673 Accuracy 0.8877\n",
            "Epoch 51 Batch 200 Loss 0.3739 Accuracy 0.8854\n",
            "Epoch 51 Batch 250 Loss 0.3795 Accuracy 0.8837\n",
            "Epoch 51 Batch 300 Loss 0.3851 Accuracy 0.8822\n",
            "Epoch 51 Batch 350 Loss 0.3900 Accuracy 0.8807\n",
            "Epoch 51 Loss 0.3908 Accuracy 0.8804\n",
            "Time taken for 1 epoch: 63.12 secs\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.2557 Accuracy 0.9309\n",
            "Epoch 52 Batch 50 Loss 0.3359 Accuracy 0.8974\n",
            "Epoch 52 Batch 100 Loss 0.3495 Accuracy 0.8936\n",
            "Epoch 52 Batch 150 Loss 0.3605 Accuracy 0.8898\n",
            "Epoch 52 Batch 200 Loss 0.3675 Accuracy 0.8874\n",
            "Epoch 52 Batch 250 Loss 0.3731 Accuracy 0.8860\n",
            "Epoch 52 Batch 300 Loss 0.3783 Accuracy 0.8841\n",
            "Epoch 52 Batch 350 Loss 0.3866 Accuracy 0.8816\n",
            "Epoch 52 Loss 0.3868 Accuracy 0.8814\n",
            "Time taken for 1 epoch: 63.03 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.2933 Accuracy 0.9021\n",
            "Epoch 53 Batch 50 Loss 0.3347 Accuracy 0.8964\n",
            "Epoch 53 Batch 100 Loss 0.3359 Accuracy 0.8949\n",
            "Epoch 53 Batch 150 Loss 0.3453 Accuracy 0.8925\n",
            "Epoch 53 Batch 200 Loss 0.3525 Accuracy 0.8904\n",
            "Epoch 53 Batch 250 Loss 0.3585 Accuracy 0.8887\n",
            "Epoch 53 Batch 300 Loss 0.3639 Accuracy 0.8872\n",
            "Epoch 53 Batch 350 Loss 0.3679 Accuracy 0.8861\n",
            "Epoch 53 Loss 0.3684 Accuracy 0.8860\n",
            "Time taken for 1 epoch: 62.95 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.3839 Accuracy 0.8838\n",
            "Epoch 54 Batch 50 Loss 0.3260 Accuracy 0.9008\n",
            "Epoch 54 Batch 100 Loss 0.3228 Accuracy 0.9024\n",
            "Epoch 54 Batch 150 Loss 0.3464 Accuracy 0.8946\n",
            "Epoch 54 Batch 200 Loss 0.3467 Accuracy 0.8941\n",
            "Epoch 54 Batch 250 Loss 0.3479 Accuracy 0.8938\n",
            "Epoch 54 Batch 300 Loss 0.3497 Accuracy 0.8931\n",
            "Epoch 54 Batch 350 Loss 0.3522 Accuracy 0.8919\n",
            "Epoch 54 Loss 0.3518 Accuracy 0.8920\n",
            "Time taken for 1 epoch: 63.58 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.3680 Accuracy 0.8920\n",
            "Epoch 55 Batch 50 Loss 0.2926 Accuracy 0.9095\n",
            "Epoch 55 Batch 100 Loss 0.2997 Accuracy 0.9073\n",
            "Epoch 55 Batch 150 Loss 0.3115 Accuracy 0.9033\n",
            "Epoch 55 Batch 200 Loss 0.3210 Accuracy 0.9006\n",
            "Epoch 55 Batch 250 Loss 0.3302 Accuracy 0.8978\n",
            "Epoch 55 Batch 300 Loss 0.3375 Accuracy 0.8950\n",
            "Epoch 55 Batch 350 Loss 0.3434 Accuracy 0.8933\n",
            "Saving checkpoint for epoch 55 at ./checkpoints/train/ckpt-11\n",
            "Epoch 55 Loss 0.3440 Accuracy 0.8932\n",
            "Time taken for 1 epoch: 65.58 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.2617 Accuracy 0.9298\n",
            "Epoch 56 Batch 50 Loss 0.3072 Accuracy 0.9068\n",
            "Epoch 56 Batch 100 Loss 0.3124 Accuracy 0.9046\n",
            "Epoch 56 Batch 150 Loss 0.3193 Accuracy 0.9021\n",
            "Epoch 56 Batch 200 Loss 0.3234 Accuracy 0.9005\n",
            "Epoch 56 Batch 250 Loss 0.3272 Accuracy 0.8995\n",
            "Epoch 56 Batch 300 Loss 0.3338 Accuracy 0.8973\n",
            "Epoch 56 Batch 350 Loss 0.3397 Accuracy 0.8956\n",
            "Epoch 56 Loss 0.3405 Accuracy 0.8954\n",
            "Time taken for 1 epoch: 64.04 secs\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.2573 Accuracy 0.9118\n",
            "Epoch 57 Batch 50 Loss 0.2989 Accuracy 0.9064\n",
            "Epoch 57 Batch 100 Loss 0.3133 Accuracy 0.9032\n",
            "Epoch 57 Batch 150 Loss 0.3199 Accuracy 0.9010\n",
            "Epoch 57 Batch 200 Loss 0.3241 Accuracy 0.8998\n",
            "Epoch 57 Batch 250 Loss 0.3285 Accuracy 0.8987\n",
            "Epoch 57 Batch 300 Loss 0.3335 Accuracy 0.8975\n",
            "Epoch 57 Batch 350 Loss 0.3367 Accuracy 0.8965\n",
            "Epoch 57 Loss 0.3371 Accuracy 0.8964\n",
            "Time taken for 1 epoch: 63.60 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.2133 Accuracy 0.9410\n",
            "Epoch 58 Batch 50 Loss 0.2744 Accuracy 0.9157\n",
            "Epoch 58 Batch 100 Loss 0.2807 Accuracy 0.9138\n",
            "Epoch 58 Batch 150 Loss 0.2902 Accuracy 0.9102\n",
            "Epoch 58 Batch 200 Loss 0.3005 Accuracy 0.9065\n",
            "Epoch 58 Batch 250 Loss 0.3096 Accuracy 0.9041\n",
            "Epoch 58 Batch 300 Loss 0.3154 Accuracy 0.9022\n",
            "Epoch 58 Batch 350 Loss 0.3194 Accuracy 0.9010\n",
            "Epoch 58 Loss 0.3196 Accuracy 0.9010\n",
            "Time taken for 1 epoch: 63.37 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.2347 Accuracy 0.9225\n",
            "Epoch 59 Batch 50 Loss 0.2836 Accuracy 0.9128\n",
            "Epoch 59 Batch 100 Loss 0.2889 Accuracy 0.9109\n",
            "Epoch 59 Batch 150 Loss 0.2932 Accuracy 0.9097\n",
            "Epoch 59 Batch 200 Loss 0.2954 Accuracy 0.9083\n",
            "Epoch 59 Batch 250 Loss 0.3032 Accuracy 0.9060\n",
            "Epoch 59 Batch 300 Loss 0.3084 Accuracy 0.9042\n",
            "Epoch 59 Batch 350 Loss 0.3125 Accuracy 0.9031\n",
            "Epoch 59 Loss 0.3134 Accuracy 0.9027\n",
            "Time taken for 1 epoch: 63.74 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.3008 Accuracy 0.9008\n",
            "Epoch 60 Batch 50 Loss 0.2816 Accuracy 0.9108\n",
            "Epoch 60 Batch 100 Loss 0.2854 Accuracy 0.9103\n",
            "Epoch 60 Batch 150 Loss 0.2918 Accuracy 0.9085\n",
            "Epoch 60 Batch 200 Loss 0.2972 Accuracy 0.9069\n",
            "Epoch 60 Batch 250 Loss 0.3053 Accuracy 0.9044\n",
            "Epoch 60 Batch 300 Loss 0.3079 Accuracy 0.9039\n",
            "Epoch 60 Batch 350 Loss 0.3114 Accuracy 0.9029\n",
            "Saving checkpoint for epoch 60 at ./checkpoints/train/ckpt-12\n",
            "Epoch 60 Loss 0.3115 Accuracy 0.9029\n",
            "Time taken for 1 epoch: 66.19 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.2298 Accuracy 0.9237\n",
            "Epoch 61 Batch 50 Loss 0.2663 Accuracy 0.9189\n",
            "Epoch 61 Batch 100 Loss 0.2719 Accuracy 0.9168\n",
            "Epoch 61 Batch 150 Loss 0.2812 Accuracy 0.9131\n",
            "Epoch 61 Batch 200 Loss 0.2879 Accuracy 0.9105\n",
            "Epoch 61 Batch 250 Loss 0.2935 Accuracy 0.9088\n",
            "Epoch 61 Batch 300 Loss 0.2956 Accuracy 0.9079\n",
            "Epoch 61 Batch 350 Loss 0.2990 Accuracy 0.9069\n",
            "Epoch 61 Loss 0.2995 Accuracy 0.9067\n",
            "Time taken for 1 epoch: 63.48 secs\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.3159 Accuracy 0.9009\n",
            "Epoch 62 Batch 50 Loss 0.2694 Accuracy 0.9174\n",
            "Epoch 62 Batch 100 Loss 0.2692 Accuracy 0.9172\n",
            "Epoch 62 Batch 150 Loss 0.2767 Accuracy 0.9149\n",
            "Epoch 62 Batch 200 Loss 0.2785 Accuracy 0.9141\n",
            "Epoch 62 Batch 250 Loss 0.2846 Accuracy 0.9124\n",
            "Epoch 62 Batch 300 Loss 0.2877 Accuracy 0.9114\n",
            "Epoch 62 Batch 350 Loss 0.2912 Accuracy 0.9104\n",
            "Epoch 62 Loss 0.2913 Accuracy 0.9104\n",
            "Time taken for 1 epoch: 62.79 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.2457 Accuracy 0.9276\n",
            "Epoch 63 Batch 50 Loss 0.2611 Accuracy 0.9185\n",
            "Epoch 63 Batch 100 Loss 0.2632 Accuracy 0.9184\n",
            "Epoch 63 Batch 150 Loss 0.2689 Accuracy 0.9166\n",
            "Epoch 63 Batch 200 Loss 0.2741 Accuracy 0.9153\n",
            "Epoch 63 Batch 250 Loss 0.2794 Accuracy 0.9136\n",
            "Epoch 63 Batch 300 Loss 0.2852 Accuracy 0.9118\n",
            "Epoch 63 Batch 350 Loss 0.2880 Accuracy 0.9107\n",
            "Epoch 63 Loss 0.2881 Accuracy 0.9107\n",
            "Time taken for 1 epoch: 62.94 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.2420 Accuracy 0.9194\n",
            "Epoch 64 Batch 50 Loss 0.2484 Accuracy 0.9232\n",
            "Epoch 64 Batch 100 Loss 0.2551 Accuracy 0.9208\n",
            "Epoch 64 Batch 150 Loss 0.2594 Accuracy 0.9192\n",
            "Epoch 64 Batch 200 Loss 0.2653 Accuracy 0.9171\n",
            "Epoch 64 Batch 250 Loss 0.2715 Accuracy 0.9153\n",
            "Epoch 64 Batch 300 Loss 0.2763 Accuracy 0.9138\n",
            "Epoch 64 Batch 350 Loss 0.2804 Accuracy 0.9126\n",
            "Epoch 64 Loss 0.2802 Accuracy 0.9126\n",
            "Time taken for 1 epoch: 63.44 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.3007 Accuracy 0.9211\n",
            "Epoch 65 Batch 50 Loss 0.2510 Accuracy 0.9240\n",
            "Epoch 65 Batch 100 Loss 0.2468 Accuracy 0.9245\n",
            "Epoch 65 Batch 150 Loss 0.2527 Accuracy 0.9226\n",
            "Epoch 65 Batch 200 Loss 0.2584 Accuracy 0.9205\n",
            "Epoch 65 Batch 250 Loss 0.2633 Accuracy 0.9188\n",
            "Epoch 65 Batch 300 Loss 0.2669 Accuracy 0.9176\n",
            "Epoch 65 Batch 350 Loss 0.2725 Accuracy 0.9159\n",
            "Saving checkpoint for epoch 65 at ./checkpoints/train/ckpt-13\n",
            "Epoch 65 Loss 0.2729 Accuracy 0.9157\n",
            "Time taken for 1 epoch: 65.75 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.2300 Accuracy 0.9453\n",
            "Epoch 66 Batch 50 Loss 0.2483 Accuracy 0.9242\n",
            "Epoch 66 Batch 100 Loss 0.2480 Accuracy 0.9238\n",
            "Epoch 66 Batch 150 Loss 0.2507 Accuracy 0.9230\n",
            "Epoch 66 Batch 200 Loss 0.2528 Accuracy 0.9224\n",
            "Epoch 66 Batch 250 Loss 0.2606 Accuracy 0.9202\n",
            "Epoch 66 Batch 300 Loss 0.2651 Accuracy 0.9185\n",
            "Epoch 66 Batch 350 Loss 0.2706 Accuracy 0.9168\n",
            "Epoch 66 Loss 0.2719 Accuracy 0.9165\n",
            "Time taken for 1 epoch: 63.23 secs\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.2328 Accuracy 0.9284\n",
            "Epoch 67 Batch 50 Loss 0.2600 Accuracy 0.9217\n",
            "Epoch 67 Batch 100 Loss 0.2496 Accuracy 0.9240\n",
            "Epoch 67 Batch 150 Loss 0.2516 Accuracy 0.9230\n",
            "Epoch 67 Batch 200 Loss 0.2516 Accuracy 0.9228\n",
            "Epoch 67 Batch 250 Loss 0.2552 Accuracy 0.9216\n",
            "Epoch 67 Batch 300 Loss 0.2595 Accuracy 0.9201\n",
            "Epoch 67 Batch 350 Loss 0.2621 Accuracy 0.9191\n",
            "Epoch 67 Loss 0.2619 Accuracy 0.9192\n",
            "Time taken for 1 epoch: 63.37 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.2242 Accuracy 0.9297\n",
            "Epoch 68 Batch 50 Loss 0.2288 Accuracy 0.9297\n",
            "Epoch 68 Batch 100 Loss 0.2449 Accuracy 0.9246\n",
            "Epoch 68 Batch 150 Loss 0.2433 Accuracy 0.9247\n",
            "Epoch 68 Batch 200 Loss 0.2464 Accuracy 0.9238\n",
            "Epoch 68 Batch 250 Loss 0.2497 Accuracy 0.9225\n",
            "Epoch 68 Batch 300 Loss 0.2550 Accuracy 0.9207\n",
            "Epoch 68 Batch 350 Loss 0.2586 Accuracy 0.9196\n",
            "Epoch 68 Loss 0.2591 Accuracy 0.9195\n",
            "Time taken for 1 epoch: 63.00 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.1909 Accuracy 0.9318\n",
            "Epoch 69 Batch 50 Loss 0.2357 Accuracy 0.9271\n",
            "Epoch 69 Batch 100 Loss 0.2320 Accuracy 0.9286\n",
            "Epoch 69 Batch 150 Loss 0.2418 Accuracy 0.9247\n",
            "Epoch 69 Batch 200 Loss 0.2425 Accuracy 0.9242\n",
            "Epoch 69 Batch 250 Loss 0.2448 Accuracy 0.9236\n",
            "Epoch 69 Batch 300 Loss 0.2479 Accuracy 0.9226\n",
            "Epoch 69 Batch 350 Loss 0.2529 Accuracy 0.9211\n",
            "Epoch 69 Loss 0.2534 Accuracy 0.9210\n",
            "Time taken for 1 epoch: 63.43 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.1978 Accuracy 0.9389\n",
            "Epoch 70 Batch 50 Loss 0.2258 Accuracy 0.9312\n",
            "Epoch 70 Batch 100 Loss 0.2309 Accuracy 0.9286\n",
            "Epoch 70 Batch 150 Loss 0.2318 Accuracy 0.9280\n",
            "Epoch 70 Batch 200 Loss 0.2355 Accuracy 0.9267\n",
            "Epoch 70 Batch 250 Loss 0.2388 Accuracy 0.9256\n",
            "Epoch 70 Batch 300 Loss 0.2415 Accuracy 0.9249\n",
            "Epoch 70 Batch 350 Loss 0.2459 Accuracy 0.9234\n",
            "Saving checkpoint for epoch 70 at ./checkpoints/train/ckpt-14\n",
            "Epoch 70 Loss 0.2459 Accuracy 0.9233\n",
            "Time taken for 1 epoch: 65.74 secs\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.2008 Accuracy 0.9342\n",
            "Epoch 71 Batch 50 Loss 0.2203 Accuracy 0.9317\n",
            "Epoch 71 Batch 100 Loss 0.2247 Accuracy 0.9304\n",
            "Epoch 71 Batch 150 Loss 0.2262 Accuracy 0.9297\n",
            "Epoch 71 Batch 200 Loss 0.2296 Accuracy 0.9287\n",
            "Epoch 71 Batch 250 Loss 0.2321 Accuracy 0.9278\n",
            "Epoch 71 Batch 300 Loss 0.2385 Accuracy 0.9257\n",
            "Epoch 71 Batch 350 Loss 0.2462 Accuracy 0.9235\n",
            "Epoch 71 Loss 0.2473 Accuracy 0.9231\n",
            "Time taken for 1 epoch: 63.20 secs\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.2740 Accuracy 0.9275\n",
            "Epoch 72 Batch 50 Loss 0.2147 Accuracy 0.9339\n",
            "Epoch 72 Batch 100 Loss 0.2155 Accuracy 0.9337\n",
            "Epoch 72 Batch 150 Loss 0.2185 Accuracy 0.9325\n",
            "Epoch 72 Batch 200 Loss 0.2217 Accuracy 0.9314\n",
            "Epoch 72 Batch 250 Loss 0.2257 Accuracy 0.9299\n",
            "Epoch 72 Batch 300 Loss 0.2286 Accuracy 0.9289\n",
            "Epoch 72 Batch 350 Loss 0.2315 Accuracy 0.9277\n",
            "Epoch 72 Loss 0.2320 Accuracy 0.9276\n",
            "Time taken for 1 epoch: 63.30 secs\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.1513 Accuracy 0.9590\n",
            "Epoch 73 Batch 50 Loss 0.2097 Accuracy 0.9364\n",
            "Epoch 73 Batch 100 Loss 0.2129 Accuracy 0.9352\n",
            "Epoch 73 Batch 150 Loss 0.2144 Accuracy 0.9347\n",
            "Epoch 73 Batch 200 Loss 0.2198 Accuracy 0.9329\n",
            "Epoch 73 Batch 250 Loss 0.2224 Accuracy 0.9317\n",
            "Epoch 73 Batch 300 Loss 0.2268 Accuracy 0.9303\n",
            "Epoch 73 Batch 350 Loss 0.2311 Accuracy 0.9288\n",
            "Epoch 73 Loss 0.2316 Accuracy 0.9286\n",
            "Time taken for 1 epoch: 62.90 secs\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.1483 Accuracy 0.9552\n",
            "Epoch 74 Batch 50 Loss 0.2043 Accuracy 0.9383\n",
            "Epoch 74 Batch 100 Loss 0.2070 Accuracy 0.9366\n",
            "Epoch 74 Batch 150 Loss 0.2110 Accuracy 0.9343\n",
            "Epoch 74 Batch 200 Loss 0.2119 Accuracy 0.9346\n",
            "Epoch 74 Batch 250 Loss 0.2184 Accuracy 0.9327\n",
            "Epoch 74 Batch 300 Loss 0.2220 Accuracy 0.9315\n",
            "Epoch 74 Batch 350 Loss 0.2247 Accuracy 0.9306\n",
            "Epoch 74 Loss 0.2249 Accuracy 0.9306\n",
            "Time taken for 1 epoch: 62.83 secs\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.1947 Accuracy 0.9378\n",
            "Epoch 75 Batch 50 Loss 0.1957 Accuracy 0.9399\n",
            "Epoch 75 Batch 100 Loss 0.2001 Accuracy 0.9379\n",
            "Epoch 75 Batch 150 Loss 0.2105 Accuracy 0.9347\n",
            "Epoch 75 Batch 200 Loss 0.2156 Accuracy 0.9332\n",
            "Epoch 75 Batch 250 Loss 0.2193 Accuracy 0.9321\n",
            "Epoch 75 Batch 300 Loss 0.2236 Accuracy 0.9304\n",
            "Epoch 75 Batch 350 Loss 0.2271 Accuracy 0.9292\n",
            "Saving checkpoint for epoch 75 at ./checkpoints/train/ckpt-15\n",
            "Epoch 75 Loss 0.2271 Accuracy 0.9291\n",
            "Time taken for 1 epoch: 65.90 secs\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.1912 Accuracy 0.9362\n",
            "Epoch 76 Batch 50 Loss 0.1926 Accuracy 0.9423\n",
            "Epoch 76 Batch 100 Loss 0.1929 Accuracy 0.9413\n",
            "Epoch 76 Batch 150 Loss 0.1995 Accuracy 0.9388\n",
            "Epoch 76 Batch 200 Loss 0.2012 Accuracy 0.9382\n",
            "Epoch 76 Batch 250 Loss 0.2066 Accuracy 0.9362\n",
            "Epoch 76 Batch 300 Loss 0.2104 Accuracy 0.9350\n",
            "Epoch 76 Batch 350 Loss 0.2138 Accuracy 0.9340\n",
            "Epoch 76 Loss 0.2140 Accuracy 0.9339\n",
            "Time taken for 1 epoch: 63.09 secs\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.1662 Accuracy 0.9504\n",
            "Epoch 77 Batch 50 Loss 0.2045 Accuracy 0.9385\n",
            "Epoch 77 Batch 100 Loss 0.2067 Accuracy 0.9365\n",
            "Epoch 77 Batch 150 Loss 0.2053 Accuracy 0.9370\n",
            "Epoch 77 Batch 200 Loss 0.2065 Accuracy 0.9363\n",
            "Epoch 77 Batch 250 Loss 0.2093 Accuracy 0.9353\n",
            "Epoch 77 Batch 300 Loss 0.2114 Accuracy 0.9347\n",
            "Epoch 77 Batch 350 Loss 0.2135 Accuracy 0.9338\n",
            "Epoch 77 Loss 0.2135 Accuracy 0.9339\n",
            "Time taken for 1 epoch: 63.50 secs\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.1579 Accuracy 0.9410\n",
            "Epoch 78 Batch 50 Loss 0.1899 Accuracy 0.9412\n",
            "Epoch 78 Batch 100 Loss 0.1908 Accuracy 0.9404\n",
            "Epoch 78 Batch 150 Loss 0.1927 Accuracy 0.9401\n",
            "Epoch 78 Batch 200 Loss 0.1986 Accuracy 0.9384\n",
            "Epoch 78 Batch 250 Loss 0.2011 Accuracy 0.9374\n",
            "Epoch 78 Batch 300 Loss 0.2046 Accuracy 0.9361\n",
            "Epoch 78 Batch 350 Loss 0.2065 Accuracy 0.9357\n",
            "Epoch 78 Loss 0.2069 Accuracy 0.9355\n",
            "Time taken for 1 epoch: 63.02 secs\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.2081 Accuracy 0.9349\n",
            "Epoch 79 Batch 50 Loss 0.1843 Accuracy 0.9438\n",
            "Epoch 79 Batch 100 Loss 0.1911 Accuracy 0.9412\n",
            "Epoch 79 Batch 150 Loss 0.1937 Accuracy 0.9398\n",
            "Epoch 79 Batch 200 Loss 0.1956 Accuracy 0.9394\n",
            "Epoch 79 Batch 250 Loss 0.1989 Accuracy 0.9383\n",
            "Epoch 79 Batch 300 Loss 0.2004 Accuracy 0.9379\n",
            "Epoch 79 Batch 350 Loss 0.2032 Accuracy 0.9370\n",
            "Epoch 79 Loss 0.2034 Accuracy 0.9369\n",
            "Time taken for 1 epoch: 62.89 secs\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.2506 Accuracy 0.9228\n",
            "Epoch 80 Batch 50 Loss 0.1885 Accuracy 0.9415\n",
            "Epoch 80 Batch 100 Loss 0.1860 Accuracy 0.9424\n",
            "Epoch 80 Batch 150 Loss 0.1903 Accuracy 0.9412\n",
            "Epoch 80 Batch 200 Loss 0.1923 Accuracy 0.9405\n",
            "Epoch 80 Batch 250 Loss 0.1947 Accuracy 0.9397\n",
            "Epoch 80 Batch 300 Loss 0.1975 Accuracy 0.9388\n",
            "Epoch 80 Batch 350 Loss 0.2005 Accuracy 0.9379\n",
            "Saving checkpoint for epoch 80 at ./checkpoints/train/ckpt-16\n",
            "Epoch 80 Loss 0.2008 Accuracy 0.9378\n",
            "Time taken for 1 epoch: 66.52 secs\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.1808 Accuracy 0.9444\n",
            "Epoch 81 Batch 50 Loss 0.1736 Accuracy 0.9465\n",
            "Epoch 81 Batch 100 Loss 0.1807 Accuracy 0.9451\n",
            "Epoch 81 Batch 150 Loss 0.1831 Accuracy 0.9439\n",
            "Epoch 81 Batch 200 Loss 0.1871 Accuracy 0.9426\n",
            "Epoch 81 Batch 250 Loss 0.1911 Accuracy 0.9413\n",
            "Epoch 81 Batch 300 Loss 0.1928 Accuracy 0.9405\n",
            "Epoch 81 Batch 350 Loss 0.1974 Accuracy 0.9394\n",
            "Epoch 81 Loss 0.1986 Accuracy 0.9390\n",
            "Time taken for 1 epoch: 63.09 secs\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.2272 Accuracy 0.9429\n",
            "Epoch 82 Batch 50 Loss 0.1879 Accuracy 0.9433\n",
            "Epoch 82 Batch 100 Loss 0.1857 Accuracy 0.9438\n",
            "Epoch 82 Batch 150 Loss 0.1846 Accuracy 0.9435\n",
            "Epoch 82 Batch 200 Loss 0.1845 Accuracy 0.9435\n",
            "Epoch 82 Batch 250 Loss 0.1894 Accuracy 0.9419\n",
            "Epoch 82 Batch 300 Loss 0.1918 Accuracy 0.9409\n",
            "Epoch 82 Batch 350 Loss 0.1940 Accuracy 0.9401\n",
            "Epoch 82 Loss 0.1939 Accuracy 0.9400\n",
            "Time taken for 1 epoch: 62.88 secs\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.2034 Accuracy 0.9399\n",
            "Epoch 83 Batch 50 Loss 0.1775 Accuracy 0.9441\n",
            "Epoch 83 Batch 100 Loss 0.1749 Accuracy 0.9459\n",
            "Epoch 83 Batch 150 Loss 0.1817 Accuracy 0.9442\n",
            "Epoch 83 Batch 200 Loss 0.1840 Accuracy 0.9433\n",
            "Epoch 83 Batch 250 Loss 0.1859 Accuracy 0.9425\n",
            "Epoch 83 Batch 300 Loss 0.1884 Accuracy 0.9415\n",
            "Epoch 83 Batch 350 Loss 0.1902 Accuracy 0.9410\n",
            "Epoch 83 Loss 0.1905 Accuracy 0.9409\n",
            "Time taken for 1 epoch: 63.62 secs\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.1515 Accuracy 0.9429\n",
            "Epoch 84 Batch 50 Loss 0.1693 Accuracy 0.9470\n",
            "Epoch 84 Batch 100 Loss 0.1745 Accuracy 0.9460\n",
            "Epoch 84 Batch 150 Loss 0.1776 Accuracy 0.9452\n",
            "Epoch 84 Batch 200 Loss 0.1848 Accuracy 0.9430\n",
            "Epoch 84 Batch 250 Loss 0.1877 Accuracy 0.9419\n",
            "Epoch 84 Batch 300 Loss 0.1880 Accuracy 0.9420\n",
            "Epoch 84 Batch 350 Loss 0.1900 Accuracy 0.9413\n",
            "Epoch 84 Loss 0.1903 Accuracy 0.9413\n",
            "Time taken for 1 epoch: 63.43 secs\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.1524 Accuracy 0.9560\n",
            "Epoch 85 Batch 50 Loss 0.1700 Accuracy 0.9468\n",
            "Epoch 85 Batch 100 Loss 0.1730 Accuracy 0.9461\n",
            "Epoch 85 Batch 150 Loss 0.1762 Accuracy 0.9448\n",
            "Epoch 85 Batch 200 Loss 0.1815 Accuracy 0.9435\n",
            "Epoch 85 Batch 250 Loss 0.1851 Accuracy 0.9421\n",
            "Epoch 85 Batch 300 Loss 0.1856 Accuracy 0.9420\n",
            "Epoch 85 Batch 350 Loss 0.1862 Accuracy 0.9420\n",
            "Saving checkpoint for epoch 85 at ./checkpoints/train/ckpt-17\n",
            "Epoch 85 Loss 0.1863 Accuracy 0.9418\n",
            "Time taken for 1 epoch: 66.52 secs\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.1451 Accuracy 0.9540\n",
            "Epoch 86 Batch 50 Loss 0.1658 Accuracy 0.9505\n",
            "Epoch 86 Batch 100 Loss 0.1673 Accuracy 0.9487\n",
            "Epoch 86 Batch 150 Loss 0.1710 Accuracy 0.9477\n",
            "Epoch 86 Batch 200 Loss 0.1742 Accuracy 0.9464\n",
            "Epoch 86 Batch 250 Loss 0.1775 Accuracy 0.9454\n",
            "Epoch 86 Batch 300 Loss 0.1789 Accuracy 0.9448\n",
            "Epoch 86 Batch 350 Loss 0.1803 Accuracy 0.9443\n",
            "Epoch 86 Loss 0.1809 Accuracy 0.9441\n",
            "Time taken for 1 epoch: 63.32 secs\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.1949 Accuracy 0.9466\n",
            "Epoch 87 Batch 50 Loss 0.1662 Accuracy 0.9479\n",
            "Epoch 87 Batch 100 Loss 0.1677 Accuracy 0.9479\n",
            "Epoch 87 Batch 150 Loss 0.1671 Accuracy 0.9484\n",
            "Epoch 87 Batch 200 Loss 0.1695 Accuracy 0.9474\n",
            "Epoch 87 Batch 250 Loss 0.1735 Accuracy 0.9462\n",
            "Epoch 87 Batch 300 Loss 0.1761 Accuracy 0.9454\n",
            "Epoch 87 Batch 350 Loss 0.1776 Accuracy 0.9448\n",
            "Epoch 87 Loss 0.1776 Accuracy 0.9449\n",
            "Time taken for 1 epoch: 63.24 secs\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.1194 Accuracy 0.9702\n",
            "Epoch 88 Batch 50 Loss 0.1582 Accuracy 0.9501\n",
            "Epoch 88 Batch 100 Loss 0.1600 Accuracy 0.9500\n",
            "Epoch 88 Batch 150 Loss 0.1644 Accuracy 0.9487\n",
            "Epoch 88 Batch 200 Loss 0.1675 Accuracy 0.9477\n",
            "Epoch 88 Batch 250 Loss 0.1704 Accuracy 0.9468\n",
            "Epoch 88 Batch 300 Loss 0.1755 Accuracy 0.9452\n",
            "Epoch 88 Batch 350 Loss 0.1780 Accuracy 0.9443\n",
            "Epoch 88 Loss 0.1780 Accuracy 0.9443\n",
            "Time taken for 1 epoch: 63.87 secs\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.1694 Accuracy 0.9449\n",
            "Epoch 89 Batch 50 Loss 0.1622 Accuracy 0.9496\n",
            "Epoch 89 Batch 100 Loss 0.1653 Accuracy 0.9485\n",
            "Epoch 89 Batch 150 Loss 0.1662 Accuracy 0.9482\n",
            "Epoch 89 Batch 200 Loss 0.1687 Accuracy 0.9477\n",
            "Epoch 89 Batch 250 Loss 0.1712 Accuracy 0.9467\n",
            "Epoch 89 Batch 300 Loss 0.1734 Accuracy 0.9461\n",
            "Epoch 89 Batch 350 Loss 0.1761 Accuracy 0.9453\n",
            "Epoch 89 Loss 0.1767 Accuracy 0.9452\n",
            "Time taken for 1 epoch: 63.64 secs\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.1556 Accuracy 0.9557\n",
            "Epoch 90 Batch 50 Loss 0.1612 Accuracy 0.9513\n",
            "Epoch 90 Batch 100 Loss 0.1679 Accuracy 0.9493\n",
            "Epoch 90 Batch 150 Loss 0.1701 Accuracy 0.9480\n",
            "Epoch 90 Batch 200 Loss 0.1714 Accuracy 0.9476\n",
            "Epoch 90 Batch 250 Loss 0.1725 Accuracy 0.9468\n",
            "Epoch 90 Batch 300 Loss 0.1738 Accuracy 0.9462\n",
            "Epoch 90 Batch 350 Loss 0.1760 Accuracy 0.9456\n",
            "Saving checkpoint for epoch 90 at ./checkpoints/train/ckpt-18\n",
            "Epoch 90 Loss 0.1760 Accuracy 0.9455\n",
            "Time taken for 1 epoch: 66.10 secs\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.1196 Accuracy 0.9663\n",
            "Epoch 91 Batch 50 Loss 0.1506 Accuracy 0.9539\n",
            "Epoch 91 Batch 100 Loss 0.1525 Accuracy 0.9531\n",
            "Epoch 91 Batch 150 Loss 0.1566 Accuracy 0.9518\n",
            "Epoch 91 Batch 200 Loss 0.1610 Accuracy 0.9504\n",
            "Epoch 91 Batch 250 Loss 0.1633 Accuracy 0.9495\n",
            "Epoch 91 Batch 300 Loss 0.1657 Accuracy 0.9488\n",
            "Epoch 91 Batch 350 Loss 0.1672 Accuracy 0.9481\n",
            "Epoch 91 Loss 0.1668 Accuracy 0.9483\n",
            "Time taken for 1 epoch: 63.53 secs\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.1343 Accuracy 0.9562\n",
            "Epoch 92 Batch 50 Loss 0.1593 Accuracy 0.9526\n",
            "Epoch 92 Batch 100 Loss 0.1541 Accuracy 0.9531\n",
            "Epoch 92 Batch 150 Loss 0.1570 Accuracy 0.9518\n",
            "Epoch 92 Batch 200 Loss 0.1610 Accuracy 0.9503\n",
            "Epoch 92 Batch 250 Loss 0.1633 Accuracy 0.9496\n",
            "Epoch 92 Batch 300 Loss 0.1662 Accuracy 0.9488\n",
            "Epoch 92 Batch 350 Loss 0.1670 Accuracy 0.9486\n",
            "Epoch 92 Loss 0.1673 Accuracy 0.9485\n",
            "Time taken for 1 epoch: 63.01 secs\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.1618 Accuracy 0.9456\n",
            "Epoch 93 Batch 50 Loss 0.1430 Accuracy 0.9551\n",
            "Epoch 93 Batch 100 Loss 0.1446 Accuracy 0.9552\n",
            "Epoch 93 Batch 150 Loss 0.1497 Accuracy 0.9534\n",
            "Epoch 93 Batch 200 Loss 0.1541 Accuracy 0.9520\n",
            "Epoch 93 Batch 250 Loss 0.1571 Accuracy 0.9510\n",
            "Epoch 93 Batch 300 Loss 0.1595 Accuracy 0.9503\n",
            "Epoch 93 Batch 350 Loss 0.1612 Accuracy 0.9499\n",
            "Epoch 93 Loss 0.1617 Accuracy 0.9497\n",
            "Time taken for 1 epoch: 63.57 secs\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.1443 Accuracy 0.9511\n",
            "Epoch 94 Batch 50 Loss 0.1425 Accuracy 0.9558\n",
            "Epoch 94 Batch 100 Loss 0.1462 Accuracy 0.9541\n",
            "Epoch 94 Batch 150 Loss 0.1532 Accuracy 0.9520\n",
            "Epoch 94 Batch 200 Loss 0.1572 Accuracy 0.9513\n",
            "Epoch 94 Batch 250 Loss 0.1594 Accuracy 0.9504\n",
            "Epoch 94 Batch 300 Loss 0.1604 Accuracy 0.9498\n",
            "Epoch 94 Batch 350 Loss 0.1633 Accuracy 0.9489\n",
            "Epoch 94 Loss 0.1636 Accuracy 0.9488\n",
            "Time taken for 1 epoch: 63.32 secs\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.1640 Accuracy 0.9468\n",
            "Epoch 95 Batch 50 Loss 0.1564 Accuracy 0.9518\n",
            "Epoch 95 Batch 100 Loss 0.1554 Accuracy 0.9516\n",
            "Epoch 95 Batch 150 Loss 0.1540 Accuracy 0.9520\n",
            "Epoch 95 Batch 200 Loss 0.1565 Accuracy 0.9515\n",
            "Epoch 95 Batch 250 Loss 0.1582 Accuracy 0.9509\n",
            "Epoch 95 Batch 300 Loss 0.1587 Accuracy 0.9508\n",
            "Epoch 95 Batch 350 Loss 0.1592 Accuracy 0.9507\n",
            "Saving checkpoint for epoch 95 at ./checkpoints/train/ckpt-19\n",
            "Epoch 95 Loss 0.1598 Accuracy 0.9505\n",
            "Time taken for 1 epoch: 66.20 secs\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.1189 Accuracy 0.9577\n",
            "Epoch 96 Batch 50 Loss 0.1476 Accuracy 0.9553\n",
            "Epoch 96 Batch 100 Loss 0.1458 Accuracy 0.9555\n",
            "Epoch 96 Batch 150 Loss 0.1473 Accuracy 0.9551\n",
            "Epoch 96 Batch 200 Loss 0.1489 Accuracy 0.9542\n",
            "Epoch 96 Batch 250 Loss 0.1520 Accuracy 0.9531\n",
            "Epoch 96 Batch 300 Loss 0.1552 Accuracy 0.9519\n",
            "Epoch 96 Batch 350 Loss 0.1555 Accuracy 0.9516\n",
            "Epoch 96 Loss 0.1559 Accuracy 0.9515\n",
            "Time taken for 1 epoch: 63.71 secs\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.1769 Accuracy 0.9478\n",
            "Epoch 97 Batch 50 Loss 0.1463 Accuracy 0.9555\n",
            "Epoch 97 Batch 100 Loss 0.1430 Accuracy 0.9557\n",
            "Epoch 97 Batch 150 Loss 0.1465 Accuracy 0.9547\n",
            "Epoch 97 Batch 200 Loss 0.1492 Accuracy 0.9538\n",
            "Epoch 97 Batch 250 Loss 0.1519 Accuracy 0.9530\n",
            "Epoch 97 Batch 300 Loss 0.1551 Accuracy 0.9519\n",
            "Epoch 97 Batch 350 Loss 0.1559 Accuracy 0.9516\n",
            "Epoch 97 Loss 0.1561 Accuracy 0.9516\n",
            "Time taken for 1 epoch: 63.01 secs\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.1235 Accuracy 0.9656\n",
            "Epoch 98 Batch 50 Loss 0.1400 Accuracy 0.9580\n",
            "Epoch 98 Batch 100 Loss 0.1386 Accuracy 0.9577\n",
            "Epoch 98 Batch 150 Loss 0.1407 Accuracy 0.9567\n",
            "Epoch 98 Batch 200 Loss 0.1424 Accuracy 0.9562\n",
            "Epoch 98 Batch 250 Loss 0.1448 Accuracy 0.9555\n",
            "Epoch 98 Batch 300 Loss 0.1489 Accuracy 0.9541\n",
            "Epoch 98 Batch 350 Loss 0.1513 Accuracy 0.9532\n",
            "Epoch 98 Loss 0.1516 Accuracy 0.9531\n",
            "Time taken for 1 epoch: 62.97 secs\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.1200 Accuracy 0.9670\n",
            "Epoch 99 Batch 50 Loss 0.1373 Accuracy 0.9587\n",
            "Epoch 99 Batch 100 Loss 0.1395 Accuracy 0.9582\n",
            "Epoch 99 Batch 150 Loss 0.1423 Accuracy 0.9566\n",
            "Epoch 99 Batch 200 Loss 0.1423 Accuracy 0.9563\n",
            "Epoch 99 Batch 250 Loss 0.1454 Accuracy 0.9553\n",
            "Epoch 99 Batch 300 Loss 0.1485 Accuracy 0.9543\n",
            "Epoch 99 Batch 350 Loss 0.1506 Accuracy 0.9536\n",
            "Epoch 99 Loss 0.1507 Accuracy 0.9535\n",
            "Time taken for 1 epoch: 63.15 secs\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.0873 Accuracy 0.9742\n",
            "Epoch 100 Batch 50 Loss 0.1392 Accuracy 0.9564\n",
            "Epoch 100 Batch 100 Loss 0.1397 Accuracy 0.9564\n",
            "Epoch 100 Batch 150 Loss 0.1402 Accuracy 0.9563\n",
            "Epoch 100 Batch 200 Loss 0.1402 Accuracy 0.9567\n",
            "Epoch 100 Batch 250 Loss 0.1417 Accuracy 0.9562\n",
            "Epoch 100 Batch 300 Loss 0.1445 Accuracy 0.9553\n",
            "Epoch 100 Batch 350 Loss 0.1464 Accuracy 0.9546\n",
            "Saving checkpoint for epoch 100 at ./checkpoints/train/ckpt-20\n",
            "Epoch 100 Loss 0.1466 Accuracy 0.9546\n",
            "Time taken for 1 epoch: 66.24 secs\n",
            "\n",
            "Epoch 101 Batch 0 Loss 0.1048 Accuracy 0.9785\n",
            "Epoch 101 Batch 50 Loss 0.1338 Accuracy 0.9584\n",
            "Epoch 101 Batch 100 Loss 0.1335 Accuracy 0.9589\n",
            "Epoch 101 Batch 150 Loss 0.1354 Accuracy 0.9581\n",
            "Epoch 101 Batch 200 Loss 0.1393 Accuracy 0.9569\n",
            "Epoch 101 Batch 250 Loss 0.1433 Accuracy 0.9555\n",
            "Epoch 101 Batch 300 Loss 0.1448 Accuracy 0.9550\n",
            "Epoch 101 Batch 350 Loss 0.1463 Accuracy 0.9545\n",
            "Epoch 101 Loss 0.1467 Accuracy 0.9544\n",
            "Time taken for 1 epoch: 63.20 secs\n",
            "\n",
            "Epoch 102 Batch 0 Loss 0.1553 Accuracy 0.9530\n",
            "Epoch 102 Batch 50 Loss 0.1430 Accuracy 0.9561\n",
            "Epoch 102 Batch 100 Loss 0.1423 Accuracy 0.9568\n",
            "Epoch 102 Batch 150 Loss 0.1470 Accuracy 0.9552\n",
            "Epoch 102 Batch 200 Loss 0.1461 Accuracy 0.9553\n",
            "Epoch 102 Batch 250 Loss 0.1458 Accuracy 0.9557\n",
            "Epoch 102 Batch 300 Loss 0.1464 Accuracy 0.9554\n",
            "Epoch 102 Batch 350 Loss 0.1471 Accuracy 0.9548\n",
            "Epoch 102 Loss 0.1475 Accuracy 0.9547\n",
            "Time taken for 1 epoch: 62.81 secs\n",
            "\n",
            "Epoch 103 Batch 0 Loss 0.0951 Accuracy 0.9680\n",
            "Epoch 103 Batch 50 Loss 0.1269 Accuracy 0.9617\n",
            "Epoch 103 Batch 100 Loss 0.1336 Accuracy 0.9600\n",
            "Epoch 103 Batch 150 Loss 0.1364 Accuracy 0.9589\n",
            "Epoch 103 Batch 200 Loss 0.1397 Accuracy 0.9580\n",
            "Epoch 103 Batch 250 Loss 0.1412 Accuracy 0.9575\n",
            "Epoch 103 Batch 300 Loss 0.1416 Accuracy 0.9570\n",
            "Epoch 103 Batch 350 Loss 0.1436 Accuracy 0.9564\n",
            "Epoch 103 Loss 0.1439 Accuracy 0.9562\n",
            "Time taken for 1 epoch: 62.85 secs\n",
            "\n",
            "Epoch 104 Batch 0 Loss 0.1338 Accuracy 0.9616\n",
            "Epoch 104 Batch 50 Loss 0.1304 Accuracy 0.9615\n",
            "Epoch 104 Batch 100 Loss 0.1297 Accuracy 0.9605\n",
            "Epoch 104 Batch 150 Loss 0.1307 Accuracy 0.9598\n",
            "Epoch 104 Batch 200 Loss 0.1341 Accuracy 0.9586\n",
            "Epoch 104 Batch 250 Loss 0.1360 Accuracy 0.9579\n",
            "Epoch 104 Batch 300 Loss 0.1380 Accuracy 0.9573\n",
            "Epoch 104 Batch 350 Loss 0.1396 Accuracy 0.9567\n",
            "Epoch 104 Loss 0.1395 Accuracy 0.9567\n",
            "Time taken for 1 epoch: 63.25 secs\n",
            "\n",
            "Epoch 105 Batch 0 Loss 0.1128 Accuracy 0.9612\n",
            "Epoch 105 Batch 50 Loss 0.1301 Accuracy 0.9605\n",
            "Epoch 105 Batch 100 Loss 0.1306 Accuracy 0.9604\n",
            "Epoch 105 Batch 150 Loss 0.1346 Accuracy 0.9593\n",
            "Epoch 105 Batch 200 Loss 0.1340 Accuracy 0.9592\n",
            "Epoch 105 Batch 250 Loss 0.1354 Accuracy 0.9586\n",
            "Epoch 105 Batch 300 Loss 0.1360 Accuracy 0.9585\n",
            "Epoch 105 Batch 350 Loss 0.1370 Accuracy 0.9580\n",
            "Saving checkpoint for epoch 105 at ./checkpoints/train/ckpt-21\n",
            "Epoch 105 Loss 0.1371 Accuracy 0.9579\n",
            "Time taken for 1 epoch: 66.14 secs\n",
            "\n",
            "Epoch 106 Batch 0 Loss 0.1607 Accuracy 0.9537\n",
            "Epoch 106 Batch 50 Loss 0.1325 Accuracy 0.9615\n",
            "Epoch 106 Batch 100 Loss 0.1334 Accuracy 0.9600\n",
            "Epoch 106 Batch 150 Loss 0.1336 Accuracy 0.9596\n",
            "Epoch 106 Batch 200 Loss 0.1338 Accuracy 0.9595\n",
            "Epoch 106 Batch 250 Loss 0.1340 Accuracy 0.9596\n",
            "Epoch 106 Batch 300 Loss 0.1342 Accuracy 0.9595\n",
            "Epoch 106 Batch 350 Loss 0.1352 Accuracy 0.9589\n",
            "Epoch 106 Loss 0.1354 Accuracy 0.9589\n",
            "Time taken for 1 epoch: 63.30 secs\n",
            "\n",
            "Epoch 107 Batch 0 Loss 0.1152 Accuracy 0.9703\n",
            "Epoch 107 Batch 50 Loss 0.1277 Accuracy 0.9612\n",
            "Epoch 107 Batch 100 Loss 0.1302 Accuracy 0.9602\n",
            "Epoch 107 Batch 150 Loss 0.1307 Accuracy 0.9602\n",
            "Epoch 107 Batch 200 Loss 0.1323 Accuracy 0.9596\n",
            "Epoch 107 Batch 250 Loss 0.1349 Accuracy 0.9589\n",
            "Epoch 107 Batch 300 Loss 0.1362 Accuracy 0.9584\n",
            "Epoch 107 Batch 350 Loss 0.1376 Accuracy 0.9580\n",
            "Epoch 107 Loss 0.1374 Accuracy 0.9580\n",
            "Time taken for 1 epoch: 63.41 secs\n",
            "\n",
            "Epoch 108 Batch 0 Loss 0.1091 Accuracy 0.9678\n",
            "Epoch 108 Batch 50 Loss 0.1169 Accuracy 0.9646\n",
            "Epoch 108 Batch 100 Loss 0.1193 Accuracy 0.9633\n",
            "Epoch 108 Batch 150 Loss 0.1223 Accuracy 0.9622\n",
            "Epoch 108 Batch 200 Loss 0.1256 Accuracy 0.9610\n",
            "Epoch 108 Batch 250 Loss 0.1294 Accuracy 0.9600\n",
            "Epoch 108 Batch 300 Loss 0.1304 Accuracy 0.9596\n",
            "Epoch 108 Batch 350 Loss 0.1327 Accuracy 0.9588\n",
            "Epoch 108 Loss 0.1329 Accuracy 0.9587\n",
            "Time taken for 1 epoch: 62.90 secs\n",
            "\n",
            "Epoch 109 Batch 0 Loss 0.1306 Accuracy 0.9599\n",
            "Epoch 109 Batch 50 Loss 0.1248 Accuracy 0.9626\n",
            "Epoch 109 Batch 100 Loss 0.1250 Accuracy 0.9623\n",
            "Epoch 109 Batch 150 Loss 0.1255 Accuracy 0.9622\n",
            "Epoch 109 Batch 200 Loss 0.1270 Accuracy 0.9613\n",
            "Epoch 109 Batch 250 Loss 0.1285 Accuracy 0.9607\n",
            "Epoch 109 Batch 300 Loss 0.1305 Accuracy 0.9599\n",
            "Epoch 109 Batch 350 Loss 0.1308 Accuracy 0.9599\n",
            "Epoch 109 Loss 0.1311 Accuracy 0.9598\n",
            "Time taken for 1 epoch: 63.30 secs\n",
            "\n",
            "Epoch 110 Batch 0 Loss 0.0791 Accuracy 0.9757\n",
            "Epoch 110 Batch 50 Loss 0.1194 Accuracy 0.9634\n",
            "Epoch 110 Batch 100 Loss 0.1253 Accuracy 0.9614\n",
            "Epoch 110 Batch 150 Loss 0.1269 Accuracy 0.9609\n",
            "Epoch 110 Batch 200 Loss 0.1283 Accuracy 0.9605\n",
            "Epoch 110 Batch 250 Loss 0.1303 Accuracy 0.9596\n",
            "Epoch 110 Batch 300 Loss 0.1305 Accuracy 0.9596\n",
            "Epoch 110 Batch 350 Loss 0.1317 Accuracy 0.9595\n",
            "Saving checkpoint for epoch 110 at ./checkpoints/train/ckpt-22\n",
            "Epoch 110 Loss 0.1317 Accuracy 0.9595\n",
            "Time taken for 1 epoch: 65.79 secs\n",
            "\n",
            "Epoch 111 Batch 0 Loss 0.1058 Accuracy 0.9664\n",
            "Epoch 111 Batch 50 Loss 0.1206 Accuracy 0.9629\n",
            "Epoch 111 Batch 100 Loss 0.1192 Accuracy 0.9634\n",
            "Epoch 111 Batch 150 Loss 0.1216 Accuracy 0.9629\n",
            "Epoch 111 Batch 200 Loss 0.1235 Accuracy 0.9620\n",
            "Epoch 111 Batch 250 Loss 0.1255 Accuracy 0.9614\n",
            "Epoch 111 Batch 300 Loss 0.1363 Accuracy 0.9587\n",
            "Epoch 111 Batch 350 Loss 0.1365 Accuracy 0.9585\n",
            "Epoch 111 Loss 0.1365 Accuracy 0.9585\n",
            "Time taken for 1 epoch: 63.46 secs\n",
            "\n",
            "Epoch 112 Batch 0 Loss 0.1103 Accuracy 0.9666\n",
            "Epoch 112 Batch 50 Loss 0.1172 Accuracy 0.9638\n",
            "Epoch 112 Batch 100 Loss 0.1176 Accuracy 0.9636\n",
            "Epoch 112 Batch 150 Loss 0.1183 Accuracy 0.9638\n",
            "Epoch 112 Batch 200 Loss 0.1194 Accuracy 0.9632\n",
            "Epoch 112 Batch 250 Loss 0.1212 Accuracy 0.9629\n",
            "Epoch 112 Batch 300 Loss 0.1220 Accuracy 0.9626\n",
            "Epoch 112 Batch 350 Loss 0.1237 Accuracy 0.9619\n",
            "Epoch 112 Loss 0.1240 Accuracy 0.9618\n",
            "Time taken for 1 epoch: 62.99 secs\n",
            "\n",
            "Epoch 113 Batch 0 Loss 0.0744 Accuracy 0.9743\n",
            "Epoch 113 Batch 50 Loss 0.1181 Accuracy 0.9631\n",
            "Epoch 113 Batch 100 Loss 0.1218 Accuracy 0.9627\n",
            "Epoch 113 Batch 150 Loss 0.1215 Accuracy 0.9629\n",
            "Epoch 113 Batch 200 Loss 0.1227 Accuracy 0.9625\n",
            "Epoch 113 Batch 250 Loss 0.1258 Accuracy 0.9615\n",
            "Epoch 113 Batch 300 Loss 0.1266 Accuracy 0.9611\n",
            "Epoch 113 Batch 350 Loss 0.1277 Accuracy 0.9605\n",
            "Epoch 113 Loss 0.1276 Accuracy 0.9606\n",
            "Time taken for 1 epoch: 62.65 secs\n",
            "\n",
            "Epoch 114 Batch 0 Loss 0.0891 Accuracy 0.9709\n",
            "Epoch 114 Batch 50 Loss 0.1116 Accuracy 0.9650\n",
            "Epoch 114 Batch 100 Loss 0.1125 Accuracy 0.9653\n",
            "Epoch 114 Batch 150 Loss 0.1184 Accuracy 0.9636\n",
            "Epoch 114 Batch 200 Loss 0.1236 Accuracy 0.9621\n",
            "Epoch 114 Batch 250 Loss 0.1239 Accuracy 0.9623\n",
            "Epoch 114 Batch 300 Loss 0.1245 Accuracy 0.9620\n",
            "Epoch 114 Batch 350 Loss 0.1255 Accuracy 0.9617\n",
            "Epoch 114 Loss 0.1258 Accuracy 0.9616\n",
            "Time taken for 1 epoch: 62.83 secs\n",
            "\n",
            "Epoch 115 Batch 0 Loss 0.0851 Accuracy 0.9705\n",
            "Epoch 115 Batch 50 Loss 0.1079 Accuracy 0.9670\n",
            "Epoch 115 Batch 100 Loss 0.1138 Accuracy 0.9643\n",
            "Epoch 115 Batch 150 Loss 0.1169 Accuracy 0.9636\n",
            "Epoch 115 Batch 200 Loss 0.1184 Accuracy 0.9633\n",
            "Epoch 115 Batch 250 Loss 0.1195 Accuracy 0.9630\n",
            "Epoch 115 Batch 300 Loss 0.1215 Accuracy 0.9623\n",
            "Epoch 115 Batch 350 Loss 0.1221 Accuracy 0.9622\n",
            "Saving checkpoint for epoch 115 at ./checkpoints/train/ckpt-23\n",
            "Epoch 115 Loss 0.1222 Accuracy 0.9622\n",
            "Time taken for 1 epoch: 66.00 secs\n",
            "\n",
            "Epoch 116 Batch 0 Loss 0.0888 Accuracy 0.9732\n",
            "Epoch 116 Batch 50 Loss 0.1120 Accuracy 0.9667\n",
            "Epoch 116 Batch 100 Loss 0.1158 Accuracy 0.9648\n",
            "Epoch 116 Batch 150 Loss 0.1175 Accuracy 0.9641\n",
            "Epoch 116 Batch 200 Loss 0.1190 Accuracy 0.9637\n",
            "Epoch 116 Batch 250 Loss 0.1216 Accuracy 0.9629\n",
            "Epoch 116 Batch 300 Loss 0.1219 Accuracy 0.9627\n",
            "Epoch 116 Batch 350 Loss 0.1225 Accuracy 0.9625\n",
            "Epoch 116 Loss 0.1229 Accuracy 0.9624\n",
            "Time taken for 1 epoch: 63.20 secs\n",
            "\n",
            "Epoch 117 Batch 0 Loss 0.1557 Accuracy 0.9510\n",
            "Epoch 117 Batch 50 Loss 0.1173 Accuracy 0.9639\n",
            "Epoch 117 Batch 100 Loss 0.1203 Accuracy 0.9629\n",
            "Epoch 117 Batch 150 Loss 0.1187 Accuracy 0.9630\n",
            "Epoch 117 Batch 200 Loss 0.1209 Accuracy 0.9626\n",
            "Epoch 117 Batch 250 Loss 0.1228 Accuracy 0.9619\n",
            "Epoch 117 Batch 300 Loss 0.1230 Accuracy 0.9616\n",
            "Epoch 117 Batch 350 Loss 0.1246 Accuracy 0.9611\n",
            "Epoch 117 Loss 0.1246 Accuracy 0.9611\n",
            "Time taken for 1 epoch: 63.36 secs\n",
            "\n",
            "Epoch 118 Batch 0 Loss 0.1103 Accuracy 0.9708\n",
            "Epoch 118 Batch 50 Loss 0.1096 Accuracy 0.9661\n",
            "Epoch 118 Batch 100 Loss 0.1125 Accuracy 0.9659\n",
            "Epoch 118 Batch 150 Loss 0.1150 Accuracy 0.9648\n",
            "Epoch 118 Batch 200 Loss 0.1174 Accuracy 0.9643\n",
            "Epoch 118 Batch 250 Loss 0.1184 Accuracy 0.9639\n",
            "Epoch 118 Batch 300 Loss 0.1209 Accuracy 0.9631\n",
            "Epoch 118 Batch 350 Loss 0.1222 Accuracy 0.9627\n",
            "Epoch 118 Loss 0.1223 Accuracy 0.9627\n",
            "Time taken for 1 epoch: 62.99 secs\n",
            "\n",
            "Epoch 119 Batch 0 Loss 0.1128 Accuracy 0.9600\n",
            "Epoch 119 Batch 50 Loss 0.1116 Accuracy 0.9656\n",
            "Epoch 119 Batch 100 Loss 0.1142 Accuracy 0.9649\n",
            "Epoch 119 Batch 150 Loss 0.1151 Accuracy 0.9650\n",
            "Epoch 119 Batch 200 Loss 0.1172 Accuracy 0.9644\n",
            "Epoch 119 Batch 250 Loss 0.1179 Accuracy 0.9641\n",
            "Epoch 119 Batch 300 Loss 0.1194 Accuracy 0.9636\n",
            "Epoch 119 Batch 350 Loss 0.1205 Accuracy 0.9631\n",
            "Epoch 119 Loss 0.1209 Accuracy 0.9629\n",
            "Time taken for 1 epoch: 62.85 secs\n",
            "\n",
            "Epoch 120 Batch 0 Loss 0.0840 Accuracy 0.9807\n",
            "Epoch 120 Batch 50 Loss 0.1037 Accuracy 0.9674\n",
            "Epoch 120 Batch 100 Loss 0.1092 Accuracy 0.9665\n",
            "Epoch 120 Batch 150 Loss 0.1107 Accuracy 0.9658\n",
            "Epoch 120 Batch 200 Loss 0.1122 Accuracy 0.9655\n",
            "Epoch 120 Batch 250 Loss 0.1129 Accuracy 0.9650\n",
            "Epoch 120 Batch 300 Loss 0.1146 Accuracy 0.9645\n",
            "Epoch 120 Batch 350 Loss 0.1167 Accuracy 0.9639\n",
            "Saving checkpoint for epoch 120 at ./checkpoints/train/ckpt-24\n",
            "Epoch 120 Loss 0.1170 Accuracy 0.9638\n",
            "Time taken for 1 epoch: 66.00 secs\n",
            "\n",
            "Epoch 121 Batch 0 Loss 0.1094 Accuracy 0.9639\n",
            "Epoch 121 Batch 50 Loss 0.1080 Accuracy 0.9667\n",
            "Epoch 121 Batch 100 Loss 0.1054 Accuracy 0.9677\n",
            "Epoch 121 Batch 150 Loss 0.1064 Accuracy 0.9676\n",
            "Epoch 121 Batch 200 Loss 0.1134 Accuracy 0.9659\n",
            "Epoch 121 Batch 250 Loss 0.1162 Accuracy 0.9647\n",
            "Epoch 121 Batch 300 Loss 0.1154 Accuracy 0.9649\n",
            "Epoch 121 Batch 350 Loss 0.1152 Accuracy 0.9649\n",
            "Epoch 121 Loss 0.1153 Accuracy 0.9648\n",
            "Time taken for 1 epoch: 63.05 secs\n",
            "\n",
            "Epoch 122 Batch 0 Loss 0.1201 Accuracy 0.9625\n",
            "Epoch 122 Batch 50 Loss 0.1050 Accuracy 0.9687\n",
            "Epoch 122 Batch 100 Loss 0.1053 Accuracy 0.9683\n",
            "Epoch 122 Batch 150 Loss 0.1082 Accuracy 0.9672\n",
            "Epoch 122 Batch 200 Loss 0.1100 Accuracy 0.9663\n",
            "Epoch 122 Batch 250 Loss 0.1123 Accuracy 0.9656\n",
            "Epoch 122 Batch 300 Loss 0.1131 Accuracy 0.9652\n",
            "Epoch 122 Batch 350 Loss 0.1142 Accuracy 0.9649\n",
            "Epoch 122 Loss 0.1145 Accuracy 0.9648\n",
            "Time taken for 1 epoch: 62.85 secs\n",
            "\n",
            "Epoch 123 Batch 0 Loss 0.0733 Accuracy 0.9777\n",
            "Epoch 123 Batch 50 Loss 0.1041 Accuracy 0.9683\n",
            "Epoch 123 Batch 100 Loss 0.1051 Accuracy 0.9681\n",
            "Epoch 123 Batch 150 Loss 0.1114 Accuracy 0.9658\n",
            "Epoch 123 Batch 200 Loss 0.1115 Accuracy 0.9655\n",
            "Epoch 123 Batch 250 Loss 0.1113 Accuracy 0.9655\n",
            "Epoch 123 Batch 300 Loss 0.1120 Accuracy 0.9653\n",
            "Epoch 123 Batch 350 Loss 0.1145 Accuracy 0.9646\n",
            "Epoch 123 Loss 0.1147 Accuracy 0.9645\n",
            "Time taken for 1 epoch: 62.91 secs\n",
            "\n",
            "Epoch 124 Batch 0 Loss 0.1012 Accuracy 0.9684\n",
            "Epoch 124 Batch 50 Loss 0.1033 Accuracy 0.9693\n",
            "Epoch 124 Batch 100 Loss 0.1028 Accuracy 0.9687\n",
            "Epoch 124 Batch 150 Loss 0.1027 Accuracy 0.9688\n",
            "Epoch 124 Batch 200 Loss 0.1041 Accuracy 0.9683\n",
            "Epoch 124 Batch 250 Loss 0.1072 Accuracy 0.9673\n",
            "Epoch 124 Batch 300 Loss 0.1087 Accuracy 0.9668\n",
            "Epoch 124 Batch 350 Loss 0.1101 Accuracy 0.9663\n",
            "Epoch 124 Loss 0.1099 Accuracy 0.9664\n",
            "Time taken for 1 epoch: 62.92 secs\n",
            "\n",
            "Epoch 125 Batch 0 Loss 0.1072 Accuracy 0.9622\n",
            "Epoch 125 Batch 50 Loss 0.1002 Accuracy 0.9682\n",
            "Epoch 125 Batch 100 Loss 0.1045 Accuracy 0.9672\n",
            "Epoch 125 Batch 150 Loss 0.1062 Accuracy 0.9673\n",
            "Epoch 125 Batch 200 Loss 0.1072 Accuracy 0.9673\n",
            "Epoch 125 Batch 250 Loss 0.1070 Accuracy 0.9674\n",
            "Epoch 125 Batch 300 Loss 0.1085 Accuracy 0.9669\n",
            "Epoch 125 Batch 350 Loss 0.1101 Accuracy 0.9664\n",
            "Saving checkpoint for epoch 125 at ./checkpoints/train/ckpt-25\n",
            "Epoch 125 Loss 0.1100 Accuracy 0.9664\n",
            "Time taken for 1 epoch: 66.16 secs\n",
            "\n",
            "Epoch 126 Batch 0 Loss 0.1050 Accuracy 0.9679\n",
            "Epoch 126 Batch 50 Loss 0.1030 Accuracy 0.9693\n",
            "Epoch 126 Batch 100 Loss 0.1030 Accuracy 0.9691\n",
            "Epoch 126 Batch 150 Loss 0.1029 Accuracy 0.9688\n",
            "Epoch 126 Batch 200 Loss 0.1046 Accuracy 0.9682\n",
            "Epoch 126 Batch 250 Loss 0.1053 Accuracy 0.9679\n",
            "Epoch 126 Batch 300 Loss 0.1065 Accuracy 0.9673\n",
            "Epoch 126 Batch 350 Loss 0.1090 Accuracy 0.9664\n",
            "Epoch 126 Loss 0.1092 Accuracy 0.9663\n",
            "Time taken for 1 epoch: 63.50 secs\n",
            "\n",
            "Epoch 127 Batch 0 Loss 0.1159 Accuracy 0.9596\n",
            "Epoch 127 Batch 50 Loss 0.1039 Accuracy 0.9676\n",
            "Epoch 127 Batch 100 Loss 0.1042 Accuracy 0.9679\n",
            "Epoch 127 Batch 150 Loss 0.1038 Accuracy 0.9678\n",
            "Epoch 127 Batch 200 Loss 0.1033 Accuracy 0.9681\n",
            "Epoch 127 Batch 250 Loss 0.1031 Accuracy 0.9680\n",
            "Epoch 127 Batch 300 Loss 0.1047 Accuracy 0.9674\n",
            "Epoch 127 Batch 350 Loss 0.1063 Accuracy 0.9669\n",
            "Epoch 127 Loss 0.1066 Accuracy 0.9668\n",
            "Time taken for 1 epoch: 63.01 secs\n",
            "\n",
            "Epoch 128 Batch 0 Loss 0.1053 Accuracy 0.9663\n",
            "Epoch 128 Batch 50 Loss 0.1013 Accuracy 0.9685\n",
            "Epoch 128 Batch 100 Loss 0.1018 Accuracy 0.9687\n",
            "Epoch 128 Batch 150 Loss 0.1024 Accuracy 0.9685\n",
            "Epoch 128 Batch 200 Loss 0.1057 Accuracy 0.9676\n",
            "Epoch 128 Batch 250 Loss 0.1075 Accuracy 0.9670\n",
            "Epoch 128 Batch 300 Loss 0.1088 Accuracy 0.9664\n",
            "Epoch 128 Batch 350 Loss 0.1092 Accuracy 0.9662\n",
            "Epoch 128 Loss 0.1091 Accuracy 0.9662\n",
            "Time taken for 1 epoch: 62.99 secs\n",
            "\n",
            "Epoch 129 Batch 0 Loss 0.0916 Accuracy 0.9684\n",
            "Epoch 129 Batch 50 Loss 0.0974 Accuracy 0.9707\n",
            "Epoch 129 Batch 100 Loss 0.0978 Accuracy 0.9706\n",
            "Epoch 129 Batch 150 Loss 0.1001 Accuracy 0.9700\n",
            "Epoch 129 Batch 200 Loss 0.1010 Accuracy 0.9695\n",
            "Epoch 129 Batch 250 Loss 0.1043 Accuracy 0.9682\n",
            "Epoch 129 Batch 300 Loss 0.1057 Accuracy 0.9677\n",
            "Epoch 129 Batch 350 Loss 0.1061 Accuracy 0.9675\n",
            "Epoch 129 Loss 0.1063 Accuracy 0.9675\n",
            "Time taken for 1 epoch: 62.71 secs\n",
            "\n",
            "Epoch 130 Batch 0 Loss 0.1001 Accuracy 0.9697\n",
            "Epoch 130 Batch 50 Loss 0.0977 Accuracy 0.9703\n",
            "Epoch 130 Batch 100 Loss 0.0967 Accuracy 0.9707\n",
            "Epoch 130 Batch 150 Loss 0.0979 Accuracy 0.9703\n",
            "Epoch 130 Batch 200 Loss 0.0987 Accuracy 0.9698\n",
            "Epoch 130 Batch 250 Loss 0.0994 Accuracy 0.9695\n",
            "Epoch 130 Batch 300 Loss 0.1006 Accuracy 0.9692\n",
            "Epoch 130 Batch 350 Loss 0.1028 Accuracy 0.9686\n",
            "Saving checkpoint for epoch 130 at ./checkpoints/train/ckpt-26\n",
            "Epoch 130 Loss 0.1028 Accuracy 0.9685\n",
            "Time taken for 1 epoch: 66.00 secs\n",
            "\n",
            "Epoch 131 Batch 0 Loss 0.0674 Accuracy 0.9805\n",
            "Epoch 131 Batch 50 Loss 0.0955 Accuracy 0.9708\n",
            "Epoch 131 Batch 100 Loss 0.0993 Accuracy 0.9692\n",
            "Epoch 131 Batch 150 Loss 0.0991 Accuracy 0.9693\n",
            "Epoch 131 Batch 200 Loss 0.1007 Accuracy 0.9690\n",
            "Epoch 131 Batch 250 Loss 0.1015 Accuracy 0.9685\n",
            "Epoch 131 Batch 300 Loss 0.1026 Accuracy 0.9682\n",
            "Epoch 131 Batch 350 Loss 0.1041 Accuracy 0.9678\n",
            "Epoch 131 Loss 0.1041 Accuracy 0.9678\n",
            "Time taken for 1 epoch: 63.47 secs\n",
            "\n",
            "Epoch 132 Batch 0 Loss 0.0956 Accuracy 0.9717\n",
            "Epoch 132 Batch 50 Loss 0.1073 Accuracy 0.9672\n",
            "Epoch 132 Batch 100 Loss 0.0994 Accuracy 0.9700\n",
            "Epoch 132 Batch 150 Loss 0.0992 Accuracy 0.9698\n",
            "Epoch 132 Batch 200 Loss 0.0995 Accuracy 0.9696\n",
            "Epoch 132 Batch 250 Loss 0.1014 Accuracy 0.9692\n",
            "Epoch 132 Batch 300 Loss 0.1025 Accuracy 0.9688\n",
            "Epoch 132 Batch 350 Loss 0.1038 Accuracy 0.9684\n",
            "Epoch 132 Loss 0.1036 Accuracy 0.9685\n",
            "Time taken for 1 epoch: 62.89 secs\n",
            "\n",
            "Epoch 133 Batch 0 Loss 0.0891 Accuracy 0.9740\n",
            "Epoch 133 Batch 50 Loss 0.0923 Accuracy 0.9714\n",
            "Epoch 133 Batch 100 Loss 0.0958 Accuracy 0.9704\n",
            "Epoch 133 Batch 150 Loss 0.0967 Accuracy 0.9702\n",
            "Epoch 133 Batch 200 Loss 0.0984 Accuracy 0.9698\n",
            "Epoch 133 Batch 250 Loss 0.1002 Accuracy 0.9694\n",
            "Epoch 133 Batch 300 Loss 0.1008 Accuracy 0.9693\n",
            "Epoch 133 Batch 350 Loss 0.1019 Accuracy 0.9688\n",
            "Epoch 133 Loss 0.1020 Accuracy 0.9688\n",
            "Time taken for 1 epoch: 62.59 secs\n",
            "\n",
            "Epoch 134 Batch 0 Loss 0.0916 Accuracy 0.9685\n",
            "Epoch 134 Batch 50 Loss 0.0919 Accuracy 0.9723\n",
            "Epoch 134 Batch 100 Loss 0.0965 Accuracy 0.9705\n",
            "Epoch 134 Batch 150 Loss 0.0972 Accuracy 0.9706\n",
            "Epoch 134 Batch 200 Loss 0.0983 Accuracy 0.9701\n",
            "Epoch 134 Batch 250 Loss 0.0996 Accuracy 0.9695\n",
            "Epoch 134 Batch 300 Loss 0.1012 Accuracy 0.9689\n",
            "Epoch 134 Batch 350 Loss 0.1019 Accuracy 0.9687\n",
            "Epoch 134 Loss 0.1019 Accuracy 0.9687\n",
            "Time taken for 1 epoch: 63.12 secs\n",
            "\n",
            "Epoch 135 Batch 0 Loss 0.0718 Accuracy 0.9801\n",
            "Epoch 135 Batch 50 Loss 0.0955 Accuracy 0.9718\n",
            "Epoch 135 Batch 100 Loss 0.0937 Accuracy 0.9721\n",
            "Epoch 135 Batch 150 Loss 0.0934 Accuracy 0.9720\n",
            "Epoch 135 Batch 200 Loss 0.0946 Accuracy 0.9717\n",
            "Epoch 135 Batch 250 Loss 0.0971 Accuracy 0.9711\n",
            "Epoch 135 Batch 300 Loss 0.0979 Accuracy 0.9707\n",
            "Epoch 135 Batch 350 Loss 0.0982 Accuracy 0.9706\n",
            "Saving checkpoint for epoch 135 at ./checkpoints/train/ckpt-27\n",
            "Epoch 135 Loss 0.0981 Accuracy 0.9706\n",
            "Time taken for 1 epoch: 66.21 secs\n",
            "\n",
            "Epoch 136 Batch 0 Loss 0.0688 Accuracy 0.9765\n",
            "Epoch 136 Batch 50 Loss 0.0941 Accuracy 0.9711\n",
            "Epoch 136 Batch 100 Loss 0.0957 Accuracy 0.9703\n",
            "Epoch 136 Batch 150 Loss 0.0943 Accuracy 0.9711\n",
            "Epoch 136 Batch 200 Loss 0.0962 Accuracy 0.9706\n",
            "Epoch 136 Batch 250 Loss 0.0973 Accuracy 0.9703\n",
            "Epoch 136 Batch 300 Loss 0.0986 Accuracy 0.9699\n",
            "Epoch 136 Batch 350 Loss 0.0996 Accuracy 0.9696\n",
            "Epoch 136 Loss 0.0997 Accuracy 0.9695\n",
            "Time taken for 1 epoch: 63.66 secs\n",
            "\n",
            "Epoch 137 Batch 0 Loss 0.0627 Accuracy 0.9806\n",
            "Epoch 137 Batch 50 Loss 0.0901 Accuracy 0.9738\n",
            "Epoch 137 Batch 100 Loss 0.0908 Accuracy 0.9730\n",
            "Epoch 137 Batch 150 Loss 0.0935 Accuracy 0.9719\n",
            "Epoch 137 Batch 200 Loss 0.0969 Accuracy 0.9710\n",
            "Epoch 137 Batch 250 Loss 0.0976 Accuracy 0.9706\n",
            "Epoch 137 Batch 300 Loss 0.0979 Accuracy 0.9704\n",
            "Epoch 137 Batch 350 Loss 0.0979 Accuracy 0.9704\n",
            "Epoch 137 Loss 0.0980 Accuracy 0.9703\n",
            "Time taken for 1 epoch: 63.57 secs\n",
            "\n",
            "Epoch 138 Batch 0 Loss 0.0764 Accuracy 0.9779\n",
            "Epoch 138 Batch 50 Loss 0.0912 Accuracy 0.9715\n",
            "Epoch 138 Batch 100 Loss 0.0899 Accuracy 0.9724\n",
            "Epoch 138 Batch 150 Loss 0.0922 Accuracy 0.9718\n",
            "Epoch 138 Batch 200 Loss 0.0934 Accuracy 0.9715\n",
            "Epoch 138 Batch 250 Loss 0.0948 Accuracy 0.9710\n",
            "Epoch 138 Batch 300 Loss 0.0964 Accuracy 0.9704\n",
            "Epoch 138 Batch 350 Loss 0.0976 Accuracy 0.9700\n",
            "Epoch 138 Loss 0.0980 Accuracy 0.9699\n",
            "Time taken for 1 epoch: 63.27 secs\n",
            "\n",
            "Epoch 139 Batch 0 Loss 0.1008 Accuracy 0.9692\n",
            "Epoch 139 Batch 50 Loss 0.0852 Accuracy 0.9735\n",
            "Epoch 139 Batch 100 Loss 0.0891 Accuracy 0.9723\n",
            "Epoch 139 Batch 150 Loss 0.0900 Accuracy 0.9720\n",
            "Epoch 139 Batch 200 Loss 0.0915 Accuracy 0.9717\n",
            "Epoch 139 Batch 250 Loss 0.0946 Accuracy 0.9705\n",
            "Epoch 139 Batch 300 Loss 0.0966 Accuracy 0.9699\n",
            "Epoch 139 Batch 350 Loss 0.0980 Accuracy 0.9696\n",
            "Epoch 139 Loss 0.0982 Accuracy 0.9695\n",
            "Time taken for 1 epoch: 63.44 secs\n",
            "\n",
            "Epoch 140 Batch 0 Loss 0.0961 Accuracy 0.9682\n",
            "Epoch 140 Batch 50 Loss 0.0898 Accuracy 0.9728\n",
            "Epoch 140 Batch 100 Loss 0.0901 Accuracy 0.9723\n",
            "Epoch 140 Batch 150 Loss 0.0922 Accuracy 0.9718\n",
            "Epoch 140 Batch 200 Loss 0.0942 Accuracy 0.9713\n",
            "Epoch 140 Batch 250 Loss 0.0962 Accuracy 0.9707\n",
            "Epoch 140 Batch 300 Loss 0.0964 Accuracy 0.9705\n",
            "Epoch 140 Batch 350 Loss 0.0974 Accuracy 0.9701\n",
            "Saving checkpoint for epoch 140 at ./checkpoints/train/ckpt-28\n",
            "Epoch 140 Loss 0.0975 Accuracy 0.9700\n",
            "Time taken for 1 epoch: 65.66 secs\n",
            "\n",
            "Epoch 141 Batch 0 Loss 0.1147 Accuracy 0.9709\n",
            "Epoch 141 Batch 50 Loss 0.0919 Accuracy 0.9721\n",
            "Epoch 141 Batch 100 Loss 0.0873 Accuracy 0.9728\n",
            "Epoch 141 Batch 150 Loss 0.0876 Accuracy 0.9730\n",
            "Epoch 141 Batch 200 Loss 0.0899 Accuracy 0.9726\n",
            "Epoch 141 Batch 250 Loss 0.0914 Accuracy 0.9722\n",
            "Epoch 141 Batch 300 Loss 0.0922 Accuracy 0.9717\n",
            "Epoch 141 Batch 350 Loss 0.0932 Accuracy 0.9714\n",
            "Epoch 141 Loss 0.0934 Accuracy 0.9714\n",
            "Time taken for 1 epoch: 63.23 secs\n",
            "\n",
            "Epoch 142 Batch 0 Loss 0.0583 Accuracy 0.9869\n",
            "Epoch 142 Batch 50 Loss 0.0868 Accuracy 0.9739\n",
            "Epoch 142 Batch 100 Loss 0.0900 Accuracy 0.9727\n",
            "Epoch 142 Batch 150 Loss 0.0923 Accuracy 0.9714\n",
            "Epoch 142 Batch 200 Loss 0.0922 Accuracy 0.9714\n",
            "Epoch 142 Batch 250 Loss 0.0936 Accuracy 0.9709\n",
            "Epoch 142 Batch 300 Loss 0.0947 Accuracy 0.9706\n",
            "Epoch 142 Batch 350 Loss 0.0950 Accuracy 0.9706\n",
            "Epoch 142 Loss 0.0948 Accuracy 0.9707\n",
            "Time taken for 1 epoch: 63.51 secs\n",
            "\n",
            "Epoch 143 Batch 0 Loss 0.0565 Accuracy 0.9837\n",
            "Epoch 143 Batch 50 Loss 0.0793 Accuracy 0.9751\n",
            "Epoch 143 Batch 100 Loss 0.0843 Accuracy 0.9741\n",
            "Epoch 143 Batch 150 Loss 0.0865 Accuracy 0.9735\n",
            "Epoch 143 Batch 200 Loss 0.0884 Accuracy 0.9729\n",
            "Epoch 143 Batch 250 Loss 0.0890 Accuracy 0.9728\n",
            "Epoch 143 Batch 300 Loss 0.0898 Accuracy 0.9726\n",
            "Epoch 143 Batch 350 Loss 0.0913 Accuracy 0.9722\n",
            "Epoch 143 Loss 0.0914 Accuracy 0.9721\n",
            "Time taken for 1 epoch: 63.08 secs\n",
            "\n",
            "Epoch 144 Batch 0 Loss 0.1378 Accuracy 0.9632\n",
            "Epoch 144 Batch 50 Loss 0.0846 Accuracy 0.9743\n",
            "Epoch 144 Batch 100 Loss 0.0832 Accuracy 0.9747\n",
            "Epoch 144 Batch 150 Loss 0.0865 Accuracy 0.9738\n",
            "Epoch 144 Batch 200 Loss 0.0888 Accuracy 0.9730\n",
            "Epoch 144 Batch 250 Loss 0.0913 Accuracy 0.9723\n",
            "Epoch 144 Batch 300 Loss 0.0927 Accuracy 0.9719\n",
            "Epoch 144 Batch 350 Loss 0.0934 Accuracy 0.9716\n",
            "Epoch 144 Loss 0.0938 Accuracy 0.9715\n",
            "Time taken for 1 epoch: 62.86 secs\n",
            "\n",
            "Epoch 145 Batch 0 Loss 0.1006 Accuracy 0.9656\n",
            "Epoch 145 Batch 50 Loss 0.0838 Accuracy 0.9744\n",
            "Epoch 145 Batch 100 Loss 0.0843 Accuracy 0.9746\n",
            "Epoch 145 Batch 150 Loss 0.0850 Accuracy 0.9742\n",
            "Epoch 145 Batch 200 Loss 0.0863 Accuracy 0.9738\n",
            "Epoch 145 Batch 250 Loss 0.0881 Accuracy 0.9732\n",
            "Epoch 145 Batch 300 Loss 0.0887 Accuracy 0.9729\n",
            "Epoch 145 Batch 350 Loss 0.0903 Accuracy 0.9722\n",
            "Saving checkpoint for epoch 145 at ./checkpoints/train/ckpt-29\n",
            "Epoch 145 Loss 0.0903 Accuracy 0.9722\n",
            "Time taken for 1 epoch: 66.05 secs\n",
            "\n",
            "Epoch 146 Batch 0 Loss 0.0661 Accuracy 0.9810\n",
            "Epoch 146 Batch 50 Loss 0.0833 Accuracy 0.9745\n",
            "Epoch 146 Batch 100 Loss 0.0869 Accuracy 0.9739\n",
            "Epoch 146 Batch 150 Loss 0.0890 Accuracy 0.9733\n",
            "Epoch 146 Batch 200 Loss 0.0895 Accuracy 0.9731\n",
            "Epoch 146 Batch 250 Loss 0.0894 Accuracy 0.9730\n",
            "Epoch 146 Batch 300 Loss 0.0908 Accuracy 0.9727\n",
            "Epoch 146 Batch 350 Loss 0.0915 Accuracy 0.9724\n",
            "Epoch 146 Loss 0.0915 Accuracy 0.9724\n",
            "Time taken for 1 epoch: 63.06 secs\n",
            "\n",
            "Epoch 147 Batch 0 Loss 0.0869 Accuracy 0.9696\n",
            "Epoch 147 Batch 50 Loss 0.0930 Accuracy 0.9716\n",
            "Epoch 147 Batch 100 Loss 0.0894 Accuracy 0.9726\n",
            "Epoch 147 Batch 150 Loss 0.0886 Accuracy 0.9731\n",
            "Epoch 147 Batch 200 Loss 0.0888 Accuracy 0.9730\n",
            "Epoch 147 Batch 250 Loss 0.0902 Accuracy 0.9726\n",
            "Epoch 147 Batch 300 Loss 0.0917 Accuracy 0.9721\n",
            "Epoch 147 Batch 350 Loss 0.0915 Accuracy 0.9721\n",
            "Epoch 147 Loss 0.0916 Accuracy 0.9720\n",
            "Time taken for 1 epoch: 63.40 secs\n",
            "\n",
            "Epoch 148 Batch 0 Loss 0.0897 Accuracy 0.9772\n",
            "Epoch 148 Batch 50 Loss 0.0829 Accuracy 0.9742\n",
            "Epoch 148 Batch 100 Loss 0.0854 Accuracy 0.9736\n",
            "Epoch 148 Batch 150 Loss 0.0871 Accuracy 0.9730\n",
            "Epoch 148 Batch 200 Loss 0.0872 Accuracy 0.9731\n",
            "Epoch 148 Batch 250 Loss 0.0870 Accuracy 0.9733\n",
            "Epoch 148 Batch 300 Loss 0.0888 Accuracy 0.9726\n",
            "Epoch 148 Batch 350 Loss 0.0899 Accuracy 0.9722\n",
            "Epoch 148 Loss 0.0900 Accuracy 0.9721\n",
            "Time taken for 1 epoch: 63.55 secs\n",
            "\n",
            "Epoch 149 Batch 0 Loss 0.0576 Accuracy 0.9865\n",
            "Epoch 149 Batch 50 Loss 0.0845 Accuracy 0.9745\n",
            "Epoch 149 Batch 100 Loss 0.0862 Accuracy 0.9744\n",
            "Epoch 149 Batch 150 Loss 0.0885 Accuracy 0.9732\n",
            "Epoch 149 Batch 200 Loss 0.0880 Accuracy 0.9731\n",
            "Epoch 149 Batch 250 Loss 0.0898 Accuracy 0.9727\n",
            "Epoch 149 Batch 300 Loss 0.0913 Accuracy 0.9722\n",
            "Epoch 149 Batch 350 Loss 0.0916 Accuracy 0.9720\n",
            "Epoch 149 Loss 0.0915 Accuracy 0.9720\n",
            "Time taken for 1 epoch: 63.23 secs\n",
            "\n",
            "Epoch 150 Batch 0 Loss 0.0636 Accuracy 0.9835\n",
            "Epoch 150 Batch 50 Loss 0.0799 Accuracy 0.9762\n",
            "Epoch 150 Batch 100 Loss 0.0814 Accuracy 0.9755\n",
            "Epoch 150 Batch 150 Loss 0.0828 Accuracy 0.9750\n",
            "Epoch 150 Batch 200 Loss 0.0833 Accuracy 0.9746\n",
            "Epoch 150 Batch 250 Loss 0.0848 Accuracy 0.9742\n",
            "Epoch 150 Batch 300 Loss 0.0865 Accuracy 0.9737\n",
            "Epoch 150 Batch 350 Loss 0.0872 Accuracy 0.9735\n",
            "Saving checkpoint for epoch 150 at ./checkpoints/train/ckpt-30\n",
            "Epoch 150 Loss 0.0872 Accuracy 0.9736\n",
            "Time taken for 1 epoch: 66.10 secs\n",
            "\n",
            "CPU times: user 1h 28min 37s, sys: 24min, total: 1h 52min 37s\n",
            "Wall time: 2h 40min 3s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Run training! Each epoch takes several mins with GPU\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  # inp -> twi, tar -> french\n",
        "  for (batch, (inp,tar)) in enumerate(train_batches):\n",
        "    train_step(inp, tar)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aij6BWpIBU52"
      },
      "source": [
        "# Build a Translator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "z0clY8WkaGZZ"
      },
      "outputs": [],
      "source": [
        "# class Translator(tf.Module):\n",
        "#     def __init__(self, tokenizers, transformer):\n",
        "#         self.tokenizers = tokenizers\n",
        "#         self.transformer = transformer\n",
        "\n",
        "#     def __call__(self, sentence, max_length=MAX_TOKENS):\n",
        "#         # The input sentence is English, hence adding the `[START]` and `[END]` tokens.\n",
        "#         sentence = tf.convert_to_tensor([sentence])\n",
        "#         sentence = self.tokenizers.eng.tokenize(sentence).to_tensor()\n",
        "\n",
        "#         encoder_input = sentence\n",
        "\n",
        "#         # As the output language is TWI, initialize the output with the\n",
        "#         # English `[START]` token.\n",
        "#         start_end = self.tokenizers.twi.tokenize([''])[0]\n",
        "#         start = start_end[0][tf.newaxis]\n",
        "#         end = start_end[1][tf.newaxis]\n",
        "\n",
        "#         # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "#         # dynamic-loop can be traced by `tf.function`.\n",
        "#         output_array = tf.TensorArray(\n",
        "#             dtype=tf.int64, size=0, dynamic_size=True)\n",
        "#         output_array = output_array.write(0, start)\n",
        "#         output = tf.transpose(output_array.stack())\n",
        "\n",
        "#         enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "#                 encoder_input, output)\n",
        "\n",
        "#         for i in tf.range(max_length):\n",
        "#             output = tf.transpose(output_array.stack())\n",
        "\n",
        "#             enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "#                 encoder_input, output)\n",
        "\n",
        "#             predictions, attention_weights = self.transformer(\n",
        "#                 encoder_input, output, False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "\n",
        "#             # Select the last token from the `seq_len` dimension.\n",
        "#             # Shape `(batch_size, 1, vocab_size)`.\n",
        "#             predictions = predictions[:, -1:, :]\n",
        "\n",
        "#             predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "#             # Concatenate the `predicted_id` to the output which is given to the\n",
        "#             # decoder as its input.\n",
        "#             output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "#             if predicted_id == end:\n",
        "#                 break\n",
        "\n",
        "#         output = tf.transpose(output_array.stack())\n",
        "#         # The output shape is `(1, tokens)`.\n",
        "#         text = self.tokenizers.twi.detokenize(output)[0]  # Shape: `()`.\n",
        "\n",
        "#         tokens = self.tokenizers.twi.lookup(output)[0]\n",
        "#         _, attention_weights = self.transformer(\n",
        "#             encoder_input, output[:, :-1], False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "\n",
        "#         return text, tokens, attention_weights\n",
        "  \n",
        "class Translator(tf.Module):\n",
        "    def __init__(self, transformer, input_processor, output_processor, max_length\n",
        "                 ):\n",
        "        self.input_processor = input_processor\n",
        "        self.output_processor = output_processor\n",
        "        self.transformer = transformer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __call__(self, sentence):\n",
        "        # The input sentence is TWI, hence adding the `[START]` and `[END]` tokens.\n",
        "        sentence = tf.convert_to_tensor([sentence])\n",
        "        sentence = self.input_processor.tokenize(sentence)\n",
        "        # trim sentence greater than Max_tokens\n",
        "        sentence = sentence[:, :self.max_length]\n",
        "        sentence = sentence.to_tensor()\n",
        "\n",
        "        encoder_input = sentence\n",
        "\n",
        "        # As the output language is ENGLISH, initialize the output with the\n",
        "        # English `[START]` token.\n",
        "        start_end = self.output_processor.tokenize([''])[0]\n",
        "        start = start_end[0][tf.newaxis]\n",
        "        end = start_end[1][tf.newaxis]\n",
        "\n",
        "        # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "        # dynamic-loop can be traced by `tf.function`.\n",
        "        output_array = tf.TensorArray(\n",
        "            dtype=tf.int64, size=0, dynamic_size=True)\n",
        "        output_array = output_array.write(0, start)\n",
        "        output = tf.transpose(output_array.stack())\n",
        "\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "            encoder_input, output)\n",
        "\n",
        "        for i in tf.range(self.max_length):\n",
        "            output = tf.transpose(output_array.stack())\n",
        "\n",
        "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "                encoder_input, output)\n",
        "\n",
        "            predictions, attention_weights = self.transformer(\n",
        "                encoder_input, output, False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "\n",
        "            # Select the last token from the `seq_len` dimension.\n",
        "            # Shape `(batch_size, 1, vocab_size)`.\n",
        "            predictions = predictions[:, -1:, :]\n",
        "\n",
        "            predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "            # Concatenate the `predicted_id` to the output which is given to the\n",
        "            # decoder as its input.\n",
        "            output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "            if predicted_id == end:\n",
        "                break\n",
        "\n",
        "        output = tf.transpose(output_array.stack())\n",
        "        # The output shape is `(1, tokens)`.\n",
        "        text = self.output_processor.detokenize(output)[0]  # Shape: `()`.\n",
        "\n",
        "        tokens = self.output_processor.lookup(output)[0]\n",
        "        _, attention_weights = self.transformer(\n",
        "            encoder_input, output[:, :-1], False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "\n",
        "        return text, tokens, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Tt1OVm3ecSNO"
      },
      "outputs": [],
      "source": [
        "#Create an instance of this Translator class\n",
        "translator = Translator(transformer,tokenizers.eng, tokenizers.twi, MAX_TOKENS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dMj9N3fBh1P"
      },
      "source": [
        "## translate example sentecnes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ysGcUmAzc_Yi"
      },
      "outputs": [],
      "source": [
        "def print_translation(sentence, tokens, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "wKmyWMcGduft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38abca6f-6bc6-437e-dd36-43b83f5529d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : i ran to your brother on the street .\n",
            "Prediction     : mituu mmirika kɔɔ wo nua no nkyɛn wɔ abɔnten so .\n",
            "Ground truth   : mituu mmirika kɔɔ wo nua no nkyɛn wɔ abɔnten so .\n"
          ]
        }
      ],
      "source": [
        "sentence =\"i ran to your brother on the street .\"\n",
        "ground_truth=\"mituu mmirika kɔɔ wo nua no nkyɛn wɔ abɔnten so .\"\n",
        "translated_text, translated_tokens, attention_weights = translator(sentence)\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "DYL7f6tu9Y0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c282c550-b6ae-4727-b998-e613976ae98c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : i want to ask you a question .\n",
            "Prediction     : mepɛ sɛ mibisa wo asɛm bi .\n",
            "Ground truth   : mepɛ sɛ mibisa wo asɛm bi .\n"
          ]
        }
      ],
      "source": [
        "ground_truth=\"mepɛ sɛ mibisa wo asɛm bi .\"\n",
        "sentence=\"i want to ask you a question .\"\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBe9E_dXBzne"
      },
      "source": [
        "# Save Translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "qMpmh77M9ufZ"
      },
      "outputs": [],
      "source": [
        "# # class to export translator\n",
        "# class ExportTranslator(tf.Module):\n",
        "#   def __init__(self, translator):\n",
        "#     self.translator = translator\n",
        "\n",
        "#   @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "#   def __call__(self, sentence):\n",
        "#     (result,\n",
        "#      tokens,\n",
        "#      attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n",
        "\n",
        "#     return result\n",
        "\n",
        "# class to export translator\n",
        "class ExportTranslator(tf.Module):\n",
        "    def __init__(self, translator, max_length):\n",
        "        self.translator = translator\n",
        "        self.max_length = max_length\n",
        "\n",
        "    @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "    def __call__(self, sentence):\n",
        "        (result,\n",
        "         tokens,\n",
        "         attention_weights) = self.translator(sentence, self.max_length)\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "89tEraoTixKG"
      },
      "outputs": [],
      "source": [
        "translator = ExportTranslator(translator,MAX_TOKENS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "1ZurB6qDFDzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0aea3edd-1093-4adc-f4bb-88d90485cb13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'lycolis y\\xc9\\x9b ade a w\\xc9\\x94de di dwuma .'"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "translator(tf.constant(\"mepɛ sɛ minya afoforo anim dom .\")).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "YMhMt9_z2ZXH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3c8d882-5a0d-4ef6-880a-70a2916de9e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn while saving (showing 5 of 332). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "module_no_signatures_path = '/content/drive/MyDrive/english_twi_translator'\n",
        "print('Saving model...')\n",
        "tf.saved_model.save(translator, module_no_signatures_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "P0jO3gtH9bih"
      },
      "outputs": [],
      "source": [
        "reloaded = tf.saved_model.load(module_no_signatures_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "adjW9Tug-ZOa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "2f43754f-9d03-42d0-bc7a-dd803e91f291"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'kɔmputa no wɔ nhomakorabea hɔ .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "reloaded('the computer is in the library .').numpy().decode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbDLGyb8clxZ"
      },
      "source": [
        "#BLEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "W8k01SazA8Cl"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "\n",
        "class Bleu():\n",
        "    def __init__(self, translator):\n",
        "        self.translator = translator\n",
        "\n",
        "    def get_bleuscore(self, testfile, referencefile, smothingfunction=None):\n",
        "        if type(testfile) == str and type(referencefile) == str:\n",
        "            # Open test file and read lines\n",
        "            f = open(testfile, \"r\")\n",
        "            hypothesis = f.readlines()\n",
        "            f.close()\n",
        "            # open refernce file and read lines\n",
        "            f = open(referencefile, \"r\")\n",
        "            reference = f.readlines()\n",
        "            f.close()\n",
        "        elif type(testfile) == list and type(referencefile) == list:\n",
        "            hypothesis = testfile\n",
        "            reference = referencefile\n",
        "        else:\n",
        "            print(f'File must be txt or python list')\n",
        "\n",
        "        # check the length of our input sentence\n",
        "        length = len(hypothesis)\n",
        "        bleu_total = 0\n",
        "        weights = (0.58,0,0,0)\n",
        "        for i in range(length):\n",
        "            hypothesis[i] = hypothesis[i]\n",
        "            reference[i] = reference[i]\n",
        "            groundtruth = reference[i].lower().replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            groundtruth = [groundtruth]\n",
        "            translated_text = self.translator(\n",
        "                hypothesis[i]).numpy().decode(\"utf-8\")\n",
        "            # print(\"Translated Text: \", translated_text)\n",
        "            # print(\"Ground Truth: \", reference[i])\n",
        "            candidate = translated_text.replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            bleu = sentence_bleu(\n",
        "                groundtruth, candidate, weights, smoothing_function=smothingfunction,auto_reweigh=True)\n",
        "            bleu_total += bleu\n",
        "\n",
        "        return f'BLEU SCORE: {bleu_total/length}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "TYHXXY10dMh5"
      },
      "outputs": [],
      "source": [
        "#iNSTANTIATE OBJECT OF BLEU CLASS AND IT SMOOTHING FUNCTION\n",
        "smooth= SmoothingFunction()\n",
        "bleu = Bleu(reloaded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "g2y6aHDfdbZK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "b1ae4842-4bcf-4041-c470-ac6c7db32fee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 17min 54s, sys: 2min 11s, total: 20min 5s\n",
            "Wall time: 12min 46s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BLEU SCORE: 0.6155615816041359'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "# ESTIMATE BLEU SCORE FROM THE TEST DATA\n",
        "# from list\n",
        "%%time\n",
        "bleu.get_bleuscore(\"/content/english_test.txt\",\"/content/twi_test.txt\", smooth.method7)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GOOGLE *API* BLEU\n"
      ],
      "metadata": {
        "id": "VRgT8luKPPuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use google API for bidirectional pivot translation of Twi and French\n",
        "# pivot language = English\n",
        "# import libraries\n",
        "from googletrans import Translator, constants\n",
        "# instantiate a translator object\n",
        "# initiate translator object\n",
        "translator = Translator()\n",
        "# Add Akan to the language supported by this package\n",
        "# Note the googletrans package has not  been updated to capture the new additions by google since May 2022\n",
        "# from https://translate.google.com/?sl=en&tl=ak&op=translate , the key and value for Twi is 'ak':'akan'\n",
        "constants.LANGUAGES['ak'] = 'akan'\n",
        "\n",
        "\n",
        "class GooglePivot:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def evaluate(self, sentences, src_key, dest_key):\n",
        "        eng_text = translator.translate(sentences, src=src_key, dest='en').text\n",
        "        print(eng_text)\n",
        "        output = translator.translate(eng_text, dest=dest_key).text\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class GoogleDirect:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def evaluate(self, sentences, src_key, dest_key):\n",
        "\n",
        "        return translator.translate(sentences, src=src_key, dest=dest_key).text"
      ],
      "metadata": {
        "id": "836KH3HJPNyW"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "\n",
        "class GoogleBleu():\n",
        "    def __init__(self, translator):\n",
        "        self.translator = translator\n",
        "\n",
        "    def get_bleuscore(self, testfile, referencefile,src_lang,dest_lang, smothingfunction=None):\n",
        "        if type(testfile) == str and type(referencefile) == str:\n",
        "            # Open test file and read lines\n",
        "            f = open(testfile, \"r\")\n",
        "            hypothesis = f.readlines()\n",
        "            f.close()\n",
        "            # open refernce file and read lines\n",
        "            f = open(referencefile, \"r\")\n",
        "            reference = f.readlines()\n",
        "            f.close()\n",
        "        elif type(testfile) == list and type(referencefile) == list:\n",
        "            hypothesis = testfile\n",
        "            reference = referencefile\n",
        "        else:\n",
        "            print(f'File must be txt or python list')\n",
        "\n",
        "        # check the length of our input sentence\n",
        "        length = len(hypothesis)\n",
        "        bleu_total = 0\n",
        "        weights = (0.58,0,0,0)\n",
        "        for i in range(length):\n",
        "            hypothesis[i] = hypothesis[i]\n",
        "            reference[i] = reference[i]\n",
        "            groundtruth = reference[i].lower().replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            groundtruth = [groundtruth]\n",
        "            translated_text = self.translator.evaluate(hypothesis[i],src_key=src_lang, dest_key=dest_lang)\n",
        "            # print(\"Translated Text: \", translated_text)\n",
        "            # print(\"Ground Truth: \", reference[i])\n",
        "            candidate = translated_text.replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            bleu = sentence_bleu(\n",
        "                groundtruth, candidate, weights, smoothing_function=smothingfunction,auto_reweigh=True)\n",
        "            bleu_total += bleu\n",
        "\n",
        "        return f'BLEU SCORE: {bleu_total/length}'"
      ],
      "metadata": {
        "id": "QHT4GJc7Sqpb"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate GoogleDirect class\n",
        "google_translate = GoogleDirect()"
      ],
      "metadata": {
        "id": "DxmDJjRkQLiI"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "google_bleu = GoogleBleu(google_translate)"
      ],
      "metadata": {
        "id": "UHjJ24E4RQ22"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "google_bleu.get_bleuscore(\"/content/english_test.txt\", \"/content/twi_test.txt\",'en','ak',smooth.method7)\n"
      ],
      "metadata": {
        "id": "v8QDG1k8UMhp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "3448926e-c79b-4a28-c7b6-7d0642c13af4"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 12.6 s, sys: 1.14 s, total: 13.7 s\n",
            "Wall time: 5min 21s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'BLEU SCORE: 0.800399985760805'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6G85u42AxVHt"
      },
      "execution_count": 57,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}