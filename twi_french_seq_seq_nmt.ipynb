{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/twi_french_seq_seq_nmt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiBmhONTYbXr"
      },
      "source": [
        "# **Machine Translation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iowPxD95Y6nE"
      },
      "source": [
        "This exercise will demonstrate how to build sequence to sequence models with attention for Twi-French machine translation. This code is based on the tensorflow implementation.The code snippet are adapted from from [[1]](https://www.tensorflow.org/text/tutorials/nmt_with_attention)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzMNK6S-ZK6s"
      },
      "source": [
        "## Install Tensorflow "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5kz0EVda0eP",
        "outputId": "788731d9-bf06-424d-d008-e47b52647bc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow-text==2.8.* in /usr/local/lib/python3.7/dist-packages (2.8.2)\n",
            "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.8.*) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.8.*) (0.12.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.17.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.26.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (14.0.6)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.21.6)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.1.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.14.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.3.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.47.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.1.1)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.5.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.1.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.0.7)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (57.4.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.23.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.4.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2022.6.15)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"tensorflow-text==2.8.*\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VESvB4rXcGAL"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yHosmTGhY5Xm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import unicodedata\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import collections\n",
        "import logging\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "import sys\n",
        "import tempfile\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import tensorflow as tf\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq9yZ-Hbc4Vj"
      },
      "source": [
        "## The Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bt3y-TB7dGzJ"
      },
      "source": [
        "The dataset for this exercise is taken from the paper ENGLISH-AKUAPEM TWI PARALLEL CORPUS and is available at [[2]](https://zenodo.org/record/4432117#.YxTaVtLP1Nj). The french translations were generated using  the googletrans python package [[3]](https://pypi.org/project/googletrans/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz5uvNaOhz0B"
      },
      "source": [
        "Perform stanadardization, unicode normalisation and add aend and start tokens to each sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "k6WR49aTT_Zj"
      },
      "outputs": [],
      "source": [
        "def read_dataset(number):\n",
        "\n",
        "    french_data = []\n",
        "    with open('/content/verified.french.txt') as file:\n",
        "\n",
        "        line = file.readline()\n",
        "        cnt = 1\n",
        "        while line:\n",
        "            french_data.append(line.strip())\n",
        "            line = file.readline()\n",
        "            cnt += 1\n",
        "\n",
        "    twi_data = []\n",
        "    with open('/content/verified.twi.txt') as file:\n",
        "\n",
        "        line = file.readline()\n",
        "        cnt = 1\n",
        "        while line:\n",
        "            twi_data.append(line.strip())\n",
        "            line = file.readline()\n",
        "            cnt += 1\n",
        "\n",
        "    return french_data[:number],twi_data[:number]\n",
        "\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def normalize_fr(s):\n",
        "    s = unicode_to_ascii(s)\n",
        "    s = re.sub(r'([!.?])', r' \\1', s)\n",
        "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "    s = re.sub(r'\\s+', r' ', s)\n",
        "    return s\n",
        "\n",
        "def normalize_twi(s):\n",
        "    s = unicode_to_ascii(s)\n",
        "    s = re.sub(r'([!.?])', r' \\1', s)\n",
        "    s = re.sub(r'[^a-zA-Z.ƆɔɛƐ!?’]+', r' ', s)\n",
        "    s = re.sub(r'\\s+', r' ', s)\n",
        "    return s\n",
        "\n",
        "def add_start_end_token(text):\n",
        "  # lower text\n",
        "  text = tf.strings.lower(text)\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text\n",
        "\n",
        "\n",
        "raw_data_fr,raw_data_twi = read_dataset(10000)\n",
        "raw_data_fr = [normalize_fr(data) for data in raw_data_fr]\n",
        "raw_data_twi = [normalize_twi(data) for data in raw_data_twi]\n",
        "\n",
        "with open('training_twi.txt', 'w') as f:\n",
        "    for line in raw_data_twi:\n",
        "        f.write(f\"{line}\\n\")\n",
        "\n",
        "with open('training_fr.txt', 'w') as f:\n",
        "    for line in raw_data_fr:\n",
        "        f.write(f\"{line}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DnvIjcG7oyGg"
      },
      "outputs": [],
      "source": [
        "# build TF datasets from input sentences in both languages\n",
        "lines_dataset_fr = tf.data.TextLineDataset('/content/training_fr.txt')\n",
        "lines_dataset_tw = tf.data.TextLineDataset('/content/training_twi.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WW2Is1gkspis",
        "outputId": "5bb3ece3-aad7-409a-c717-88206fc9cab4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 10000 samples in training data\n"
          ]
        }
      ],
      "source": [
        "NUM_PAIRS = 0\n",
        "for fr in lines_dataset_fr:\n",
        "  NUM_PAIRS += 1\n",
        "print('There are ' + str(NUM_PAIRS) + ' samples in training data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ui8oq9nsqb_",
        "outputId": "f451cbc8-3a6b-4067-aaf6-a53722ff5ffc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Twi:  Nea onni ho adwempa no de adwumaden na ɛba .\n",
            "Twi:  Na biribiara nni hɛ a metumi ayɔ\n",
            "Twi:  Kwaku hui se John ne Abena kurakura wɛn nsa .\n",
            "Twi:  So wubetumi atena ha akosi nnɛnmienu npaamu aduasa awiaberɔ .\n",
            "Twi:  Wonni mmre\n",
            "Twi:  Na ɛtotɛ nkosua aduonu abien .\n",
            "Twi:  Minni nkrante .\n",
            "Twi:  Dɔ n nti na abofra no suɔ\n",
            "Twi:  Me na mewɔ ha .\n",
            "Twi:  Na wonte ase .\n",
            "Twi:  Kwaku to dwom yiye .\n",
            "Twi:  Anadwo biara ɛfrɛ no .\n",
            "Twi:  Kwaku fii ayaresabea hɔ kɔe .\n",
            "Twi:  So mu denneennen\n",
            "Twi:  Wubetumi adi nkonim wɔ ɔko no nyinaa mu nanso wubetumi ahwere ɔko no .\n",
            "Twi:  Kwaku bɔɔ mmɔden sɛ obegyae ɔko a ɛkɔɔ so wɔ Abena ne John ntam no .\n",
            "Twi:  M adwene ne sɛ mɛte eyi ase akɔ Franse kasa mu .\n",
            "Twi:  Na megyae .\n",
            "Twi:  Obiara ani gyee n akokoduru no ho .\n",
            "Twi:  Na nsɔmmisa no ɔyɔ den .\n"
          ]
        }
      ],
      "source": [
        "# verify Twi file interpreted correctly\n",
        "for twi in lines_dataset_tw.take(20):\n",
        "  print(\"Twi: \", twi.numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aqr1wwhxs48T",
        "outputId": "1c3565ed-31ee-46bb-e8bd-edb5582c8351"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "French:  Ce qui lui manque en charisme elle le compense par avec travail acharne .\n",
            "French:  il n y avait rien je pourrais faire a ce sujet .\n",
            "French:  Kwaku vit John et Abena se tenir la main .\n",
            "French:  Pouvez vous rester jusqu a h ?\n",
            "French:  Vous n avez pas beaucoup de temps .\n",
            "French:  Elle a achete deux douzaines d ufs .\n",
            "French:  Je n ai pas d epee .\n",
            "French:  Pourquoi cette bebe pleure t il ?\n",
            "French:  J appartiens ici .\n",
            "French:  Vous n a pas pas compris .\n",
            "French:  Kwaku chante plutot bien .\n",
            "French:  Il l appelle tous les soirs .\n",
            "French:  Kwaku est sorti de l hopital .\n",
            "French:  Accrochez vous bien .\n",
            "French:  Vous pouvez gagner toutes les batailles encore perdre la guerre .\n",
            "French:  Kwaku a essaye de briser le combat entre Abena et John .\n",
            "French:  Je pense le faire traduire en francais\n",
            "French:  J arreterai .\n",
            "French:  Tout le monde admirait son courage .\n",
            "French:  c est une question difficile\n"
          ]
        }
      ],
      "source": [
        "# verify Hebrew file interpreted correctly\n",
        "for fr in lines_dataset_fr.take(20):\n",
        "  print(\"French: \", fr.numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_PBtVPLDtJ99"
      },
      "outputs": [],
      "source": [
        "# combine languages into single dataset\n",
        "combined = tf.data.Dataset.zip((lines_dataset_tw, lines_dataset_fr))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZAKOpv1tx_W",
        "outputId": "355d3200-ded4-402b-94f2-c4c5682dab15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Twi:  Nea onni ho adwempa no de adwumaden na ɛba .\n",
            "French:  Ce qui lui manque en charisme elle le compense par avec travail acharne .\n",
            "Twi:  Na biribiara nni hɛ a metumi ayɔ\n",
            "French:  il n y avait rien je pourrais faire a ce sujet .\n",
            "Twi:  Kwaku hui se John ne Abena kurakura wɛn nsa .\n",
            "French:  Kwaku vit John et Abena se tenir la main .\n",
            "Twi:  So wubetumi atena ha akosi nnɛnmienu npaamu aduasa awiaberɔ .\n",
            "French:  Pouvez vous rester jusqu a h ?\n",
            "Twi:  Wonni mmre\n",
            "French:  Vous n avez pas beaucoup de temps .\n",
            "Twi:  Na ɛtotɛ nkosua aduonu abien .\n",
            "French:  Elle a achete deux douzaines d ufs .\n",
            "Twi:  Minni nkrante .\n",
            "French:  Je n ai pas d epee .\n",
            "Twi:  Dɔ n nti na abofra no suɔ\n",
            "French:  Pourquoi cette bebe pleure t il ?\n",
            "Twi:  Me na mewɔ ha .\n",
            "French:  J appartiens ici .\n",
            "Twi:  Na wonte ase .\n",
            "French:  Vous n a pas pas compris .\n",
            "Twi:  Kwaku to dwom yiye .\n",
            "French:  Kwaku chante plutot bien .\n",
            "Twi:  Anadwo biara ɛfrɛ no .\n",
            "French:  Il l appelle tous les soirs .\n",
            "Twi:  Kwaku fii ayaresabea hɔ kɔe .\n",
            "French:  Kwaku est sorti de l hopital .\n",
            "Twi:  So mu denneennen\n",
            "French:  Accrochez vous bien .\n",
            "Twi:  Wubetumi adi nkonim wɔ ɔko no nyinaa mu nanso wubetumi ahwere ɔko no .\n",
            "French:  Vous pouvez gagner toutes les batailles encore perdre la guerre .\n",
            "Twi:  Kwaku bɔɔ mmɔden sɛ obegyae ɔko a ɛkɔɔ so wɔ Abena ne John ntam no .\n",
            "French:  Kwaku a essaye de briser le combat entre Abena et John .\n",
            "Twi:  M adwene ne sɛ mɛte eyi ase akɔ Franse kasa mu .\n",
            "French:  Je pense le faire traduire en francais\n",
            "Twi:  Na megyae .\n",
            "French:  J arreterai .\n",
            "Twi:  Obiara ani gyee n akokoduru no ho .\n",
            "French:  Tout le monde admirait son courage .\n",
            "Twi:  Na nsɔmmisa no ɔyɔ den .\n",
            "French:  c est une question difficile\n"
          ]
        }
      ],
      "source": [
        "# verify combined dataset is correct\n",
        "for tw,fr in combined.take(20):\n",
        "  print(\"Twi: \", tw.numpy().decode('utf-8'))\n",
        "  print(\"French: \", fr.numpy().decode('utf-8'))\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZySEcMyuxeS"
      },
      "source": [
        "## Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7BbRQUXRt3U2"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = NUM_PAIRS \n",
        "BATCH_SIZE = 64\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "  ds\n",
        "  .cache()\n",
        "  .shuffle(BUFFER_SIZE)\n",
        "  .batch(BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCxrWPy7vTEZ",
        "outputId": "92b887d4-6e78-46bd-b459-870f93eb6998"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.string, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "dataset = make_batches(combined)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjQA-sw7wUCD",
        "outputId": "48238d4f-a9f8-49f6-c268-282ee6d73dbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'Bere a Asamoah yii mfonini a w\\xc9\\x94atwa no mfonini no fii h\\xc9\\x94 no \\xc9\\x94hw\\xc9\\x9b\\xc9\\x9b mu yiye .'\n",
            " b'Minnim \\xc9\\x9bho ns\\xc9\\x9bm nyinaa .'\n",
            " b'\\xc6\\x90s\\xc9\\x9b s\\xc9\\x9b woka kyer\\xc9\\x9b Asamoah s\\xc9\\x9b Aku ny\\xc9\\x9b n adwene s\\xc9\\x9b \\xc9\\x94b\\xc9\\x9bk\\xc9\\x94 .'\n",
            " b'Asamoah de atuo maa Boatemaa' b'Wo nhoma no adane .'], shape=(5,), dtype=string)\n",
            "\n",
            "tf.Tensor(\n",
            "[b'Asamoah a enleve la photo encadree de la cheminee et l a regardee de plus pres .'\n",
            " b'Je ne connais pas les details .'\n",
            " b'Tu devrais dire a Asamoah qu Aku n a pas l intention d y aller .'\n",
            " b'Asamoah tendit une arme a Boatemaa .' b'Votre livre est a l envers .'], shape=(5,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "for  example_input_batch,example_target_batch in dataset.take(1):\n",
        "  print(example_input_batch[:5])\n",
        "  print()\n",
        "  print(example_target_batch[:5])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7e86KIYCwkMW"
      },
      "outputs": [],
      "source": [
        "# add start and end tokens\n",
        "def tf_start_and_end_tokens(text):\n",
        "  # Split accented characters.\n",
        "  #text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QqdmE0uyFDt",
        "outputId": "52443735-bc19-45f2-ee6a-d4906b3a8cce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ɛhe na saa frankaa yi fi ba ?\n",
            "[START] Ɛhe na saa frankaa yi fi ba ? [END]\n"
          ]
        }
      ],
      "source": [
        "# verify if start and end tokens\n",
        "example_text = tf.constant('Ɛhe na saa frankaa yi fi ba ?')\n",
        "print(example_text.numpy().decode())\n",
        "print(tf_start_and_end_tokens(example_text).numpy().decode())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oajmdm0UFITv"
      },
      "source": [
        "## Text Vectorization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjXS7QSTFZdz"
      },
      "source": [
        "This standardization function will be wrapped up in a tf.keras.layers.TextVectorization layer which will handle the vocabulary extraction and conversion of input text to sequences of tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9rpmQpXHFT-j"
      },
      "outputs": [],
      "source": [
        "max_vocab_size = 5000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVUf_-3JF1Ju",
        "outputId": "307142de-a480-4ca5-e63b-9d54f945f18a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'no', 'sɛ', 'a', 'na', 'so']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Process twi as input\n",
        "input_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_start_and_end_tokens,\n",
        "    max_tokens=max_vocab_size)\n",
        "input_text_processor.adapt(lines_dataset_tw)\n",
        "# Here are the first 10 words from the vocabulary:\n",
        "input_text_processor.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFxUzyAFGh8i",
        "outputId": "095e5ea4-477b-44a6-ed13-5f9f119df2d3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'a', 'de', 'je', 'est', 'asamoah']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Process french as output\n",
        "output_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_start_and_end_tokens,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "output_text_processor.adapt(lines_dataset_fr)\n",
        "output_text_processor.get_vocabulary()[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6U3VVe80InAk"
      },
      "source": [
        "Now these layers can convert a batch of strings into a batch of token IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FtuwuAZHZlm",
        "outputId": "f7ef3da9-a86e-431e-9c69-4cb4cbcfb54b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 10), dtype=int64, numpy=\n",
              "array([[   2,   19,    7,   10,  804,  172,    7, 1757,    5,  172],\n",
              "       [   2,  132,  303,  135,   38,    4,    3,    0,    0,    0],\n",
              "       [   2,   64,    6,  270,   86,   10,    6,  130,   70,   29]])>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "example_tokens = input_text_processor(example_input_batch)\n",
        "example_tokens[:3, :10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvOlTahyJakK"
      },
      "source": [
        "The get_vocabulary method can be used to convert token IDs back to text:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "IjyocNAbJIJy",
        "outputId": "506463ae-0d2e-4d75-e184-10997ea5c839"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[START] bere a asamoah yii mfonini a wɔatwa no mfonini no fii hɔ no ɔhwɛɛ mu yiye . [END]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
        "tokens = input_vocab[example_tokens[0].numpy()]\n",
        "' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1L4JnexLLPR"
      },
      "source": [
        "The returned token IDs are zero-padded. This can easily be turned into a mask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "IqYmGYMsLL0s",
        "outputId": "ab00c9ea-5039-42e6-fb8b-5294f86c9a50"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Mask')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAcmUlEQVR4nO3deZRc5Znf8e/T1S21QEJCC7LQgvBImLCYTayeeFgyNl4mMMlYscdxlBnNURZ7QnImGZPxHNszx2cOPjMZzMSccWTjIC9sIfaBBMcYy2DsYZUAI1YDsgQSQhICJDVoq6onf9SVTyHUz+2u21V1367f5xyd7qqn7r0vzdtPv/XU+97X3B0REUlPX7cbICIirVECFxFJlBK4iEiilMBFRBKlBC4ikiglcBGRRCmBt5GZXWhmm7rdDpHUmNk9ZvZH3W5H2SmBj5CZDTX9q5vZnqbHn+xy237d2bM/GvWmtm0ys1vM7OxutlHGHzPbYGb7zWzmIc8/amZuZgu707LeoQQ+Qu4++eA/4EXgd5qe+26323eIl7N2TgHOA54BfmZml3S3WTIO/Qr4xMEHZnYqcET3mtNblMALMrOJZvYVM3s5+/cVM5s4zGv/g5k9ZWbzsuP+xsxeNLOtZvY1M5uUve7CbOT8J2a2zcy2mNkfjLZt3rDJ3T8PfAP4cnZ+M7Ors3PvMrN1ZnZKkZ+D9KxvA/+q6fEy4FsHH5jZR7IR+S4ze8nMvtgUGzSz75jZDjN7w8weNrPZh17AzOaY2eNm9l/a+R+SIiXw4j5HY5R7OnAacA7w54e+yMw+D/xr4LfcfRNwFXBCdtwiYC7w+aZD3gVMzZ5fDlxrZkcXaOf3gDPN7EjgA8D7s+tPBZYCOwqcW3rXA8BRZvaPzKwCfBz4TlP8TRoJfhrwEeDfmdnlWWwZjf43H5gB/FtgT/PJzex44KfAV939r9v5H5IiJfDiPgn8pbtvc/ftwF8An2qKm5n9LY2keZG7bzczA1YA/8ndX3P33cBf0ej8Bx3IznvA3X8ADAHvKdDOlwGj8Yt0gEZ55UTA3P1pd99S4NzS2w6Own8beBrYfDDg7ve4+zp3r7v748CNwG9l4QM0Evcid6+5+1p339V03pOAu4EvuPvKTvyHpKa/2w0YB44FNjY93pg9d9A0Gsn6X7j7zuy5WTTqhGsbuRxoJNdK03E73L3a9PgtYHKBds4FHHjD3X9iZl8FrgWOM7PvAf/5kF8ekZH6NnAvcDxN5RMAMzuXxrvNU4AJwETgfzUdNx+4ycym0Ri5f87dD2TxTwLPA7e2+z8gVRqBF/cycFzT4wXZcwe9DnwU+J9m9r7suVdpvFU82d2nZf+mZh88tsvvAo+4+5sA7v537n4WjVHOCYDqi9ISd99I48PMD9Mo1TW7AbgdmO/uU4Gv0RiskL27/At3Pwm4gMbvSXM9/Ys0flduyMozcggl8OJuBP7czGZl06k+z9trgLj7PTRGE98zs3PcvQ58HbjazI4BMLO5ZvbBsWxY9mHlXDP7AvBHwJ9lz59tZuea2QCNGuVeoD6W15aesxy4+OAAockU4DV332tm5wC/fzBgZheZ2alZct5Fo6TS3A8PAB8DjgS+ZWbKV4fQD6S4LwFrgMeBdcAj2XNv4+53AX8I/B8zOxP4LI23hw+Y2S7gxxSrcTc71syGaNTNHwZOBS509x9l8aNo/AF5nUbJZwegD4ikZe7+gruvOUzo3wN/aWa7aQxubmmKvYtGeWQXjdr5T2mUVZrPux/4Z8Bs4JtK4m9n2tBBRCRN+msmIpIoJXARkUQpgYuIJEoJXEQkUR1dyDPBJvogR7bl3E0LYg4r78Na64v/lnlds+zKbjevv+ruszp93ZnTK75w/kCnL1sav3xc965qt+H6dkcT+CBHcu4wN8SzSjxP32u1MN43YUIYr+/fHx8/Ke6E9bfeCuPSfT/2Wzfmv2rsLZw/wEN3LujGpUvhg8ee1u0mjHvD9W2VUEREEqUELiKSqNLczMpySiDklEC8XmxBUt/8Y8O4T5kUx9esK3R9kVTd+fIvCh2vEkzrNAIXEUmUEriISKKUwEVEElWaGnh9z54wnjfNEC82T7s2Na5xs/apOH5+Th3vgbhG3r/o+DBefe6F+PwiiVINvXUagYuIJEoJXEQkUSMqoWT71X2Dxr52TmNjgmeBm4GFwAZgqbu/3mpDKpPj3cRqb+ashMwpofQvencYrz5UcBrg/cXeBqpE0h2d6Nu9rpdLHO020hH4NcAP3f1E4DQau2dcCax298XA6uyxSGrUtyVZuQnczKYC7weug8YWR+7+BnAZsCp72Srg8nY1UqQd1LcldSMpoRwPbKexq/ppwFrgCmC2u2/JXvMKjT3r3sHMVgArAAYZ/oZRnrPS8sAHzgrjA3c+HMarL2wI4/3z5sbHb94SxmsXnxHGK6vXhnHpipb7dnO/XjC3NJO5SilvlolKLK0bSQmlHzgT+Ht3P4PGLuZve0vpjXu1HnYtu7uvdPcl7r5kgIlF2ysyllru2839etaMnCmuIm0ykgS+Cdjk7g9mj2+l0em3mtkcgOzrtvY0UaRt1LclabkJ3N1fAV4ys/dkT10CPAXcDizLnlsG3NaWFoq0ifq2pG6kxbs/Br5rZhOA9cAf0Ej+t5jZcmAjsLQ9TWzIq3H3HzsnjFdfjmvY1U2bR92mZqpxJ6vrfbvbVINO14gSuLs/Biw5TOjw2+uIJEJ9W1KmlZgiIokqzfyn89cMhfEH3jcjjNdnHR3G+wfjGTDV9RvCuMh4pZtJpUsjcBGRRCmBi4gkSglcRCRRpamBP7j05DBeG4rv1rf3uPhuhoPbB8K4bYxX01VmTA/j1W3b4+OnTg3jtZ07w7hIWRWtoRfVyzV4jcBFRBKlBC4ikqjSlFB8S87tJnI2bDjyp8+G8bwSxWHvxNXkrbPjPSsn3BGXUGxKXOJBJRSRw+rlEkkejcBFRBKlBC4ikqjSlFBYEN+MinVxiaG2a/cYNuadJtzxUKHji94sS6RdVKJIl0bgIiKJUgIXEUmUEriISKJKUwOvrXsmjFemx3cbpBZPM6wPxXc79FotPr/IONXulZSqsbePRuAiIolSAhcRSVRpSiiVyfFKRd+7L4zX33orvoDpb5VIK1QCKS9lNRGRRCmBi4gkSglcRCRRpamB13Km+fUvmBfG6y/GNfD+mTkbMmx/NYz3TRwM41Tiv4V5Nfq889f37Y2vL9Im3d6wIU8v1+g1AhcRSZQSuIhIokZUQjGzDcBuoAZU3X2JmU0HbgYWAhuApe7+eqsN6TvtpDC+6ZJpYXzgzQVhfMbKB0bdpmb1/fvjF+RsOJF7fpVIuqITfbvX9XKJo91GMwK/yN1Pd/cl2eMrgdXuvhhYnT0WSZH6tiSpSAnlMmBV9v0q4PLizREpBfVtScJIZ6E48CMzc+B/uPtKYLa7b8nirwCzD3egma0AVgAMcsTwFxiMmzL96QNhfMIP14bxvgkTwni7SyRSWi317eZ+vWBuaSZzdYVKJN0z0p73m+6+2cyOAe4ys7fdOtDdPfsFeIfsF2IlwFE2PW/vYJFOa6lvN/frJacNql9LV4yohOLum7Ov24DvA+cAW81sDkD2NWdbeZHyUd+WlOUmcDM70symHPwe+ADwBHA7sCx72TLgtnY1UqQd1LcldSMpocwGvm9mB19/g7v/0MweBm4xs+XARmBpkYb4g4+H8biCne/ABSeH8crdcQ09byVo7eWtYdyrcQ0/b8OK2muaxdYGHenb4123V2r2cg0+N4G7+3rgHT8hd98BXNKORol0gvq2pE4rMUVEElWa+U+VU94TxmtPPBvGn7v23DC++NMPjrpNzaovbip0fB6VSKRberkEkTqNwEVEEqUELiKSKCVwEZFElaYGblviDRWsfyCML/7Mw4WO91otjG/74/PC+DF/d18YFymrdk8DVI29fTQCFxFJlBK4iEiiSlNCqe7YEcaLrlS0SryW0yqVMF60RNJ/3PwwXt34UqHzi5RV0RKNSjDD0whcRCRRSuAiIokqTQmlb+JgGK/v3BWfwOK/RbsuPz2MT745Z8/Mn8Q3s+LieKWmSiTSq1QCaR+NwEVEEqUELiKSKCVwEZFElaYGbpNyauBv7C10/mmPbA/j1bwT5NS4RXqVatzdoxG4iEiilMBFRBJVmhIK898VhvsH4qbWh94M49XnfzXqJomISiRlphG4iEiilMBFRBKlBC4ikqjS1MBr654J43l3C8zbkKHdKpMnh3GbPi2MV196OYz35XwG8OzX3hvGF//hmjAuMpx2b/hQdmX+DEAjcBGRRCmBi4gkasQlFDOrAGuAze7+UTM7HrgJmAGsBT7l7vtbbUje3Qh3/vMzwviUGx8M4/0zp4fx6vZ4T868EkntzbfCOENDcTxHfX/8o1WJpDXt7tdSXJlLGN02mhH4FcDTTY+/DFzt7ouA14HlY9kwkQ5Rv5ZkjSiBm9k84CPAN7LHBlwM3Jq9ZBVweTsaKNIu6teSupGWUL4C/CkwJXs8A3jD3Q/eA2oTMPdwB5rZCmAFwCBHDHuBvFkkU264f4RNPby8EkmeWk4JZP9HzgnjE+54KIz3zzvsj+/Xqps2h3FpyZj06wVzSzOZa1zKmwXTyyWW3BG4mX0U2Obua1u5gLuvdPcl7r5kgImtnEJkzI1lv541I57iKtIuIxk6vA/4p2b2YWAQOAq4BphmZv3ZaGUeoCGipET9WpKXOwJ39//q7vPcfSHwceAn7v5J4G7g97KXLQNua1srRcaY+rWMB0WKd58FbjKzLwGPAtcVaUheDbzvrFPCeH3tE0UuT+XUE8N47YlfhvG8Gnce1bhLY0z7tRTXyzXuPKNK4O5+D3BP9v16IP7kTiQB6teSKq3EFBFJVGnmP/XPPzaMV3NKJFuvuCCMz77mvrgBm7fGca/HcZEepRJH92gELiKSKCVwEZFEKYGLiCSqNDXwAwtmhvH+KcMvw4cR1Lhz1F57vdDxImWlGvX4pRG4iEiilMBFRBJVmhKK/fyxMO4TJhQ6f96GEXkbJvTlXb8S/y2sv5Wz4YNIm6S+p6VKQMPTCFxEJFFK4CIiiSpNCaV20VnxC+5u6bbNv1bft7erx4t0i0oQ45dG4CIiiVICFxFJlBK4iEiiSlMDH/iHdWHcK/G+g173+AI5dxPsX3hcGK9u2Bif3+K/hdZnYTxvQwuRVmka4filEbiISKKUwEVEElWaEkr9QLXQ8X2DE+Pz79kTxouWSPJU3r0wvv5zLxQ6v8h4VbQENJ5LMBqBi4gkSglcRCRRSuAiIokqTQ08d9PgnBp0fe++MN4/Y0YYr+7YkXP5YtMAVeOWXjWea9DdphG4iEiilMBFRBKVW0Ixs0HgXmBi9vpb3f0LZnY8cBMwA1gLfMrd410Rouv0D4Rxrx7IOUFOiWXozTBemTo1jNd27oyvL8npVN8e71Qi6Z6RjMD3ARe7+2nA6cClZnYe8GXgandfBLwOLG9fM0XaQn1bkpabwL1hKHs4kP1z4GLg1uz5VcDlbWmhSJuob0vqRjQLxcwqNN5KLgKuBV4A3nD3g8snNwFzhzl2BbACYJAjhr9IziyU5685L4wvuuKBML73n7w3jE+446Ewvvv3zw/jR92yJozbhLhExInHh+H6I0/Gx0tLWu3bzf16wdzyTObqhm7fLKuXSzgj+hDT3WvufjowDzgHOHGkF3D3le6+xN2XDBAvdxfptFb7dnO/njUjvlOmSLuMahaKu78B3A2cD0wzs4NDj3nA5jFum0jHqG9LinITuJnNMrNp2feTgN8GnqbR2X8ve9ky4LZ2NVKkHdS3JXUjKd7NAVZltcI+4BZ3/79m9hRwk5l9CXgUuK5IQ/JWMubVuPOmEebVuPOmMU654f4wnrOdRP40SNW4u6EjfXu86+UadLflJnB3fxw44zDPr6dRMxRJkvq2pE4rMUVEElWa+U/9vxFPo6uujzdc6DvzpDj+ymvx+Te/HMZFepVKJOWlEbiISKKUwEVEEqUELiKSqNLUwPfPPzqM9+XUwP0Xz4bxas40vr4z4hp6/dGnwrjIeKVNhctLI3ARkUQpgYuIJKo0JZTKz+K3aZ5zt0I/+9Qwbg/nlECe+VUcz1F0QwqrxDdEylup+tryC8L4jjPj40/8bLwStDY0FMZFhtPtuxW2WzdLRBqBi4gkSglcRCRR5SmhHDc/jA+dckwYn3TH2jCeV4KozJoRxut798Xnz5vlMnEwjOdt+GCD8b3Up193Xxif+Z34+vVTF4dx1qyL4yI9Kq9E1M4Si0bgIiKJUgIXEUmUEriISKJKUwOvbXwpjA+u3xDGKwvmhfHqi5viBlSrcTxnGmOe+r698Qvy4rt3t/f6qnFLi7TSsns0AhcRSZQSuIhIokpTQrng0T1h/Bc74xLJ0D/OKZHkqG5/tdDxIt2iEkbv0ghcRCRRSuAiIolSAhcRSVRpauD/cFq81BviGnXfpElhvL4nrrH3HXFEfPxbb4VxkW4p+93+VKNvH43ARUQSpQQuIpKo3BKKmc0HvgXMBhxY6e7XmNl04GZgIbABWOrur7faEDvr5DDuOSsFD5wX72lZuTu+W+H+C+LjJ26NNzSorXsmjOexc98bxv3Bx3NOkPO3OGclaWV6vCdp7bWW/9eWVqf6dq8re4mn3bp9N8Iq8CfufhJwHvBpMzsJuBJY7e6LgdXZY5GUqG9L0nITuLtvcfdHsu93A08Dc4HLgFXZy1YBl7erkSLtoL4tqRvVLBQzWwicATwIzHb3LVnoFRpvQw93zApgBcAgw8/0OHB0vGFBXkMre3JuRpVjw+/EV1h0RbESSZ7cEknuCYrdbGs8lkhGY7R9u7lfL5hbmslcXaFZJt0z4g8xzWwy8L+B/+juu5pj7u40aojv4O4r3X2Juy8ZIE7SIt3QSt9u7tezZsQbUou0y4gSuJkN0Ojg33X372VPbzWzOVl8DrCtPU0UaR/1bUlZbgI3MwOuA552979tCt0OLMu+XwbcNvbNE2kf9W1J3UiKd+8DPgWsM7PHsuf+DLgKuMXMlgMbgaWFGnLXmjg+a2YYrz4QT1Xqnx1vinzCnz4WxtFKzfGoI317vOvmpr69LjeBu/vPARsmfMnYNkekc9S3JXVaiSkikqjSzH/qP25+GK9v3xEff+LiMF595rlRt0mkF6jEkS6NwEVEEqUELiKSKCVwEZFElaYG/uxfxdMEFy17JT7BgZyl9AXv1icyXmkaYLo0AhcRSZQSuIhIokpTQllwzGthvG9SvGdm9YVfjWVzRJKhEkfv0ghcRCRRSuAiIokqTQll8GM7w3ht9+4wbv0DcXxCHM+7GVX/uxeG8er6DWE8T2XatDBee+ONQueX8Wu87zmpEtHwNAIXEUmUEriISKKUwEVEElWaGni7a7w2YUIc338gjOfVuPuPnRMf//KWMK4at8jhaaXo8DQCFxFJlBK4iEiiSlNC6Z83N4xXN8clCOsbbmeshvrQm2Hcq3EJZf+Hzo6vf1e8p2Zf3p6ae/aG8bybba3/m/PD+OIvrgvjtaGh+PoiJdXuaZRlLtFoBC4ikiglcBGRRCmBi4gkqjQ18LxNiysnLgrjtad/GcatUgnj/bOPCeP8v4fDcN+pJ4bxoUVTw/iUe58P49Ud8c/nhL9+IYyTczdH3oxvJaANL6RXlXkao0bgIiKJUgIXEUlUbgnFzL4JfBTY5u6nZM9NB24GFgIbgKXu/nqhllTivyW1aZMKnd7rHsarW7cVOn9t3TNhfFI8i4+cHT1zFW1/L+pY305cmafR9bqRjMCvBy495LkrgdXuvhhYnT0WSc31qG9LwnITuLvfCxy639llwKrs+1XA5WPcLpG2U9+W1LU6C2W2ux9cGvkKMHu4F5rZCmAFwCDDr0bMu9kU98efBFcmTw7jWmkoIzSivt3crxfMLc1kLukxhT/EdHcHhi0wu/tKd1/i7ksGmFj0ciIdE/Xt5n49a0Y8RVWkXVpN4FvNbA5A9lWfoMl4ob4tyWg1gd8OLMu+XwbcNjbNEek69W1JxkimEd4IXAjMNLNNwBeAq4BbzGw5sBFYWrgl898Vx3M2PPjE2mfD+Jdu/VgYX/i5++Lry7jTsb4t0ia5CdzdPzFM6JIxbotIR6lvS+q0ElNEJFGlmf+Ut5Kx74yTwvgNp8crNReeEd+sKe9mV16rhfG+vD03J8Xtq+3cGZ8/5/j6nj3x8afFP7/6L54K49K72r1hQup0MysRERk1JXARkUQpgYuIJKo0NfA8fZvi9RS1/fvj49fENV63+G/ZgQ+eGcYnrI43Na7n1bhzauj1vfvC+IEPxpsuD9wZb0ghIodX5rsxagQuIpIoJXARkUQlU0Kpbn+10PF9Rwx/J0QA9sUliok/ezKM16sH4vPnlGjqOSWgPHklnL4pU8J4bffuQtcXkc7TCFxEJFFK4CIiiUqmhFJ0peQPnv15GM/9pLlgiQOvFzs+7/Q5JZza7pwSj8gwyjwLo9dpBC4ikiglcBGRRCmBi4gkKpkauNeH3XYTgM1XXhDGP7QwXunYNymusefd7U9EpNM0AhcRSZQSuIhIokpTQrH+gTCeN03uuK/He2LmTeIrWiLJ23DBD1TjeMENI+r79obxysknhPHak78M49K7en1DhzJPo9QIXEQkUUrgIiKJUgIXEUlUeWrgA3FTKtOnhfH67qE4nlODLqrd0wzzatx5VOOWVpW5BtzrNAIXEUmUEriISKIKlVDM7FLgGqACfMPdr2r1XHkliKIlCjv3vWG878n1Ybw2FJdotn0mXgl6zFfvC+OVyZMLXT93mmFeCSnnbom7/uX5Yfyo79wfnz9nQ4t2361xtMayb6eu16cRlkFlzuGfb3kEbmYV4FrgQ8BJwCfM7KRWzydSFurbkooiJZRzgOfdfb277wduAi4bm2aJdJX6tiShSAllLvBS0+NNwLmHvsjMVgArsof7fuy3PlHgmq174NaRvGom0Nrmm/99ROcf3si2pBy+ffGWnsV9u+DPL74X2Vg5bozOk9u3D+3XlTnPdadfj0zr/bpzyt7GbrfvsH277dMI3X0lsBLAzNa4+5J2X7NVal8xZW/fWFK/Hltlb2NZ21ekhLIZmN/0eF72nEjq1LclCUUS+MPAYjM73swmAB8Hbh+bZol0lfq2JKHlEoq7V83sM8CdNKZafdPdn8w5bGWr1+sQta+YsrdvRFro22X/7y57+6D8bSxl+8y9M58uiYjI2NJKTBGRRCmBi4gkqiMJ3MwuNbNnzex5M7uyE9ccDTPbYGbrzOwxM1vT7fYAmNk3zWybmT3R9Nx0M7vLzJ7Lvh5dsvZ90cw2Zz/Hx8zsw91qX6eob4+6PerXY6jtCTyhZckXufvpJZrreT1w6SHPXQmsdvfFwOrscbdczzvbB3B19nM83d1/0OE2dZT6dkuuR/16zHRiBK5lyS1w93uB1w55+jJgVfb9KuDyjjaqyTDt6zXq26Okfj22OpHAD7cseW4HrjsaDvzIzNZmS6TLara7b8m+fwWY3c3GDOMzZvZ49la0a2+FO0R9e2yoX7dIH2I2/Ka7n0njrfCnzez93W5QHm/M/yzbHNC/B34DOB3YAvy37jZHSKxvq1+PTicSeOmXJbv75uzrNuD7NN4al9FWM5sDkH3d1uX2vI27b3X3mrvXga9T3p/jWFHfHhvq1y3qRAIv9bJkMzvSzKYc/B74AFDWO8vdDizLvl8G3NbFtrzDwV/CzO9S3p/jWFHfHhvq1y3qxN0IW1ly30mzge+bGTR+Hje4+w+72yQwsxuBC4GZZrYJ+AJwFXCLmS0HNgJLS9a+C83sdBpvgTcA/6Zb7esE9e3RU78eW1pKLyKSKH2IKSKSKCVwEZFEKYGLiCRKCVxEJFFK4CIiiVICFxFJlBK4iEii/j9xLX/S0EG2ZwAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(example_tokens)\n",
        "plt.title('Token IDs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens != 0)\n",
        "plt.title('Mask')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "DX8nNOhVLTIs"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 256\n",
        "units = 1024"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VpENMJ4L3rv"
      },
      "source": [
        "## The Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Di-H5ePYLwBw"
      },
      "outputs": [],
      "source": [
        "# Creating the Encoder RNN\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "\n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # The GRU RNN layer processes those vectors sequentially.\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   # Return the sequence and state\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "  def call(self, tokens, state=None):\n",
        "    # shape_checker = ShapeChecker()\n",
        "    # shape_checker(tokens, ('batch', 's'))\n",
        "\n",
        "    # 2. The embedding layer looks up the embedding for each token.\n",
        "    vectors = self.embedding(tokens)\n",
        "    #shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
        "\n",
        "    # 3. The GRU processes the embedding sequence.\n",
        "    #    output shape: (batch, s, enc_units)\n",
        "    #    state shape: (batch, enc_units)\n",
        "    output, state = self.gru(vectors, initial_state=state)\n",
        "    # shape_checker(output, ('batch', 's', 'enc_units'))\n",
        "    # shape_checker(state, ('batch', 'enc_units'))\n",
        "\n",
        "    # 4. Returns the new sequence and its state.\n",
        "    return output, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnxVZsuBRDaa",
        "outputId": "d42f7a3f-e188-4ff4-9d52-fe1988f1e2c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch, shape (batch): (64,)\n",
            "Input batch tokens, shape (batch, s): (64, 19)\n",
            "Encoder output, shape (batch, s, units): (64, 19, 1024)\n",
            "Encoder state, shape (batch, units): (64, 1024)\n"
          ]
        }
      ],
      "source": [
        "# Convert the input text to tokens.\n",
        "example_tokens = input_text_processor(example_input_batch)\n",
        "\n",
        "# Encode the input sequence.\n",
        "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)\n",
        "example_enc_output, example_enc_state = encoder(example_tokens)\n",
        "\n",
        "print(f'Input batch, shape (batch): {example_input_batch.shape}')\n",
        "print(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\n",
        "print(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\n",
        "print(f'Encoder state, shape (batch, units): {example_enc_state.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90MC-kwiRnri"
      },
      "source": [
        "## The attention head"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Stog9zP8Sf1B"
      },
      "source": [
        "This tutorial uses Bahdanau's additive attention [[3]](https://arxiv.org/pdf/1409.0473.pdf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "w5_qeRKnReHp"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    # For Eqn. (4), the  Bahdanau attention\n",
        "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "    self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "    # shape_checker = ShapeChecker()\n",
        "    # shape_checker(query, ('batch', 't', 'query_units'))\n",
        "    # shape_checker(value, ('batch', 's', 'value_units'))\n",
        "    # shape_checker(mask, ('batch', 's'))\n",
        "\n",
        "    # From Eqn. (4), `W1@ht`.\n",
        "    w1_query = self.W1(query)\n",
        "    #shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
        "\n",
        "    # From Eqn. (4), `W2@hs`.\n",
        "    w2_key = self.W2(value)\n",
        "    #shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask=[query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "    # shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
        "    # shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "    return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "OTrrd7F3TNkF"
      },
      "outputs": [],
      "source": [
        "attention_layer = BahdanauAttention(units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0vpLscGTZsY",
        "outputId": "26bcba35-b705-474c-a1b9-6e514b703a1d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 19])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "(example_tokens != 0).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BhUWPqYTmTh",
        "outputId": "97a54771-b439-4916-9b31-213c4924faa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention result shape: (batch_size, query_seq_length, units):           (64, 2, 1024)\n",
            "Attention weights shape: (batch_size, query_seq_length, value_seq_length): (64, 2, 19)\n"
          ]
        }
      ],
      "source": [
        "# Later, the decoder will generate this attention query\n",
        "example_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n",
        "\n",
        "# Attend to the encoded tokens\n",
        "context_vector, attention_weights = attention_layer(\n",
        "    query=example_attention_query,\n",
        "    value=example_enc_output,\n",
        "    mask=(example_tokens != 0))\n",
        "\n",
        "print(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\n",
        "print(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej4jZw5DT5CC"
      },
      "source": [
        "The attention weights should sum to 1.0 for each sequence.\n",
        "\n",
        "Here are the attention weights across the sequences at t=0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "Ur0kVlIdTopS",
        "outputId": "c1846938-9be6-4121-919e-f62228875877"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Mask')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(attention_weights[:, 0, :])\n",
        "plt.title('Attention weights')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens != 0)\n",
        "plt.title('Mask')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "_4z0vW4cUuMq"
      },
      "outputs": [],
      "source": [
        "attention_slice = attention_weights[0, 0].numpy()\n",
        "attention_slice = attention_slice[attention_slice != 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxOEbgxeT_5V",
        "outputId": "db3c0df2-e714-492a-87e7-34026b016b89"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 2, 19])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "attention_weights.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "QkqQ3TLfUN1Z",
        "outputId": "a77508f9-86bf-46ed-ade8-33d908650fd9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7ff59dbd4150>]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.suptitle('Attention weights for one sequence')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "a1 = plt.subplot(1, 2, 1)\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "# freeze the xlim\n",
        "plt.xlim(plt.xlim())\n",
        "plt.xlabel('Attention weights')\n",
        "\n",
        "a2 = plt.subplot(1, 2, 2)\n",
        "plt.bar(range(len(attention_slice)), attention_slice)\n",
        "plt.xlabel('Attention weights, zoomed')\n",
        "\n",
        "# zoom in\n",
        "top = max(a1.get_ylim())\n",
        "zoom = 0.85*top\n",
        "a2.set_ylim([0.90*top, top])\n",
        "a1.plot(a1.get_xlim(), [zoom, zoom], color='k')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbWnoOzEU3MT"
      },
      "source": [
        "## The Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "PbyHUAj6U7WQ"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.dec_units = dec_units\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    # For Step 1. The embedding layer converts token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # For Step 2. The RNN keeps track of what's been generated so far.\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # For step 3. The RNN output will be the query for the attention layer.\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    # For step 4. Eqn. (3): converting `ct` to `at`\n",
        "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
        "                                    use_bias=False)\n",
        "\n",
        "    # For step 5. This fully connected layer produces the logits for each\n",
        "    # output token.\n",
        "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "MSnSww0IVjGh"
      },
      "outputs": [],
      "source": [
        "class DecoderInput(typing.NamedTuple):\n",
        "  new_tokens: Any\n",
        "  enc_output: Any\n",
        "  mask: Any\n",
        "\n",
        "class DecoderOutput(typing.NamedTuple):\n",
        "  logits: Any\n",
        "  attention_weights: Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "kYI8FMFXZf-d"
      },
      "outputs": [],
      "source": [
        "def call(self,\n",
        "         inputs: DecoderInput,\n",
        "         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
        "  # shape_checker = ShapeChecker()\n",
        "  # shape_checker(inputs.new_tokens, ('batch', 't'))\n",
        "  # shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n",
        "  # shape_checker(inputs.mask, ('batch', 's'))\n",
        "\n",
        "  # if state is not None:\n",
        "  #   shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "  # Step 1. Lookup the embeddings\n",
        "  vectors = self.embedding(inputs.new_tokens)\n",
        "  #shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n",
        "\n",
        "  # Step 2. Process one step with the RNN\n",
        "  rnn_output, state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "  # shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n",
        "  # shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "  # Step 3. Use the RNN output as the query for the attention over the\n",
        "  # encoder output.\n",
        "  context_vector, attention_weights = self.attention(\n",
        "      query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
        "  # shape_checker(context_vector, ('batch', 't', 'dec_units'))\n",
        "  # shape_checker(attention_weights, ('batch', 't', 's'))\n",
        "\n",
        "  # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
        "  #     [ct; ht] shape: (batch t, value_units + query_units)\n",
        "  context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "\n",
        "  # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
        "  attention_vector = self.Wc(context_and_rnn_output)\n",
        "  #shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n",
        "\n",
        "  # Step 5. Generate logit predictions:\n",
        "  logits = self.fc(attention_vector)\n",
        "  #shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n",
        "\n",
        "  return DecoderOutput(logits, attention_weights), state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "QTXyNYisZoFO"
      },
      "outputs": [],
      "source": [
        "Decoder.call = call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "NLzLKIeSXO7z"
      },
      "outputs": [],
      "source": [
        "# VERIFY DECODER\n",
        "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "kDB6Q-eGYQDC"
      },
      "outputs": [],
      "source": [
        "# Convert the target sequence, and collect the \"[START]\" tokens\n",
        "example_output_tokens = output_text_processor(example_target_batch)\n",
        "\n",
        "start_index = output_text_processor.get_vocabulary().index('[START]')\n",
        "first_token = tf.constant([[start_index]] * example_output_tokens.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bn4GA2ewYdFf",
        "outputId": "626ee85f-dcf9-4f24-dbc8-cf950b4cac17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits shape: (batch_size, t, output_vocab_size) (64, 1, 5000)\n",
            "state shape: (batch_size, dec_units) (64, 1024)\n"
          ]
        }
      ],
      "source": [
        "# Run the decoder\n",
        "dec_result, dec_state = decoder(\n",
        "    inputs = DecoderInput(new_tokens=first_token,\n",
        "                          enc_output=example_enc_output,\n",
        "                          mask=(example_tokens != 0)),\n",
        "    state = example_enc_state\n",
        ")\n",
        "\n",
        "print(f'logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\n",
        "print(f'state shape: (batch_size, dec_units) {dec_state.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "jRRN-ruJbKz9"
      },
      "outputs": [],
      "source": [
        "# Sample a token according to the logits:\n",
        "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0kkz-B7bb8B",
        "outputId": "829bd807-d4eb-4448-8a34-94928ce2eed0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['echec'],\n",
              "       ['autour'],\n",
              "       ['detestent'],\n",
              "       ['italie'],\n",
              "       ['metal']], dtype='<U18')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "#Decode the token as the first word of the output:\n",
        "vocab = np.array(output_text_processor.get_vocabulary())\n",
        "first_word = vocab[sampled_token.numpy()]\n",
        "first_word[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VioOXI96b9fo"
      },
      "source": [
        "Now use the decoder to generate a second set of logits.\n",
        "\n",
        "\n",
        "1.   Pass the same enc_output and mask, these haven't changed.\n",
        "2.   Pass the sampled token as new_tokens\n",
        "3.   Pass the decoder_state the decoder returned last time, so the RNN continues with a memory of where it left off last time\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "8I4I5n_Eb-kW"
      },
      "outputs": [],
      "source": [
        "dec_result, dec_state = decoder(\n",
        "    DecoderInput(sampled_token,\n",
        "                 example_enc_output,\n",
        "                 mask=(example_tokens != 0)),\n",
        "    state=dec_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H6V04MecmOj",
        "outputId": "26afeb9e-72fe-4d66-a710-a3ea51db5269"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['parait'],\n",
              "       ['chemise'],\n",
              "       ['repandit'],\n",
              "       ['chanteuse'],\n",
              "       ['couru']], dtype='<U18')"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)\n",
        "first_word = vocab[sampled_token.numpy()]\n",
        "first_word[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaBjysj4dDEs"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "vcvKHLJOcw9V"
      },
      "outputs": [],
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "    # shape_checker = ShapeChecker()\n",
        "    # shape_checker(y_true, ('batch', 't'))\n",
        "    # shape_checker(y_pred, ('batch', 't', 'logits'))\n",
        "\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "    #shape_checker(loss, ('batch', 't'))\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    #shape_checker(mask, ('batch', 't'))\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "5fmgvJmRdQB0"
      },
      "outputs": [],
      "source": [
        "class TrainTranslator(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units,\n",
        "               input_text_processor,\n",
        "               output_text_processor,\n",
        "               use_tf_function=True):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder\n",
        "    encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "    decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "    self.use_tf_function = use_tf_function\n",
        "    #self.shape_checker = ShapeChecker()\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    #.shape_checker = ShapeChecker()\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_train_step(inputs)\n",
        "    else:\n",
        "      return self._train_step(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "E49BBN_VdvhJ"
      },
      "outputs": [],
      "source": [
        "# Implement preprocessing step to:\n",
        "# Receive a batch of input_text, target_text from the tf.data.Dataset.\n",
        "# Convert those raw text inputs to token-embeddings and masks.\n",
        "def _preprocess(self, input_text, target_text):\n",
        "  # self.shape_checker(input_text, ('batch',))\n",
        "  # self.shape_checker(target_text, ('batch',))\n",
        "\n",
        "  # Convert the text to token IDs\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  target_tokens = self.output_text_processor(target_text)\n",
        "  # self.shape_checker(input_tokens, ('batch', 's'))\n",
        "  # self.shape_checker(target_tokens, ('batch', 't'))\n",
        "\n",
        "  # Convert IDs to masks.\n",
        "  input_mask = input_tokens != 0\n",
        "  #self.shape_checker(input_mask, ('batch', 's'))\n",
        "\n",
        "  target_mask = target_tokens != 0\n",
        "  #self.shape_checker(target_mask, ('batch', 't'))\n",
        "\n",
        "  return input_tokens, input_mask, target_tokens, target_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "lGRmKo1cfXy0"
      },
      "outputs": [],
      "source": [
        "TrainTranslator._preprocess = _preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Jdd-CMzjfbaI"
      },
      "outputs": [],
      "source": [
        "# the function The _train_step:\n",
        "#Run the encoder on the input_tokens to get the encoder_output and encoder_state.\n",
        "#Initialize the decoder state and loss.\n",
        "#Loop over the target_tokens:\n",
        "#   Run the decoder one step at a time.\n",
        "#   Calculate the loss for each step.\n",
        "#Accumulate the average loss.\n",
        "#Calculate the gradient of the loss and use the optimizer to apply updates to the model's trainable_variables.\n",
        "def _train_step(self, inputs):\n",
        "  input_text, target_text = inputs\n",
        "\n",
        "  (input_tokens, input_mask,\n",
        "   target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
        "\n",
        "  max_target_length = tf.shape(target_tokens)[1]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Encode the input\n",
        "    enc_output, enc_state = self.encoder(input_tokens)\n",
        "    # self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
        "    # self.shape_checker(enc_state, ('batch', 'enc_units'))\n",
        "\n",
        "    # Initialize the decoder's state to the encoder's final state.\n",
        "    # This only works if the encoder and decoder have the same number of\n",
        "    # units.\n",
        "    dec_state = enc_state\n",
        "    loss = tf.constant(0.0)\n",
        "\n",
        "    for t in tf.range(max_target_length-1):\n",
        "      # Pass in two tokens from the target sequence:\n",
        "      # 1. The current input to the decoder.\n",
        "      # 2. The target for the decoder's next prediction.\n",
        "      new_tokens = target_tokens[:, t:t+2]\n",
        "      step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
        "                                             enc_output, dec_state)\n",
        "      loss = loss + step_loss\n",
        "\n",
        "    # Average the loss over all non padding tokens.\n",
        "    average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "  # Apply an optimization step\n",
        "  variables = self.trainable_variables\n",
        "  gradients = tape.gradient(average_loss, variables)\n",
        "  self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  # Return a dict mapping metric names to current value\n",
        "  return {'batch_loss': average_loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "3Qr69EKBgQ6h"
      },
      "outputs": [],
      "source": [
        "TrainTranslator._train_step = _train_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "tX6ZkCh7g7Fg"
      },
      "outputs": [],
      "source": [
        "#The _loop_step method, added below, executes the decoder and calculates the incremental loss and new decoder state (dec_state).\n",
        "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
        "  input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
        "\n",
        "  # Run the decoder one step.\n",
        "  decoder_input = DecoderInput(new_tokens=input_token,\n",
        "                               enc_output=enc_output,\n",
        "                               mask=input_mask)\n",
        "\n",
        "  dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
        "  # self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n",
        "  # self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n",
        "  # self.shape_checker(dec_state, ('batch', 'dec_units'))\n",
        "\n",
        "  # `self.loss` returns the total for non-padded tokens\n",
        "  y = target_token\n",
        "  y_pred = dec_result.logits\n",
        "  step_loss = self.loss(y, y_pred)\n",
        "\n",
        "  return step_loss, dec_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "sO34tTQahMkI"
      },
      "outputs": [],
      "source": [
        "TrainTranslator._loop_step = _loop_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "h79zTlAThWi_"
      },
      "outputs": [],
      "source": [
        "translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        "    use_tf_function=False)\n",
        "# Configure the loss and optimizer\n",
        "translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUlHXR2AhaJE",
        "outputId": "3b9b3d03-a773-4e40-8221-a1192ad1019a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8.517193191416238"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "np.log(output_text_processor.vocabulary_size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsDXQza2hqP7",
        "outputId": "3a081f3f-5578-4d7a-914a-32c6773f4350"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.702392>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.674491>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.6219454>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=7.4706807>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=6.919408>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.0313168>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=5.4071965>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.5035076>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.2916613>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.2763476>}\n",
            "\n",
            "CPU times: user 2min 9s, sys: 3.29 s, total: 2min 13s\n",
            "Wall time: 1min 18s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "for n in range(10):\n",
        "  print(translator.train_step([example_input_batch, example_target_batch]))\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3fDBmxOjHyY"
      },
      "source": [
        "While it's easier to debug without a tf.function it does give a performance boost. So now that the _train_step method is working, try the tf.function-wrapped _tf_train_step, to maximize performance while training:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "2ao9Ksdwih9h"
      },
      "outputs": [],
      "source": [
        "@tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
        "def _tf_train_step(self, inputs):\n",
        "  return self._train_step(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "0S6Z_ajojXx8"
      },
      "outputs": [],
      "source": [
        "TrainTranslator._tf_train_step = _tf_train_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "EpxCtiPFjNxA"
      },
      "outputs": [],
      "source": [
        "translator.use_tf_function = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMULdUPEkFiX",
        "outputId": "5e200e96-659e-43a0-c384-34d8a21c46b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.304758>}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "translator.train_step([example_input_batch, example_target_batch])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LI2tQMCXkKMx",
        "outputId": "5a58c1d1-02d2-497a-8ec6-4293f6bf2b4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.3337536>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.3174896>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.2627645>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.1902537>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.118381>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.055052>}\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "for n in range(10):\n",
        "  print(translator.train_step([example_input_batch, example_target_batch]))\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJwzm9kYkm8Q"
      },
      "source": [
        "A good test of a new model is to see that it can overfit a single batch of input. Try it, the loss should quickly go to zero:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXq0I7tkkhON"
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "for n in range(100):\n",
        "  print('.', end='')\n",
        "  logs = translator.train_step([example_input_batch, example_target_batch])\n",
        "  losses.append(logs['batch_loss'].numpy())\n",
        "\n",
        "print()\n",
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N66xbPpJkubg"
      },
      "outputs": [],
      "source": [
        "# build new copy of translator to train\n",
        "train_translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "train_translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fX71r-PlMv1"
      },
      "outputs": [],
      "source": [
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key):\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_end(self, n, logs):\n",
        "    self.logs.append(logs[self.key])\n",
        "\n",
        "batch_loss = BatchLogs('batch_loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hb40flt9mBX0"
      },
      "outputs": [],
      "source": [
        "train_translator.fit(dataset, epochs=100,\n",
        "                     callbacks=[batch_loss])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYpGUEL7mFz6"
      },
      "outputs": [],
      "source": [
        "plt.plot(batch_loss.logs)\n",
        "plt.ylim([0, 3])\n",
        "plt.xlabel('Batch #')\n",
        "plt.ylabel('CE/token')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fiiWVrHom7Lh"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.Module):\n",
        "\n",
        "  def __init__(self, encoder, decoder, input_text_processor,\n",
        "               output_text_processor):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "\n",
        "    self.output_token_string_from_index = (\n",
        "        tf.keras.layers.StringLookup(\n",
        "            vocabulary=output_text_processor.get_vocabulary(),\n",
        "            mask_token='',\n",
        "            invert=True))\n",
        "\n",
        "    # The output should never generate padding, unknown, or start.\n",
        "    index_from_string = tf.keras.layers.StringLookup(\n",
        "        vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
        "    token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
        "\n",
        "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    self.token_mask = token_mask\n",
        "\n",
        "    self.start_token = index_from_string(tf.constant('[START]'))\n",
        "    self.end_token = index_from_string(tf.constant('[END]'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMT9Y4j8nDx6"
      },
      "outputs": [],
      "source": [
        "translator = Translator(\n",
        "    encoder=train_translator.encoder,\n",
        "    decoder=train_translator.decoder,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mChNybeSnnJt"
      },
      "source": [
        "## Convert token IDs to tex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABlX1N1fnPOE"
      },
      "outputs": [],
      "source": [
        "def tokens_to_text(self, result_tokens):\n",
        "  # shape_checker = ShapeChecker()\n",
        "  # shape_checker(result_tokens, ('batch', 't'))\n",
        "  result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
        "  # shape_checker(result_text_tokens, ('batch', 't'))\n",
        "\n",
        "  result_text = tf.strings.reduce_join(result_text_tokens,\n",
        "                                       axis=1, separator=' ')\n",
        "  #shape_checker(result_text, ('batch'))\n",
        "\n",
        "  result_text = tf.strings.strip(result_text)\n",
        "  #shape_checker(result_text, ('batch',))\n",
        "  return result_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Twa3joiBn-oJ"
      },
      "outputs": [],
      "source": [
        "Translator.tokens_to_text = tokens_to_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fHuw9a8oEjH"
      },
      "outputs": [],
      "source": [
        "#Input some random token IDs and see what it generates:\n",
        "example_output_tokens = tf.random.uniform(\n",
        "    shape=[5, 2], minval=0, dtype=tf.int64,\n",
        "    maxval=output_text_processor.vocabulary_size())\n",
        "translator.tokens_to_text(example_output_tokens).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_X34mZJobL4"
      },
      "source": [
        "Sample from the decoder's predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nU8cEayOoUnw"
      },
      "outputs": [],
      "source": [
        "def sample(self, logits, temperature):\n",
        "  # shape_checker = ShapeChecker()\n",
        "  # # 't' is usually 1 here.\n",
        "  # shape_checker(logits, ('batch', 't', 'vocab'))\n",
        "  # shape_checker(self.token_mask, ('vocab',))\n",
        "\n",
        "  token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "  #shape_checker(token_mask, ('batch', 't', 'vocab'), broadcast=True)\n",
        "\n",
        "  # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
        "  logits = tf.where(self.token_mask, -np.inf, logits)\n",
        "\n",
        "  if temperature == 0.0:\n",
        "    new_tokens = tf.argmax(logits, axis=-1)\n",
        "  else:\n",
        "    logits = tf.squeeze(logits, axis=1)\n",
        "    new_tokens = tf.random.categorical(logits/temperature,\n",
        "                                        num_samples=1)\n",
        "\n",
        "  #shape_checker(new_tokens, ('batch', 't'))\n",
        "\n",
        "  return new_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChfSjfe-on3-"
      },
      "outputs": [],
      "source": [
        "Translator.sample = sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G26hc3mEotML"
      },
      "outputs": [],
      "source": [
        "# Test run this function on some random inputs:\n",
        "\n",
        "example_logits = tf.random.normal([5, 1, output_text_processor.vocabulary_size()])\n",
        "example_output_tokens = translator.sample(example_logits, temperature=1.0)\n",
        "example_output_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbA3djpto1Ck"
      },
      "outputs": [],
      "source": [
        "# Implement the translation loop\n",
        "def translate_unrolled(self,\n",
        "                       input_text, *,\n",
        "                       max_length=50,\n",
        "                       return_attention=True,\n",
        "                       temperature=1.0):\n",
        "  batch_size = tf.shape(input_text)[0]\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "  dec_state = enc_state\n",
        "  new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "\n",
        "  result_tokens = []\n",
        "  attention = []\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "\n",
        "  for _ in range(max_length):\n",
        "    dec_input = DecoderInput(new_tokens=new_tokens,\n",
        "                             enc_output=enc_output,\n",
        "                             mask=(input_tokens!=0))\n",
        "\n",
        "    dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
        "\n",
        "    attention.append(dec_result.attention_weights)\n",
        "\n",
        "    new_tokens = self.sample(dec_result.logits, temperature)\n",
        "\n",
        "    # If a sequence produces an `end_token`, set it `done`\n",
        "    done = done | (new_tokens == self.end_token)\n",
        "    # Once a sequence is done it only produces 0-padding.\n",
        "    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "\n",
        "    # Collect the generated tokens\n",
        "    result_tokens.append(new_tokens)\n",
        "\n",
        "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  # Convert the list of generates token ids to a list of strings.\n",
        "  result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "  result_text = self.tokens_to_text(result_tokens)\n",
        "\n",
        "  if return_attention:\n",
        "    attention_stack = tf.concat(attention, axis=1)\n",
        "    return {'text': result_text, 'attention': attention_stack}\n",
        "  else:\n",
        "    return {'text': result_text}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsuZeVSSpX-g"
      },
      "outputs": [],
      "source": [
        "Translator.translate = translate_unrolled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7Udh2-vpYqg"
      },
      "outputs": [],
      "source": [
        "# Run it on a simple input:\n",
        "%%time\n",
        "input_text = tf.constant([\n",
        "    'Yɛn agyanom Nyankopɔn .', # \"Dieu de nos pères.\"\n",
        "    'Eyi ne m’asetra .', # \"C'est ma vie.\"\"\n",
        "])\n",
        "\n",
        "result = translator.translate(\n",
        "    input_text = input_text)\n",
        "\n",
        "print(result['text'][0].numpy().decode())\n",
        "print(result['text'][1].numpy().decode())\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi2FCVlaqkV8"
      },
      "outputs": [],
      "source": [
        "@tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
        "def tf_translate(self, input_text):\n",
        "  return self.translate(input_text)\n",
        "\n",
        "Translator.tf_translate = tf_translate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqHmHDmBq_9I"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "result = translator.tf_translate(\n",
        "    input_text = input_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RvnlerYrOoF"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "result = translator.tf_translate(\n",
        "    input_text = input_text)\n",
        "\n",
        "print(result['text'][0].numpy().decode())\n",
        "print(result['text'][1].numpy().decode())\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19Ul18HRrsCv"
      },
      "outputs": [],
      "source": [
        "a = result['attention'][0]\n",
        "\n",
        "print(np.sum(a, axis=-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLOSGsqZrsx1"
      },
      "outputs": [],
      "source": [
        "_ = plt.bar(range(len(a[0, :])), a[0, :])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNkM4SIor2df"
      },
      "outputs": [],
      "source": [
        "plt.imshow(np.array(a), vmin=0.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCi_N-mcr9K0"
      },
      "outputs": [],
      "source": [
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  sentence = tf_start_and_end_tokens(sentence).numpy().decode().split()\n",
        "  predicted_sentence = predicted_sentence.numpy().decode().split() + ['[END]']\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "  attention = attention[:len(predicted_sentence), :len(sentence)]\n",
        "\n",
        "  ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  ax.set_xlabel('Input text')\n",
        "  ax.set_ylabel('Output text')\n",
        "  plt.suptitle('Attention weights')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0mHKLz0sGUO"
      },
      "outputs": [],
      "source": [
        "i=0\n",
        "plot_attention(result['attention'][i], input_text[i], result['text'][i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5xqO8PtsULH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9908_M7FsmH8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcTInn7IzuhWN8FCAD5PaX",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}