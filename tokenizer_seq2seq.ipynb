{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "https://github.com/gyasifred/msc-thesis/blob/main/tokenizer_seq2seq.ipynb",
      "authorship_tag": "ABX9TyOSjfeED3qmXpoTMaT2PJg5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/tokenizer_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install Tensorflow"
      ],
      "metadata": {
        "id": "8cxg6yRcGORb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_-r4nUYtGBiF"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U tensorflow-text-nightly==2.11.0.dev20220817\n",
        "!pip install -U -q tf-nightly==2.11.0.dev20220817"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import tensorflow as tf\n",
        "import tensorflow_text as tf_text\n",
        "import pickle"
      ],
      "metadata": {
        "id": "xs6iNy4aGr6O"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess data"
      ],
      "metadata": {
        "id": "1dLG-GYUHFOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code was adapted from https://github.com/GhanaNLP/kasa/blob/master/Kasa/Preprocessing.py\n",
        "# A subclass of the kasafranse for preprocessing data\n",
        "# import required library\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "class Preprocessing:\n",
        "    # dummy initialization method\n",
        "    def __init__(self):\n",
        "        # initialize with some default parameters here later\n",
        "        pass\n",
        "\n",
        "    # read in parallel twi - english dataset\n",
        "    def read_parallel_dataset(self, filepath_1, filepath_2, filepath_3=None):\n",
        "        if filepath_3 != None:\n",
        "            # read first language data\n",
        "            lang_1 = []\n",
        "            with open(filepath_1, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_1.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            # read second language data\n",
        "            lang_2 = []\n",
        "            with open(filepath_2, encoding='utf-8') as file:\n",
        "\n",
        "                # twi=file.read()\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_2.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "            # Read third Language data\n",
        "            lang_3 = []\n",
        "            with open(filepath_3, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_3.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            return lang_1, lang_2, lang_3\n",
        "            \n",
        "        else:\n",
        "            # read first language data\n",
        "            lang_1 = []\n",
        "            with open(filepath_1, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_1.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            # read second language data\n",
        "            lang_2 = []\n",
        "            with open(filepath_2, encoding='utf-8') as file:\n",
        "\n",
        "                # twi=file.read()\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_2.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            return lang_1, lang_2\n",
        "\n",
        "    # Define a helper function to remove string accents\n",
        "\n",
        "    def removeStringAccent(self, s):\n",
        "        return ''.join(\n",
        "            c for c in unicodedata.normalize('NFD', s)\n",
        "            if unicodedata.category(c) != 'Mn'\n",
        "        )\n",
        "\n",
        "    # normalize input twi sentence\n",
        "    def normalize_twi(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = s.lower()\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.ƆɔɛƐ!?’]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "\n",
        "    # normalize input french sentence\n",
        "    def normalize_FrEn(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = s.lower()\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "    \n",
        "    def writeTotxt(self,destination,data):\n",
        "        with open(destination, 'w') as f:\n",
        "            for line in data:\n",
        "                 f.write(f\"{line}\\n\")"
      ],
      "metadata": {
        "id": "D6CSnIU8Gu4O"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import preprocessing class \n",
        "# Create an instance of tft preprocessing class\n",
        "preprocessor = Preprocessing()\n",
        "\n",
        "# Read raw parallel dataset\n",
        "raw_data_twi, raw_data_fr, raw_eng_data = preprocessor.read_parallel_dataset(\n",
        "    filepath_1='/content/drive/MyDrive/verified_twi.txt',\n",
        "    filepath_2='/content/drive/MyDrive/verified_english_french.txt',\n",
        "    filepath_3='/content/drive/MyDrive/verified_english.txt'\n",
        ")\n",
        "\n",
        "# Normalize the raw data\n",
        "raw_data_fr = [preprocessor.normalize_FrEn(data) for data in raw_data_fr]\n",
        "raw_data_twi = [preprocessor.normalize_twi(data) for data in raw_data_twi]\n",
        "raw_data_eng = [preprocessor.normalize_FrEn(data) for data in raw_eng_data ]"
      ],
      "metadata": {
        "id": "uxy9nkAgIXEH"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write the preprocess dataset to a file\n",
        "preprocessor.writeTotxt('raw_data_twi.txt',raw_data_twi)\n",
        "preprocessor.writeTotxt('raw_data_fr.txt',raw_data_fr)\n",
        "preprocessor.writeTotxt('raw_data_eng.txt',raw_data_eng)"
      ],
      "metadata": {
        "id": "Ui-fJeLTL2Qh"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Tokenizer"
      ],
      "metadata": {
        "id": "__VF5hRuMHGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "build tf dataset"
      ],
      "metadata": {
        "id": "XUhfH9B0COBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read the raw datasets\n",
        "lines_dataset_fr = tf.data.TextLineDataset('/content/raw_data_fr.txt')\n",
        "lines_dataset_tw = tf.data.TextLineDataset('/content/raw_data_twi.txt')\n",
        "lines_dataset_eng= tf.data.TextLineDataset('/content/raw_data_eng.txt')"
      ],
      "metadata": {
        "id": "iipLHwNKMFvv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vecrorization"
      ],
      "metadata": {
        "id": "IEEndFz9MiZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add start and end tokens\n",
        "def tf_start_and_end_tokens(text):\n",
        "    # Split accented characters.\n",
        "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "    text = tf.strings.lower(text)\n",
        "    # Strip whitespace.\n",
        "    text = tf.strings.strip(text)\n",
        "\n",
        "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "    return text\n",
        "\n",
        "# set maximum vocaburary size\n",
        "max_vocab_size = 10000\n",
        "# Process twi as input\n",
        "twi_tokenizer = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_start_and_end_tokens,\n",
        "    max_tokens=max_vocab_size)\n",
        "twi_tokenizer.adapt(lines_dataset_tw)\n",
        "\n",
        "# Process french as output\n",
        "french_tokenizer = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_start_and_end_tokens,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "french_tokenizer.adapt(lines_dataset_fr)\n",
        "\n",
        "# Process french as output\n",
        "english_tokenizer = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_start_and_end_tokens,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "english_tokenizer.adapt(lines_dataset_eng)"
      ],
      "metadata": {
        "id": "0lJ6SK6CMhpX"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#verify tokenizer\n",
        "# Print few lines of our tokenizers vocabulary and length\n",
        "print(f'French Tokenizer:',french_tokenizer.get_vocabulary()[:10])\n",
        "print(f'French Tokenizer size:',len(french_tokenizer.get_vocabulary()))\n",
        "\n",
        "print()\n",
        "print(f'TWI Tokenizer:',twi_tokenizer.get_vocabulary()[-10:])\n",
        "print(f'TWI Tokenizer size:',len(twi_tokenizer.get_vocabulary()))\n",
        "print()\n",
        "print(f'English Tokenizer:',english_tokenizer.get_vocabulary()[-10:])\n",
        "print(f'English Tokenizer size:',len(english_tokenizer.get_vocabulary()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVTDyrQYM9G3",
        "outputId": "363e3da9-b4df-44f5-a596-3eb45c3b5c9b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "French Tokenizer: ['', '[UNK]', '[START]', '[END]', '.', 'a', 'de', 'je', 'est', 'il']\n",
            "French Tokenizer size: 9553\n",
            "\n",
            "TWI Tokenizer: ['abamu', 'abambu', 'abada', 'abaafo', 'aakwantuo', 'aa', '.r', '.meda', '.ma', '.abena']\n",
            "TWI Tokenizer size: 7551\n",
            "\n",
            "English Tokenizer: ['abstract', 'abstained', 'absorbs', 'absences', 'abiding', 'abhor', 'abducted', 'abbreviation', '.r', '.a']\n",
            "English Tokenizer size: 7479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test on simple sentenct\n",
        "twi_tokenizer(\"Dɔ n nti na abofra no suɔ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4nmp0XbN1QJ",
        "outputId": "98cbc4e4-3e0c-412a-b630-23c89468c1e8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(9,), dtype=int64, numpy=array([   2,  442,   35,   59,    8,  216,    5, 5293,    3])>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "twi_tokenizer.get_vocabulary()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV5E6asYOgyi",
        "outputId": "89b09410-d955-46fd-d0c9-3e1780dde529"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'no', 'sɛ', 'a', 'na', 'so']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Tokenizer"
      ],
      "metadata": {
        "id": "zrYYhUikNC9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#save Twi Tokenizer\n",
        "# Pickle the config,vocabs and weights\n",
        "pickle.dump({'config': twi_tokenizer.get_config(),\n",
        "             'vocabulary':twi_tokenizer.get_vocabulary(),\n",
        "             'weights': twi_tokenizer.get_weights()}\n",
        "            , open(\"/content/drive/MyDrive/twi_tokenizer.pkl\", \"wb\"))\n"
      ],
      "metadata": {
        "id": "wdRD0Di2PMS7"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save French Tokenizer\n",
        "#save Twi Tokenizer\n",
        "# Pickle the config,vocabs and weights\n",
        "pickle.dump({'config': french_tokenizer.get_config(),\n",
        "             'vocabulary':french_tokenizer.get_vocabulary(),\n",
        "             'weights': french_tokenizer.get_weights()}\n",
        "            , open(\"/content/drive/MyDrive/french_tokenizer.pkl\", \"wb\"))\n"
      ],
      "metadata": {
        "id": "p956NkZ_QsW_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save English Tokenizer\n",
        "#save Twi Tokenizer\n",
        "# Pickle the config,vocabs and weights\n",
        "pickle.dump({'config': english_tokenizer.get_config(),\n",
        "             'vocabulary':english_tokenizer.get_vocabulary(),\n",
        "             'weights': english_tokenizer.get_weights()}\n",
        "            , open(\"/content/drive/MyDrive/english_tokenizer.pkl\", \"wb\"))\n"
      ],
      "metadata": {
        "id": "lNYe7BDsFwye"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reload Tokenizers and test"
      ],
      "metadata": {
        "id": "HDlODP9SQ-PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loadtokenizer(filepath):\n",
        "    tmp = pickle.load(open(filepath, \"rb\"))\n",
        "    temp = tf.keras.layers.TextVectorization.from_config(tmp['config'])\n",
        "    # You have to call `adapt` with some dummy data (BUG in Keras)\n",
        "    temp.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
        "    temp.set_weights(tmp['weights'])\n",
        "    temp.set_vocabulary(tmp['vocabulary'])\n",
        "    return temp\n",
        "  "
      ],
      "metadata": {
        "id": "lzPBQmiPGyd0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reload french tokenizer and test\n",
        "french_tokenizer1 = loadtokenizer('/content/drive/MyDrive/french_tokenizer.pkl')\n",
        "# test on simple sentence\n",
        "french_tokenizer1(\"Vous devriez parler au professeur vous meme .\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggVdY6kKG7xy",
        "outputId": "e680c25e-23d7-4fd3-bfbf-e01a8ac06762"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([  2,  13, 408, 102,  45, 315,  13, 116,   4,   3])>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reload twi tokenizer and test\n",
        "twi_tokenizer1 = loadtokenizer('/content/drive/MyDrive/twi_tokenizer.pkl')\n",
        "# test on simple sentence\n",
        "twi_tokenizer1(\"Dɔ n nti na abofra no suɔ\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZhzP6xIrIPjc",
        "outputId": "518e96f2-5e4b-46fd-9c20-e46a995d9354"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(9,), dtype=int64, numpy=array([   2,  442,   35,   59,    8,  216,    5, 5293,    3])>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reload english tokenizer and test\n",
        "english_tokenizer1 = loadtokenizer('/content/drive/MyDrive/english_tokenizer.pkl')\n",
        "# test on simple sentence\n",
        "english_tokenizer1(\"The true meaning of life\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bS_6wZobIiIe",
        "outputId": "43218ed3-6f9a-4b57-99be-0e30483b6a14"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(7,), dtype=int64, numpy=array([   2,    6,  259, 1481,   18,  180,    3])>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    }
  ]
}