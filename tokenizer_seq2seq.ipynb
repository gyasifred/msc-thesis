{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMKJPUskl1hedL2RVqolqki",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/tokenizer_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Tensorflow"
      ],
      "metadata": {
        "id": "8cxg6yRcGORb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_-r4nUYtGBiF"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U tensorflow-text-nightly==2.11.0.dev20220817\n",
        "!pip install -U -q tf-nightly==2.11.0.dev20220817"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_text as tf_text\n",
        "import pickle"
      ],
      "metadata": {
        "id": "xs6iNy4aGr6O"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess data"
      ],
      "metadata": {
        "id": "1dLG-GYUHFOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code was adapted from https://github.com/GhanaNLP/kasa/blob/master/Kasa/Preprocessing.py\n",
        "# import required library\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "class Preprocessing:\n",
        "    # dummy initialization method\n",
        "    def __init__(self):\n",
        "        # initialize with some default parameters here later\n",
        "        pass\n",
        "\n",
        "    # read in parallel twi - french dataset\n",
        "    def read_parallel_dataset(self, filepath_1, filepath_2):\n",
        "\n",
        "        # read language 1\n",
        "        lang_1 = []\n",
        "        with open(filepath_1, encoding='utf-8') as file:\n",
        "            line = file.readline()\n",
        "            cnt = 1\n",
        "            while line:\n",
        "                lang_1.append(line.strip())\n",
        "                line = file.readline()\n",
        "                cnt += 1\n",
        "\n",
        "        # read language 2\n",
        "        lang_2 = []\n",
        "        with open(filepath_2, encoding='utf-8') as file:\n",
        "\n",
        "            # twi=file.read()\n",
        "            line = file.readline()\n",
        "            cnt = 1\n",
        "            while line:\n",
        "                lang_2.append(line.strip())\n",
        "                line = file.readline()\n",
        "                cnt += 1\n",
        "\n",
        "      \n",
        "\n",
        "        return lang_1, lang_2\n",
        "\n",
        "    # Define a helper function to remove string accents\n",
        "\n",
        "    def removeStringAccent(self, s):\n",
        "        return ''.join(\n",
        "            c for c in unicodedata.normalize('NFD', s)\n",
        "            if unicodedata.category(c) != 'Mn'\n",
        "        )\n",
        "\n",
        "    # normalize input twi sentence\n",
        "    def normalize_twi(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.ƆɔɛƐ!?’]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "\n",
        "    # normalize input french and English sentence\n",
        "    def normalize_fr(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s"
      ],
      "metadata": {
        "id": "D6CSnIU8Gu4O"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = Preprocessing()\n",
        "\n",
        "# Read raw parallel dataset\n",
        "raw_data_twi, raw_data_fr = preprocessor.read_parallel_dataset(\n",
        "    filepath_1='/content/verified_twi.txt',\n",
        "    filepath_2='/content/verified_french.txt')"
      ],
      "metadata": {
        "id": "uxy9nkAgIXEH"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the raw data\n",
        "raw_data_fr = [preprocessor.normalize_fr(data) for data in raw_data_fr]\n",
        "raw_data_twi = [preprocessor.normalize_twi(data) for data in raw_data_twi]"
      ],
      "metadata": {
        "id": "tHjWVzy-LT6B"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to write text to txt file\n",
        "def writeTotxt(destination,data):\n",
        "  with open(destination, 'w') as f:\n",
        "    for line in data:\n",
        "        f.write(f\"{line}\\n\")"
      ],
      "metadata": {
        "id": "s-cyvMtwLv7F"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write the preprocess french and twi lines to a file\n",
        "writeTotxt('twi_lines.txt',raw_data_twi)\n",
        "writeTotxt('fr_lines.txt',raw_data_fr)"
      ],
      "metadata": {
        "id": "Ui-fJeLTL2Qh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Tokenizer"
      ],
      "metadata": {
        "id": "__VF5hRuMHGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# buld tf dataset\n",
        "full_dataset_fr = tf.data.TextLineDataset('/content/fr_lines.txt')\n",
        "full_dataset_tw = tf.data.TextLineDataset('/content/twi_lines.txt')"
      ],
      "metadata": {
        "id": "iipLHwNKMFvv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vecrorization"
      ],
      "metadata": {
        "id": "IEEndFz9MiZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add start and end tokens\n",
        "def tf_start_and_end_tokens(text):\n",
        "    # Split accented characters.\n",
        "    #text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "    text = tf.strings.lower(text)\n",
        "    # Strip whitespace.\n",
        "    text = tf.strings.strip(text)\n",
        "\n",
        "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "    return text\n",
        "\n",
        "# set maximum vocaburary size\n",
        "max_vocab_size = 10000\n",
        "# Process twi as input\n",
        "twi_tokenizer = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_start_and_end_tokens,\n",
        "    max_tokens=max_vocab_size)\n",
        "twi_tokenizer.adapt(full_dataset_tw)\n",
        "\n",
        "# Process french as output\n",
        "french_tokenizer = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_start_and_end_tokens,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "french_tokenizer.adapt(full_dataset_fr)"
      ],
      "metadata": {
        "id": "0lJ6SK6CMhpX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#verify tokenizer\n",
        "# Print few lines of our tokenizers vocabulary and length\n",
        "print(f'French Tokenizer:',french_tokenizer.get_vocabulary()[:10])\n",
        "print(f'French Tokenizer size:',len(french_tokenizer.get_vocabulary()))\n",
        "\n",
        "print()\n",
        "print(f'TWI Tokenizer:',twi_tokenizer.get_vocabulary()[-10:])\n",
        "print(f'TWI Tokenizer size:',len(twi_tokenizer.get_vocabulary()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVTDyrQYM9G3",
        "outputId": "240103c2-f20c-4372-9bee-d9b6b023149b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "French Tokenizer: ['', '[UNK]', '[START]', '[END]', '.', 'a', 'de', 'je', 'est', 'il']\n",
            "French Tokenizer size: 9570\n",
            "\n",
            "TWI Tokenizer: ['abamu', 'abambu', 'abada', 'abaafo', 'aakwantuo', 'aa', '.r', '.meda', '.ma', '.abena']\n",
            "TWI Tokenizer size: 7750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test on simple sentenct\n",
        "twi_tokenizer(\"Dɔ n nti na abofra no suɔ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4nmp0XbN1QJ",
        "outputId": "8868f35a-836b-424d-e55b-e2511a6cc2e5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(9,), dtype=int64, numpy=array([   2,  440,   33,   60,    8,  215,    5, 5491,    3])>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "twi_tokenizer.get_vocabulary()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV5E6asYOgyi",
        "outputId": "eac312a5-eb1e-47a3-850a-6287616d250e"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'no', 'sɛ', 'a', 'na', 'so']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save Tokenizer"
      ],
      "metadata": {
        "id": "zrYYhUikNC9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#save Twi Tokenizer\n",
        "# Pickle the config,vocabs and weights\n",
        "pickle.dump({'config': twi_tokenizer.get_config(),\n",
        "             'vocabulary':twi_tokenizer.get_vocabulary(),\n",
        "             'weights': twi_tokenizer.get_weights()}\n",
        "            , open(\"twiwords_tokenizer.pkl\", \"wb\"))\n"
      ],
      "metadata": {
        "id": "wdRD0Di2PMS7"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save French Tokenizer\n",
        "#save Twi Tokenizer\n",
        "# Pickle the config,vocabs and weights\n",
        "pickle.dump({'config': french_tokenizer.get_config(),\n",
        "             'vocabulary':french_tokenizer.get_vocabulary(),\n",
        "             'weights': french_tokenizer.get_weights()}\n",
        "            , open(\"frenchwords_tokenizer.pkl\", \"wb\"))\n"
      ],
      "metadata": {
        "id": "p956NkZ_QsW_"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reload Tokenizers and test"
      ],
      "metadata": {
        "id": "HDlODP9SQ-PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#reload twi tokenizer\n",
        "tw_t= pickle.load(open(\"twiwords_tokenizer.pkl\", \"rb\"))\n",
        "new_twi_tokenizer = tf.keras.layers.TextVectorization.from_config(tw_t['config'])\n",
        "# You have to call `adapt` with some dummy data (BUG in Keras)\n",
        "new_twi_tokenizer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
        "new_twi_tokenizer.set_weights(tw_t['weights'])\n",
        "new_twi_tokenizer.set_vocabulary(tw_t['vocabulary'])\n"
      ],
      "metadata": {
        "id": "Py7VDKJwRQTx"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test new twi_tokenizer\n",
        "#test on simple sentenct\n",
        "new_twi_tokenizer(\"Dɔ n nti na abofra no suɔ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOLyyu-sTB6l",
        "outputId": "e2428312-a716-4b02-817c-68837dc57655"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(9,), dtype=int64, numpy=array([   2,  440,   33,   60,    8,  215,    5, 5491,    3])>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get vocabulary\n",
        "new_twi_tokenizer.get_vocabulary()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQB_1wAoTWCS",
        "outputId": "07ba96c2-7a48-4d3f-aec8-629daa468e52"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'no', 'sɛ', 'a', 'na', 'so']"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# reload French Tokenizer\n",
        "#reload twi tokenizer\n",
        "tw_t= pickle.load(open(\"frenchwords_tokenizer.pkl\", \"rb\"))\n",
        "new_french_tokenizer = tf.keras.layers.TextVectorization.from_config(tw_t['config'])\n",
        "# You have to call `adapt` with some dummy data (BUG in Keras)\n",
        "new_french_tokenizer.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
        "new_french_tokenizer.set_weights(tw_t['weights'])\n",
        "new_french_tokenizer.set_vocabulary(tw_t['vocabulary'])\n"
      ],
      "metadata": {
        "id": "n_C4HTevNHiN"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test new french_tokenizer\n",
        "#test on simple sentenct\n",
        "new_twi_tokenizer(\"Elle a acheté deux douzaines d'œufs.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVvnbUOaU-Et",
        "outputId": "447cfbdb-e1bf-4e42-c273-a12391d3a10f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(8,), dtype=int64, numpy=array([2, 1, 7, 1, 1, 1, 1, 3])>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get vocabulary\n",
        "new_french_tokenizer.get_vocabulary()[-10:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uV4XXixVg3c",
        "outputId": "1b4791d0-4814-4115-fe2c-3c26354880ac"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['abattre',\n",
              " 'abasourdis',\n",
              " 'abasourdi',\n",
              " 'abandonnerait',\n",
              " 'abandonnerais',\n",
              " 'abandonnee',\n",
              " 'abandonna',\n",
              " 'abaisser',\n",
              " '.r',\n",
              " '.a']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    }
  ]
}