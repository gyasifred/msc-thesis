{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/french_twi_seq2seq_nmt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygqAsWIXdaHh"
      },
      "source": [
        "This exercise will demonstrate how to build sequence to sequence models with attention for French-Twi machine translation. This code is based on the tensorflow implementation of the paper [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025v5) (Luong et al., 2015).The code snippet are adapted from from [[1]](https://www.tensorflow.org/text/tutorials/nmt_with_attention)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq-OJN6ybfEP"
      },
      "source": [
        "# Import Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPsStl-KdJmv",
        "outputId": "e868782d-0614-4809-9477-1dee0de1d339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-text==2.8.*\n",
            "  Downloading tensorflow_text-2.8.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 12.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.8.*) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text==2.8.*) (2.8.2+zzzcolab20220719082949)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.14.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.2.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.15.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.26.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.21.6)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (14.0.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.1.1)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.0.7)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.5.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (57.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.3.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.48.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text==2.8.*) (3.2.0)\n",
            "Installing collected packages: tensorflow-text\n",
            "Successfully installed tensorflow-text-2.8.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2022.6.15)\n",
            "Collecting hstspreload\n",
            "  Downloading hstspreload-2022.9.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 27.3 MB/s \n",
            "\u001b[?25hCollecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16367 sha256=be730b429899ddcfc51a8b80f156afe15fa23a8c10ed00bf0424c720a232870f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/be/fe/93a6a40ffe386e16089e44dad9018ebab9dc4cb9eb7eab65ae\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hyperframe, hpack, sniffio, h2, h11, rfc3986, httpcore, hstspreload, httpx, googletrans\n",
            "Successfully installed googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2022.9.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install \"tensorflow-text==2.8.*\"\n",
        "!pip3 install googletrans==3.1.0a0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YET1BasJdKr9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-k6W5V1Jg44R"
      },
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Nh5Z6XSTfREU"
      },
      "outputs": [],
      "source": [
        "# This code was adapted from https://github.com/GhanaNLP/kasa/blob/master/Kasa/Preprocessing.py\n",
        "# A subclass of the kasafrench for preprocessing data\n",
        "# import required library\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "class Preprocessing:\n",
        "    # dummy initialization method\n",
        "    def __init__(self):\n",
        "        # initialize with some default parameters here later\n",
        "        pass\n",
        "\n",
        "    # read in parallel twi - english dataset\n",
        "    def read_parallel_dataset(self, filepath_1, filepath_2, filepath_3=None):\n",
        "        if filepath_3 != None:\n",
        "            # read first language data\n",
        "            lang_1 = []\n",
        "            with open(filepath_1, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_1.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            # read second language data\n",
        "            lang_2 = []\n",
        "            with open(filepath_2, encoding='utf-8') as file:\n",
        "\n",
        "                # twi=file.read()\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_2.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "            # Read third Language data\n",
        "            lang_3 = []\n",
        "            with open(filepath_3, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_3.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            return lang_1, lang_2, lang_3\n",
        "            \n",
        "        else:\n",
        "            # read first language data\n",
        "            lang_1 = []\n",
        "            with open(filepath_1, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_1.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            # read second language data\n",
        "            lang_2 = []\n",
        "            with open(filepath_2, encoding='utf-8') as file:\n",
        "\n",
        "                # twi=file.read()\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_2.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            return lang_1, lang_2\n",
        "\n",
        "    # Define a helper function to remove string accents\n",
        "\n",
        "    def removeStringAccent(self, s):\n",
        "        return ''.join(\n",
        "            c for c in unicodedata.normalize('NFD', s)\n",
        "            if unicodedata.category(c) != 'Mn'\n",
        "        )\n",
        "\n",
        "    # normalize input twi sentence\n",
        "    def normalize_twi(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = s.lower()\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.ƆɔɛƐ!?’]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "\n",
        "    # normalize input french sentence\n",
        "    def normalize_FrEn(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = s.lower()\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "    \n",
        "    def writeTotxt(self,destination,data):\n",
        "        with open(destination, 'w') as f:\n",
        "            for line in data:\n",
        "                 f.write(f\"{line}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XL_-wwIOhEjY"
      },
      "outputs": [],
      "source": [
        "# import preprocessing class \n",
        "# Create an instance of tft preprocessing class\n",
        "preprocessor = Preprocessing()\n",
        "\n",
        "# Read raw parallel dataset\n",
        "raw_data_twi, raw_data_fr = preprocessor.read_parallel_dataset(\n",
        "    filepath_1='/content/drive/MyDrive/verified_twi.txt',\n",
        "    filepath_2='/content/drive/MyDrive/verified_french.txt'\n",
        ")\n",
        "\n",
        "# Normalize the raw data\n",
        "raw_data_fr = [preprocessor.normalize_FrEn(data) for data in raw_data_fr]\n",
        "raw_data_twi = [preprocessor.normalize_twi(data) for data in raw_data_twi]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Tokenizer"
      ],
      "metadata": {
        "id": "Wk43j3wxrqRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define functions to help reload tokenizer.\n",
        "# add start and end tokens\n",
        "#This functions was use in building the tokenizer\n",
        "#Need to use custmise class to save tokenizer instead\n",
        "\n",
        "def tf_start_and_end_tokens(text):\n",
        "    # Split accented characters.\n",
        "    text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "    text = tf.strings.lower(text)\n",
        "    # Strip whitespace.\n",
        "    text = tf.strings.strip(text)\n",
        "\n",
        "    text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "    return text\n",
        "    \n",
        "# function to load tokenizer\n",
        "def loadtokenizer(filepath):\n",
        "    tmp = pickle.load(open(filepath, \"rb\"))\n",
        "    temp = tf.keras.layers.TextVectorization.from_config(tmp['config'])\n",
        "    # You have to call `adapt` with some dummy data (BUG in Keras)\n",
        "    temp.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
        "    temp.set_weights(tmp['weights'])\n",
        "    temp.set_vocabulary(tmp['vocabulary'])\n",
        "    return temp\n"
      ],
      "metadata": {
        "id": "IgcUbiSASeqg"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load Twi tokenizer\n",
        "twi_tokenizer = loadtokenizer(\"/content/drive/MyDrive/twi_tokenizer .pkl\")\n",
        "#load french tokenizer\n",
        "french_tokenizer = loadtokenizer(\"/content/drive/MyDrive/french_tokenizer .pkl\")\n"
      ],
      "metadata": {
        "id": "MFF13DWys_2I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify tokenizers\n",
        "# Print few lines of our tokenizers vocabulary and length\n",
        "print(f'French Tokenizer:',french_tokenizer.get_vocabulary()[:10])\n",
        "print(f'French Tokenizer size:',len(french_tokenizer.get_vocabulary()))\n",
        "\n",
        "print()\n",
        "print(f'TWI Tokenizer:',twi_tokenizer.get_vocabulary()[-10:])\n",
        "print(f'TWI Tokenizer size:',len(twi_tokenizer.get_vocabulary()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vQkzK52TG_d",
        "outputId": "fc2dcab0-c40d-4e97-d1da-041c88c199e0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "French Tokenizer: ['', '[UNK]', '[START]', '[END]', '.', 'a', 'de', 'je', 'est', 'il']\n",
            "French Tokenizer size: 9553\n",
            "\n",
            "TWI Tokenizer: ['abamu', 'abambu', 'abada', 'abaafo', 'aakwantuo', 'aa', '.r', '.meda', '.ma', '.abena']\n",
            "TWI Tokenizer size: 7551\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G80o2o2FjZYD"
      },
      "source": [
        "# Create Train and Test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qIAtMONkhNcF"
      },
      "outputs": [],
      "source": [
        "# split the dataset into training and test sets\n",
        "# Keep 10% of the data as test set\n",
        "train_twi,test_twi,train_fr,test_fr = train_test_split(raw_data_twi,raw_data_fr, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2aFkzc8sj5vO"
      },
      "outputs": [],
      "source": [
        "# write the preprocess traning and test dataset to a file\n",
        "preprocessor.writeTotxt('train_twi.txt',train_twi)\n",
        "preprocessor.writeTotxt('train_fr.txt',train_fr)\n",
        "preprocessor.writeTotxt('test_twi.txt',test_twi)\n",
        "preprocessor.writeTotxt('test_fr.txt',test_fr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8H8yu3ykCy9"
      },
      "source": [
        "## Build tf Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fqIWNiixj93A"
      },
      "outputs": [],
      "source": [
        "# build tf datasets from traning and test sentences in both languages\n",
        "train_dataset_fr = tf.data.TextLineDataset('/content/train_fr.txt')\n",
        "train_dataset_tw = tf.data.TextLineDataset('/content/train_twi.txt')\n",
        "\n",
        "val_dataset_fr = tf.data.TextLineDataset('/content/test_fr.txt')\n",
        "val_dataset_tw = tf.data.TextLineDataset('/content/test_twi.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jnQ1hMH_kPm1"
      },
      "outputs": [],
      "source": [
        "# combine languages into single dataset\n",
        "trained_combined = tf.data.Dataset.zip((train_dataset_fr,train_dataset_tw))\n",
        "val_combined = tf.data.Dataset.zip((val_dataset_fr,train_dataset_tw))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkpIb3EXkU_O",
        "outputId": "4e6cf18c-8271-4be5-ce77-149d662367ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "French:  les premiers colons americains sont arrives au eme siecle .\n",
            "Twi:  amerika atubrafo a wodi kan no duu hɔ wɔ afeha a ɛto so no mu .\n",
            "French:  vous devez vous laver les mains .\n",
            "Twi:  ɛsɛ sɛ wohohoro wo nsa .\n",
            "French:  appiah horrow est le jour de paie .\n",
            "Twi:  da a wɔde tua ka no yɛ da koro akatua .\n",
            "French:  je comprends pourquoi tu aimes asamoah .\n",
            "Twi:  mitumi hu nea enti a w ani gye asamoa ho\n",
            "French:  il avait un jean .\n",
            "Twi:  ɔhyɛ jeans attade\n"
          ]
        }
      ],
      "source": [
        "# verify trained_combined dataset is correct\n",
        "for fr,tw in trained_combined.take(5):\n",
        "  print(\"French: \", fr.numpy().decode('utf-8'))\n",
        "  print(\"Twi: \", tw.numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pr47kMLGlGan"
      },
      "source": [
        "## Create Training Batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ARuJ-j3dkaMV"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(train_fr) \n",
        "BATCH_SIZE = 64\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "  ds\n",
        "  .cache()\n",
        "  .shuffle(BUFFER_SIZE)\n",
        "  .batch(BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "G4E48b8fnR_Q"
      },
      "outputs": [],
      "source": [
        "# train batches\n",
        "trained_dataset = make_batches(trained_combined)\n",
        "# test batches\n",
        "val_dataset = make_batches(val_combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnmDbsDDnW1G",
        "outputId": "e819dbe3-e8c0-4c1d-d426-511a67429d3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'vous semblez aimer les fruits .'\n",
            " b'elle cuisait du pain et des gateaux au four .'\n",
            " b'nous devons essayer de proteger l environnement .'\n",
            " b'donnons un gros coup de main a asamoah .'\n",
            " b'je travaille dans une ville pres de rome .'], shape=(5,), dtype=string)\n",
            "\n",
            "tf.Tensor(\n",
            "[b'\\xc9\\x9bte s\\xc9\\x9b nea w ani gye aduaba ho .'\n",
            " b'\\xc9\\x94too paanoo ne keeki w\\xc9\\x94 fononoo no mu .'\n",
            " b'\\xc9\\x9bs\\xc9\\x9b s\\xc9\\x9b y\\xc9\\x9bb\\xc9\\x94 mm\\xc9\\x94den b\\xc9\\x94 nne\\xc9\\x9bma a atwa y\\xc9\\x9bn ho ahyia ho ban .'\n",
            " b'ma y\\xc9\\x9bmfa nsamb k\\xc9\\x9bse bi mmr\\xc9\\x9b wo .'\n",
            " b'mey\\xc9\\x9b adwuma w\\xc9\\x94 kurow bi a \\xc9\\x9bb\\xc9\\x9bn roma mu .'], shape=(5,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "# verify input and target\n",
        "for  example_input_batch,example_target_batch in trained_dataset.take(1):\n",
        "  print(example_input_batch[:5])\n",
        "  print()\n",
        "  print(example_target_batch[:5])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whFNrwDByRkO",
        "outputId": "7bb384b2-a579-4b0f-e382-fa6a3954c753"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 10), dtype=int64, numpy=\n",
              "array([[   2,   13, 1470, 1446,   25,  976,    4,    3,    0,    0],\n",
              "       [   2,   30, 8558,   37,  865,   40,   39, 2832,   45, 2439],\n",
              "       [   2,   28,  354,  591,    6, 1373,   19, 3503,    4,    3]])>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Verify tokenizer\n",
        "example_tokens = french_tokenizer(example_input_batch)\n",
        "example_tokens[:3, :10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CoaRtkchybGn",
        "outputId": "9791297b-a55d-4592-fba8-f2bb35d5a703"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[START] vous semblez aimer les fruits . [END]              '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "input_vocab = np.array(french_tokenizer.get_vocabulary())\n",
        "tokens = input_vocab[example_tokens[0].numpy()]\n",
        "' '.join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JTiUeXsy3bA"
      },
      "source": [
        "The returned token IDs are zero-padded. This can easily be turned into a mask:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "62XxlkwayyeI",
        "outputId": "ccc8ad8b-bea1-4583-af6d-f2336e4f9df7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Mask')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(example_tokens)\n",
        "plt.title('Token IDs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens != 0)\n",
        "plt.title('Mask')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOnz10TWzDeK"
      },
      "source": [
        "# Encoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "iLWhTwqK6Nhj"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_units = enc_units\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "\n",
        "        # The embedding layer converts tokens to vectors\n",
        "        self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
        "                                                   embedding_dim)\n",
        "\n",
        "        # The GRU RNN layer processes those vectors sequentially.\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                         # Return the sequence and state\n",
        "                                         return_sequences=True,\n",
        "                                         return_state=True,\n",
        "                                         recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, tokens, state=None):\n",
        "        # The embedding layer looks up the embedding for each token.\n",
        "        vectors = self.embedding(tokens)\n",
        "\n",
        "        # The GRU processes the embedding sequence.\n",
        "        #    output shape: (batch, s, enc_units)\n",
        "        #    state shape: (batch, enc_units)\n",
        "        output, state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "        # 4. Returns the new sequence and its state.\n",
        "        return output, state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0AcFzlXhDjBn"
      },
      "source": [
        "# Create Attention Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "G55yaW0U-w_X"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "        self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "    def call(self, query, value, mask):\n",
        "        w1_query = self.W1(query)\n",
        "        w2_key = self.W2(value)\n",
        "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "        value_mask = mask\n",
        "\n",
        "        context_vector, attention_weights = self.attention(\n",
        "            inputs=[w1_query, value, w2_key],\n",
        "            mask=[query_mask, value_mask],\n",
        "            return_attention_scores=True,\n",
        "        )\n",
        "        return context_vector, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afqcLYScH3ab"
      },
      "source": [
        "# Decoder layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "MTQH_4OsEgAa"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dec_units = dec_units\n",
        "        self.output_vocab_size = output_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # For Step 1. The embedding layer converts token IDs to vectors\n",
        "        self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
        "                                                   embedding_dim)\n",
        "\n",
        "        # For Step 2. The RNN keeps track of what's been generated so far.\n",
        "        self.gru= tf.keras.layers.GRU(self.dec_units,\n",
        "                                         return_sequences=True,\n",
        "                                         return_state=True,\n",
        "                                         recurrent_initializer='glorot_uniform')\n",
        "\n",
        "        # For step 3. The RNN output will be the query for the attention layer.\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "        # For step 4. Eqn. (3): converting `ct` to `at`\n",
        "        self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
        "                                        use_bias=False)\n",
        "\n",
        "        # For step 5. This fully connected layer produces the logits for each\n",
        "        # output token.\n",
        "        self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
        "\n",
        "\n",
        "class DecoderInput(typing.NamedTuple):\n",
        "    new_tokens: Any\n",
        "    enc_output: Any\n",
        "    mask: Any\n",
        "\n",
        "\n",
        "class DecoderOutput(typing.NamedTuple):\n",
        "    logits: Any\n",
        "    attention_weights: Any\n",
        "\n",
        "\n",
        "def call(self,\n",
        "         inputs: DecoderInput,\n",
        "         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
        "\n",
        "    # Step 1. Lookup the embeddings\n",
        "    vectors = self.embedding(inputs.new_tokens)\n",
        "    # Step 2. Process one step with the RNN\n",
        "    rnn_output,state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "    # Step 3. Use the RNN output as the query for the attention over the\n",
        "    # encoder output.\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
        "    # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
        "    #     [ct; ht] shape: (batch t, value_units + query_units)\n",
        "    context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "\n",
        "    # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
        "    attention_vector = self.Wc(context_and_rnn_output)\n",
        "    # Step 5. Generate logit predictions:\n",
        "    logits = self.fc(attention_vector)\n",
        "    return DecoderOutput(logits, attention_weights), state\n",
        "    \n",
        "\n",
        "Decoder.call = call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wl0R2Zg6SkRI"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "pgyqEVm2Ku8T"
      },
      "outputs": [],
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self):\n",
        "        self.name = 'masked_loss'\n",
        "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=True, reduction='none')\n",
        "\n",
        "    def __call__(self, y_true, y_pred):\n",
        "\n",
        "        # Calculate the loss for each item in the batch.\n",
        "        loss = self.loss(y_true, y_pred)\n",
        "\n",
        "        # Mask off the losses on padding.\n",
        "        mask = tf.cast(y_true != 0, tf.float32)\n",
        "        loss *= mask\n",
        "\n",
        "        # Return the total.\n",
        "        return tf.reduce_sum(loss)\n",
        "\n",
        "\n",
        "class TrainTranslator(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units,\n",
        "                 input_text_processor,\n",
        "                 output_text_processor,\n",
        "                 use_tf_function=True):\n",
        "        super().__init__()\n",
        "        # Build the encoder and decoder\n",
        "        encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                          embedding_dim, units)\n",
        "        decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                          embedding_dim, units)\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.input_text_processor = input_text_processor\n",
        "        self.output_text_processor = output_text_processor\n",
        "        self.use_tf_function = use_tf_function\n",
        "\n",
        "    def train_step(self, inputs):\n",
        "        # .shape_checker = ShapeChecker()\n",
        "        if self.use_tf_function:\n",
        "            return self._tf_train_step(inputs)\n",
        "        else:\n",
        "            return self._train_step(inputs)\n",
        "\n",
        "    # Implement preprocessing step to:\n",
        "    # Receive a batch of input_text, target_text from the tf.data.Dataset.\n",
        "    # Convert those raw text inputs to token-embeddings and masks.\n",
        "    def _preprocess(self, input_text, target_text):\n",
        "        # Convert the text to token IDs\n",
        "        input_tokens = self.input_text_processor(input_text)\n",
        "        target_tokens = self.output_text_processor(target_text)\n",
        "        # Convert IDs to masks.\n",
        "        input_mask = input_tokens != 0\n",
        "\n",
        "        target_mask = target_tokens != 0\n",
        "        return input_tokens, input_mask, target_tokens, target_mask\n",
        "\n",
        "    # the function The _train_step:\n",
        "    # Run the encoder on the input_tokens to get the encoder_output and encoder_state.\n",
        "    # Initialize the decoder state and loss.\n",
        "    # Loop over the target_tokens:\n",
        "    #   Run the decoder one step at a time.\n",
        "    #   Calculate the loss for each step.\n",
        "    # Accumulate the average loss.\n",
        "    # Calculate the gradient of the loss and use the optimizer to apply updates to the model's trainable_variables.\n",
        "\n",
        "    def _train_step(self, inputs):\n",
        "        input_text, target_text = inputs\n",
        "\n",
        "        (input_tokens, input_mask,\n",
        "         target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
        "\n",
        "        max_target_length = tf.shape(target_tokens)[1]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # Encode the input\n",
        "            enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "            # Initialize the decoder's state to the encoder's final state.\n",
        "            # This only works if the encoder and decoder have the same number of\n",
        "            # units.\n",
        "            dec_state = enc_state\n",
        "            loss = tf.constant(0.0)\n",
        "\n",
        "            for t in tf.range(max_target_length-1):\n",
        "                # Pass in two tokens from the target sequence:\n",
        "                # 1. The current input to the decoder.\n",
        "                # 2. The target for the decoder's next prediction.\n",
        "                new_tokens = target_tokens[:, t:t+2]\n",
        "                step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
        "                                                       enc_output, dec_state)\n",
        "                loss = loss + step_loss\n",
        "\n",
        "            # Average the loss over all non padding tokens.\n",
        "            average_loss = loss / \\\n",
        "                tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "        # Apply an optimization step\n",
        "        variables = self.trainable_variables\n",
        "        gradients = tape.gradient(average_loss, variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "        # Return a dict mapping metric names to current value\n",
        "        return {'batch_loss': average_loss}\n",
        "\n",
        "    # The _loop_step method, added below, executes the decoder and calculates the incremental loss and new decoder state (dec_state).\n",
        "\n",
        "    def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
        "        input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
        "\n",
        "        # Run the decoder one step.\n",
        "        decoder_input = DecoderInput(new_tokens=input_token,\n",
        "                                     enc_output=enc_output,\n",
        "                                     mask=input_mask)\n",
        "\n",
        "        dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
        "\n",
        "        # `self.loss` returns the total for non-padded tokens\n",
        "        y = target_token\n",
        "        y_pred = dec_result.logits\n",
        "        step_loss = self.loss(y, y_pred)\n",
        "\n",
        "        return step_loss, dec_state\n",
        "\n",
        "    @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
        "    def _tf_train_step(self, inputs):\n",
        "        return self._train_step(inputs)\n",
        "\n",
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key):\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_end(self, n, logs):\n",
        "    self.logs.append(logs[self.key])\n",
        "\n",
        "batch_loss = BatchLogs('batch_loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "BI0V2_YNSpO2"
      },
      "outputs": [],
      "source": [
        "# SET INPUT AND OUTPUT PROCESSOR\n",
        "input_text_processor = french_tokenizer\n",
        "output_text_processor = twi_tokenizer\n",
        "\n",
        "# set Hyerperameters\n",
        "embedding_dim = 256\n",
        "units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "31yGNhKaaSU8"
      },
      "outputs": [],
      "source": [
        "train_translator = TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        "    use_tf_function=False)\n",
        "# Configure the loss and optimizer\n",
        "train_translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")\n",
        "train_translator.use_tf_function = True\n",
        "\n",
        "def loadtokenizer(filepath):\n",
        "    tmp = pickle.load(open(filepath, \"rb\"))\n",
        "    temp = tf.keras.layers.TextVectorization.from_config(tmp['config'])\n",
        "    # You have to call `adapt` with some dummy data (BUG in Keras)\n",
        "    temp.adapt(tf.data.Dataset.from_tensor_slices([\"xyz\"]))\n",
        "    temp.set_weights(tmp['weights'])\n",
        "    temp.set_vocabulary(tmp['vocabulary'])\n",
        "    return temp\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRCC98HFb7zG",
        "outputId": "14d5457e-5d1f-4a39-c5f4-c16781067cdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "358/358 [==============================] - 134s 325ms/step - batch_loss: 4.2692\n",
            "Epoch 2/100\n",
            "358/358 [==============================] - 110s 308ms/step - batch_loss: 3.1297\n",
            "Epoch 3/100\n",
            "358/358 [==============================] - 109s 305ms/step - batch_loss: 2.3496\n",
            "Epoch 4/100\n",
            "358/358 [==============================] - 111s 311ms/step - batch_loss: 1.7624\n",
            "Epoch 5/100\n",
            "358/358 [==============================] - 111s 310ms/step - batch_loss: 1.3089\n",
            "Epoch 6/100\n",
            "358/358 [==============================] - 110s 306ms/step - batch_loss: 0.9624\n",
            "Epoch 7/100\n",
            "358/358 [==============================] - 109s 306ms/step - batch_loss: 0.6924\n",
            "Epoch 8/100\n",
            "358/358 [==============================] - 109s 306ms/step - batch_loss: 0.4870\n",
            "Epoch 9/100\n",
            "358/358 [==============================] - 110s 307ms/step - batch_loss: 0.3352\n",
            "Epoch 10/100\n",
            "358/358 [==============================] - 110s 306ms/step - batch_loss: 0.2342\n",
            "Epoch 11/100\n",
            "358/358 [==============================] - 107s 300ms/step - batch_loss: 0.1682\n",
            "Epoch 12/100\n",
            "358/358 [==============================] - 109s 304ms/step - batch_loss: 0.1251\n",
            "Epoch 13/100\n",
            "358/358 [==============================] - 110s 307ms/step - batch_loss: 0.1023\n",
            "Epoch 14/100\n",
            "358/358 [==============================] - 110s 308ms/step - batch_loss: 0.0920\n",
            "Epoch 15/100\n",
            "358/358 [==============================] - 110s 306ms/step - batch_loss: 0.1012\n",
            "Epoch 16/100\n",
            "358/358 [==============================] - 106s 296ms/step - batch_loss: 0.1039\n",
            "Epoch 17/100\n",
            "358/358 [==============================] - 108s 302ms/step - batch_loss: 0.1006\n",
            "Epoch 18/100\n",
            "358/358 [==============================] - 107s 299ms/step - batch_loss: 0.0897\n",
            "Epoch 19/100\n",
            "358/358 [==============================] - 107s 298ms/step - batch_loss: 0.0763\n",
            "Epoch 20/100\n",
            "358/358 [==============================] - 105s 294ms/step - batch_loss: 0.0676\n",
            "Epoch 21/100\n",
            "358/358 [==============================] - 106s 297ms/step - batch_loss: 0.0613\n",
            "Epoch 22/100\n",
            "358/358 [==============================] - 106s 295ms/step - batch_loss: 0.0623\n",
            "Epoch 23/100\n",
            "358/358 [==============================] - 108s 301ms/step - batch_loss: 0.0655\n",
            "Epoch 24/100\n",
            "358/358 [==============================] - 107s 300ms/step - batch_loss: 0.0781\n",
            "Epoch 25/100\n",
            "358/358 [==============================] - 107s 298ms/step - batch_loss: 0.0805\n",
            "Epoch 26/100\n",
            "358/358 [==============================] - 108s 301ms/step - batch_loss: 0.0711\n",
            "Epoch 27/100\n",
            "358/358 [==============================] - 109s 306ms/step - batch_loss: 0.0661\n",
            "Epoch 28/100\n",
            "358/358 [==============================] - 111s 309ms/step - batch_loss: 0.0579\n",
            "Epoch 29/100\n",
            "358/358 [==============================] - 107s 300ms/step - batch_loss: 0.0459\n",
            "Epoch 30/100\n",
            "358/358 [==============================] - 107s 298ms/step - batch_loss: 0.0491\n",
            "Epoch 31/100\n",
            "358/358 [==============================] - 108s 303ms/step - batch_loss: 0.0545\n",
            "Epoch 32/100\n",
            "358/358 [==============================] - 107s 300ms/step - batch_loss: 0.0613\n",
            "Epoch 33/100\n",
            "358/358 [==============================] - 108s 302ms/step - batch_loss: 0.0680\n",
            "Epoch 34/100\n",
            "358/358 [==============================] - 108s 303ms/step - batch_loss: 0.0678\n",
            "Epoch 35/100\n",
            "358/358 [==============================] - 106s 296ms/step - batch_loss: 0.0630\n",
            "Epoch 36/100\n",
            "358/358 [==============================] - 107s 297ms/step - batch_loss: 0.0564\n",
            "Epoch 37/100\n",
            "358/358 [==============================] - 109s 303ms/step - batch_loss: 0.0506\n",
            "Epoch 38/100\n",
            "358/358 [==============================] - 108s 303ms/step - batch_loss: 0.0445\n",
            "Epoch 39/100\n",
            "358/358 [==============================] - 109s 304ms/step - batch_loss: 0.0442\n",
            "Epoch 40/100\n",
            "358/358 [==============================] - 107s 300ms/step - batch_loss: 0.0479\n",
            "Epoch 41/100\n",
            "358/358 [==============================] - 106s 296ms/step - batch_loss: 0.0585\n",
            "Epoch 42/100\n",
            "358/358 [==============================] - 107s 299ms/step - batch_loss: 0.0636\n",
            "Epoch 43/100\n",
            "358/358 [==============================] - 107s 299ms/step - batch_loss: 0.0662\n",
            "Epoch 44/100\n",
            "358/358 [==============================] - 108s 302ms/step - batch_loss: 0.0609\n",
            "Epoch 45/100\n",
            "358/358 [==============================] - 106s 295ms/step - batch_loss: 0.0497\n",
            "Epoch 46/100\n",
            "358/358 [==============================] - 108s 302ms/step - batch_loss: 0.0430\n",
            "Epoch 47/100\n",
            "358/358 [==============================] - 108s 302ms/step - batch_loss: 0.0431\n",
            "Epoch 48/100\n",
            "358/358 [==============================] - 109s 303ms/step - batch_loss: 0.0443\n",
            "Epoch 49/100\n",
            "358/358 [==============================] - 108s 300ms/step - batch_loss: 0.0470\n",
            "Epoch 50/100\n",
            "261/358 [====================>.........] - ETA: 28s - batch_loss: 0.0556"
          ]
        }
      ],
      "source": [
        "train_translator.fit(trained_dataset, epochs=100,\n",
        "                     callbacks=[batch_loss])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej3vGAWRb9Pt"
      },
      "outputs": [],
      "source": [
        "plt.plot(batch_loss.logs)\n",
        "plt.ylim([0, 3])\n",
        "plt.xlabel('Batch #')\n",
        "plt.ylabel('CE/token')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translator"
      ],
      "metadata": {
        "id": "IXqJ52MDyMNt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4NpHontQwGV"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Translator(tf.Module):\n",
        "\n",
        "    def __init__(self, encoder, decoder, input_text_processor,\n",
        "                 output_text_processor):\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.input_text_processor = input_text_processor\n",
        "        self.output_text_processor = output_text_processor\n",
        "\n",
        "        self.output_token_string_from_index = (\n",
        "            tf.keras.layers.StringLookup(\n",
        "                vocabulary=output_text_processor.get_vocabulary(),\n",
        "                mask_token='',\n",
        "                invert=True))\n",
        "\n",
        "        # The output should never generate padding, unknown, or start.\n",
        "        index_from_string = tf.keras.layers.StringLookup(\n",
        "            vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
        "        token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
        "\n",
        "        token_mask = np.zeros(\n",
        "            [index_from_string.vocabulary_size()], dtype=np.bool)\n",
        "        token_mask[np.array(token_mask_ids)] = True\n",
        "        self.token_mask = token_mask\n",
        "\n",
        "        self.start_token = index_from_string(tf.constant('[START]'))\n",
        "        self.end_token = index_from_string(tf.constant('[END]'))\n",
        "\n",
        "    def tokens_to_text(self, result_tokens):\n",
        "        result_text_tokens = self.output_token_string_from_index(\n",
        "            result_tokens)\n",
        "        result_text = tf.strings.reduce_join(result_text_tokens,\n",
        "                                             axis=1, separator=' ')\n",
        "        result_text = tf.strings.strip(result_text)\n",
        "        return result_text\n",
        "\n",
        "    def sample(self, logits, temperature):\n",
        "        token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "        logits = tf.where(self.token_mask, -np.inf, logits)\n",
        "\n",
        "        if temperature == 0.0:\n",
        "            new_tokens = tf.argmax(logits, axis=-1)\n",
        "        else:\n",
        "            logits = tf.squeeze(logits, axis=1)\n",
        "            new_tokens = tf.random.categorical(logits/temperature,\n",
        "                                               num_samples=1)\n",
        "\n",
        "        return new_tokens\n",
        "\n",
        "    def translate(self,\n",
        "                  input_text, *,\n",
        "                  max_length=50,\n",
        "                  return_attention=True,\n",
        "                  temperature=1.0):\n",
        "        batch_size = tf.shape(input_text)[0]\n",
        "        input_tokens = self.input_text_processor(input_text)\n",
        "        enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "        dec_state = enc_state\n",
        "        new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "\n",
        "        result_tokens = []\n",
        "        attention = []\n",
        "        done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            dec_input = DecoderInput(new_tokens=new_tokens,\n",
        "                                     enc_output=enc_output,\n",
        "                                     mask=(input_tokens != 0))\n",
        "\n",
        "            dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
        "\n",
        "            attention.append(dec_result.attention_weights)\n",
        "\n",
        "            new_tokens = self.sample(dec_result.logits, temperature)\n",
        "\n",
        "            # If a sequence produces an `end_token`, set it `done`\n",
        "            done = done | (new_tokens == self.end_token)\n",
        "            # Once a sequence is done it only produces 0-padding.\n",
        "            new_tokens = tf.where(done, tf.constant(\n",
        "                0, dtype=tf.int64), new_tokens)\n",
        "\n",
        "            # Collect the generated tokens\n",
        "            result_tokens.append(new_tokens)\n",
        "\n",
        "            if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "                break\n",
        "\n",
        "        # Convert the list of generates token ids to a list of strings.\n",
        "        result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "        result_text = self.tokens_to_text(result_tokens)\n",
        "\n",
        "        if return_attention:\n",
        "            attention_stack = tf.concat(attention, axis=1)\n",
        "            return {'text': result_text, 'attention': attention_stack}\n",
        "        else:\n",
        "            return {'text': result_text}\n",
        "\n",
        "    @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
        "    def __call__(self, input_text):\n",
        "        return self.translate(input_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate a translator\n",
        "translate = Translator(\n",
        "    encoder=train_translator.encoder,\n",
        "    decoder=train_translator.decoder,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        ")"
      ],
      "metadata": {
        "id": "BtJgUoquyZO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run it on a simple input:\n",
        "%%time\n",
        "input_text = tf.constant([\n",
        "    \"Dieu de nos pères.\",#Yɛn agyanom Nyankopɔn .'\n",
        "    \"C'est ma vie.\",  # 'Eyi ne m’asetra .'\n",
        "])\n",
        "\n",
        "result = translate(\n",
        "    input_text = input_text)\n",
        "\n",
        "print(result['text'][0].numpy().decode())\n",
        "print(result['text'][1].numpy().decode())\n",
        "print()"
      ],
      "metadata": {
        "id": "KxEOJS2sygHN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set up a function to plot attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  sentence = tf_start_and_end_tokens(sentence).numpy().decode().split()\n",
        "  predicted_sentence = predicted_sentence.numpy().decode().split() + ['[END]']\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "\n",
        "  attention = attention[:len(predicted_sentence), :len(sentence)]\n",
        "\n",
        "  ax.matshow(attention, cmap='viridis', vmin=0.0)\n",
        "\n",
        "  fontdict = {'fontsize': 14}\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "  ax.set_xlabel('Input text')\n",
        "  ax.set_ylabel('Output text')\n",
        "  plt.suptitle('Attention weights')"
      ],
      "metadata": {
        "id": "R_kbJUQ91Gwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify plot attention\n",
        "i=0\n",
        "plot_attention(result['attention'][i], input_text[i], result['text'][i])"
      ],
      "metadata": {
        "id": "a6rfUCLP2zXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Model"
      ],
      "metadata": {
        "id": "svoGwXHY3EJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(translate, '/content/drive/MyDrive/fr_tw_seq2seq_translator',\n",
        "                    signatures={'serving_default': translate.__call__})"
      ],
      "metadata": {
        "id": "NHFcFkWJ3CcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded = tf.saved_model.load('/content/drive/MyDrive/fr_tw_seq2seq_translator')\n",
        "result = reloaded(input_text)"
      ],
      "metadata": {
        "id": "xpxhhNo09naB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLEU\n"
      ],
      "metadata": {
        "id": "UGP9mZqTvBzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "\n",
        "class Bleu():\n",
        "    def __init__(self, translator):\n",
        "        self.translator = translator\n",
        "\n",
        "    def get_bleuscore(self, testfile, referencefile, smothingfunction=None):\n",
        "        if type(testfile) == str and type(referencefile) == str:\n",
        "            # Open test file and read lines\n",
        "            f = open(testfile, \"r\")\n",
        "            hypothesis = f.readlines()\n",
        "            f.close()\n",
        "            # open refernce file and read lines\n",
        "            f = open(referencefile, \"r\")\n",
        "            reference = f.readlines()\n",
        "            f.close()\n",
        "        elif type(testfile) == list and type(referencefile) == list:\n",
        "            hypothesis = testfile\n",
        "            reference = referencefile\n",
        "        else:\n",
        "            print(f'File must be txt or python list')\n",
        "\n",
        "        # check the length of our input sentence\n",
        "        length = len(hypothesis)\n",
        "        bleu_total = np.array([0., 0., 0.])\n",
        "        weights = [(1./2, 1./2), (1./3, 1./3, 1./3),\n",
        "                   (1./4, 1./4, 1./4, 1./4)]\n",
        "        for i in range(length):\n",
        "            hypothesis[i] = hypothesis[i]\n",
        "            reference[i] = reference[i]\n",
        "            groundtruth = reference[i].lower().replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            groundtruth = [groundtruth]\n",
        "            translated_text = self.translator(\n",
        "                hypothesis[i]).numpy().decode(\"utf-8\")\n",
        "            # print(\"Translated Text: \", translated_text)\n",
        "            # print(\"Ground Truth: \", reference[i])\n",
        "            candidate = translated_text.replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            bleu = np.array(sentence_bleu(\n",
        "                groundtruth, candidate, weights, smoothing_function=smothingfunction))\n",
        "            bleu_total += bleu\n",
        "\n",
        "        return f'2-GRAMS: {bleu_total[0]/length}',f'3-GRAMS: {bleu_total[1]/length}',f'4-GRAMS: {bleu_total[2]/length}'"
      ],
      "metadata": {
        "id": "SguFKYfLvD10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#iNSTANTIATE OBJECT OF BLEU CLASS AND IT SMOOTHING FUNCTION\n",
        "smooth= SmoothingFunction()\n",
        "bleu = Bleu(reloaded)"
      ],
      "metadata": {
        "id": "cfjG3A_BvJvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ESTIMATE BLEU SCORE FROM THE TEST DATA\n",
        "# from list\n",
        "bleu.get_bleuscore(test_fr,test_twi,smooth.method2)"
      ],
      "metadata": {
        "id": "FlTY1ZYtvPRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GoogleAPI bleu"
      ],
      "metadata": {
        "id": "v8ChexIEvcq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use google API for bidirectional pivot translation of Twi and French\n",
        "# pivot language = English\n",
        "# import libraries\n",
        "from googletrans import Translator, constants\n",
        "# instantiate a translator object\n",
        "# initiate translator object\n",
        "translator = Translator()\n",
        "# Add Akan to the language supported by this package\n",
        "# Note the googletrans package has not  been updated to capture the new additions by google since May 2022\n",
        "# from https://translate.google.com/?sl=en&tl=ak&op=translate , the key and value for Twi is 'ak':'akan'\n",
        "constants.LANGUAGES['ak'] = 'akan'\n",
        "\n",
        "\n",
        "class GooglePivot:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def evaluate(self, sentences, src_key, dest_key):\n",
        "        eng_text = translator.translate(sentences, src=src_key, dest='en').text\n",
        "        print(eng_text)\n",
        "        output = translator.translate(eng_text, dest=dest_key).text\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class GoogleDirect:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def evaluate(self, sentences, src_key, dest_key):\n",
        "\n",
        "        return translator.translate(sentences, src=src_key, dest=dest_key).text"
      ],
      "metadata": {
        "id": "dsy_Y8eTvbto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "\n",
        "class GoogleBleu():\n",
        "    def __init__(self, translator):\n",
        "        self.translator = translator\n",
        "\n",
        "    def get_bleuscore(self, testfile, referencefile,src_lang,dest_lang, smothingfunction=None):\n",
        "        if type(testfile) == str and type(referencefile) == str:\n",
        "            # Open test file and read lines\n",
        "            f = open(testfile, \"r\")\n",
        "            hypothesis = f.readlines()\n",
        "            f.close()\n",
        "            # open refernce file and read lines\n",
        "            f = open(referencefile, \"r\")\n",
        "            reference = f.readlines()\n",
        "            f.close()\n",
        "        elif type(testfile) == list and type(referencefile) == list:\n",
        "            hypothesis = testfile\n",
        "            reference = referencefile\n",
        "        else:\n",
        "            print(f'File must be txt or python list')\n",
        "\n",
        "        # check the length of our input sentence\n",
        "        length = len(hypothesis)\n",
        "        bleu_total = np.array([0., 0., 0.])\n",
        "        weights = [(1./2, 1./2), (1./3, 1./3, 1./3),\n",
        "                   (1./4, 1./4, 1./4, 1./4)]\n",
        "        for i in range(length):\n",
        "            hypothesis[i] = hypothesis[i]\n",
        "            reference[i] = reference[i]\n",
        "            groundtruth = reference[i].lower().replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            groundtruth = [groundtruth]\n",
        "            translated_text = self.translator.evaluate(hypothesis[i],src_key=src_lang, dest_key=dest_lang)\n",
        "            # print(\"Translated Text: \", translated_text)\n",
        "            # print(\"Ground Truth: \", reference[i])\n",
        "            candidate = translated_text.replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            bleu = np.array(sentence_bleu(\n",
        "                groundtruth, candidate, weights, smoothing_function=smothingfunction))\n",
        "            bleu_total += bleu\n",
        "\n",
        "        return f'2-GRAMS: {bleu_total[0]/length}',f'3-GRAMS: {bleu_total[1]/length}',f'4-GRAMS: {bleu_total[2]/length}'"
      ],
      "metadata": {
        "id": "IJ6yxd05vnyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate GoogleDirect class\n",
        "google_translate = GoogleDirect()\n",
        "google_bleu = GoogleBleu(google_translate)"
      ],
      "metadata": {
        "id": "ywpKmrvKvsc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "google_bleu.get_bleuscore(test_fr,test_twi,'fr','ak',smooth.method2)"
      ],
      "metadata": {
        "id": "rofXQmYIv33v"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}