{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/transformer__twi_english.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qFzQxA29DJp"
      },
      "source": [
        "This notebook is based on the implementation of the paper [Attention is All you Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017). The code are adapted from the Tenssorflow repository at https://www.tensorflow.org/text/tutorials/transformer#build_the_transformer and a big thanks to [[Crater David]](https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:37370749) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hblEgiSfLEck"
      },
      "source": [
        "# Install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naWgZqfLsOiN",
        "outputId": "f876908f-371b-450a-ca62-db514fd54f11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 5.9 MB 24.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 582.0 MB 13 kB/s \n",
            "\u001b[K     |████████████████████████████████| 439 kB 66.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 46.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 50.0 MB/s \n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading googletrans-3.1.0a0.tar.gz (19 kB)\n",
            "Collecting httpx==0.13.3\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Collecting httpcore==0.9.*\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2022.6.15)\n",
            "Collecting hstspreload\n",
            "  Downloading hstspreload-2022.9.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 62.5 MB/s \n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting rfc3986<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting h11<0.10,>=0.8\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.5 MB/s \n",
            "\u001b[?25hCollecting h2==3.*\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-py3-none-any.whl size=16367 sha256=470dd9e6a29e68266d84bbe998b717ffd41f2ad6d9c8f25321740293fbaf92dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/be/fe/93a6a40ffe386e16089e44dad9018ebab9dc4cb9eb7eab65ae\n",
            "Successfully built googletrans\n",
            "Installing collected packages: hyperframe, hpack, sniffio, h2, h11, rfc3986, httpcore, hstspreload, httpx, googletrans\n",
            "Successfully installed googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2022.9.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U tensorflow-text-nightly==2.11.0.dev20220817\n",
        "!pip install -U -q tf-nightly==2.11.0.dev20220817\n",
        "!pip3 install googletrans==3.1.0a0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY7seog2LQOL"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gwubBvId4uGK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W18p7g2lLatA"
      },
      "source": [
        "# preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ovdl-UER8hMX"
      },
      "outputs": [],
      "source": [
        "# This code was adapted from https://github.com/GhanaNLP/kasa/blob/master/Kasa/Preprocessing.py\n",
        "# A subclass of the kasafrench for preprocessing data\n",
        "# import required library\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "class Preprocessing:\n",
        "    # dummy initialization method\n",
        "    def __init__(self):\n",
        "        # initialize with some default parameters here later\n",
        "        pass\n",
        "\n",
        "    # read in parallel twi - english dataset\n",
        "    def read_parallel_dataset(self, filepath_1, filepath_2, filepath_3=None):\n",
        "        if filepath_3 != None:\n",
        "            # read first language data\n",
        "            lang_1 = []\n",
        "            with open(filepath_1, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_1.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            # read second language data\n",
        "            lang_2 = []\n",
        "            with open(filepath_2, encoding='utf-8') as file:\n",
        "\n",
        "                # twi=file.read()\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_2.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "            # Read third Language data\n",
        "            lang_3 = []\n",
        "            with open(filepath_3, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_3.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            return lang_1, lang_2, lang_3\n",
        "            \n",
        "        else:\n",
        "            # read first language data\n",
        "            lang_1 = []\n",
        "            with open(filepath_1, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_1.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            # read second language data\n",
        "            lang_2 = []\n",
        "            with open(filepath_2, encoding='utf-8') as file:\n",
        "\n",
        "                # twi=file.read()\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_2.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            return lang_1, lang_2\n",
        "\n",
        "    # Define a helper function to remove string accents\n",
        "\n",
        "    def removeStringAccent(self, s):\n",
        "        return ''.join(\n",
        "            c for c in unicodedata.normalize('NFD', s)\n",
        "            if unicodedata.category(c) != 'Mn'\n",
        "        )\n",
        "\n",
        "    # normalize input twi sentence\n",
        "    def normalize_twi(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = s.lower()\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.ƆɔɛƐ!?’]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "\n",
        "    # normalize input french sentence\n",
        "    def normalize_FrEn(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = s.lower()\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "    \n",
        "    def writeTotxt(self,destination,data):\n",
        "        with open(destination, 'w') as f:\n",
        "            for line in data:\n",
        "                 f.write(f\"{line}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "B4asafNW9HIt"
      },
      "outputs": [],
      "source": [
        "# import preprocessing class \n",
        "# Create an instance of tft preprocessing class\n",
        "preprocessor = Preprocessing()\n",
        "\n",
        "# Read raw parallel dataset\n",
        "raw_data_twi, raw_data_en = preprocessor.read_parallel_dataset(\n",
        "    filepath_1='/content/drive/MyDrive/verified_twi.txt',\n",
        "    filepath_2='/content/drive/MyDrive/verified_english.txt'\n",
        ")\n",
        "\n",
        "# Normalize the raw data\n",
        "raw_data_en = [preprocessor.normalize_FrEn(data) for data in raw_data_en]\n",
        "raw_data_twi = [preprocessor.normalize_twi(data) for data in raw_data_twi]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au81TkNOLwnB"
      },
      "source": [
        "# split data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5l08UvTL9mtx"
      },
      "outputs": [],
      "source": [
        "# split the dataset into training and test sets\n",
        "# Keep 10% of the data as test set\n",
        "train_twi,test_twi,train_en,test_en = train_test_split(raw_data_twi,raw_data_en, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3q4djANP-9g6"
      },
      "outputs": [],
      "source": [
        "# write the preprocess traning and test dataset to a file\n",
        "preprocessor.writeTotxt('train_twi.txt',train_twi)\n",
        "preprocessor.writeTotxt('train_en.txt',train_en)\n",
        "preprocessor.writeTotxt('test_twi.txt',test_twi)\n",
        "preprocessor.writeTotxt('test_en.txt',test_en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao2l87AzL_kE"
      },
      "source": [
        "# Build tf dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0WZrSEP9_ISJ"
      },
      "outputs": [],
      "source": [
        "# build tf datasets from traning and test sentences in both languages\n",
        "train_dataset_en = tf.data.TextLineDataset('/content/train_en.txt')\n",
        "train_dataset_tw = tf.data.TextLineDataset('/content/train_twi.txt')\n",
        "\n",
        "val_dataset_en = tf.data.TextLineDataset('/content/test_en.txt')\n",
        "val_dataset_tw = tf.data.TextLineDataset('/content/test_twi.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mmksbF-CAEXg"
      },
      "outputs": [],
      "source": [
        "# combine languages into single dataset\n",
        "trained_combined = tf.data.Dataset.zip((train_dataset_tw, train_dataset_en))\n",
        "val_combined = tf.data.Dataset.zip((val_dataset_tw, val_dataset_en))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZJ-hiEUBLDu",
        "outputId": "d3a56d8c-02b0-419c-fb13-2fa28094ab2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Twi:  amerika atubrafo a wodi kan no duu hɔ wɔ afeha a ɛto so no mu .\n",
            "English:  the first american colonists arrived in the th century .\n",
            "Twi:  ɛsɛ sɛ wohohoro wo nsa .\n",
            "English:  you need to wash your hands .\n",
            "Twi:  da a wɔde tua ka no yɛ da koro akatua .\n",
            "English:  appiah horrow is payday .\n",
            "Twi:  mitumi hu nea enti a w ani gye asamoa ho\n",
            "English:  i can see why you like asamoah .\n",
            "Twi:  ɔhyɛ jeans attade\n",
            "English:  he had jeans on .\n"
          ]
        }
      ],
      "source": [
        "# verify trained_combined dataset is correct\n",
        "for tw,en in trained_combined.take(5):\n",
        "  print(\"Twi: \", tw.numpy().decode('utf-8'))\n",
        "  print(\"English: \", en.numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaxFg3TZMIK8"
      },
      "source": [
        "# load tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjb-3AU1BnrK"
      },
      "source": [
        "\n",
        "\n",
        "1. The tokenizer is which was build from the parallel corpus.\n",
        "2. The code are available on the link https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/build_subword_tokenizer.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aR_vgyQgBo_T"
      },
      "outputs": [],
      "source": [
        "model_named = '/content/drive/MyDrive/translate_frengtwi_converter'\n",
        "# Verify tokenizer model can be reloaded\n",
        "tokenizers = tf.saved_model.load(model_named)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdHMl3icEjos",
        "outputId": "5535c277-983d-4082-c4c3-ce94e37f2cfe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'i', b'love', b'student', b'life', b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Verify tokenizer works on french sentence\n",
        "tokens = tokenizers.eng.tokenize(['I LOVE STUDENT LIFE'])\n",
        "text_tokens = tokenizers.eng.lookup(tokens)\n",
        "text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSOMXh_de04Q",
        "outputId": "ac796c9b-8308-41ac-adc1-3d53ab6e8dce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i love student life\n"
          ]
        }
      ],
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.eng.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBNqSqeSfChq",
        "outputId": "4b4da44d-66bf-4a3c-d81d-2d6b8c88fad2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'na', b'mmarimaa', b'ne', b'mmeawa', b'pii', b'w\\xc9\\x94',\n",
              "  b'agu', b'##di', b'##bea', b'h\\xc9\\x94', b'.', b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Verify tokenizer works on twi sentence\n",
        "tokens = tokenizers.twi.tokenize(['Na mmarimaa ne mmeawa pii wɔ agudibea hɔ .'])\n",
        "text_tokens = tokenizers.twi.lookup(tokens)\n",
        "text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lS1xMYQfQR9",
        "outputId": "dc58e590-a492-4fb6-e390-0f4ca738842b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "na mmarimaa ne mmeawa pii wɔ agudibea hɔ .\n"
          ]
        }
      ],
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.twi.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQVo6G3OMfac"
      },
      "source": [
        "# Check sentence with maximum length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvoh1vRgCxSb"
      },
      "source": [
        "By checking the maximum length of sentence, you can suitably select the MAX_TOKENS. The MAX_TOKKENS help drops examples longer than the maximum number of tokens (MAX_TOKENS). Without limiting the size of sequences, the performance may be negatively affected.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VJxJuNufTCB",
        "outputId": "4926cb3d-bba8-455e-f2a8-a4dacbbbadd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".........................................................................................................................................................................................................................................................................................................................................................................................................................................................................."
          ]
        }
      ],
      "source": [
        "#The distribution of tokens per example in the dataset is as follows:\n",
        "\n",
        "lengths = []\n",
        "\n",
        "for twi_examples,en_examples in trained_combined.batch(50):\n",
        "  twi_tokens = tokenizers.twi.tokenize(twi_examples)\n",
        "  lengths.append(twi_tokens.row_lengths())\n",
        "\n",
        "  en_tokens = tokenizers.eng.tokenize(en_examples)\n",
        "  lengths.append(en_tokens.row_lengths())\n",
        "  print('.', end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "ri-qmhyyiBSa",
        "outputId": "d6ace3af-0bd3-48e9-ef01-6fc6c2b218dc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "all_lengths = np.concatenate(lengths)\n",
        "\n",
        "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
        "plt.ylim(plt.ylim())\n",
        "max_length = max(all_lengths)\n",
        "plt.plot([max_length, max_length], plt.ylim())\n",
        "plt.title(f'Maximum tokens per example: {max_length}');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DYwhdWAMpnW"
      },
      "source": [
        "# Tokenize the training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GC7UlwEeiNTO"
      },
      "outputs": [],
      "source": [
        "MAX_TOKENS = 50\n",
        "\n",
        "def filter_max_tokens(l1, l2):\n",
        "  num_tokens = tf.maximum(tf.shape(l1)[1],tf.shape(l2)[1])\n",
        "  return num_tokens < MAX_TOKENS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Routine to tokenize sentence pairs; needed to make sentence batches for training\n",
        "def tokenize_pairs(twi,en):\n",
        "  tw = tokenizers.twi.tokenize(twi)\n",
        "  # Convert from ragged to dense, padding with zeros.\n",
        "  tw = tw.to_tensor()\n",
        "  en = tokenizers.eng.tokenize(en)\n",
        "  # Convert from ragged to dense, padding with zeros.\n",
        "  en = en.to_tensor()\n",
        "  return tw,en"
      ],
      "metadata": {
        "id": "AORJvDkcOhxC"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "EWMySNLzklc_"
      },
      "outputs": [],
      "source": [
        "# create training batches\n",
        "BUFFER_SIZE = len(train_twi)\n",
        "BATCH_SIZE = 64\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .filter(filter_max_tokens)\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "# Create training and validation set batches.\n",
        "train_batches = make_batches(trained_combined)\n",
        "val_batches = make_batches(val_combined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2MhJY21RqcJ"
      },
      "source": [
        "Write the test batch to file. This will come in handy later if you prefer to translate from txt file and estimate the translator [BLEU](https://aclanthology.org/P02-1040.pdf) score. The translator will run to error for any for any sentence with the shape greater than the MAX_TOKENS used for training. It adivisable to use the trimmed sentences for testting to avoid such occurrance\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2V_Rh8fFSvPV"
      },
      "outputs": [],
      "source": [
        "twi_test = []\n",
        "en_test= []\n",
        "\n",
        "for twi_batches,en_batches in val_batches:\n",
        "    for tw in tokenizers.twi.detokenize(twi_batches):\n",
        "      twi_test.append(tw.numpy().decode(\"utf-8\"))\n",
        "    for en in tokenizers.eng.detokenize(en_batches):\n",
        "      en_test.append(en.numpy().decode(\"utf-8\"))\n",
        "\n",
        "\n",
        "preprocessor.writeTotxt('/content/drive/MyDrive/twi_testing_set.txt',test_twi)\n",
        "preprocessor.writeTotxt('/content/drive/MyDrive/english_testing_set.txt',en_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNFMseSdM3wr"
      },
      "source": [
        "# Positional encoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5du74jWFF5WK"
      },
      "source": [
        "Positional encodings are added to the embeddings to give the model some information about the relative position of the tokens in the sentence so the model can learn to recognize the word order.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "KXtWVxfeb6px"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "  \n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # Apply the sine function to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # Apply the cosine function to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXL_HTJWG44o"
      },
      "source": [
        "# Create a point-wise feed-forward network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "db4kXyfGb_l5"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(\n",
        "  d_model, # Input/output dimensionality.\n",
        "  dff # Inner-layer dimensionality.\n",
        "  ):\n",
        "\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # Shape `(batch_size, seq_len, dff)`.\n",
        "      tf.keras.layers.Dense(d_model)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "  ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSEO7HUZHppR"
      },
      "source": [
        "# Build Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtydO3sOIRE-"
      },
      "source": [
        "### Multihead Attention layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "BSHsspXXIaUR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create mask for padding tokens so translator ignores them\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "\n",
        "# Mask future tokens so translator cannot see future\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "\n",
        "# Create final masks\n",
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead)\n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "      q: query shape == (..., seq_len_q, depth)\n",
        "      k: key shape == (..., seq_len_k, depth)\n",
        "      v: value shape == (..., seq_len_v, depth_v)\n",
        "      mask: Float tensor with shape broadcastable\n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "      output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    # (..., seq_len_q, seq_len_k)\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(\n",
        "        scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# Add attention layers to create multi-head attention\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, *, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # (batch_size, num_heads, seq_len_q, depth)\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        # (batch_size, num_heads, seq_len_k, depth)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        # (batch_size, num_heads, seq_len_v, depth)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        # (batch_size, seq_len_q, num_heads, depth)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        # (batch_size, seq_len_q, d_model)\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "        # Build Transformer encoder layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvNLzFWpJw6m"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "bCdZS_nJcGVW"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "EBZNB3MacRCF"
      },
      "outputs": [],
      "source": [
        "#Build Transformer encoder from encoder layer\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "              maximum_position_encoding=MAX_TOKENS,rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndsnLCOwRgcY"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "fnXVTpI_cgzR"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "UEuoBDtJcp7Q"
      },
      "outputs": [],
      "source": [
        "#Build Transformer decoder from decoder layer\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "              maximum_position_encoding,rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size,d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjFbGSvJUQY6"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_6sd1ofVcyl4"
      },
      "outputs": [],
      "source": [
        "# Build full Transformer\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_vocab_size, pe_input=MAX_TOKENS, pe_target=MAX_TOKENS, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.encoder= Encoder(num_layers, d_model, num_heads, dff,input_vocab_size, pe_input, rate)\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff,target_vocab_size, pe_target, rate)\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "  \n",
        "  def call(self, inp, tar, training, enc_padding_mask,look_ahead_mask, dec_padding_mask):\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask) # (batch_size, inp_seq_len, d_model)\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    final_output = self.final_layer(dec_output) # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    return final_output, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPhqoBhwWYZ9"
      },
      "source": [
        "# Optimizer and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "y79jHzBnA7wG"
      },
      "outputs": [],
      "source": [
        "# Set learning rate schedule\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g11C2K3tXFyP"
      },
      "source": [
        "# Set hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-q51YVNusr5_"
      },
      "outputs": [],
      "source": [
        "num_layers = 6\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fGafl4DXYct"
      },
      "source": [
        "# Instantiate a Transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "9DJxw765BYHi"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=tokenizers.twi.get_vocab_size().numpy(),\n",
        "    target_vocab_size=tokenizers.eng.get_vocab_size().numpy(),\n",
        "    rate=dropout_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aGTknsnYDCC"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "BF0wsHaaJVKC"
      },
      "outputs": [],
      "source": [
        "# Instantiate learning rate and set optimizer\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "HpLaySRfSy89"
      },
      "outputs": [],
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "-PHWy8sdGoR8"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = './checkpoints/train'\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# If a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "VAj0P_T0ZcBV"
      },
      "outputs": [],
      "source": [
        "# Choose number of training epochs\n",
        "EPOCHS = 120\n",
        "train_step_signature = [\n",
        "  tf.TensorSpec(shape=(None, None), dtype=tf.int64), \n",
        "  tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp,True,enc_padding_mask,combined_mask,dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7k6GwwEIi_A",
        "outputId": "1da66ccd-de65-4098-f804-0bcc3f935f5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 7.7080 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 6.9948 Accuracy 0.0657\n",
            "Epoch 1 Batch 100 Loss 6.5943 Accuracy 0.0846\n",
            "Epoch 1 Batch 150 Loss 6.3166 Accuracy 0.1118\n",
            "Epoch 1 Batch 200 Loss 6.0671 Accuracy 0.1406\n",
            "Epoch 1 Batch 250 Loss 5.8820 Accuracy 0.1597\n",
            "Epoch 1 Batch 300 Loss 5.7312 Accuracy 0.1745\n",
            "Epoch 1 Batch 350 Loss 5.6040 Accuracy 0.1870\n",
            "Epoch 1 Loss 5.5918 Accuracy 0.1883\n",
            "Time taken for 1 epoch: 86.05 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 4.8860 Accuracy 0.2538\n",
            "Epoch 2 Batch 50 Loss 4.7387 Accuracy 0.2712\n",
            "Epoch 2 Batch 100 Loss 4.6741 Accuracy 0.2808\n",
            "Epoch 2 Batch 150 Loss 4.6155 Accuracy 0.2878\n",
            "Epoch 2 Batch 200 Loss 4.5447 Accuracy 0.2955\n",
            "Epoch 2 Batch 250 Loss 4.4884 Accuracy 0.3003\n",
            "Epoch 2 Batch 300 Loss 4.4290 Accuracy 0.3056\n",
            "Epoch 2 Batch 350 Loss 4.3723 Accuracy 0.3107\n",
            "Epoch 2 Loss 4.3656 Accuracy 0.3112\n",
            "Time taken for 1 epoch: 63.80 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 3.9160 Accuracy 0.3617\n",
            "Epoch 3 Batch 50 Loss 3.9005 Accuracy 0.3485\n",
            "Epoch 3 Batch 100 Loss 3.8594 Accuracy 0.3525\n",
            "Epoch 3 Batch 150 Loss 3.8305 Accuracy 0.3548\n",
            "Epoch 3 Batch 200 Loss 3.8075 Accuracy 0.3575\n",
            "Epoch 3 Batch 250 Loss 3.7802 Accuracy 0.3601\n",
            "Epoch 3 Batch 300 Loss 3.7578 Accuracy 0.3616\n",
            "Epoch 3 Batch 350 Loss 3.7341 Accuracy 0.3634\n",
            "Epoch 3 Loss 3.7329 Accuracy 0.3634\n",
            "Time taken for 1 epoch: 64.19 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 3.6686 Accuracy 0.3299\n",
            "Epoch 4 Batch 50 Loss 3.4558 Accuracy 0.3877\n",
            "Epoch 4 Batch 100 Loss 3.4354 Accuracy 0.3896\n",
            "Epoch 4 Batch 150 Loss 3.4335 Accuracy 0.3890\n",
            "Epoch 4 Batch 200 Loss 3.4283 Accuracy 0.3891\n",
            "Epoch 4 Batch 250 Loss 3.4137 Accuracy 0.3908\n",
            "Epoch 4 Batch 300 Loss 3.3966 Accuracy 0.3920\n",
            "Epoch 4 Batch 350 Loss 3.3842 Accuracy 0.3932\n",
            "Epoch 4 Loss 3.3827 Accuracy 0.3933\n",
            "Time taken for 1 epoch: 63.39 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 3.0823 Accuracy 0.4399\n",
            "Epoch 5 Batch 50 Loss 3.1466 Accuracy 0.4172\n",
            "Epoch 5 Batch 100 Loss 3.1564 Accuracy 0.4159\n",
            "Epoch 5 Batch 150 Loss 3.1461 Accuracy 0.4169\n",
            "Epoch 5 Batch 200 Loss 3.1427 Accuracy 0.4166\n",
            "Epoch 5 Batch 250 Loss 3.1362 Accuracy 0.4173\n",
            "Epoch 5 Batch 300 Loss 3.1308 Accuracy 0.4172\n",
            "Epoch 5 Batch 350 Loss 3.1219 Accuracy 0.4178\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
            "Epoch 5 Loss 3.1202 Accuracy 0.4179\n",
            "Time taken for 1 epoch: 67.36 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 2.8657 Accuracy 0.4720\n",
            "Epoch 6 Batch 50 Loss 2.8732 Accuracy 0.4435\n",
            "Epoch 6 Batch 100 Loss 2.8678 Accuracy 0.4439\n",
            "Epoch 6 Batch 150 Loss 2.8680 Accuracy 0.4438\n",
            "Epoch 6 Batch 200 Loss 2.8787 Accuracy 0.4424\n",
            "Epoch 6 Batch 250 Loss 2.8823 Accuracy 0.4423\n",
            "Epoch 6 Batch 300 Loss 2.8905 Accuracy 0.4405\n",
            "Epoch 6 Batch 350 Loss 2.8868 Accuracy 0.4407\n",
            "Epoch 6 Loss 2.8872 Accuracy 0.4407\n",
            "Time taken for 1 epoch: 63.36 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.8353 Accuracy 0.4350\n",
            "Epoch 7 Batch 50 Loss 2.6601 Accuracy 0.4636\n",
            "Epoch 7 Batch 100 Loss 2.6677 Accuracy 0.4634\n",
            "Epoch 7 Batch 150 Loss 2.6847 Accuracy 0.4612\n",
            "Epoch 7 Batch 200 Loss 2.6960 Accuracy 0.4597\n",
            "Epoch 7 Batch 250 Loss 2.6974 Accuracy 0.4590\n",
            "Epoch 7 Batch 300 Loss 2.7027 Accuracy 0.4582\n",
            "Epoch 7 Batch 350 Loss 2.7054 Accuracy 0.4582\n",
            "Epoch 7 Loss 2.7071 Accuracy 0.4579\n",
            "Time taken for 1 epoch: 63.02 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.6010 Accuracy 0.4567\n",
            "Epoch 8 Batch 50 Loss 2.4936 Accuracy 0.4830\n",
            "Epoch 8 Batch 100 Loss 2.4910 Accuracy 0.4845\n",
            "Epoch 8 Batch 150 Loss 2.5166 Accuracy 0.4812\n",
            "Epoch 8 Batch 200 Loss 2.5367 Accuracy 0.4781\n",
            "Epoch 8 Batch 250 Loss 2.5403 Accuracy 0.4775\n",
            "Epoch 8 Batch 300 Loss 2.5524 Accuracy 0.4750\n",
            "Epoch 8 Batch 350 Loss 2.5602 Accuracy 0.4741\n",
            "Epoch 8 Loss 2.5602 Accuracy 0.4740\n",
            "Time taken for 1 epoch: 63.55 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.5058 Accuracy 0.4491\n",
            "Epoch 9 Batch 50 Loss 2.3735 Accuracy 0.4929\n",
            "Epoch 9 Batch 100 Loss 2.3774 Accuracy 0.4956\n",
            "Epoch 9 Batch 150 Loss 2.4036 Accuracy 0.4923\n",
            "Epoch 9 Batch 200 Loss 2.4198 Accuracy 0.4897\n",
            "Epoch 9 Batch 250 Loss 2.4282 Accuracy 0.4887\n",
            "Epoch 9 Batch 300 Loss 2.4366 Accuracy 0.4875\n",
            "Epoch 9 Batch 350 Loss 2.4544 Accuracy 0.4848\n",
            "Epoch 9 Loss 2.4569 Accuracy 0.4844\n",
            "Time taken for 1 epoch: 62.97 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 2.4383 Accuracy 0.4816\n",
            "Epoch 10 Batch 50 Loss 2.2993 Accuracy 0.5004\n",
            "Epoch 10 Batch 100 Loss 2.3078 Accuracy 0.4998\n",
            "Epoch 10 Batch 150 Loss 2.3387 Accuracy 0.4964\n",
            "Epoch 10 Batch 200 Loss 2.3472 Accuracy 0.4958\n",
            "Epoch 10 Batch 250 Loss 2.3624 Accuracy 0.4942\n",
            "Epoch 10 Batch 300 Loss 2.3766 Accuracy 0.4927\n",
            "Epoch 10 Batch 350 Loss 2.3846 Accuracy 0.4920\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
            "Epoch 10 Loss 2.3869 Accuracy 0.4916\n",
            "Time taken for 1 epoch: 66.43 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 2.0365 Accuracy 0.5527\n",
            "Epoch 11 Batch 50 Loss 2.2115 Accuracy 0.5156\n",
            "Epoch 11 Batch 100 Loss 2.2452 Accuracy 0.5104\n",
            "Epoch 11 Batch 150 Loss 2.2691 Accuracy 0.5076\n",
            "Epoch 11 Batch 200 Loss 2.2933 Accuracy 0.5041\n",
            "Epoch 11 Batch 250 Loss 2.3146 Accuracy 0.5008\n",
            "Epoch 11 Batch 300 Loss 2.3316 Accuracy 0.4981\n",
            "Epoch 11 Batch 350 Loss 2.3525 Accuracy 0.4952\n",
            "Epoch 11 Loss 2.3556 Accuracy 0.4948\n",
            "Time taken for 1 epoch: 62.98 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.9389 Accuracy 0.5607\n",
            "Epoch 12 Batch 50 Loss 2.1992 Accuracy 0.5098\n",
            "Epoch 12 Batch 100 Loss 2.2332 Accuracy 0.5057\n",
            "Epoch 12 Batch 150 Loss 2.2563 Accuracy 0.5023\n",
            "Epoch 12 Batch 200 Loss 2.2812 Accuracy 0.4995\n",
            "Epoch 12 Batch 250 Loss 2.2962 Accuracy 0.4985\n",
            "Epoch 12 Batch 300 Loss 2.3161 Accuracy 0.4958\n",
            "Epoch 12 Batch 350 Loss 2.3266 Accuracy 0.4946\n",
            "Epoch 12 Loss 2.3281 Accuracy 0.4944\n",
            "Time taken for 1 epoch: 63.08 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 2.0380 Accuracy 0.5422\n",
            "Epoch 13 Batch 50 Loss 2.1002 Accuracy 0.5279\n",
            "Epoch 13 Batch 100 Loss 2.1532 Accuracy 0.5189\n",
            "Epoch 13 Batch 150 Loss 2.1785 Accuracy 0.5156\n",
            "Epoch 13 Batch 200 Loss 2.1958 Accuracy 0.5129\n",
            "Epoch 13 Batch 250 Loss 2.2089 Accuracy 0.5112\n",
            "Epoch 13 Batch 300 Loss 2.2198 Accuracy 0.5096\n",
            "Epoch 13 Batch 350 Loss 2.2268 Accuracy 0.5091\n",
            "Epoch 13 Loss 2.2285 Accuracy 0.5088\n",
            "Time taken for 1 epoch: 62.82 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.9181 Accuracy 0.5684\n",
            "Epoch 14 Batch 50 Loss 2.0586 Accuracy 0.5309\n",
            "Epoch 14 Batch 100 Loss 2.0852 Accuracy 0.5279\n",
            "Epoch 14 Batch 150 Loss 2.0987 Accuracy 0.5261\n",
            "Epoch 14 Batch 200 Loss 2.1247 Accuracy 0.5223\n",
            "Epoch 14 Batch 250 Loss 2.1345 Accuracy 0.5213\n",
            "Epoch 14 Batch 300 Loss 2.1331 Accuracy 0.5223\n",
            "Epoch 14 Batch 350 Loss 2.1351 Accuracy 0.5223\n",
            "Epoch 14 Loss 2.1370 Accuracy 0.5220\n",
            "Time taken for 1 epoch: 63.39 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.8350 Accuracy 0.5762\n",
            "Epoch 15 Batch 50 Loss 1.9306 Accuracy 0.5534\n",
            "Epoch 15 Batch 100 Loss 1.9850 Accuracy 0.5456\n",
            "Epoch 15 Batch 150 Loss 2.0042 Accuracy 0.5403\n",
            "Epoch 15 Batch 200 Loss 2.0148 Accuracy 0.5391\n",
            "Epoch 15 Batch 250 Loss 2.0336 Accuracy 0.5362\n",
            "Epoch 15 Batch 300 Loss 2.0393 Accuracy 0.5355\n",
            "Epoch 15 Batch 350 Loss 2.0385 Accuracy 0.5357\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
            "Epoch 15 Loss 2.0384 Accuracy 0.5357\n",
            "Time taken for 1 epoch: 65.28 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.7042 Accuracy 0.5832\n",
            "Epoch 16 Batch 50 Loss 1.8054 Accuracy 0.5757\n",
            "Epoch 16 Batch 100 Loss 1.8368 Accuracy 0.5685\n",
            "Epoch 16 Batch 150 Loss 1.8521 Accuracy 0.5668\n",
            "Epoch 16 Batch 200 Loss 1.8801 Accuracy 0.5628\n",
            "Epoch 16 Batch 250 Loss 1.8944 Accuracy 0.5603\n",
            "Epoch 16 Batch 300 Loss 1.9053 Accuracy 0.5580\n",
            "Epoch 16 Batch 350 Loss 1.9185 Accuracy 0.5561\n",
            "Epoch 16 Loss 1.9195 Accuracy 0.5560\n",
            "Time taken for 1 epoch: 62.70 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.7053 Accuracy 0.5767\n",
            "Epoch 17 Batch 50 Loss 1.7268 Accuracy 0.5887\n",
            "Epoch 17 Batch 100 Loss 1.7416 Accuracy 0.5836\n",
            "Epoch 17 Batch 150 Loss 1.7605 Accuracy 0.5812\n",
            "Epoch 17 Batch 200 Loss 1.7815 Accuracy 0.5779\n",
            "Epoch 17 Batch 250 Loss 1.8041 Accuracy 0.5734\n",
            "Epoch 17 Batch 300 Loss 1.8116 Accuracy 0.5722\n",
            "Epoch 17 Batch 350 Loss 1.8211 Accuracy 0.5702\n",
            "Epoch 17 Loss 1.8219 Accuracy 0.5700\n",
            "Time taken for 1 epoch: 62.93 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.4082 Accuracy 0.6507\n",
            "Epoch 18 Batch 50 Loss 1.5853 Accuracy 0.6143\n",
            "Epoch 18 Batch 100 Loss 1.6144 Accuracy 0.6080\n",
            "Epoch 18 Batch 150 Loss 1.6468 Accuracy 0.6014\n",
            "Epoch 18 Batch 200 Loss 1.6659 Accuracy 0.5975\n",
            "Epoch 18 Batch 250 Loss 1.6888 Accuracy 0.5931\n",
            "Epoch 18 Batch 300 Loss 1.7023 Accuracy 0.5905\n",
            "Epoch 18 Batch 350 Loss 1.7191 Accuracy 0.5872\n",
            "Epoch 18 Loss 1.7211 Accuracy 0.5868\n",
            "Time taken for 1 epoch: 62.78 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.4934 Accuracy 0.6244\n",
            "Epoch 19 Batch 50 Loss 1.5257 Accuracy 0.6191\n",
            "Epoch 19 Batch 100 Loss 1.5394 Accuracy 0.6180\n",
            "Epoch 19 Batch 150 Loss 1.5636 Accuracy 0.6135\n",
            "Epoch 19 Batch 200 Loss 1.5713 Accuracy 0.6126\n",
            "Epoch 19 Batch 250 Loss 1.5881 Accuracy 0.6097\n",
            "Epoch 19 Batch 300 Loss 1.6069 Accuracy 0.6062\n",
            "Epoch 19 Batch 350 Loss 1.6262 Accuracy 0.6028\n",
            "Epoch 19 Loss 1.6283 Accuracy 0.6024\n",
            "Time taken for 1 epoch: 63.24 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.3475 Accuracy 0.6528\n",
            "Epoch 20 Batch 50 Loss 1.4218 Accuracy 0.6415\n",
            "Epoch 20 Batch 100 Loss 1.4448 Accuracy 0.6365\n",
            "Epoch 20 Batch 150 Loss 1.4763 Accuracy 0.6303\n",
            "Epoch 20 Batch 200 Loss 1.4906 Accuracy 0.6276\n",
            "Epoch 20 Batch 250 Loss 1.5014 Accuracy 0.6251\n",
            "Epoch 20 Batch 300 Loss 1.5211 Accuracy 0.6211\n",
            "Epoch 20 Batch 350 Loss 1.5356 Accuracy 0.6182\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
            "Epoch 20 Loss 1.5371 Accuracy 0.6181\n",
            "Time taken for 1 epoch: 65.23 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.1611 Accuracy 0.6989\n",
            "Epoch 21 Batch 50 Loss 1.3372 Accuracy 0.6566\n",
            "Epoch 21 Batch 100 Loss 1.3620 Accuracy 0.6513\n",
            "Epoch 21 Batch 150 Loss 1.3775 Accuracy 0.6493\n",
            "Epoch 21 Batch 200 Loss 1.3903 Accuracy 0.6470\n",
            "Epoch 21 Batch 250 Loss 1.4087 Accuracy 0.6427\n",
            "Epoch 21 Batch 300 Loss 1.4218 Accuracy 0.6402\n",
            "Epoch 21 Batch 350 Loss 1.4373 Accuracy 0.6370\n",
            "Epoch 21 Loss 1.4417 Accuracy 0.6361\n",
            "Time taken for 1 epoch: 63.11 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 1.2554 Accuracy 0.6765\n",
            "Epoch 22 Batch 50 Loss 1.2632 Accuracy 0.6705\n",
            "Epoch 22 Batch 100 Loss 1.2890 Accuracy 0.6635\n",
            "Epoch 22 Batch 150 Loss 1.3016 Accuracy 0.6615\n",
            "Epoch 22 Batch 200 Loss 1.3186 Accuracy 0.6581\n",
            "Epoch 22 Batch 250 Loss 1.3310 Accuracy 0.6557\n",
            "Epoch 22 Batch 300 Loss 1.3467 Accuracy 0.6527\n",
            "Epoch 22 Batch 350 Loss 1.3611 Accuracy 0.6498\n",
            "Epoch 22 Loss 1.3636 Accuracy 0.6493\n",
            "Time taken for 1 epoch: 62.84 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 1.2285 Accuracy 0.6864\n",
            "Epoch 23 Batch 50 Loss 1.1675 Accuracy 0.6903\n",
            "Epoch 23 Batch 100 Loss 1.1847 Accuracy 0.6859\n",
            "Epoch 23 Batch 150 Loss 1.2099 Accuracy 0.6804\n",
            "Epoch 23 Batch 200 Loss 1.2251 Accuracy 0.6776\n",
            "Epoch 23 Batch 250 Loss 1.2523 Accuracy 0.6716\n",
            "Epoch 23 Batch 300 Loss 1.2722 Accuracy 0.6678\n",
            "Epoch 23 Batch 350 Loss 1.2886 Accuracy 0.6643\n",
            "Epoch 23 Loss 1.2902 Accuracy 0.6641\n",
            "Time taken for 1 epoch: 62.95 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.9647 Accuracy 0.7475\n",
            "Epoch 24 Batch 50 Loss 1.0923 Accuracy 0.7086\n",
            "Epoch 24 Batch 100 Loss 1.1299 Accuracy 0.6994\n",
            "Epoch 24 Batch 150 Loss 1.1633 Accuracy 0.6908\n",
            "Epoch 24 Batch 200 Loss 1.1801 Accuracy 0.6877\n",
            "Epoch 24 Batch 250 Loss 1.1970 Accuracy 0.6840\n",
            "Epoch 24 Batch 300 Loss 1.2116 Accuracy 0.6807\n",
            "Epoch 24 Batch 350 Loss 1.2322 Accuracy 0.6760\n",
            "Epoch 24 Loss 1.2339 Accuracy 0.6757\n",
            "Time taken for 1 epoch: 62.85 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.9679 Accuracy 0.7308\n",
            "Epoch 25 Batch 50 Loss 1.0453 Accuracy 0.7191\n",
            "Epoch 25 Batch 100 Loss 1.0820 Accuracy 0.7100\n",
            "Epoch 25 Batch 150 Loss 1.0939 Accuracy 0.7065\n",
            "Epoch 25 Batch 200 Loss 1.1153 Accuracy 0.7014\n",
            "Epoch 25 Batch 250 Loss 1.1293 Accuracy 0.6982\n",
            "Epoch 25 Batch 300 Loss 1.1457 Accuracy 0.6941\n",
            "Epoch 25 Batch 350 Loss 1.1633 Accuracy 0.6901\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-5\n",
            "Epoch 25 Loss 1.1637 Accuracy 0.6900\n",
            "Time taken for 1 epoch: 69.98 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 1.1235 Accuracy 0.6860\n",
            "Epoch 26 Batch 50 Loss 0.9954 Accuracy 0.7261\n",
            "Epoch 26 Batch 100 Loss 1.0164 Accuracy 0.7212\n",
            "Epoch 26 Batch 150 Loss 1.0422 Accuracy 0.7148\n",
            "Epoch 26 Batch 200 Loss 1.0616 Accuracy 0.7104\n",
            "Epoch 26 Batch 250 Loss 1.0846 Accuracy 0.7054\n",
            "Epoch 26 Batch 300 Loss 1.0975 Accuracy 0.7027\n",
            "Epoch 26 Batch 350 Loss 1.1065 Accuracy 0.7009\n",
            "Epoch 26 Loss 1.1081 Accuracy 0.7005\n",
            "Time taken for 1 epoch: 63.34 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 1.1359 Accuracy 0.7038\n",
            "Epoch 27 Batch 50 Loss 0.9320 Accuracy 0.7411\n",
            "Epoch 27 Batch 100 Loss 0.9591 Accuracy 0.7360\n",
            "Epoch 27 Batch 150 Loss 0.9807 Accuracy 0.7299\n",
            "Epoch 27 Batch 200 Loss 0.9991 Accuracy 0.7249\n",
            "Epoch 27 Batch 250 Loss 1.0129 Accuracy 0.7215\n",
            "Epoch 27 Batch 300 Loss 1.0374 Accuracy 0.7160\n",
            "Epoch 27 Batch 350 Loss 1.0496 Accuracy 0.7127\n",
            "Epoch 27 Loss 1.0515 Accuracy 0.7122\n",
            "Time taken for 1 epoch: 62.82 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.8907 Accuracy 0.7630\n",
            "Epoch 28 Batch 50 Loss 0.8976 Accuracy 0.7519\n",
            "Epoch 28 Batch 100 Loss 0.9143 Accuracy 0.7475\n",
            "Epoch 28 Batch 150 Loss 0.9308 Accuracy 0.7421\n",
            "Epoch 28 Batch 200 Loss 0.9443 Accuracy 0.7386\n",
            "Epoch 28 Batch 250 Loss 0.9632 Accuracy 0.7340\n",
            "Epoch 28 Batch 300 Loss 0.9773 Accuracy 0.7307\n",
            "Epoch 28 Batch 350 Loss 0.9880 Accuracy 0.7279\n",
            "Epoch 28 Loss 0.9871 Accuracy 0.7280\n",
            "Time taken for 1 epoch: 62.81 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.8020 Accuracy 0.7790\n",
            "Epoch 29 Batch 50 Loss 0.8456 Accuracy 0.7616\n",
            "Epoch 29 Batch 100 Loss 0.8748 Accuracy 0.7543\n",
            "Epoch 29 Batch 150 Loss 0.8875 Accuracy 0.7501\n",
            "Epoch 29 Batch 200 Loss 0.8995 Accuracy 0.7471\n",
            "Epoch 29 Batch 250 Loss 0.9115 Accuracy 0.7441\n",
            "Epoch 29 Batch 300 Loss 0.9232 Accuracy 0.7413\n",
            "Epoch 29 Batch 350 Loss 0.9405 Accuracy 0.7372\n",
            "Epoch 29 Loss 0.9423 Accuracy 0.7369\n",
            "Time taken for 1 epoch: 62.39 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.8564 Accuracy 0.7593\n",
            "Epoch 30 Batch 50 Loss 0.8281 Accuracy 0.7633\n",
            "Epoch 30 Batch 100 Loss 0.8400 Accuracy 0.7625\n",
            "Epoch 30 Batch 150 Loss 0.8446 Accuracy 0.7616\n",
            "Epoch 30 Batch 200 Loss 0.8621 Accuracy 0.7565\n",
            "Epoch 30 Batch 250 Loss 0.8753 Accuracy 0.7529\n",
            "Epoch 30 Batch 300 Loss 0.8863 Accuracy 0.7503\n",
            "Epoch 30 Batch 350 Loss 0.8927 Accuracy 0.7487\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-6\n",
            "Epoch 30 Loss 0.8943 Accuracy 0.7483\n",
            "Time taken for 1 epoch: 65.86 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.7989 Accuracy 0.7741\n",
            "Epoch 31 Batch 50 Loss 0.7494 Accuracy 0.7816\n",
            "Epoch 31 Batch 100 Loss 0.7667 Accuracy 0.7770\n",
            "Epoch 31 Batch 150 Loss 0.7946 Accuracy 0.7714\n",
            "Epoch 31 Batch 200 Loss 0.8107 Accuracy 0.7671\n",
            "Epoch 31 Batch 250 Loss 0.8281 Accuracy 0.7629\n",
            "Epoch 31 Batch 300 Loss 0.8397 Accuracy 0.7605\n",
            "Epoch 31 Batch 350 Loss 0.8481 Accuracy 0.7586\n",
            "Epoch 31 Loss 0.8488 Accuracy 0.7585\n",
            "Time taken for 1 epoch: 62.70 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.6419 Accuracy 0.7978\n",
            "Epoch 32 Batch 50 Loss 0.7367 Accuracy 0.7864\n",
            "Epoch 32 Batch 100 Loss 0.7342 Accuracy 0.7872\n",
            "Epoch 32 Batch 150 Loss 0.7566 Accuracy 0.7816\n",
            "Epoch 32 Batch 200 Loss 0.7716 Accuracy 0.7775\n",
            "Epoch 32 Batch 250 Loss 0.7826 Accuracy 0.7742\n",
            "Epoch 32 Batch 300 Loss 0.7970 Accuracy 0.7706\n",
            "Epoch 32 Batch 350 Loss 0.8057 Accuracy 0.7686\n",
            "Epoch 32 Loss 0.8065 Accuracy 0.7683\n",
            "Time taken for 1 epoch: 62.77 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.8484 Accuracy 0.7572\n",
            "Epoch 33 Batch 50 Loss 0.7042 Accuracy 0.7971\n",
            "Epoch 33 Batch 100 Loss 0.7081 Accuracy 0.7957\n",
            "Epoch 33 Batch 150 Loss 0.7187 Accuracy 0.7925\n",
            "Epoch 33 Batch 200 Loss 0.7378 Accuracy 0.7880\n",
            "Epoch 33 Batch 250 Loss 0.7539 Accuracy 0.7838\n",
            "Epoch 33 Batch 300 Loss 0.7707 Accuracy 0.7791\n",
            "Epoch 33 Batch 350 Loss 0.7822 Accuracy 0.7759\n",
            "Epoch 33 Loss 0.7827 Accuracy 0.7757\n",
            "Time taken for 1 epoch: 62.77 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.6711 Accuracy 0.8198\n",
            "Epoch 34 Batch 50 Loss 0.6681 Accuracy 0.8055\n",
            "Epoch 34 Batch 100 Loss 0.6851 Accuracy 0.8011\n",
            "Epoch 34 Batch 150 Loss 0.7041 Accuracy 0.7958\n",
            "Epoch 34 Batch 200 Loss 0.7120 Accuracy 0.7935\n",
            "Epoch 34 Batch 250 Loss 0.7248 Accuracy 0.7902\n",
            "Epoch 34 Batch 300 Loss 0.7364 Accuracy 0.7870\n",
            "Epoch 34 Batch 350 Loss 0.7451 Accuracy 0.7846\n",
            "Epoch 34 Loss 0.7466 Accuracy 0.7843\n",
            "Time taken for 1 epoch: 62.13 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.5125 Accuracy 0.8370\n",
            "Epoch 35 Batch 50 Loss 0.6214 Accuracy 0.8180\n",
            "Epoch 35 Batch 100 Loss 0.6401 Accuracy 0.8126\n",
            "Epoch 35 Batch 150 Loss 0.6590 Accuracy 0.8069\n",
            "Epoch 35 Batch 200 Loss 0.6806 Accuracy 0.8005\n",
            "Epoch 35 Batch 250 Loss 0.6926 Accuracy 0.7974\n",
            "Epoch 35 Batch 300 Loss 0.7010 Accuracy 0.7950\n",
            "Epoch 35 Batch 350 Loss 0.7112 Accuracy 0.7922\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-7\n",
            "Epoch 35 Loss 0.7128 Accuracy 0.7917\n",
            "Time taken for 1 epoch: 65.55 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.6080 Accuracy 0.8256\n",
            "Epoch 36 Batch 50 Loss 0.5963 Accuracy 0.8217\n",
            "Epoch 36 Batch 100 Loss 0.6130 Accuracy 0.8180\n",
            "Epoch 36 Batch 150 Loss 0.6247 Accuracy 0.8144\n",
            "Epoch 36 Batch 200 Loss 0.6393 Accuracy 0.8109\n",
            "Epoch 36 Batch 250 Loss 0.6490 Accuracy 0.8084\n",
            "Epoch 36 Batch 300 Loss 0.6595 Accuracy 0.8050\n",
            "Epoch 36 Batch 350 Loss 0.6657 Accuracy 0.8034\n",
            "Epoch 36 Loss 0.6661 Accuracy 0.8032\n",
            "Time taken for 1 epoch: 62.34 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.5459 Accuracy 0.8404\n",
            "Epoch 37 Batch 50 Loss 0.5586 Accuracy 0.8354\n",
            "Epoch 37 Batch 100 Loss 0.5760 Accuracy 0.8297\n",
            "Epoch 37 Batch 150 Loss 0.5952 Accuracy 0.8244\n",
            "Epoch 37 Batch 200 Loss 0.6056 Accuracy 0.8211\n",
            "Epoch 37 Batch 250 Loss 0.6184 Accuracy 0.8177\n",
            "Epoch 37 Batch 300 Loss 0.6301 Accuracy 0.8145\n",
            "Epoch 37 Batch 350 Loss 0.6391 Accuracy 0.8120\n",
            "Epoch 37 Loss 0.6396 Accuracy 0.8120\n",
            "Time taken for 1 epoch: 62.80 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.6986 Accuracy 0.7775\n",
            "Epoch 38 Batch 50 Loss 0.5617 Accuracy 0.8300\n",
            "Epoch 38 Batch 100 Loss 0.5639 Accuracy 0.8307\n",
            "Epoch 38 Batch 150 Loss 0.5744 Accuracy 0.8278\n",
            "Epoch 38 Batch 200 Loss 0.5852 Accuracy 0.8249\n",
            "Epoch 38 Batch 250 Loss 0.5935 Accuracy 0.8221\n",
            "Epoch 38 Batch 300 Loss 0.6033 Accuracy 0.8195\n",
            "Epoch 38 Batch 350 Loss 0.6128 Accuracy 0.8170\n",
            "Epoch 38 Loss 0.6134 Accuracy 0.8170\n",
            "Time taken for 1 epoch: 62.56 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.5393 Accuracy 0.8394\n",
            "Epoch 39 Batch 50 Loss 0.5109 Accuracy 0.8476\n",
            "Epoch 39 Batch 100 Loss 0.5303 Accuracy 0.8427\n",
            "Epoch 39 Batch 150 Loss 0.5366 Accuracy 0.8410\n",
            "Epoch 39 Batch 200 Loss 0.5500 Accuracy 0.8365\n",
            "Epoch 39 Batch 250 Loss 0.5624 Accuracy 0.8325\n",
            "Epoch 39 Batch 300 Loss 0.5743 Accuracy 0.8289\n",
            "Epoch 39 Batch 350 Loss 0.5871 Accuracy 0.8254\n",
            "Epoch 39 Loss 0.5898 Accuracy 0.8247\n",
            "Time taken for 1 epoch: 63.16 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.5641 Accuracy 0.8399\n",
            "Epoch 40 Batch 50 Loss 0.5135 Accuracy 0.8453\n",
            "Epoch 40 Batch 100 Loss 0.5151 Accuracy 0.8450\n",
            "Epoch 40 Batch 150 Loss 0.5254 Accuracy 0.8423\n",
            "Epoch 40 Batch 200 Loss 0.5383 Accuracy 0.8384\n",
            "Epoch 40 Batch 250 Loss 0.5450 Accuracy 0.8362\n",
            "Epoch 40 Batch 300 Loss 0.5513 Accuracy 0.8344\n",
            "Epoch 40 Batch 350 Loss 0.5607 Accuracy 0.8317\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-8\n",
            "Epoch 40 Loss 0.5626 Accuracy 0.8311\n",
            "Time taken for 1 epoch: 65.36 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.4223 Accuracy 0.8630\n",
            "Epoch 41 Batch 50 Loss 0.4739 Accuracy 0.8585\n",
            "Epoch 41 Batch 100 Loss 0.4896 Accuracy 0.8528\n",
            "Epoch 41 Batch 150 Loss 0.5063 Accuracy 0.8478\n",
            "Epoch 41 Batch 200 Loss 0.5131 Accuracy 0.8451\n",
            "Epoch 41 Batch 250 Loss 0.5220 Accuracy 0.8426\n",
            "Epoch 41 Batch 300 Loss 0.5306 Accuracy 0.8400\n",
            "Epoch 41 Batch 350 Loss 0.5399 Accuracy 0.8374\n",
            "Epoch 41 Loss 0.5409 Accuracy 0.8371\n",
            "Time taken for 1 epoch: 63.23 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.4400 Accuracy 0.8631\n",
            "Epoch 42 Batch 50 Loss 0.4557 Accuracy 0.8619\n",
            "Epoch 42 Batch 100 Loss 0.4711 Accuracy 0.8576\n",
            "Epoch 42 Batch 150 Loss 0.4831 Accuracy 0.8541\n",
            "Epoch 42 Batch 200 Loss 0.4915 Accuracy 0.8518\n",
            "Epoch 42 Batch 250 Loss 0.5010 Accuracy 0.8489\n",
            "Epoch 42 Batch 300 Loss 0.5126 Accuracy 0.8455\n",
            "Epoch 42 Batch 350 Loss 0.5211 Accuracy 0.8431\n",
            "Epoch 42 Loss 0.5218 Accuracy 0.8429\n",
            "Time taken for 1 epoch: 62.04 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.4281 Accuracy 0.8647\n",
            "Epoch 43 Batch 50 Loss 0.4365 Accuracy 0.8659\n",
            "Epoch 43 Batch 100 Loss 0.4525 Accuracy 0.8623\n",
            "Epoch 43 Batch 150 Loss 0.4640 Accuracy 0.8589\n",
            "Epoch 43 Batch 200 Loss 0.4773 Accuracy 0.8550\n",
            "Epoch 43 Batch 250 Loss 0.4879 Accuracy 0.8518\n",
            "Epoch 43 Batch 300 Loss 0.4950 Accuracy 0.8497\n",
            "Epoch 43 Batch 350 Loss 0.5050 Accuracy 0.8468\n",
            "Epoch 43 Loss 0.5066 Accuracy 0.8463\n",
            "Time taken for 1 epoch: 62.25 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.4268 Accuracy 0.8726\n",
            "Epoch 44 Batch 50 Loss 0.4472 Accuracy 0.8654\n",
            "Epoch 44 Batch 100 Loss 0.4499 Accuracy 0.8643\n",
            "Epoch 44 Batch 150 Loss 0.4559 Accuracy 0.8617\n",
            "Epoch 44 Batch 200 Loss 0.4658 Accuracy 0.8587\n",
            "Epoch 44 Batch 250 Loss 0.4751 Accuracy 0.8560\n",
            "Epoch 44 Batch 300 Loss 0.4796 Accuracy 0.8544\n",
            "Epoch 44 Batch 350 Loss 0.4846 Accuracy 0.8527\n",
            "Epoch 44 Loss 0.4858 Accuracy 0.8521\n",
            "Time taken for 1 epoch: 62.60 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.3970 Accuracy 0.8782\n",
            "Epoch 45 Batch 50 Loss 0.4122 Accuracy 0.8734\n",
            "Epoch 45 Batch 100 Loss 0.4317 Accuracy 0.8693\n",
            "Epoch 45 Batch 150 Loss 0.4407 Accuracy 0.8665\n",
            "Epoch 45 Batch 200 Loss 0.4515 Accuracy 0.8634\n",
            "Epoch 45 Batch 250 Loss 0.4641 Accuracy 0.8600\n",
            "Epoch 45 Batch 300 Loss 0.4737 Accuracy 0.8571\n",
            "Epoch 45 Batch 350 Loss 0.4786 Accuracy 0.8555\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/train/ckpt-9\n",
            "Epoch 45 Loss 0.4790 Accuracy 0.8553\n",
            "Time taken for 1 epoch: 64.95 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.4287 Accuracy 0.8704\n",
            "Epoch 46 Batch 50 Loss 0.4011 Accuracy 0.8790\n",
            "Epoch 46 Batch 100 Loss 0.4093 Accuracy 0.8756\n",
            "Epoch 46 Batch 150 Loss 0.4244 Accuracy 0.8714\n",
            "Epoch 46 Batch 200 Loss 0.4345 Accuracy 0.8677\n",
            "Epoch 46 Batch 250 Loss 0.4447 Accuracy 0.8649\n",
            "Epoch 46 Batch 300 Loss 0.4527 Accuracy 0.8622\n",
            "Epoch 46 Batch 350 Loss 0.4600 Accuracy 0.8598\n",
            "Epoch 46 Loss 0.4607 Accuracy 0.8596\n",
            "Time taken for 1 epoch: 63.26 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.4074 Accuracy 0.8750\n",
            "Epoch 47 Batch 50 Loss 0.4045 Accuracy 0.8763\n",
            "Epoch 47 Batch 100 Loss 0.4064 Accuracy 0.8757\n",
            "Epoch 47 Batch 150 Loss 0.4142 Accuracy 0.8730\n",
            "Epoch 47 Batch 200 Loss 0.4248 Accuracy 0.8701\n",
            "Epoch 47 Batch 250 Loss 0.4287 Accuracy 0.8690\n",
            "Epoch 47 Batch 300 Loss 0.4363 Accuracy 0.8668\n",
            "Epoch 47 Batch 350 Loss 0.4444 Accuracy 0.8643\n",
            "Epoch 47 Loss 0.4452 Accuracy 0.8641\n",
            "Time taken for 1 epoch: 62.11 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.3103 Accuracy 0.9031\n",
            "Epoch 48 Batch 50 Loss 0.3830 Accuracy 0.8824\n",
            "Epoch 48 Batch 100 Loss 0.3836 Accuracy 0.8827\n",
            "Epoch 48 Batch 150 Loss 0.4002 Accuracy 0.8775\n",
            "Epoch 48 Batch 200 Loss 0.4069 Accuracy 0.8759\n",
            "Epoch 48 Batch 250 Loss 0.4130 Accuracy 0.8737\n",
            "Epoch 48 Batch 300 Loss 0.4207 Accuracy 0.8716\n",
            "Epoch 48 Batch 350 Loss 0.4272 Accuracy 0.8694\n",
            "Epoch 48 Loss 0.4281 Accuracy 0.8692\n",
            "Time taken for 1 epoch: 62.72 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.3593 Accuracy 0.8916\n",
            "Epoch 49 Batch 50 Loss 0.3835 Accuracy 0.8822\n",
            "Epoch 49 Batch 100 Loss 0.3878 Accuracy 0.8810\n",
            "Epoch 49 Batch 150 Loss 0.3907 Accuracy 0.8803\n",
            "Epoch 49 Batch 200 Loss 0.3944 Accuracy 0.8792\n",
            "Epoch 49 Batch 250 Loss 0.3971 Accuracy 0.8783\n",
            "Epoch 49 Batch 300 Loss 0.4034 Accuracy 0.8764\n",
            "Epoch 49 Batch 350 Loss 0.4105 Accuracy 0.8744\n",
            "Epoch 49 Loss 0.4116 Accuracy 0.8741\n",
            "Time taken for 1 epoch: 62.59 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.3320 Accuracy 0.8974\n",
            "Epoch 50 Batch 50 Loss 0.3619 Accuracy 0.8876\n",
            "Epoch 50 Batch 100 Loss 0.3694 Accuracy 0.8865\n",
            "Epoch 50 Batch 150 Loss 0.3751 Accuracy 0.8843\n",
            "Epoch 50 Batch 200 Loss 0.3831 Accuracy 0.8820\n",
            "Epoch 50 Batch 250 Loss 0.3903 Accuracy 0.8796\n",
            "Epoch 50 Batch 300 Loss 0.3957 Accuracy 0.8779\n",
            "Epoch 50 Batch 350 Loss 0.4021 Accuracy 0.8762\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-10\n",
            "Epoch 50 Loss 0.4022 Accuracy 0.8761\n",
            "Time taken for 1 epoch: 65.82 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.3597 Accuracy 0.8853\n",
            "Epoch 51 Batch 50 Loss 0.3384 Accuracy 0.8953\n",
            "Epoch 51 Batch 100 Loss 0.3500 Accuracy 0.8925\n",
            "Epoch 51 Batch 150 Loss 0.3541 Accuracy 0.8912\n",
            "Epoch 51 Batch 200 Loss 0.3618 Accuracy 0.8890\n",
            "Epoch 51 Batch 250 Loss 0.3721 Accuracy 0.8860\n",
            "Epoch 51 Batch 300 Loss 0.3797 Accuracy 0.8835\n",
            "Epoch 51 Batch 350 Loss 0.3844 Accuracy 0.8818\n",
            "Epoch 51 Loss 0.3846 Accuracy 0.8817\n",
            "Time taken for 1 epoch: 62.29 secs\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.2751 Accuracy 0.9299\n",
            "Epoch 52 Batch 50 Loss 0.3291 Accuracy 0.8986\n",
            "Epoch 52 Batch 100 Loss 0.3301 Accuracy 0.8976\n",
            "Epoch 52 Batch 150 Loss 0.3422 Accuracy 0.8944\n",
            "Epoch 52 Batch 200 Loss 0.3518 Accuracy 0.8913\n",
            "Epoch 52 Batch 250 Loss 0.3595 Accuracy 0.8884\n",
            "Epoch 52 Batch 300 Loss 0.3659 Accuracy 0.8865\n",
            "Epoch 52 Batch 350 Loss 0.3685 Accuracy 0.8859\n",
            "Epoch 52 Loss 0.3691 Accuracy 0.8858\n",
            "Time taken for 1 epoch: 62.45 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.2891 Accuracy 0.9029\n",
            "Epoch 53 Batch 50 Loss 0.3219 Accuracy 0.8994\n",
            "Epoch 53 Batch 100 Loss 0.3311 Accuracy 0.8975\n",
            "Epoch 53 Batch 150 Loss 0.3355 Accuracy 0.8964\n",
            "Epoch 53 Batch 200 Loss 0.3443 Accuracy 0.8936\n",
            "Epoch 53 Batch 250 Loss 0.3486 Accuracy 0.8922\n",
            "Epoch 53 Batch 300 Loss 0.3531 Accuracy 0.8908\n",
            "Epoch 53 Batch 350 Loss 0.3603 Accuracy 0.8886\n",
            "Epoch 53 Loss 0.3607 Accuracy 0.8885\n",
            "Time taken for 1 epoch: 62.87 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.3627 Accuracy 0.8894\n",
            "Epoch 54 Batch 50 Loss 0.3063 Accuracy 0.9058\n",
            "Epoch 54 Batch 100 Loss 0.3173 Accuracy 0.9025\n",
            "Epoch 54 Batch 150 Loss 0.3247 Accuracy 0.8998\n",
            "Epoch 54 Batch 200 Loss 0.3344 Accuracy 0.8967\n",
            "Epoch 54 Batch 250 Loss 0.3383 Accuracy 0.8952\n",
            "Epoch 54 Batch 300 Loss 0.3440 Accuracy 0.8933\n",
            "Epoch 54 Batch 350 Loss 0.3493 Accuracy 0.8918\n",
            "Epoch 54 Loss 0.3500 Accuracy 0.8916\n",
            "Time taken for 1 epoch: 62.65 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.2359 Accuracy 0.9272\n",
            "Epoch 55 Batch 50 Loss 0.2989 Accuracy 0.9087\n",
            "Epoch 55 Batch 100 Loss 0.3045 Accuracy 0.9060\n",
            "Epoch 55 Batch 150 Loss 0.3158 Accuracy 0.9025\n",
            "Epoch 55 Batch 200 Loss 0.3215 Accuracy 0.9002\n",
            "Epoch 55 Batch 250 Loss 0.3268 Accuracy 0.8980\n",
            "Epoch 55 Batch 300 Loss 0.3321 Accuracy 0.8964\n",
            "Epoch 55 Batch 350 Loss 0.3376 Accuracy 0.8949\n",
            "Saving checkpoint for epoch 55 at ./checkpoints/train/ckpt-11\n",
            "Epoch 55 Loss 0.3386 Accuracy 0.8946\n",
            "Time taken for 1 epoch: 65.38 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.2477 Accuracy 0.9186\n",
            "Epoch 56 Batch 50 Loss 0.2990 Accuracy 0.9069\n",
            "Epoch 56 Batch 100 Loss 0.3056 Accuracy 0.9048\n",
            "Epoch 56 Batch 150 Loss 0.3111 Accuracy 0.9030\n",
            "Epoch 56 Batch 200 Loss 0.3179 Accuracy 0.9011\n",
            "Epoch 56 Batch 250 Loss 0.3224 Accuracy 0.8998\n",
            "Epoch 56 Batch 300 Loss 0.3275 Accuracy 0.8984\n",
            "Epoch 56 Batch 350 Loss 0.3331 Accuracy 0.8965\n",
            "Epoch 56 Loss 0.3335 Accuracy 0.8963\n",
            "Time taken for 1 epoch: 62.45 secs\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.3583 Accuracy 0.8903\n",
            "Epoch 57 Batch 50 Loss 0.2970 Accuracy 0.9084\n",
            "Epoch 57 Batch 100 Loss 0.2971 Accuracy 0.9079\n",
            "Epoch 57 Batch 150 Loss 0.3005 Accuracy 0.9069\n",
            "Epoch 57 Batch 200 Loss 0.3047 Accuracy 0.9053\n",
            "Epoch 57 Batch 250 Loss 0.3117 Accuracy 0.9034\n",
            "Epoch 57 Batch 300 Loss 0.3162 Accuracy 0.9018\n",
            "Epoch 57 Batch 350 Loss 0.3191 Accuracy 0.9009\n",
            "Epoch 57 Loss 0.3195 Accuracy 0.9008\n",
            "Time taken for 1 epoch: 62.81 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.2762 Accuracy 0.9192\n",
            "Epoch 58 Batch 50 Loss 0.2720 Accuracy 0.9134\n",
            "Epoch 58 Batch 100 Loss 0.2810 Accuracy 0.9116\n",
            "Epoch 58 Batch 150 Loss 0.2852 Accuracy 0.9102\n",
            "Epoch 58 Batch 200 Loss 0.2941 Accuracy 0.9079\n",
            "Epoch 58 Batch 250 Loss 0.3013 Accuracy 0.9056\n",
            "Epoch 58 Batch 300 Loss 0.3059 Accuracy 0.9040\n",
            "Epoch 58 Batch 350 Loss 0.3111 Accuracy 0.9023\n",
            "Epoch 58 Loss 0.3116 Accuracy 0.9022\n",
            "Time taken for 1 epoch: 62.63 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.2388 Accuracy 0.9313\n",
            "Epoch 59 Batch 50 Loss 0.2687 Accuracy 0.9167\n",
            "Epoch 59 Batch 100 Loss 0.2821 Accuracy 0.9127\n",
            "Epoch 59 Batch 150 Loss 0.2833 Accuracy 0.9126\n",
            "Epoch 59 Batch 200 Loss 0.2921 Accuracy 0.9101\n",
            "Epoch 59 Batch 250 Loss 0.2962 Accuracy 0.9088\n",
            "Epoch 59 Batch 300 Loss 0.2996 Accuracy 0.9077\n",
            "Epoch 59 Batch 350 Loss 0.3014 Accuracy 0.9071\n",
            "Epoch 59 Loss 0.3020 Accuracy 0.9068\n",
            "Time taken for 1 epoch: 62.56 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.2327 Accuracy 0.9260\n",
            "Epoch 60 Batch 50 Loss 0.2649 Accuracy 0.9179\n",
            "Epoch 60 Batch 100 Loss 0.2737 Accuracy 0.9148\n",
            "Epoch 60 Batch 150 Loss 0.2780 Accuracy 0.9133\n",
            "Epoch 60 Batch 200 Loss 0.2825 Accuracy 0.9121\n",
            "Epoch 60 Batch 250 Loss 0.2877 Accuracy 0.9104\n",
            "Epoch 60 Batch 300 Loss 0.2907 Accuracy 0.9097\n",
            "Epoch 60 Batch 350 Loss 0.2965 Accuracy 0.9080\n",
            "Saving checkpoint for epoch 60 at ./checkpoints/train/ckpt-12\n",
            "Epoch 60 Loss 0.2973 Accuracy 0.9078\n",
            "Time taken for 1 epoch: 65.89 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.1786 Accuracy 0.9491\n",
            "Epoch 61 Batch 50 Loss 0.2556 Accuracy 0.9214\n",
            "Epoch 61 Batch 100 Loss 0.2628 Accuracy 0.9196\n",
            "Epoch 61 Batch 150 Loss 0.2727 Accuracy 0.9164\n",
            "Epoch 61 Batch 200 Loss 0.2750 Accuracy 0.9151\n",
            "Epoch 61 Batch 250 Loss 0.2771 Accuracy 0.9146\n",
            "Epoch 61 Batch 300 Loss 0.2808 Accuracy 0.9132\n",
            "Epoch 61 Batch 350 Loss 0.2865 Accuracy 0.9114\n",
            "Epoch 61 Loss 0.2869 Accuracy 0.9113\n",
            "Time taken for 1 epoch: 62.20 secs\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.2278 Accuracy 0.9334\n",
            "Epoch 62 Batch 50 Loss 0.2600 Accuracy 0.9198\n",
            "Epoch 62 Batch 100 Loss 0.2584 Accuracy 0.9208\n",
            "Epoch 62 Batch 150 Loss 0.2634 Accuracy 0.9187\n",
            "Epoch 62 Batch 200 Loss 0.2662 Accuracy 0.9177\n",
            "Epoch 62 Batch 250 Loss 0.2705 Accuracy 0.9165\n",
            "Epoch 62 Batch 300 Loss 0.2744 Accuracy 0.9152\n",
            "Epoch 62 Batch 350 Loss 0.2803 Accuracy 0.9134\n",
            "Epoch 62 Loss 0.2805 Accuracy 0.9132\n",
            "Time taken for 1 epoch: 62.50 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.1940 Accuracy 0.9366\n",
            "Epoch 63 Batch 50 Loss 0.2299 Accuracy 0.9268\n",
            "Epoch 63 Batch 100 Loss 0.2397 Accuracy 0.9250\n",
            "Epoch 63 Batch 150 Loss 0.2496 Accuracy 0.9226\n",
            "Epoch 63 Batch 200 Loss 0.2550 Accuracy 0.9208\n",
            "Epoch 63 Batch 250 Loss 0.2582 Accuracy 0.9201\n",
            "Epoch 63 Batch 300 Loss 0.2625 Accuracy 0.9190\n",
            "Epoch 63 Batch 350 Loss 0.2679 Accuracy 0.9173\n",
            "Epoch 63 Loss 0.2685 Accuracy 0.9171\n",
            "Time taken for 1 epoch: 61.96 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.2459 Accuracy 0.9374\n",
            "Epoch 64 Batch 50 Loss 0.2365 Accuracy 0.9273\n",
            "Epoch 64 Batch 100 Loss 0.2381 Accuracy 0.9270\n",
            "Epoch 64 Batch 150 Loss 0.2462 Accuracy 0.9245\n",
            "Epoch 64 Batch 200 Loss 0.2507 Accuracy 0.9229\n",
            "Epoch 64 Batch 250 Loss 0.2581 Accuracy 0.9204\n",
            "Epoch 64 Batch 300 Loss 0.2620 Accuracy 0.9191\n",
            "Epoch 64 Batch 350 Loss 0.2658 Accuracy 0.9180\n",
            "Epoch 64 Loss 0.2666 Accuracy 0.9177\n",
            "Time taken for 1 epoch: 62.86 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.2078 Accuracy 0.9347\n",
            "Epoch 65 Batch 50 Loss 0.2357 Accuracy 0.9263\n",
            "Epoch 65 Batch 100 Loss 0.2406 Accuracy 0.9245\n",
            "Epoch 65 Batch 150 Loss 0.2459 Accuracy 0.9230\n",
            "Epoch 65 Batch 200 Loss 0.2491 Accuracy 0.9223\n",
            "Epoch 65 Batch 250 Loss 0.2541 Accuracy 0.9205\n",
            "Epoch 65 Batch 300 Loss 0.2581 Accuracy 0.9192\n",
            "Epoch 65 Batch 350 Loss 0.2613 Accuracy 0.9183\n",
            "Saving checkpoint for epoch 65 at ./checkpoints/train/ckpt-13\n",
            "Epoch 65 Loss 0.2616 Accuracy 0.9182\n",
            "Time taken for 1 epoch: 65.05 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.2115 Accuracy 0.9331\n",
            "Epoch 66 Batch 50 Loss 0.2244 Accuracy 0.9302\n",
            "Epoch 66 Batch 100 Loss 0.2309 Accuracy 0.9285\n",
            "Epoch 66 Batch 150 Loss 0.2355 Accuracy 0.9269\n",
            "Epoch 66 Batch 200 Loss 0.2387 Accuracy 0.9257\n",
            "Epoch 66 Batch 250 Loss 0.2443 Accuracy 0.9239\n",
            "Epoch 66 Batch 300 Loss 0.2485 Accuracy 0.9227\n",
            "Epoch 66 Batch 350 Loss 0.2534 Accuracy 0.9212\n",
            "Epoch 66 Loss 0.2539 Accuracy 0.9210\n",
            "Time taken for 1 epoch: 62.58 secs\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.1664 Accuracy 0.9660\n",
            "Epoch 67 Batch 50 Loss 0.2205 Accuracy 0.9311\n",
            "Epoch 67 Batch 100 Loss 0.2282 Accuracy 0.9291\n",
            "Epoch 67 Batch 150 Loss 0.2303 Accuracy 0.9281\n",
            "Epoch 67 Batch 200 Loss 0.2385 Accuracy 0.9261\n",
            "Epoch 67 Batch 250 Loss 0.2466 Accuracy 0.9235\n",
            "Epoch 67 Batch 300 Loss 0.2529 Accuracy 0.9218\n",
            "Epoch 67 Batch 350 Loss 0.2571 Accuracy 0.9205\n",
            "Epoch 67 Loss 0.2576 Accuracy 0.9203\n",
            "Time taken for 1 epoch: 62.28 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.3388 Accuracy 0.9036\n",
            "Epoch 68 Batch 50 Loss 0.2254 Accuracy 0.9305\n",
            "Epoch 68 Batch 100 Loss 0.2239 Accuracy 0.9300\n",
            "Epoch 68 Batch 150 Loss 0.2262 Accuracy 0.9293\n",
            "Epoch 68 Batch 200 Loss 0.2319 Accuracy 0.9274\n",
            "Epoch 68 Batch 250 Loss 0.2344 Accuracy 0.9266\n",
            "Epoch 68 Batch 300 Loss 0.2386 Accuracy 0.9253\n",
            "Epoch 68 Batch 350 Loss 0.2428 Accuracy 0.9242\n",
            "Epoch 68 Loss 0.2431 Accuracy 0.9241\n",
            "Time taken for 1 epoch: 62.28 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.1732 Accuracy 0.9518\n",
            "Epoch 69 Batch 50 Loss 0.2026 Accuracy 0.9390\n",
            "Epoch 69 Batch 100 Loss 0.2098 Accuracy 0.9361\n",
            "Epoch 69 Batch 150 Loss 0.2166 Accuracy 0.9335\n",
            "Epoch 69 Batch 200 Loss 0.2223 Accuracy 0.9311\n",
            "Epoch 69 Batch 250 Loss 0.2296 Accuracy 0.9289\n",
            "Epoch 69 Batch 300 Loss 0.2336 Accuracy 0.9275\n",
            "Epoch 69 Batch 350 Loss 0.2388 Accuracy 0.9259\n",
            "Epoch 69 Loss 0.2394 Accuracy 0.9257\n",
            "Time taken for 1 epoch: 62.69 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.1947 Accuracy 0.9335\n",
            "Epoch 70 Batch 50 Loss 0.2099 Accuracy 0.9359\n",
            "Epoch 70 Batch 100 Loss 0.2102 Accuracy 0.9352\n",
            "Epoch 70 Batch 150 Loss 0.2169 Accuracy 0.9335\n",
            "Epoch 70 Batch 200 Loss 0.2200 Accuracy 0.9320\n",
            "Epoch 70 Batch 250 Loss 0.2238 Accuracy 0.9305\n",
            "Epoch 70 Batch 300 Loss 0.2269 Accuracy 0.9296\n",
            "Epoch 70 Batch 350 Loss 0.2305 Accuracy 0.9281\n",
            "Saving checkpoint for epoch 70 at ./checkpoints/train/ckpt-14\n",
            "Epoch 70 Loss 0.2308 Accuracy 0.9280\n",
            "Time taken for 1 epoch: 65.29 secs\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.1632 Accuracy 0.9447\n",
            "Epoch 71 Batch 50 Loss 0.2078 Accuracy 0.9358\n",
            "Epoch 71 Batch 100 Loss 0.2078 Accuracy 0.9353\n",
            "Epoch 71 Batch 150 Loss 0.2120 Accuracy 0.9343\n",
            "Epoch 71 Batch 200 Loss 0.2162 Accuracy 0.9328\n",
            "Epoch 71 Batch 250 Loss 0.2195 Accuracy 0.9320\n",
            "Epoch 71 Batch 300 Loss 0.2231 Accuracy 0.9306\n",
            "Epoch 71 Batch 350 Loss 0.2256 Accuracy 0.9298\n",
            "Epoch 71 Loss 0.2262 Accuracy 0.9296\n",
            "Time taken for 1 epoch: 62.66 secs\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.2107 Accuracy 0.9375\n",
            "Epoch 72 Batch 50 Loss 0.2019 Accuracy 0.9375\n",
            "Epoch 72 Batch 100 Loss 0.2026 Accuracy 0.9368\n",
            "Epoch 72 Batch 150 Loss 0.2086 Accuracy 0.9352\n",
            "Epoch 72 Batch 200 Loss 0.2149 Accuracy 0.9329\n",
            "Epoch 72 Batch 250 Loss 0.2153 Accuracy 0.9330\n",
            "Epoch 72 Batch 300 Loss 0.2174 Accuracy 0.9323\n",
            "Epoch 72 Batch 350 Loss 0.2200 Accuracy 0.9315\n",
            "Epoch 72 Loss 0.2200 Accuracy 0.9314\n",
            "Time taken for 1 epoch: 62.05 secs\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.1825 Accuracy 0.9399\n",
            "Epoch 73 Batch 50 Loss 0.1970 Accuracy 0.9378\n",
            "Epoch 73 Batch 100 Loss 0.1996 Accuracy 0.9380\n",
            "Epoch 73 Batch 150 Loss 0.2046 Accuracy 0.9364\n",
            "Epoch 73 Batch 200 Loss 0.2089 Accuracy 0.9350\n",
            "Epoch 73 Batch 250 Loss 0.2109 Accuracy 0.9344\n",
            "Epoch 73 Batch 300 Loss 0.2140 Accuracy 0.9333\n",
            "Epoch 73 Batch 350 Loss 0.2174 Accuracy 0.9322\n",
            "Epoch 73 Loss 0.2180 Accuracy 0.9320\n",
            "Time taken for 1 epoch: 62.96 secs\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.1645 Accuracy 0.9424\n",
            "Epoch 74 Batch 50 Loss 0.1915 Accuracy 0.9412\n",
            "Epoch 74 Batch 100 Loss 0.1921 Accuracy 0.9406\n",
            "Epoch 74 Batch 150 Loss 0.1962 Accuracy 0.9392\n",
            "Epoch 74 Batch 200 Loss 0.2014 Accuracy 0.9376\n",
            "Epoch 74 Batch 250 Loss 0.2053 Accuracy 0.9363\n",
            "Epoch 74 Batch 300 Loss 0.2074 Accuracy 0.9357\n",
            "Epoch 74 Batch 350 Loss 0.2103 Accuracy 0.9347\n",
            "Epoch 74 Loss 0.2107 Accuracy 0.9345\n",
            "Time taken for 1 epoch: 62.29 secs\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.1194 Accuracy 0.9577\n",
            "Epoch 75 Batch 50 Loss 0.1909 Accuracy 0.9407\n",
            "Epoch 75 Batch 100 Loss 0.1935 Accuracy 0.9396\n",
            "Epoch 75 Batch 150 Loss 0.2025 Accuracy 0.9367\n",
            "Epoch 75 Batch 200 Loss 0.2051 Accuracy 0.9354\n",
            "Epoch 75 Batch 250 Loss 0.2072 Accuracy 0.9347\n",
            "Epoch 75 Batch 300 Loss 0.2107 Accuracy 0.9338\n",
            "Epoch 75 Batch 350 Loss 0.2121 Accuracy 0.9334\n",
            "Saving checkpoint for epoch 75 at ./checkpoints/train/ckpt-15\n",
            "Epoch 75 Loss 0.2128 Accuracy 0.9331\n",
            "Time taken for 1 epoch: 65.36 secs\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.2420 Accuracy 0.9380\n",
            "Epoch 76 Batch 50 Loss 0.1980 Accuracy 0.9393\n",
            "Epoch 76 Batch 100 Loss 0.1952 Accuracy 0.9401\n",
            "Epoch 76 Batch 150 Loss 0.1976 Accuracy 0.9391\n",
            "Epoch 76 Batch 200 Loss 0.1997 Accuracy 0.9381\n",
            "Epoch 76 Batch 250 Loss 0.2017 Accuracy 0.9373\n",
            "Epoch 76 Batch 300 Loss 0.2038 Accuracy 0.9366\n",
            "Epoch 76 Batch 350 Loss 0.2064 Accuracy 0.9359\n",
            "Epoch 76 Loss 0.2064 Accuracy 0.9360\n",
            "Time taken for 1 epoch: 63.09 secs\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.1828 Accuracy 0.9401\n",
            "Epoch 77 Batch 50 Loss 0.1757 Accuracy 0.9458\n",
            "Epoch 77 Batch 100 Loss 0.1826 Accuracy 0.9441\n",
            "Epoch 77 Batch 150 Loss 0.1907 Accuracy 0.9414\n",
            "Epoch 77 Batch 200 Loss 0.1943 Accuracy 0.9403\n",
            "Epoch 77 Batch 250 Loss 0.1968 Accuracy 0.9394\n",
            "Epoch 77 Batch 300 Loss 0.1994 Accuracy 0.9386\n",
            "Epoch 77 Batch 350 Loss 0.2029 Accuracy 0.9373\n",
            "Epoch 77 Loss 0.2033 Accuracy 0.9371\n",
            "Time taken for 1 epoch: 62.20 secs\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.1662 Accuracy 0.9395\n",
            "Epoch 78 Batch 50 Loss 0.1825 Accuracy 0.9437\n",
            "Epoch 78 Batch 100 Loss 0.1825 Accuracy 0.9434\n",
            "Epoch 78 Batch 150 Loss 0.1843 Accuracy 0.9427\n",
            "Epoch 78 Batch 200 Loss 0.1885 Accuracy 0.9415\n",
            "Epoch 78 Batch 250 Loss 0.1920 Accuracy 0.9405\n",
            "Epoch 78 Batch 300 Loss 0.1929 Accuracy 0.9401\n",
            "Epoch 78 Batch 350 Loss 0.1961 Accuracy 0.9391\n",
            "Epoch 78 Loss 0.1968 Accuracy 0.9389\n",
            "Time taken for 1 epoch: 62.53 secs\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.1854 Accuracy 0.9512\n",
            "Epoch 79 Batch 50 Loss 0.1794 Accuracy 0.9442\n",
            "Epoch 79 Batch 100 Loss 0.1738 Accuracy 0.9461\n",
            "Epoch 79 Batch 150 Loss 0.1818 Accuracy 0.9436\n",
            "Epoch 79 Batch 200 Loss 0.1849 Accuracy 0.9424\n",
            "Epoch 79 Batch 250 Loss 0.1886 Accuracy 0.9414\n",
            "Epoch 79 Batch 300 Loss 0.1918 Accuracy 0.9406\n",
            "Epoch 79 Batch 350 Loss 0.1956 Accuracy 0.9393\n",
            "Epoch 79 Loss 0.1962 Accuracy 0.9390\n",
            "Time taken for 1 epoch: 62.13 secs\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.1760 Accuracy 0.9517\n",
            "Epoch 80 Batch 50 Loss 0.1734 Accuracy 0.9470\n",
            "Epoch 80 Batch 100 Loss 0.1751 Accuracy 0.9461\n",
            "Epoch 80 Batch 150 Loss 0.1786 Accuracy 0.9453\n",
            "Epoch 80 Batch 200 Loss 0.1822 Accuracy 0.9440\n",
            "Epoch 80 Batch 250 Loss 0.1860 Accuracy 0.9428\n",
            "Epoch 80 Batch 300 Loss 0.1887 Accuracy 0.9421\n",
            "Epoch 80 Batch 350 Loss 0.1904 Accuracy 0.9413\n",
            "Saving checkpoint for epoch 80 at ./checkpoints/train/ckpt-16\n",
            "Epoch 80 Loss 0.1907 Accuracy 0.9412\n",
            "Time taken for 1 epoch: 65.79 secs\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.1886 Accuracy 0.9327\n",
            "Epoch 81 Batch 50 Loss 0.1704 Accuracy 0.9480\n",
            "Epoch 81 Batch 100 Loss 0.1803 Accuracy 0.9459\n",
            "Epoch 81 Batch 150 Loss 0.1825 Accuracy 0.9442\n",
            "Epoch 81 Batch 200 Loss 0.1835 Accuracy 0.9436\n",
            "Epoch 81 Batch 250 Loss 0.1862 Accuracy 0.9425\n",
            "Epoch 81 Batch 300 Loss 0.1889 Accuracy 0.9416\n",
            "Epoch 81 Batch 350 Loss 0.1905 Accuracy 0.9409\n",
            "Epoch 81 Loss 0.1909 Accuracy 0.9407\n",
            "Time taken for 1 epoch: 62.37 secs\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.1543 Accuracy 0.9480\n",
            "Epoch 82 Batch 50 Loss 0.1712 Accuracy 0.9468\n",
            "Epoch 82 Batch 100 Loss 0.1717 Accuracy 0.9466\n",
            "Epoch 82 Batch 150 Loss 0.1724 Accuracy 0.9462\n",
            "Epoch 82 Batch 200 Loss 0.1767 Accuracy 0.9449\n",
            "Epoch 82 Batch 250 Loss 0.1804 Accuracy 0.9437\n",
            "Epoch 82 Batch 300 Loss 0.1827 Accuracy 0.9432\n",
            "Epoch 82 Batch 350 Loss 0.1848 Accuracy 0.9426\n",
            "Epoch 82 Loss 0.1850 Accuracy 0.9424\n",
            "Time taken for 1 epoch: 62.32 secs\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.2567 Accuracy 0.9198\n",
            "Epoch 83 Batch 50 Loss 0.1633 Accuracy 0.9503\n",
            "Epoch 83 Batch 100 Loss 0.1688 Accuracy 0.9486\n",
            "Epoch 83 Batch 150 Loss 0.1722 Accuracy 0.9472\n",
            "Epoch 83 Batch 200 Loss 0.1758 Accuracy 0.9455\n",
            "Epoch 83 Batch 250 Loss 0.1789 Accuracy 0.9445\n",
            "Epoch 83 Batch 300 Loss 0.1809 Accuracy 0.9439\n",
            "Epoch 83 Batch 350 Loss 0.1831 Accuracy 0.9432\n",
            "Epoch 83 Loss 0.1834 Accuracy 0.9431\n",
            "Time taken for 1 epoch: 62.71 secs\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.1574 Accuracy 0.9498\n",
            "Epoch 84 Batch 50 Loss 0.1638 Accuracy 0.9492\n",
            "Epoch 84 Batch 100 Loss 0.1655 Accuracy 0.9487\n",
            "Epoch 84 Batch 150 Loss 0.1674 Accuracy 0.9484\n",
            "Epoch 84 Batch 200 Loss 0.1681 Accuracy 0.9481\n",
            "Epoch 84 Batch 250 Loss 0.1721 Accuracy 0.9467\n",
            "Epoch 84 Batch 300 Loss 0.1740 Accuracy 0.9462\n",
            "Epoch 84 Batch 350 Loss 0.1755 Accuracy 0.9455\n",
            "Epoch 84 Loss 0.1763 Accuracy 0.9453\n",
            "Time taken for 1 epoch: 62.23 secs\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.1723 Accuracy 0.9483\n",
            "Epoch 85 Batch 50 Loss 0.1528 Accuracy 0.9538\n",
            "Epoch 85 Batch 100 Loss 0.1568 Accuracy 0.9526\n",
            "Epoch 85 Batch 150 Loss 0.1626 Accuracy 0.9503\n",
            "Epoch 85 Batch 200 Loss 0.1633 Accuracy 0.9497\n",
            "Epoch 85 Batch 250 Loss 0.1646 Accuracy 0.9491\n",
            "Epoch 85 Batch 300 Loss 0.1693 Accuracy 0.9475\n",
            "Epoch 85 Batch 350 Loss 0.1724 Accuracy 0.9466\n",
            "Saving checkpoint for epoch 85 at ./checkpoints/train/ckpt-17\n",
            "Epoch 85 Loss 0.1725 Accuracy 0.9465\n",
            "Time taken for 1 epoch: 65.54 secs\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.2084 Accuracy 0.9403\n",
            "Epoch 86 Batch 50 Loss 0.1589 Accuracy 0.9514\n",
            "Epoch 86 Batch 100 Loss 0.1547 Accuracy 0.9525\n",
            "Epoch 86 Batch 150 Loss 0.1586 Accuracy 0.9511\n",
            "Epoch 86 Batch 200 Loss 0.1615 Accuracy 0.9498\n",
            "Epoch 86 Batch 250 Loss 0.1643 Accuracy 0.9490\n",
            "Epoch 86 Batch 300 Loss 0.1662 Accuracy 0.9483\n",
            "Epoch 86 Batch 350 Loss 0.1676 Accuracy 0.9479\n",
            "Epoch 86 Loss 0.1681 Accuracy 0.9477\n",
            "Time taken for 1 epoch: 62.35 secs\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.1037 Accuracy 0.9705\n",
            "Epoch 87 Batch 50 Loss 0.1561 Accuracy 0.9526\n",
            "Epoch 87 Batch 100 Loss 0.1620 Accuracy 0.9509\n",
            "Epoch 87 Batch 150 Loss 0.1602 Accuracy 0.9513\n",
            "Epoch 87 Batch 200 Loss 0.1634 Accuracy 0.9499\n",
            "Epoch 87 Batch 250 Loss 0.1662 Accuracy 0.9489\n",
            "Epoch 87 Batch 300 Loss 0.1683 Accuracy 0.9482\n",
            "Epoch 87 Batch 350 Loss 0.1710 Accuracy 0.9473\n",
            "Epoch 87 Loss 0.1710 Accuracy 0.9472\n",
            "Time taken for 1 epoch: 62.59 secs\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.1473 Accuracy 0.9556\n",
            "Epoch 88 Batch 50 Loss 0.1483 Accuracy 0.9548\n",
            "Epoch 88 Batch 100 Loss 0.1491 Accuracy 0.9542\n",
            "Epoch 88 Batch 150 Loss 0.1548 Accuracy 0.9521\n",
            "Epoch 88 Batch 200 Loss 0.1592 Accuracy 0.9505\n",
            "Epoch 88 Batch 250 Loss 0.1646 Accuracy 0.9487\n",
            "Epoch 88 Batch 300 Loss 0.1668 Accuracy 0.9480\n",
            "Epoch 88 Batch 350 Loss 0.1686 Accuracy 0.9475\n",
            "Epoch 88 Loss 0.1686 Accuracy 0.9475\n",
            "Time taken for 1 epoch: 62.37 secs\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.1759 Accuracy 0.9474\n",
            "Epoch 89 Batch 50 Loss 0.1476 Accuracy 0.9542\n",
            "Epoch 89 Batch 100 Loss 0.1503 Accuracy 0.9533\n",
            "Epoch 89 Batch 150 Loss 0.1516 Accuracy 0.9529\n",
            "Epoch 89 Batch 200 Loss 0.1540 Accuracy 0.9523\n",
            "Epoch 89 Batch 250 Loss 0.1564 Accuracy 0.9517\n",
            "Epoch 89 Batch 300 Loss 0.1598 Accuracy 0.9506\n",
            "Epoch 89 Batch 350 Loss 0.1619 Accuracy 0.9499\n",
            "Epoch 89 Loss 0.1625 Accuracy 0.9498\n",
            "Time taken for 1 epoch: 62.57 secs\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.1199 Accuracy 0.9605\n",
            "Epoch 90 Batch 50 Loss 0.1398 Accuracy 0.9583\n",
            "Epoch 90 Batch 100 Loss 0.1460 Accuracy 0.9565\n",
            "Epoch 90 Batch 150 Loss 0.1497 Accuracy 0.9549\n",
            "Epoch 90 Batch 200 Loss 0.1528 Accuracy 0.9540\n",
            "Epoch 90 Batch 250 Loss 0.1552 Accuracy 0.9532\n",
            "Epoch 90 Batch 300 Loss 0.1569 Accuracy 0.9524\n",
            "Epoch 90 Batch 350 Loss 0.1600 Accuracy 0.9515\n",
            "Saving checkpoint for epoch 90 at ./checkpoints/train/ckpt-18\n",
            "Epoch 90 Loss 0.1604 Accuracy 0.9513\n",
            "Time taken for 1 epoch: 65.69 secs\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.1501 Accuracy 0.9528\n",
            "Epoch 91 Batch 50 Loss 0.1454 Accuracy 0.9555\n",
            "Epoch 91 Batch 100 Loss 0.1451 Accuracy 0.9549\n",
            "Epoch 91 Batch 150 Loss 0.1508 Accuracy 0.9528\n",
            "Epoch 91 Batch 200 Loss 0.1527 Accuracy 0.9522\n",
            "Epoch 91 Batch 250 Loss 0.1538 Accuracy 0.9520\n",
            "Epoch 91 Batch 300 Loss 0.1567 Accuracy 0.9512\n",
            "Epoch 91 Batch 350 Loss 0.1580 Accuracy 0.9507\n",
            "Epoch 91 Loss 0.1587 Accuracy 0.9505\n",
            "Time taken for 1 epoch: 62.39 secs\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.1829 Accuracy 0.9550\n",
            "Epoch 92 Batch 50 Loss 0.1530 Accuracy 0.9544\n",
            "Epoch 92 Batch 100 Loss 0.1476 Accuracy 0.9550\n",
            "Epoch 92 Batch 150 Loss 0.1491 Accuracy 0.9542\n",
            "Epoch 92 Batch 200 Loss 0.1503 Accuracy 0.9538\n",
            "Epoch 92 Batch 250 Loss 0.1524 Accuracy 0.9529\n",
            "Epoch 92 Batch 300 Loss 0.1541 Accuracy 0.9524\n",
            "Epoch 92 Batch 350 Loss 0.1564 Accuracy 0.9515\n",
            "Epoch 92 Loss 0.1567 Accuracy 0.9514\n",
            "Time taken for 1 epoch: 62.55 secs\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.1342 Accuracy 0.9631\n",
            "Epoch 93 Batch 50 Loss 0.1439 Accuracy 0.9559\n",
            "Epoch 93 Batch 100 Loss 0.1438 Accuracy 0.9553\n",
            "Epoch 93 Batch 150 Loss 0.1444 Accuracy 0.9554\n",
            "Epoch 93 Batch 200 Loss 0.1468 Accuracy 0.9548\n",
            "Epoch 93 Batch 250 Loss 0.1471 Accuracy 0.9548\n",
            "Epoch 93 Batch 300 Loss 0.1496 Accuracy 0.9542\n",
            "Epoch 93 Batch 350 Loss 0.1530 Accuracy 0.9532\n",
            "Epoch 93 Loss 0.1533 Accuracy 0.9531\n",
            "Time taken for 1 epoch: 62.04 secs\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.1469 Accuracy 0.9497\n",
            "Epoch 94 Batch 50 Loss 0.1378 Accuracy 0.9572\n",
            "Epoch 94 Batch 100 Loss 0.1409 Accuracy 0.9566\n",
            "Epoch 94 Batch 150 Loss 0.1432 Accuracy 0.9563\n",
            "Epoch 94 Batch 200 Loss 0.1446 Accuracy 0.9558\n",
            "Epoch 94 Batch 250 Loss 0.1478 Accuracy 0.9546\n",
            "Epoch 94 Batch 300 Loss 0.1489 Accuracy 0.9543\n",
            "Epoch 94 Batch 350 Loss 0.1515 Accuracy 0.9534\n",
            "Epoch 94 Loss 0.1516 Accuracy 0.9533\n",
            "Time taken for 1 epoch: 62.70 secs\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.1680 Accuracy 0.9515\n",
            "Epoch 95 Batch 50 Loss 0.1390 Accuracy 0.9561\n",
            "Epoch 95 Batch 100 Loss 0.1367 Accuracy 0.9579\n",
            "Epoch 95 Batch 150 Loss 0.1403 Accuracy 0.9563\n",
            "Epoch 95 Batch 200 Loss 0.1435 Accuracy 0.9555\n",
            "Epoch 95 Batch 250 Loss 0.1449 Accuracy 0.9553\n",
            "Epoch 95 Batch 300 Loss 0.1455 Accuracy 0.9550\n",
            "Epoch 95 Batch 350 Loss 0.1480 Accuracy 0.9543\n",
            "Saving checkpoint for epoch 95 at ./checkpoints/train/ckpt-19\n",
            "Epoch 95 Loss 0.1481 Accuracy 0.9543\n",
            "Time taken for 1 epoch: 64.97 secs\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.1352 Accuracy 0.9514\n",
            "Epoch 96 Batch 50 Loss 0.1302 Accuracy 0.9594\n",
            "Epoch 96 Batch 100 Loss 0.1325 Accuracy 0.9594\n",
            "Epoch 96 Batch 150 Loss 0.1371 Accuracy 0.9578\n",
            "Epoch 96 Batch 200 Loss 0.1419 Accuracy 0.9561\n",
            "Epoch 96 Batch 250 Loss 0.1436 Accuracy 0.9555\n",
            "Epoch 96 Batch 300 Loss 0.1456 Accuracy 0.9549\n",
            "Epoch 96 Batch 350 Loss 0.1484 Accuracy 0.9541\n",
            "Epoch 96 Loss 0.1486 Accuracy 0.9540\n",
            "Time taken for 1 epoch: 62.46 secs\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.1498 Accuracy 0.9522\n",
            "Epoch 97 Batch 50 Loss 0.1276 Accuracy 0.9616\n",
            "Epoch 97 Batch 100 Loss 0.1326 Accuracy 0.9593\n",
            "Epoch 97 Batch 150 Loss 0.1366 Accuracy 0.9579\n",
            "Epoch 97 Batch 200 Loss 0.1397 Accuracy 0.9570\n",
            "Epoch 97 Batch 250 Loss 0.1421 Accuracy 0.9562\n",
            "Epoch 97 Batch 300 Loss 0.1440 Accuracy 0.9555\n",
            "Epoch 97 Batch 350 Loss 0.1471 Accuracy 0.9545\n",
            "Epoch 97 Loss 0.1476 Accuracy 0.9543\n",
            "Time taken for 1 epoch: 63.00 secs\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.1247 Accuracy 0.9579\n",
            "Epoch 98 Batch 50 Loss 0.1445 Accuracy 0.9557\n",
            "Epoch 98 Batch 100 Loss 0.1417 Accuracy 0.9566\n",
            "Epoch 98 Batch 150 Loss 0.1371 Accuracy 0.9584\n",
            "Epoch 98 Batch 200 Loss 0.1376 Accuracy 0.9580\n",
            "Epoch 98 Batch 250 Loss 0.1402 Accuracy 0.9572\n",
            "Epoch 98 Batch 300 Loss 0.1435 Accuracy 0.9563\n",
            "Epoch 98 Batch 350 Loss 0.1442 Accuracy 0.9560\n",
            "Epoch 98 Loss 0.1441 Accuracy 0.9560\n",
            "Time taken for 1 epoch: 62.28 secs\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.0951 Accuracy 0.9737\n",
            "Epoch 99 Batch 50 Loss 0.1395 Accuracy 0.9572\n",
            "Epoch 99 Batch 100 Loss 0.1363 Accuracy 0.9583\n",
            "Epoch 99 Batch 150 Loss 0.1397 Accuracy 0.9571\n",
            "Epoch 99 Batch 200 Loss 0.1410 Accuracy 0.9569\n",
            "Epoch 99 Batch 250 Loss 0.1409 Accuracy 0.9569\n",
            "Epoch 99 Batch 300 Loss 0.1441 Accuracy 0.9559\n",
            "Epoch 99 Batch 350 Loss 0.1451 Accuracy 0.9554\n",
            "Epoch 99 Loss 0.1454 Accuracy 0.9553\n",
            "Time taken for 1 epoch: 62.55 secs\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.1199 Accuracy 0.9632\n",
            "Epoch 100 Batch 50 Loss 0.1269 Accuracy 0.9594\n",
            "Epoch 100 Batch 100 Loss 0.1274 Accuracy 0.9599\n",
            "Epoch 100 Batch 150 Loss 0.1287 Accuracy 0.9596\n",
            "Epoch 100 Batch 200 Loss 0.1320 Accuracy 0.9586\n",
            "Epoch 100 Batch 250 Loss 0.1337 Accuracy 0.9582\n",
            "Epoch 100 Batch 300 Loss 0.1363 Accuracy 0.9575\n",
            "Epoch 100 Batch 350 Loss 0.1385 Accuracy 0.9570\n",
            "Saving checkpoint for epoch 100 at ./checkpoints/train/ckpt-20\n",
            "Epoch 100 Loss 0.1389 Accuracy 0.9569\n",
            "Time taken for 1 epoch: 64.82 secs\n",
            "\n",
            "Epoch 101 Batch 0 Loss 0.1108 Accuracy 0.9594\n",
            "Epoch 101 Batch 50 Loss 0.1300 Accuracy 0.9606\n",
            "Epoch 101 Batch 100 Loss 0.1287 Accuracy 0.9613\n",
            "Epoch 101 Batch 150 Loss 0.1303 Accuracy 0.9601\n",
            "Epoch 101 Batch 200 Loss 0.1309 Accuracy 0.9599\n",
            "Epoch 101 Batch 250 Loss 0.1344 Accuracy 0.9588\n",
            "Epoch 101 Batch 300 Loss 0.1367 Accuracy 0.9581\n",
            "Epoch 101 Batch 350 Loss 0.1397 Accuracy 0.9570\n",
            "Epoch 101 Loss 0.1397 Accuracy 0.9570\n",
            "Time taken for 1 epoch: 62.41 secs\n",
            "\n",
            "Epoch 102 Batch 0 Loss 0.0911 Accuracy 0.9704\n",
            "Epoch 102 Batch 50 Loss 0.1258 Accuracy 0.9615\n",
            "Epoch 102 Batch 100 Loss 0.1250 Accuracy 0.9618\n",
            "Epoch 102 Batch 150 Loss 0.1271 Accuracy 0.9612\n",
            "Epoch 102 Batch 200 Loss 0.1307 Accuracy 0.9601\n",
            "Epoch 102 Batch 250 Loss 0.1328 Accuracy 0.9594\n",
            "Epoch 102 Batch 300 Loss 0.1344 Accuracy 0.9589\n",
            "Epoch 102 Batch 350 Loss 0.1369 Accuracy 0.9581\n",
            "Epoch 102 Loss 0.1368 Accuracy 0.9581\n",
            "Time taken for 1 epoch: 63.26 secs\n",
            "\n",
            "Epoch 103 Batch 0 Loss 0.1275 Accuracy 0.9626\n",
            "Epoch 103 Batch 50 Loss 0.1222 Accuracy 0.9623\n",
            "Epoch 103 Batch 100 Loss 0.1276 Accuracy 0.9609\n",
            "Epoch 103 Batch 150 Loss 0.1294 Accuracy 0.9603\n",
            "Epoch 103 Batch 200 Loss 0.1321 Accuracy 0.9593\n",
            "Epoch 103 Batch 250 Loss 0.1330 Accuracy 0.9589\n",
            "Epoch 103 Batch 300 Loss 0.1346 Accuracy 0.9585\n",
            "Epoch 103 Batch 350 Loss 0.1360 Accuracy 0.9580\n",
            "Epoch 103 Loss 0.1364 Accuracy 0.9578\n",
            "Time taken for 1 epoch: 62.62 secs\n",
            "\n",
            "Epoch 104 Batch 0 Loss 0.0762 Accuracy 0.9797\n",
            "Epoch 104 Batch 50 Loss 0.1186 Accuracy 0.9630\n",
            "Epoch 104 Batch 100 Loss 0.1188 Accuracy 0.9627\n",
            "Epoch 104 Batch 150 Loss 0.1212 Accuracy 0.9617\n",
            "Epoch 104 Batch 200 Loss 0.1247 Accuracy 0.9609\n",
            "Epoch 104 Batch 250 Loss 0.1284 Accuracy 0.9600\n",
            "Epoch 104 Batch 300 Loss 0.1306 Accuracy 0.9594\n",
            "Epoch 104 Batch 350 Loss 0.1319 Accuracy 0.9591\n",
            "Epoch 104 Loss 0.1320 Accuracy 0.9591\n",
            "Time taken for 1 epoch: 62.61 secs\n",
            "\n",
            "Epoch 105 Batch 0 Loss 0.1145 Accuracy 0.9705\n",
            "Epoch 105 Batch 50 Loss 0.1182 Accuracy 0.9639\n",
            "Epoch 105 Batch 100 Loss 0.1175 Accuracy 0.9639\n",
            "Epoch 105 Batch 150 Loss 0.1208 Accuracy 0.9630\n",
            "Epoch 105 Batch 200 Loss 0.1243 Accuracy 0.9619\n",
            "Epoch 105 Batch 250 Loss 0.1272 Accuracy 0.9610\n",
            "Epoch 105 Batch 300 Loss 0.1295 Accuracy 0.9603\n",
            "Epoch 105 Batch 350 Loss 0.1308 Accuracy 0.9600\n",
            "Saving checkpoint for epoch 105 at ./checkpoints/train/ckpt-21\n",
            "Epoch 105 Loss 0.1309 Accuracy 0.9599\n",
            "Time taken for 1 epoch: 65.17 secs\n",
            "\n",
            "Epoch 106 Batch 0 Loss 0.1060 Accuracy 0.9719\n",
            "Epoch 106 Batch 50 Loss 0.1163 Accuracy 0.9642\n",
            "Epoch 106 Batch 100 Loss 0.1183 Accuracy 0.9637\n",
            "Epoch 106 Batch 150 Loss 0.1199 Accuracy 0.9633\n",
            "Epoch 106 Batch 200 Loss 0.1221 Accuracy 0.9628\n",
            "Epoch 106 Batch 250 Loss 0.1245 Accuracy 0.9621\n",
            "Epoch 106 Batch 300 Loss 0.1261 Accuracy 0.9619\n",
            "Epoch 106 Batch 350 Loss 0.1283 Accuracy 0.9610\n",
            "Epoch 106 Loss 0.1286 Accuracy 0.9609\n",
            "Time taken for 1 epoch: 62.64 secs\n",
            "\n",
            "Epoch 107 Batch 0 Loss 0.1062 Accuracy 0.9645\n",
            "Epoch 107 Batch 50 Loss 0.1195 Accuracy 0.9637\n",
            "Epoch 107 Batch 100 Loss 0.1208 Accuracy 0.9626\n",
            "Epoch 107 Batch 150 Loss 0.1231 Accuracy 0.9622\n",
            "Epoch 107 Batch 200 Loss 0.1237 Accuracy 0.9618\n",
            "Epoch 107 Batch 250 Loss 0.1253 Accuracy 0.9615\n",
            "Epoch 107 Batch 300 Loss 0.1267 Accuracy 0.9610\n",
            "Epoch 107 Batch 350 Loss 0.1291 Accuracy 0.9604\n",
            "Epoch 107 Loss 0.1293 Accuracy 0.9602\n",
            "Time taken for 1 epoch: 62.14 secs\n",
            "\n",
            "Epoch 108 Batch 0 Loss 0.1111 Accuracy 0.9674\n",
            "Epoch 108 Batch 50 Loss 0.1177 Accuracy 0.9632\n",
            "Epoch 108 Batch 100 Loss 0.1181 Accuracy 0.9630\n",
            "Epoch 108 Batch 150 Loss 0.1187 Accuracy 0.9635\n",
            "Epoch 108 Batch 200 Loss 0.1206 Accuracy 0.9630\n",
            "Epoch 108 Batch 250 Loss 0.1231 Accuracy 0.9624\n",
            "Epoch 108 Batch 300 Loss 0.1253 Accuracy 0.9617\n",
            "Epoch 108 Batch 350 Loss 0.1271 Accuracy 0.9611\n",
            "Epoch 108 Loss 0.1268 Accuracy 0.9612\n",
            "Time taken for 1 epoch: 62.29 secs\n",
            "\n",
            "Epoch 109 Batch 0 Loss 0.1315 Accuracy 0.9597\n",
            "Epoch 109 Batch 50 Loss 0.1146 Accuracy 0.9647\n",
            "Epoch 109 Batch 100 Loss 0.1143 Accuracy 0.9651\n",
            "Epoch 109 Batch 150 Loss 0.1168 Accuracy 0.9641\n",
            "Epoch 109 Batch 200 Loss 0.1200 Accuracy 0.9631\n",
            "Epoch 109 Batch 250 Loss 0.1215 Accuracy 0.9626\n",
            "Epoch 109 Batch 300 Loss 0.1223 Accuracy 0.9626\n",
            "Epoch 109 Batch 350 Loss 0.1240 Accuracy 0.9620\n",
            "Epoch 109 Loss 0.1242 Accuracy 0.9620\n",
            "Time taken for 1 epoch: 62.46 secs\n",
            "\n",
            "Epoch 110 Batch 0 Loss 0.0724 Accuracy 0.9774\n",
            "Epoch 110 Batch 50 Loss 0.1110 Accuracy 0.9650\n",
            "Epoch 110 Batch 100 Loss 0.1098 Accuracy 0.9655\n",
            "Epoch 110 Batch 150 Loss 0.1134 Accuracy 0.9647\n",
            "Epoch 110 Batch 200 Loss 0.1134 Accuracy 0.9648\n",
            "Epoch 110 Batch 250 Loss 0.1156 Accuracy 0.9642\n",
            "Epoch 110 Batch 300 Loss 0.1171 Accuracy 0.9638\n",
            "Epoch 110 Batch 350 Loss 0.1200 Accuracy 0.9628\n",
            "Saving checkpoint for epoch 110 at ./checkpoints/train/ckpt-22\n",
            "Epoch 110 Loss 0.1202 Accuracy 0.9627\n",
            "Time taken for 1 epoch: 64.84 secs\n",
            "\n",
            "Epoch 111 Batch 0 Loss 0.1137 Accuracy 0.9569\n",
            "Epoch 111 Batch 50 Loss 0.1106 Accuracy 0.9656\n",
            "Epoch 111 Batch 100 Loss 0.1127 Accuracy 0.9652\n",
            "Epoch 111 Batch 150 Loss 0.1137 Accuracy 0.9653\n",
            "Epoch 111 Batch 200 Loss 0.1152 Accuracy 0.9650\n",
            "Epoch 111 Batch 250 Loss 0.1172 Accuracy 0.9644\n",
            "Epoch 111 Batch 300 Loss 0.1197 Accuracy 0.9635\n",
            "Epoch 111 Batch 350 Loss 0.1207 Accuracy 0.9634\n",
            "Epoch 111 Loss 0.1206 Accuracy 0.9634\n",
            "Time taken for 1 epoch: 62.77 secs\n",
            "\n",
            "Epoch 112 Batch 0 Loss 0.1030 Accuracy 0.9681\n",
            "Epoch 112 Batch 50 Loss 0.1082 Accuracy 0.9667\n",
            "Epoch 112 Batch 100 Loss 0.1110 Accuracy 0.9666\n",
            "Epoch 112 Batch 150 Loss 0.1139 Accuracy 0.9655\n",
            "Epoch 112 Batch 200 Loss 0.1154 Accuracy 0.9648\n",
            "Epoch 112 Batch 250 Loss 0.1163 Accuracy 0.9644\n",
            "Epoch 112 Batch 300 Loss 0.1167 Accuracy 0.9642\n",
            "Epoch 112 Batch 350 Loss 0.1184 Accuracy 0.9637\n",
            "Epoch 112 Loss 0.1182 Accuracy 0.9637\n",
            "Time taken for 1 epoch: 62.04 secs\n",
            "\n",
            "Epoch 113 Batch 0 Loss 0.1180 Accuracy 0.9625\n",
            "Epoch 113 Batch 50 Loss 0.1080 Accuracy 0.9672\n",
            "Epoch 113 Batch 100 Loss 0.1105 Accuracy 0.9665\n",
            "Epoch 113 Batch 150 Loss 0.1106 Accuracy 0.9668\n",
            "Epoch 113 Batch 200 Loss 0.1121 Accuracy 0.9663\n",
            "Epoch 113 Batch 250 Loss 0.1139 Accuracy 0.9653\n",
            "Epoch 113 Batch 300 Loss 0.1158 Accuracy 0.9647\n",
            "Epoch 113 Batch 350 Loss 0.1174 Accuracy 0.9643\n",
            "Epoch 113 Loss 0.1174 Accuracy 0.9643\n",
            "Time taken for 1 epoch: 62.97 secs\n",
            "\n",
            "Epoch 114 Batch 0 Loss 0.1392 Accuracy 0.9545\n",
            "Epoch 114 Batch 50 Loss 0.1104 Accuracy 0.9656\n",
            "Epoch 114 Batch 100 Loss 0.1112 Accuracy 0.9656\n",
            "Epoch 114 Batch 150 Loss 0.1125 Accuracy 0.9653\n",
            "Epoch 114 Batch 200 Loss 0.1130 Accuracy 0.9655\n",
            "Epoch 114 Batch 250 Loss 0.1148 Accuracy 0.9649\n",
            "Epoch 114 Batch 300 Loss 0.1168 Accuracy 0.9643\n",
            "Epoch 114 Batch 350 Loss 0.1185 Accuracy 0.9638\n",
            "Epoch 114 Loss 0.1186 Accuracy 0.9637\n",
            "Time taken for 1 epoch: 62.38 secs\n",
            "\n",
            "Epoch 115 Batch 0 Loss 0.1115 Accuracy 0.9713\n",
            "Epoch 115 Batch 50 Loss 0.0998 Accuracy 0.9705\n",
            "Epoch 115 Batch 100 Loss 0.1072 Accuracy 0.9673\n",
            "Epoch 115 Batch 150 Loss 0.1085 Accuracy 0.9670\n",
            "Epoch 115 Batch 200 Loss 0.1104 Accuracy 0.9666\n",
            "Epoch 115 Batch 250 Loss 0.1122 Accuracy 0.9660\n",
            "Epoch 115 Batch 300 Loss 0.1147 Accuracy 0.9650\n",
            "Epoch 115 Batch 350 Loss 0.1158 Accuracy 0.9648\n",
            "Saving checkpoint for epoch 115 at ./checkpoints/train/ckpt-23\n",
            "Epoch 115 Loss 0.1161 Accuracy 0.9648\n",
            "Time taken for 1 epoch: 64.96 secs\n",
            "\n",
            "Epoch 116 Batch 0 Loss 0.0768 Accuracy 0.9751\n",
            "Epoch 116 Batch 50 Loss 0.1025 Accuracy 0.9698\n",
            "Epoch 116 Batch 100 Loss 0.1069 Accuracy 0.9688\n",
            "Epoch 116 Batch 150 Loss 0.1076 Accuracy 0.9678\n",
            "Epoch 116 Batch 200 Loss 0.1096 Accuracy 0.9668\n",
            "Epoch 116 Batch 250 Loss 0.1114 Accuracy 0.9661\n",
            "Epoch 116 Batch 300 Loss 0.1136 Accuracy 0.9654\n",
            "Epoch 116 Batch 350 Loss 0.1142 Accuracy 0.9651\n",
            "Epoch 116 Loss 0.1143 Accuracy 0.9651\n",
            "Time taken for 1 epoch: 62.73 secs\n",
            "\n",
            "Epoch 117 Batch 0 Loss 0.1074 Accuracy 0.9620\n",
            "Epoch 117 Batch 50 Loss 0.1031 Accuracy 0.9678\n",
            "Epoch 117 Batch 100 Loss 0.1039 Accuracy 0.9675\n",
            "Epoch 117 Batch 150 Loss 0.1049 Accuracy 0.9674\n",
            "Epoch 117 Batch 200 Loss 0.1072 Accuracy 0.9670\n",
            "Epoch 117 Batch 250 Loss 0.1096 Accuracy 0.9664\n",
            "Epoch 117 Batch 300 Loss 0.1124 Accuracy 0.9658\n",
            "Epoch 117 Batch 350 Loss 0.1139 Accuracy 0.9654\n",
            "Epoch 117 Loss 0.1140 Accuracy 0.9654\n",
            "Time taken for 1 epoch: 62.30 secs\n",
            "\n",
            "Epoch 118 Batch 0 Loss 0.1212 Accuracy 0.9621\n",
            "Epoch 118 Batch 50 Loss 0.1066 Accuracy 0.9683\n",
            "Epoch 118 Batch 100 Loss 0.1036 Accuracy 0.9688\n",
            "Epoch 118 Batch 150 Loss 0.1045 Accuracy 0.9684\n",
            "Epoch 118 Batch 200 Loss 0.1070 Accuracy 0.9675\n",
            "Epoch 118 Batch 250 Loss 0.1092 Accuracy 0.9667\n",
            "Epoch 118 Batch 300 Loss 0.1100 Accuracy 0.9664\n",
            "Epoch 118 Batch 350 Loss 0.1112 Accuracy 0.9660\n",
            "Epoch 118 Loss 0.1113 Accuracy 0.9660\n",
            "Time taken for 1 epoch: 62.39 secs\n",
            "\n",
            "Epoch 119 Batch 0 Loss 0.1088 Accuracy 0.9716\n",
            "Epoch 119 Batch 50 Loss 0.1028 Accuracy 0.9688\n",
            "Epoch 119 Batch 100 Loss 0.1036 Accuracy 0.9686\n",
            "Epoch 119 Batch 150 Loss 0.1048 Accuracy 0.9683\n",
            "Epoch 119 Batch 200 Loss 0.1062 Accuracy 0.9676\n",
            "Epoch 119 Batch 250 Loss 0.1077 Accuracy 0.9671\n",
            "Epoch 119 Batch 300 Loss 0.1085 Accuracy 0.9670\n",
            "Epoch 119 Batch 350 Loss 0.1100 Accuracy 0.9666\n",
            "Epoch 119 Loss 0.1101 Accuracy 0.9665\n",
            "Time taken for 1 epoch: 62.35 secs\n",
            "\n",
            "Epoch 120 Batch 0 Loss 0.0930 Accuracy 0.9735\n",
            "Epoch 120 Batch 50 Loss 0.1004 Accuracy 0.9703\n",
            "Epoch 120 Batch 100 Loss 0.1004 Accuracy 0.9700\n",
            "Epoch 120 Batch 150 Loss 0.1016 Accuracy 0.9692\n",
            "Epoch 120 Batch 200 Loss 0.1023 Accuracy 0.9690\n",
            "Epoch 120 Batch 250 Loss 0.1044 Accuracy 0.9685\n",
            "Epoch 120 Batch 300 Loss 0.1051 Accuracy 0.9681\n",
            "Epoch 120 Batch 350 Loss 0.1063 Accuracy 0.9677\n",
            "Saving checkpoint for epoch 120 at ./checkpoints/train/ckpt-24\n",
            "Epoch 120 Loss 0.1064 Accuracy 0.9677\n",
            "Time taken for 1 epoch: 65.18 secs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run training! Each epoch takes several mins with GPU\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  # inp -> twi, tar -> french\n",
        "  for (batch, (inp,tar)) in enumerate(train_batches):\n",
        "    train_step(inp, tar)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aij6BWpIBU52"
      },
      "source": [
        "# Build a Translator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "z0clY8WkaGZZ"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.Module):\n",
        "    def __init__(self, tokenizers, transformer):\n",
        "        self.tokenizers = tokenizers\n",
        "        self.transformer = transformer\n",
        "\n",
        "    def __call__(self, sentence, max_length=MAX_TOKENS):\n",
        "        # The input sentence is TWI, hence adding the `[START]` and `[END]` tokens.\n",
        "        sentence = tf.convert_to_tensor([sentence])\n",
        "        sentence = self.tokenizers.twi.tokenize(sentence).to_tensor()\n",
        "\n",
        "        encoder_input = sentence\n",
        "\n",
        "        # As the output language is ENGLISH, initialize the output with the\n",
        "        # English `[START]` token.\n",
        "        start_end = self.tokenizers.eng.tokenize([''])[0]\n",
        "        start = start_end[0][tf.newaxis]\n",
        "        end = start_end[1][tf.newaxis]\n",
        "\n",
        "        # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "        # dynamic-loop can be traced by `tf.function`.\n",
        "        output_array = tf.TensorArray(\n",
        "            dtype=tf.int64, size=0, dynamic_size=True)\n",
        "        output_array = output_array.write(0, start)\n",
        "        output = tf.transpose(output_array.stack())\n",
        "\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "                encoder_input, output)\n",
        "\n",
        "        for i in tf.range(max_length):\n",
        "            output = tf.transpose(output_array.stack())\n",
        "\n",
        "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "                encoder_input, output)\n",
        "\n",
        "            predictions, attention_weights = self.transformer(\n",
        "                encoder_input, output, False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "\n",
        "            # Select the last token from the `seq_len` dimension.\n",
        "            # Shape `(batch_size, 1, vocab_size)`.\n",
        "            predictions = predictions[:, -1:, :]\n",
        "\n",
        "            predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "            # Concatenate the `predicted_id` to the output which is given to the\n",
        "            # decoder as its input.\n",
        "            output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "            if predicted_id == end:\n",
        "                break\n",
        "\n",
        "        output = tf.transpose(output_array.stack())\n",
        "        # The output shape is `(1, tokens)`.\n",
        "        text = self.tokenizers.eng.detokenize(output)[0]  # Shape: `()`.\n",
        "\n",
        "        tokens = self.tokenizers.eng.lookup(output)[0]\n",
        "        _, attention_weights = self.transformer(\n",
        "            encoder_input, output[:, :-1], False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "\n",
        "        return text, tokens, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Tt1OVm3ecSNO"
      },
      "outputs": [],
      "source": [
        "#Create an instance of this Translator class\n",
        "translator = Translator(tokenizers, transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dMj9N3fBh1P"
      },
      "source": [
        "## translate example sentecnes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "ysGcUmAzc_Yi"
      },
      "outputs": [],
      "source": [
        "def print_translation(sentence, tokens, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "wKmyWMcGduft",
        "outputId": "477297f3-f55c-488f-f901-bc664f99c3ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : mituu mmirika kɔɔ wo nua no nkyɛn wɔ abɔnten so .\n",
            "Prediction     : i ran into your brother on the street .\n",
            "Ground truth   : i ran to your brother on the street .\n"
          ]
        }
      ],
      "source": [
        "sentence =\"mituu mmirika kɔɔ wo nua no nkyɛn wɔ abɔnten so .\"\n",
        "ground_truth= \"i ran to your brother on the street .\"\n",
        "translated_text, translated_tokens, attention_weights = translator(sentence)\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "DYL7f6tu9Y0N",
        "outputId": "1f480c08-9457-43b3-9ca1-310320ee0abe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : mepɛ sɛ mibisa wo asɛm bi .\n",
            "Prediction     : i want to ask you a question .\n",
            "Ground truth   : i want to ask you a question .\n"
          ]
        }
      ],
      "source": [
        "sentence=\"mepɛ sɛ mibisa wo asɛm bi .\"\n",
        "ground_truth=\"i want to ask you a question .\"\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBe9E_dXBzne"
      },
      "source": [
        "# Save Translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "qMpmh77M9ufZ"
      },
      "outputs": [],
      "source": [
        "# class to export translator\n",
        "class ExportTranslator(tf.Module):\n",
        "  def __init__(self, translator):\n",
        "    self.translator = translator\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def __call__(self, sentence):\n",
        "    (result,\n",
        "     tokens,\n",
        "     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "89tEraoTixKG"
      },
      "outputs": [],
      "source": [
        "translator = ExportTranslator(translator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "1ZurB6qDFDzh",
        "outputId": "61abd5f5-be78-4a18-fe9e-a798265fa75f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'i want to visit old people .'"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "translator(tf.constant(\"mepɛ sɛ minya afoforo anim dom .\")).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "YMhMt9_z2ZXH",
        "outputId": "cdbecf7c-c695-4a5e-d451-e1c0405ab2fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn while saving (showing 5 of 332). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "module_no_signatures_path = '/content/drive/MyDrive/twi_english_translator'\n",
        "print('Saving model...')\n",
        "tf.saved_model.save(translator, module_no_signatures_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "P0jO3gtH9bih"
      },
      "outputs": [],
      "source": [
        "reloaded = tf.saved_model.load(module_no_signatures_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "adjW9Tug-ZOa",
        "outputId": "18d0c809-aea1-4cf6-fbbc-8d49f24fe83e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the computer is in the library .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "reloaded(\"kɔmputa no wɔ nhomakorabea hɔ .\").numpy().decode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbDLGyb8clxZ"
      },
      "source": [
        "#BLEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "W8k01SazA8Cl"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "\n",
        "class Bleu():\n",
        "    def __init__(self, translator):\n",
        "        self.translator = translator\n",
        "\n",
        "    def get_bleuscore(self, testfile, referencefile, smothingfunction=None):\n",
        "        if type(testfile) == str and type(referencefile) == str:\n",
        "            # Open test file and read lines\n",
        "            f = open(testfile, \"r\")\n",
        "            hypothesis = f.readlines()\n",
        "            f.close()\n",
        "            # open refernce file and read lines\n",
        "            f = open(referencefile, \"r\")\n",
        "            reference = f.readlines()\n",
        "            f.close()\n",
        "        elif type(testfile) == list and type(referencefile) == list:\n",
        "            hypothesis = testfile\n",
        "            reference = referencefile\n",
        "        else:\n",
        "            print(f'File must be txt or python list')\n",
        "\n",
        "        # check the length of our input sentence\n",
        "        length = len(hypothesis)\n",
        "        bleu_total = np.array([0., 0., 0.])\n",
        "        weights = [(1./2, 1./2), (1./3, 1./3, 1./3),\n",
        "                   (1./4, 1./4, 1./4, 1./4)]\n",
        "        for i in range(length):\n",
        "            hypothesis[i] = hypothesis[i]\n",
        "            reference[i] = reference[i]\n",
        "            groundtruth = reference[i].lower().replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            groundtruth = [groundtruth]\n",
        "            translated_text = self.translator(\n",
        "                hypothesis[i]).numpy().decode(\"utf-8\")\n",
        "            # print(\"Translated Text: \", translated_text)\n",
        "            # print(\"Ground Truth: \", reference[i])\n",
        "            candidate = translated_text.replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            bleu = np.array(sentence_bleu(\n",
        "                groundtruth, candidate, weights, smoothing_function=smothingfunction))\n",
        "            bleu_total += bleu\n",
        "\n",
        "        return f'2-GRAMS: {bleu_total[0]/length}',f'3-GRAMS: {bleu_total[1]/length}',f'4-GRAMS: {bleu_total[2]/length}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "TYHXXY10dMh5"
      },
      "outputs": [],
      "source": [
        "#iNSTANTIATE OBJECT OF BLEU CLASS AND IT SMOOTHING FUNCTION\n",
        "smooth= SmoothingFunction()\n",
        "bleu = Bleu(reloaded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "g2y6aHDfdbZK",
        "outputId": "d6bb1d51-5ded-42e7-e895-2f4134fe1df4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 19min 29s, sys: 2min 17s, total: 21min 46s\n",
            "Wall time: 13min 54s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2-GRAMS: 0.33169997119947403',\n",
              " '3-GRAMS: 0.3052104625159434',\n",
              " '4-GRAMS: 0.2933785622572871')"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "# ESTIMATE BLEU SCORE FROM THE TEST DATA\n",
        "# from list\n",
        "%%time\n",
        "bleu.get_bleuscore(twi_test,en_test,smooth.method7)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GOOGLE *API* BLEU\n"
      ],
      "metadata": {
        "id": "VRgT8luKPPuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use google API for bidirectional pivot translation of Twi and French\n",
        "# pivot language = English\n",
        "# import libraries\n",
        "from googletrans import Translator, constants\n",
        "# instantiate a translator object\n",
        "# initiate translator object\n",
        "translator = Translator()\n",
        "# Add Akan to the language supported by this package\n",
        "# Note the googletrans package has not  been updated to capture the new additions by google since May 2022\n",
        "# from https://translate.google.com/?sl=en&tl=ak&op=translate , the key and value for Twi is 'ak':'akan'\n",
        "constants.LANGUAGES['ak'] = 'akan'\n",
        "\n",
        "\n",
        "class GooglePivot:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def evaluate(self, sentences, src_key, dest_key):\n",
        "        eng_text = translator.translate(sentences, src=src_key, dest='en').text\n",
        "        print(eng_text)\n",
        "        output = translator.translate(eng_text, dest=dest_key).text\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class GoogleDirect:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def evaluate(self, sentences, src_key, dest_key):\n",
        "\n",
        "        return translator.translate(sentences, src=src_key, dest=dest_key).text"
      ],
      "metadata": {
        "id": "836KH3HJPNyW"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "\n",
        "class GoogleBleu():\n",
        "    def __init__(self, translator):\n",
        "        self.translator = translator\n",
        "\n",
        "    def get_bleuscore(self, testfile, referencefile,src_lang,dest_lang, smothingfunction=None):\n",
        "        if type(testfile) == str and type(referencefile) == str:\n",
        "            # Open test file and read lines\n",
        "            f = open(testfile, \"r\")\n",
        "            hypothesis = f.readlines()\n",
        "            f.close()\n",
        "            # open refernce file and read lines\n",
        "            f = open(referencefile, \"r\")\n",
        "            reference = f.readlines()\n",
        "            f.close()\n",
        "        elif type(testfile) == list and type(referencefile) == list:\n",
        "            hypothesis = testfile\n",
        "            reference = referencefile\n",
        "        else:\n",
        "            print(f'File must be txt or python list')\n",
        "\n",
        "        # check the length of our input sentence\n",
        "        length = len(hypothesis)\n",
        "        bleu_total = np.array([0., 0., 0.])\n",
        "        weights = [(1./2, 1./2), (1./3, 1./3, 1./3),\n",
        "                   (1./4, 1./4, 1./4, 1./4)]\n",
        "        for i in range(length):\n",
        "            hypothesis[i] = hypothesis[i]\n",
        "            reference[i] = reference[i]\n",
        "            groundtruth = reference[i].lower().replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            groundtruth = [groundtruth]\n",
        "            translated_text = self.translator.evaluate(hypothesis[i],src_key=src_lang, dest_key=dest_lang)\n",
        "            # print(\"Translated Text: \", translated_text)\n",
        "            # print(\"Ground Truth: \", reference[i])\n",
        "            candidate = translated_text.replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            bleu = np.array(sentence_bleu(\n",
        "                groundtruth, candidate, weights, smoothing_function=smothingfunction))\n",
        "            bleu_total += bleu\n",
        "\n",
        "        return f'2-GRAMS: {bleu_total[0]/length}',f'3-GRAMS: {bleu_total[1]/length}',f'4-GRAMS: {bleu_total[2]/length}'"
      ],
      "metadata": {
        "id": "QHT4GJc7Sqpb"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate GoogleDirect class\n",
        "google_translate = GoogleDirect()"
      ],
      "metadata": {
        "id": "DxmDJjRkQLiI"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "google_bleu = GoogleBleu(google_translate)"
      ],
      "metadata": {
        "id": "UHjJ24E4RQ22"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "google_bleu.get_bleuscore(twi_test,en_test,'ak','en',smooth.method7)\n"
      ],
      "metadata": {
        "id": "v8QDG1k8UMhp",
        "outputId": "7bda1043-d7e0-4e23-8262-808fc0202537",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 11.8 s, sys: 1.07 s, total: 12.9 s\n",
            "Wall time: 4min 13s\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('2-GRAMS: 0.477344605509058',\n",
              " '3-GRAMS: 0.43823509012106626',\n",
              " '4-GRAMS: 0.4123773653096391')"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}