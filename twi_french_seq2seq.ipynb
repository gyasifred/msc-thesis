{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN34p2WOhD+R+zCIK/3qONc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/twi_french_seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This exercise will demonstrate how to build sequence to sequence models with attention for Twi-French machine translation. This code is based on the tensorflow implementation of the paper [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025v5) (Luong et al., 2015).The code snippet are adapted from from [[1]](https://www.tensorflow.org/text/tutorials/nmt_with_attention)."
      ],
      "metadata": {
        "id": "ygqAsWIXdaHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Tensorflow"
      ],
      "metadata": {
        "id": "Hq-OJN6ybfEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U tensorflow-text-nightly==2.11.0.dev20220817\n",
        "!pip install -U -q tf-nightly==2.11.0.dev20220817"
      ],
      "metadata": {
        "id": "qPsStl-KdJmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import typing\n",
        "from typing import Any, Tuple\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_text as tf_text\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "YET1BasJdKr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess data"
      ],
      "metadata": {
        "id": "-k6W5V1Jg44R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This code was adapted from https://github.com/GhanaNLP/kasa/blob/master/Kasa/Preprocessing.py\n",
        "# A subclass of the TFT  for preprocessing data\n",
        "# import required library\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "class Preprocessing:\n",
        "    # dummy initialization method\n",
        "    def __init__(self):\n",
        "        # initialize with some default parameters here later\n",
        "        pass\n",
        "\n",
        "    # read in parallel twi - french dataset\n",
        "    def read_parallel_dataset(self, filepath_twi, filepath_french):\n",
        "\n",
        "        # read french data\n",
        "        french_data = []\n",
        "        with open(filepath_french, encoding='utf-8') as file:\n",
        "            line = file.readline()\n",
        "            cnt = 1\n",
        "            while line:\n",
        "                french_data.append(line.strip())\n",
        "                line = file.readline()\n",
        "                cnt += 1\n",
        "\n",
        "        # read twi data\n",
        "        twi_data = []\n",
        "        with open(filepath_twi, encoding='utf-8') as file:\n",
        "\n",
        "            # twi=file.read()\n",
        "            line = file.readline()\n",
        "            cnt = 1\n",
        "            while line:\n",
        "                twi_data.append(line.strip())\n",
        "                line = file.readline()\n",
        "                cnt += 1\n",
        "\n",
        "        return twi_data, french_data\n",
        "\n",
        "    # Define a helper function to remove string accents\n",
        "\n",
        "    def removeStringAccent(self, s):\n",
        "        return ''.join(\n",
        "            c for c in unicodedata.normalize('NFD', s)\n",
        "            if unicodedata.category(c) != 'Mn'\n",
        "        )\n",
        "\n",
        "    # normalize input twi sentence\n",
        "    def normalize_twi(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.ƆɔɛƐ!?’]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "\n",
        "    # normalize input french sentence\n",
        "    def normalize_fr(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s"
      ],
      "metadata": {
        "id": "Nh5Z6XSTfREU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of tft preprocessing class\n",
        "\n",
        "TwiFrPreprocessor = Preprocessing()\n",
        "\n",
        "# Read raw parallel dataset\n",
        "raw_data_twi,raw_data_fr = TwiFrPreprocessor.read_parallel_dataset(\n",
        "        filepath_twi='/content/verified_twi.txt',\n",
        "        filepath_french='/content/verified_french.txt')\n",
        "\n",
        "# Normalize the raw data\n",
        "raw_data_fr = [TwiFrPreprocessor.normalize_fr(data) for data in raw_data_fr]\n",
        "raw_data_twi = [TwiFrPreprocessor.normalize_twi(data) for data in raw_data_twi]"
      ],
      "metadata": {
        "id": "XL_-wwIOhEjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Train and Test sets"
      ],
      "metadata": {
        "id": "G80o2o2FjZYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the dataset into training and test sets\n",
        "# Keep 20% of the data as test set\n",
        "train_twi,test_twi,train_fr,test_fr = train_test_split(raw_data_twi,raw_data_fr, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "qIAtMONkhNcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define function to write text to txt file\n",
        "def writeTotxt(destination,data):\n",
        "  with open(destination, 'w') as f:\n",
        "    for line in data:\n",
        "        f.write(f\"{line}\\n\")"
      ],
      "metadata": {
        "id": "TsQrtWQvjpsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write the preprocess traning and test dataset to a file\n",
        "writeTotxt('train_twi.txt',train_twi)\n",
        "writeTotxt('train_fr.txt',train_fr)\n",
        "writeTotxt('test_twi.txt',test_twi)\n",
        "writeTotxt('test_fr.txt',test_fr)"
      ],
      "metadata": {
        "id": "2aFkzc8sj5vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build tf Dataset"
      ],
      "metadata": {
        "id": "j8H8yu3ykCy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build tf datasets from traning and test sentences in both languages\n",
        "train_dataset_fr = tf.data.TextLineDataset('/content/train_fr.txt')\n",
        "train_dataset_tw = tf.data.TextLineDataset('/content/train_twi.txt')\n",
        "\n",
        "val_dataset_fr = tf.data.TextLineDataset('/content/test_fr.txt')\n",
        "val_dataset_tw = tf.data.TextLineDataset('/content/test_twi.txt')"
      ],
      "metadata": {
        "id": "fqIWNiixj93A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine languages into single dataset\n",
        "trained_combined = tf.data.Dataset.zip((train_dataset_tw, train_dataset_fr))\n",
        "val_combined = tf.data.Dataset.zip((val_dataset_tw, val_dataset_fr))"
      ],
      "metadata": {
        "id": "jnQ1hMH_kPm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify trained_combined dataset is correct\n",
        "for tw,fr in trained_combined.take(5):\n",
        "  print(\"Twi: \", tw.numpy().decode('utf-8'))\n",
        "  print(\"French: \", fr.numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkpIb3EXkU_O",
        "outputId": "5948b78b-ae57-48d9-bb2c-659cc2cffbae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Twi:  Dɛn na mefi ase ?\n",
            "French:  Par quoi dois je commencer ?\n",
            "Twi:  Meresua sikasɛm ho ade wɔ suapɔn\n",
            "French:  J etudie l economie au college .\n",
            "Twi:  Saa asubɔnten yi mu nnɔ pii saa bere no .\n",
            "French:  Cette riviere devient peu profonde a cet endroit .\n",
            "Twi:  Asamoah ntumi nhu bere tenten a ɛsɛ sɛ ɔtwɛn Araba .\n",
            "French:  Asamoah se demanda combien de temps il devrait attendre Araba .\n",
            "Twi:  Dodow a me mfe rekɔ anim no dodow no ara na mekae nneɛma a amma saa da .\n",
            "French:  Plus je vieillis plus je me souviens clairement de choses qui ne se sont jamais produites .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Training Batches"
      ],
      "metadata": {
        "id": "pr47kMLGlGan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = len(train_twi) \n",
        "BATCH_SIZE = 64\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "  ds\n",
        "  .cache()\n",
        "  .shuffle(BUFFER_SIZE)\n",
        "  .batch(BATCH_SIZE))"
      ],
      "metadata": {
        "id": "ARuJ-j3dkaMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train batches\n",
        "trained_dataset = make_batches(trained_combined)\n",
        "# test batches\n",
        "trained_dataset = make_batches(val_combined)"
      ],
      "metadata": {
        "id": "G4E48b8fnR_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# verify input and target\n",
        "for  example_input_batch,example_target_batch in trained_dataset.take(1):\n",
        "  print(example_input_batch[:5])\n",
        "  print()\n",
        "  print(example_target_batch[:5])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gnmDbsDDnW1G",
        "outputId": "809b2889-cc7c-475e-f57e-b44053c5870e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'Bue so'\n",
            " b'W\\xc9\\x94gyee apam no too mu s\\xc9\\x9b w\\xc9\\x94de bedi dwuma'\n",
            " b'Ne ho pere no s\\xc9\\x9b \\xc9\\x94b\\xc9\\x9by\\xc9\\x9b adwuma pii .'\n",
            " b'Obiara ntumi nguan .' b'Merenhu obiara bio .'], shape=(5,), dtype=string)\n",
            "\n",
            "tf.Tensor(\n",
            "[b'Ouvrez le .' b'Le traite a ete approuve .'\n",
            " b'Il s epuise tellement a travailler .' b'Il n y a pas d issue .'\n",
            " b'Je ne vais voir personne d autre .'], shape=(5,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Vectorization\n"
      ],
      "metadata": {
        "id": "FfvIpXsKtVUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add start and end tokens\n",
        "def tf_start_and_end_tokens(text):\n",
        "  # Split accented characters.\n",
        "  #text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  text = tf.strings.lower(text)\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
        "  return text"
      ],
      "metadata": {
        "id": "9aNO4AlqnlhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab_size = 5000"
      ],
      "metadata": {
        "id": "2zafUQw-tNhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process twi as input\n",
        "twi_tokenizer = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_start_and_end_tokens,\n",
        "    max_tokens=max_vocab_size)\n",
        "twi_tokenizer.adapt(train_dataset_tw)\n",
        "# Here are the first 10 words from the vocabulary:\n",
        "twi_tokenizer.get_vocabulary()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdQSaYL9uNV5",
        "outputId": "de7a4f95-6ad9-4802-e603-98e6cad7a83f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'no', 'sɛ', 'a', 'na', 'so']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process french as output\n",
        "french_tokenizer = tf.keras.layers.TextVectorization(\n",
        "    standardize=tf_start_and_end_tokens,\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "french_tokenizer.adapt(train_dataset_fr)\n",
        "french_tokenizer.get_vocabulary()[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AN-JU6zEukE4",
        "outputId": "10726a42-ab3d-48c4-e2c3-920813a9d21c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['', '[UNK]', '[START]', '[END]', '.', 'a', 'de', 'je', 'est', 'il']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify tokenizer\n",
        "example_tokens = twi_tokenizer(example_input_batch)\n",
        "example_tokens[:3, :10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whFNrwDByRkO",
        "outputId": "839bf62e-caf7-404a-9771-fa7469091be0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 10), dtype=int64, numpy=\n",
              "array([[   2,  792,    9,    3,    0,    0,    0,    0,    0,    0],\n",
              "       [   2,    1, 1912,    5,  262,   14,    6,   86,  548,  124],\n",
              "       [   2,   13,   12,  952,    5,    6,  496,   44,   33,    4]])>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_vocab = np.array(twi_tokenizer.get_vocabulary())\n",
        "tokens = input_vocab[example_tokens[0].numpy()]\n",
        "' '.join(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "CoaRtkchybGn",
        "outputId": "1c18bdb3-ef79-465b-dcf0-c0168e557562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[START] bue so [END]                '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The returned token IDs are zero-padded. This can easily be turned into a mask:"
      ],
      "metadata": {
        "id": "3JTiUeXsy3bA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.subplot(1, 2, 1)\n",
        "plt.pcolormesh(example_tokens)\n",
        "plt.title('Token IDs')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.pcolormesh(example_tokens != 0)\n",
        "plt.title('Mask')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "62XxlkwayyeI",
        "outputId": "211f4087-fa0d-47c6-c654-dee5bbfde76d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Mask')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder\n"
      ],
      "metadata": {
        "id": "QOnz10TWzDeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_units = enc_units\n",
        "        self.input_vocab_size = input_vocab_size\n",
        "\n",
        "        # The embedding layer converts tokens to vectors\n",
        "        self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
        "                                                   embedding_dim)\n",
        "\n",
        "        # The GRU RNN layer processes those vectors sequentially.\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                         # Return the sequence and state\n",
        "                                         return_sequences=True,\n",
        "                                         return_state=True,\n",
        "                                         recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, tokens, state=None):\n",
        "        # The embedding layer looks up the embedding for each token.\n",
        "        vectors = self.embedding(tokens)\n",
        "\n",
        "        # The GRU processes the embedding sequence.\n",
        "        #    output shape: (batch, s, enc_units)\n",
        "        #    state shape: (batch, enc_units)\n",
        "        output, state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "        # 4. Returns the new sequence and its state.\n",
        "        return output, state"
      ],
      "metadata": {
        "id": "iLWhTwqK6Nhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Attention Layer"
      ],
      "metadata": {
        "id": "0AcFzlXhDjBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super().__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "        self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "        self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "    def call(self, query, value, mask):\n",
        "        w1_query = self.W1(query)\n",
        "        w2_key = self.W2(value)\n",
        "        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "        value_mask = mask\n",
        "\n",
        "        context_vector, attention_weights = self.attention(\n",
        "            inputs=[w1_query, value, w2_key],\n",
        "            mask=[query_mask, value_mask],\n",
        "            return_attention_scores=True,\n",
        "        )\n",
        "        return context_vector, attention_weights\n"
      ],
      "metadata": {
        "id": "G55yaW0U-w_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoder layer"
      ],
      "metadata": {
        "id": "afqcLYScH3ab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dec_units = dec_units\n",
        "        self.output_vocab_size = output_vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # For Step 1. The embedding layer converts token IDs to vectors\n",
        "        self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
        "                                                   embedding_dim)\n",
        "\n",
        "        # For Step 2. The RNN keeps track of what's been generated so far.\n",
        "        self.gru= tf.keras.layers.GRU(self.dec_units,\n",
        "                                         return_sequences=True,\n",
        "                                         return_state=True,\n",
        "                                         recurrent_initializer='glorot_uniform')\n",
        "\n",
        "        # For step 3. The RNN output will be the query for the attention layer.\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "        # For step 4. Eqn. (3): converting `ct` to `at`\n",
        "        self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
        "                                        use_bias=False)\n",
        "\n",
        "        # For step 5. This fully connected layer produces the logits for each\n",
        "        # output token.\n",
        "        self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
        "\n",
        "\n",
        "class DecoderInput(typing.NamedTuple):\n",
        "    new_tokens: Any\n",
        "    enc_output: Any\n",
        "    mask: Any\n",
        "\n",
        "\n",
        "class DecoderOutput(typing.NamedTuple):\n",
        "    logits: Any\n",
        "    attention_weights: Any\n",
        "\n",
        "\n",
        "def call(self,\n",
        "         inputs: DecoderInput,\n",
        "         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
        "\n",
        "    # Step 1. Lookup the embeddings\n",
        "    vectors = self.embedding(inputs.new_tokens)\n",
        "    # Step 2. Process one step with the RNN\n",
        "    rnn_output,state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "    # Step 3. Use the RNN output as the query for the attention over the\n",
        "    # encoder output.\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
        "    # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
        "    #     [ct; ht] shape: (batch t, value_units + query_units)\n",
        "    context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "\n",
        "    # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
        "    attention_vector = self.Wc(context_and_rnn_output)\n",
        "    # Step 5. Generate logit predictions:\n",
        "    logits = self.fc(attention_vector)\n",
        "    return DecoderOutput(logits, attention_weights), state\n"
      ],
      "metadata": {
        "id": "MTQH_4OsEgAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Decoder.call = call"
      ],
      "metadata": {
        "id": "oN705AnxIDb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Wl0R2Zg6SkRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self):\n",
        "        self.name = 'masked_loss'\n",
        "        self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "            from_logits=True, reduction='none')\n",
        "\n",
        "    def __call__(self, y_true, y_pred):\n",
        "\n",
        "        # Calculate the loss for each item in the batch.\n",
        "        loss = self.loss(y_true, y_pred)\n",
        "\n",
        "        # Mask off the losses on padding.\n",
        "        mask = tf.cast(y_true != 0, tf.float32)\n",
        "        loss *= mask\n",
        "\n",
        "        # Return the total.\n",
        "        return tf.reduce_sum(loss)\n",
        "\n",
        "\n",
        "class TrainTranslator(tf.keras.Model):\n",
        "    def __init__(self, embedding_dim, units,\n",
        "                 input_text_processor,\n",
        "                 output_text_processor,\n",
        "                 use_tf_function=True):\n",
        "        super().__init__()\n",
        "        # Build the encoder and decoder\n",
        "        encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                          embedding_dim, units)\n",
        "        decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                          embedding_dim, units)\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.input_text_processor = input_text_processor\n",
        "        self.output_text_processor = output_text_processor\n",
        "        self.use_tf_function = use_tf_function\n",
        "\n",
        "    def train_step(self, inputs):\n",
        "        # .shape_checker = ShapeChecker()\n",
        "        if self.use_tf_function:\n",
        "            return self._tf_train_step(inputs)\n",
        "        else:\n",
        "            return self._train_step(inputs)\n",
        "\n",
        "# Implement preprocessing step to:\n",
        "# Receive a batch of input_text, target_text from the tf.data.Dataset.\n",
        "# Convert those raw text inputs to token-embeddings and masks.\n",
        "\n",
        "\n",
        "def _preprocess(self, input_text, target_text):\n",
        "    # Convert the text to token IDs\n",
        "    input_tokens = self.input_text_processor(input_text)\n",
        "    target_tokens = self.output_text_processor(target_text)\n",
        "    # Convert IDs to masks.\n",
        "    input_mask = input_tokens != 0\n",
        "\n",
        "    target_mask = target_tokens != 0\n",
        "    return input_tokens, input_mask, target_tokens, target_mask\n",
        "\n",
        "\n",
        "TrainTranslator._preprocess = _preprocess\n",
        "\n",
        "# the function The _train_step:\n",
        "# Run the encoder on the input_tokens to get the encoder_output and encoder_state.\n",
        "# Initialize the decoder state and loss.\n",
        "# Loop over the target_tokens:\n",
        "#   Run the decoder one step at a time.\n",
        "#   Calculate the loss for each step.\n",
        "# Accumulate the average loss.\n",
        "# Calculate the gradient of the loss and use the optimizer to apply updates to the model's trainable_variables.\n",
        "\n",
        "\n",
        "def _train_step(self, inputs):\n",
        "    input_text, target_text = inputs\n",
        "\n",
        "    (input_tokens, input_mask,\n",
        "     target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
        "\n",
        "    max_target_length = tf.shape(target_tokens)[1]\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Encode the input\n",
        "        enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "        # Initialize the decoder's state to the encoder's final state.\n",
        "        # This only works if the encoder and decoder have the same number of\n",
        "        # units.\n",
        "        dec_state = enc_state\n",
        "        loss = tf.constant(0.0)\n",
        "\n",
        "        for t in tf.range(max_target_length-1):\n",
        "            # Pass in two tokens from the target sequence:\n",
        "            # 1. The current input to the decoder.\n",
        "            # 2. The target for the decoder's next prediction.\n",
        "            new_tokens = target_tokens[:, t:t+2]\n",
        "            step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
        "                                                   enc_output, dec_state)\n",
        "            loss = loss + step_loss\n",
        "\n",
        "        # Average the loss over all non padding tokens.\n",
        "        average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "    # Apply an optimization step\n",
        "    variables = self.trainable_variables\n",
        "    gradients = tape.gradient(average_loss, variables)\n",
        "    self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    # Return a dict mapping metric names to current value\n",
        "    return {'batch_loss': average_loss}\n",
        "\n",
        "\n",
        "TrainTranslator._train_step = _train_step\n",
        "\n",
        "# The _loop_step method, added below, executes the decoder and calculates the incremental loss and new decoder state (dec_state).\n",
        "\n",
        "\n",
        "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
        "    input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
        "\n",
        "    # Run the decoder one step.\n",
        "    decoder_input = DecoderInput(new_tokens=input_token,\n",
        "                                 enc_output=enc_output,\n",
        "                                 mask=input_mask)\n",
        "\n",
        "    dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
        "\n",
        "    # `self.loss` returns the total for non-padded tokens\n",
        "    y = target_token\n",
        "    y_pred = dec_result.logits\n",
        "    step_loss = self.loss(y, y_pred)\n",
        "\n",
        "    return step_loss, dec_state\n",
        "\n",
        "TrainTranslator._loop_step = _loop_step"
      ],
      "metadata": {
        "id": "pgyqEVm2Ku8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set Hyerperameters\n",
        "embedding_dim = 256\n",
        "units = 1024"
      ],
      "metadata": {
        "id": "BI0V2_YNSpO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_translator= TrainTranslator(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=twi_tokenizer,\n",
        "    output_text_processor=french_tokenizer,\n",
        "    use_tf_function=False)\n",
        "# Configure the loss and optimizer\n",
        "train_translator.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ],
      "metadata": {
        "id": "31yGNhKaaSU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
        "def _tf_train_step(self, inputs):\n",
        "  return self._train_step(inputs)"
      ],
      "metadata": {
        "id": "l0UWvkChaVsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainTranslator._tf_train_step = _tf_train_step"
      ],
      "metadata": {
        "id": "Aa0IpCQza1nN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_translator.use_tf_function = True"
      ],
      "metadata": {
        "id": "SZv0M7StbGlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key):\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_end(self, n, logs):\n",
        "    self.logs.append(logs[self.key])\n",
        "\n",
        "batch_loss = BatchLogs('batch_loss')"
      ],
      "metadata": {
        "id": "ElmBkfIWej8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_translator.fit(trained_dataset, epochs=100,\n",
        "                     callbacks=[batch_loss])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRCC98HFb7zG",
        "outputId": "5cf2b556-3e7d-4b6e-d752-4104c561ead7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80/80 [==============================] - 636s 8s/step - batch_loss: 5.0322\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(batch_loss.logs)\n",
        "plt.ylim([0, 3])\n",
        "plt.xlabel('Batch #')\n",
        "plt.ylabel('CE/token')"
      ],
      "metadata": {
        "id": "ej3vGAWRb9Pt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}