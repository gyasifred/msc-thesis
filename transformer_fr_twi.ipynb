{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/transformer_fr_twi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is based on the implementation of the paper [Attention is All you Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017). The code are adapted from the Tenssorflow repository at https://www.tensorflow.org/text/tutorials/transformer#build_the_transformer and a big thanks to [[Crater David]](https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:37370749) \n"
      ],
      "metadata": {
        "id": "6qFzQxA29DJp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hblEgiSfLEck"
      },
      "source": [
        "# Install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U tensorflow-text-nightly==2.11.0.dev20220817\n",
        "!pip install -U -q tf-nightly==2.11.0.dev20220817"
      ],
      "metadata": {
        "id": "naWgZqfLsOiN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae659242-aaa7-45d3-fbcd-2468a3c4b72b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 5.9 MB 14.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 582.0 MB 14 kB/s \n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 56.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 439 kB 67.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 54.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY7seog2LQOL"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwubBvId4uGK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W18p7g2lLatA"
      },
      "source": [
        "# preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovdl-UER8hMX"
      },
      "outputs": [],
      "source": [
        "# This code was adapted from https://github.com/GhanaNLP/kasa/blob/master/Kasa/Preprocessing.py\n",
        "# A subclass of the TFT  for preprocessing data\n",
        "# import required library\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "class Preprocessing:\n",
        "    # dummy initialization method\n",
        "    def __init__(self):\n",
        "        # initialize with some default parameters here later\n",
        "        pass\n",
        "\n",
        "    # read in parallel twi - french dataset\n",
        "    def read_parallel_dataset(self, filepath_twi, filepath_french):\n",
        "\n",
        "        # read french data\n",
        "        french_data = []\n",
        "        with open(filepath_french, encoding='utf-8') as file:\n",
        "            line = file.readline()\n",
        "            cnt = 1\n",
        "            while line:\n",
        "                french_data.append(line.strip())\n",
        "                line = file.readline()\n",
        "                cnt += 1\n",
        "\n",
        "        # read twi data\n",
        "        twi_data = []\n",
        "        with open(filepath_twi, encoding='utf-8') as file:\n",
        "\n",
        "            # twi=file.read()\n",
        "            line = file.readline()\n",
        "            cnt = 1\n",
        "            while line:\n",
        "                twi_data.append(line.strip())\n",
        "                line = file.readline()\n",
        "                cnt += 1\n",
        "\n",
        "        return twi_data, french_data\n",
        "\n",
        "    # Define a helper function to remove string accents\n",
        "\n",
        "    def removeStringAccent(self, s):\n",
        "        return ''.join(\n",
        "            c for c in unicodedata.normalize('NFD', s)\n",
        "            if unicodedata.category(c) != 'Mn'\n",
        "        )\n",
        "\n",
        "    # normalize input twi sentence\n",
        "    def normalize_twi(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.ƆɔɛƐ!?’]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "\n",
        "    # normalize input french sentence\n",
        "    def normalize_fr(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4asafNW9HIt"
      },
      "outputs": [],
      "source": [
        "# Create an instance of tft preprocessing class\n",
        "\n",
        "TwiFrPreprocessor = Preprocessing()\n",
        "\n",
        "# Read raw parallel dataset\n",
        "raw_data_twi,raw_data_fr = TwiFrPreprocessor.read_parallel_dataset(\n",
        "        filepath_twi='/content/verified_twi.txt',\n",
        "        filepath_french='/content/verified_french.txt')\n",
        "\n",
        "# Normalize the raw data\n",
        "raw_data_fr = [TwiFrPreprocessor.normalize_fr(data) for data in raw_data_fr]\n",
        "raw_data_twi = [TwiFrPreprocessor.normalize_twi(data) for data in raw_data_twi]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au81TkNOLwnB"
      },
      "source": [
        "# split data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5l08UvTL9mtx"
      },
      "outputs": [],
      "source": [
        "# split the dataset into training and test sets\n",
        "# Keep 20% of the data as test set\n",
        "train_twi,test_twi,train_fr,test_fr = train_test_split(raw_data_twi,raw_data_fr, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEbWCsNi9wao"
      },
      "outputs": [],
      "source": [
        "# define function to write text to txt file\n",
        "def writeTotxt(destination,data):\n",
        "  with open(destination, 'w') as f:\n",
        "    for line in data:\n",
        "        f.write(f\"{line}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q4djANP-9g6"
      },
      "outputs": [],
      "source": [
        "# write the preprocess traning and test dataset to a file\n",
        "writeTotxt('train_twi.txt',train_twi)\n",
        "writeTotxt('train_fr.txt',train_fr)\n",
        "writeTotxt('test_twi.txt',test_twi)\n",
        "writeTotxt('test_fr.txt',test_fr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao2l87AzL_kE"
      },
      "source": [
        "# Build tf dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WZrSEP9_ISJ"
      },
      "outputs": [],
      "source": [
        "# build tf datasets from traning and test sentences in both languages\n",
        "train_dataset_fr = tf.data.TextLineDataset('/content/train_fr.txt')\n",
        "train_dataset_tw = tf.data.TextLineDataset('/content/train_twi.txt')\n",
        "\n",
        "val_dataset_fr = tf.data.TextLineDataset('/content/test_fr.txt')\n",
        "val_dataset_tw = tf.data.TextLineDataset('/content/test_twi.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmksbF-CAEXg"
      },
      "outputs": [],
      "source": [
        "# combine languages into single dataset\n",
        "trained_combined = tf.data.Dataset.zip((train_dataset_fr, train_dataset_tw))\n",
        "val_combined = tf.data.Dataset.zip((val_dataset_fr, val_dataset_tw))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZJ-hiEUBLDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36e7cae3-d7d7-413b-bf98-b5b7600984bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "French:  Par quoi dois je commencer ?\n",
            "Twi:  Dɛn na mefi ase ?\n",
            "French:  J etudie l economie au college .\n",
            "Twi:  Meresua sikasɛm ho ade wɔ suapɔn\n",
            "French:  Cette riviere devient peu profonde a cet endroit .\n",
            "Twi:  Saa asubɔnten yi mu nnɔ pii saa bere no .\n",
            "French:  Asamoah se demanda combien de temps il devrait attendre Araba .\n",
            "Twi:  Asamoah ntumi nhu bere tenten a ɛsɛ sɛ ɔtwɛn Araba .\n",
            "French:  Plus je vieillis plus je me souviens clairement de choses qui ne se sont jamais produites .\n",
            "Twi:  Dodow a me mfe rekɔ anim no dodow no ara na mekae nneɛma a amma saa da .\n"
          ]
        }
      ],
      "source": [
        "# verify trained_combined dataset is correct\n",
        "for  fr,tw in trained_combined.take(5):\n",
        "  print(\"French: \", fr.numpy().decode('utf-8'))\n",
        "  print(\"Twi: \", tw.numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaxFg3TZMIK8"
      },
      "source": [
        "# load tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. The tokenizer is which was build from the parallel corpus.\n",
        "2. The code are available on the link https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/build_subword_tokenizer.ipynb\n"
      ],
      "metadata": {
        "id": "Vjb-3AU1BnrK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR_vgyQgBo_T"
      },
      "outputs": [],
      "source": [
        "model_named = '/content/drive/MyDrive/translate_fr_twi_converter'\n",
        "# Verify tokenizer model can be reloaded\n",
        "tokenizers = tf.saved_model.load(model_named)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdHMl3icEjos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cca25b06-4749-463a-bafe-ffd3494902fa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'je', b'##e', b'##he', b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Verify tokenizer works on french sentence\n",
        "tokens = tokenizers.fr.tokenize(['je suis étudiant'])\n",
        "text_tokens = tokenizers.fr.lookup(tokens)\n",
        "text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSOMXh_de04Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab476faa-8d5b-46be-c0ce-5c70fd634c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "je suis etudiant\n"
          ]
        }
      ],
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.fr.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBNqSqeSfChq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef2c517-3170-49c6-a242-eab39ef21ad1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'na', b'atwa', b'biara', b'mu',\n",
              "  b'\\xc9\\x94b\\xc9\\x9bk\\xc9\\x94', b'wob\\xc9\\x9by\\xc9\\x9b', b'pii',\n",
              "  b'w\\xc9\\x94', b'a', b'nufusu', b'anaa', b'polisifo', b'h\\xc9\\x94', b'.',\n",
              "  b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Verify tokenizer works on twi sentence\n",
        "tokens = tokenizers.twi.tokenize(['Na mmarimaa ne mmeawa pii wɔ agudibea hɔ .'])\n",
        "text_tokens = tokenizers.twi.lookup(tokens)\n",
        "text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lS1xMYQfQR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "356c8b2d-9a2c-4c03-b855-c3ad87c3a6de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "na mmarimaa ne mmeawa pii wɔ agudibea hɔ .\n"
          ]
        }
      ],
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.twi.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQVo6G3OMfac"
      },
      "source": [
        "# Check sentence with maximum length"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By checking the maximum length of sentence, you can suitably select the MAX_TOKENS. The MAX_TOKKENS help drops examples longer than the maximum number of tokens (MAX_TOKENS). Without limiting the size of sequences, the performance may be negatively affected.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vvoh1vRgCxSb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VJxJuNufTCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4981ac67-a9e4-4103-bdcd-221f8097e261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "......................................................................................................................................................................................................................................................................................................................................................................................................................."
          ]
        }
      ],
      "source": [
        "#The distribution of tokens per example in the dataset is as follows:\n",
        "\n",
        "lengths = []\n",
        "\n",
        "for fr_examples,twi_examples in trained_combined.batch(50):\n",
        "  twi_tokens = tokenizers.twi.tokenize(twi_examples)\n",
        "  lengths.append(twi_tokens.row_lengths())\n",
        "\n",
        "  fr_tokens = tokenizers.fr.tokenize(fr_examples)\n",
        "  lengths.append(fr_tokens.row_lengths())\n",
        "  print('.', end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri-qmhyyiBSa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "36fb84c9-bf8c-4628-8489-fff40deb9f49"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "all_lengths = np.concatenate(lengths)\n",
        "\n",
        "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
        "plt.ylim(plt.ylim())\n",
        "max_length = max(all_lengths)\n",
        "plt.plot([max_length, max_length], plt.ylim())\n",
        "plt.title(f'Maximum tokens per example: {max_length}');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DYwhdWAMpnW"
      },
      "source": [
        "# Tokenize the training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5piHJYkWjKii"
      },
      "outputs": [],
      "source": [
        "# function to drops examples longer than the maximum number of tokens (MAX_TOKENS).\n",
        "MAX_TOKENS = 50\n",
        "\n",
        "def filter_max_tokens(l1, l2):\n",
        "  num_tokens = tf.maximum(tf.shape(l1)[1],tf.shape(l2)[1])\n",
        "  return num_tokens < MAX_TOKENS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GC7UlwEeiNTO"
      },
      "outputs": [],
      "source": [
        "# Routine to tokenize sentence pairs; needed to make sentence batches for training\n",
        "def tokenize_pairs(fr,twi):\n",
        "  fr = tokenizers.fr.tokenize(fr)\n",
        "  # Convert from ragged to dense, padding with zeros.\n",
        "  fr = fr.to_tensor()\n",
        "  tw = tokenizers.twi.tokenize(twi)\n",
        "  # Convert from ragged to dense, padding with zeros.\n",
        "  tw = tw.to_tensor()\n",
        "  return fr,tw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWMySNLzklc_"
      },
      "outputs": [],
      "source": [
        "# create training batches\n",
        "BUFFER_SIZE = len(train_fr)\n",
        "BATCH_SIZE = 64\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .filter(filter_max_tokens)\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "# Create training and validation set batches.\n",
        "train_batches = make_batches(trained_combined)\n",
        "val_batches = make_batches(val_combined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNFMseSdM3wr"
      },
      "source": [
        "# Positional encoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional encodings are added to the embeddings to give the model some information about the relative position of the tokens in the sentence so the model can learn to recognize the word order.\n",
        "\n"
      ],
      "metadata": {
        "id": "5du74jWFF5WK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "  \n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # Apply the sine function to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # Apply the cosine function to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "KXtWVxfeb6px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a point-wise feed-forward network"
      ],
      "metadata": {
        "id": "mXL_HTJWG44o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def point_wise_feed_forward_network(\n",
        "  d_model, # Input/output dimensionality.\n",
        "  dff # Inner-layer dimensionality.\n",
        "  ):\n",
        "\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # Shape `(batch_size, seq_len, dff)`.\n",
        "      tf.keras.layers.Dense(d_model)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "  ])"
      ],
      "metadata": {
        "id": "db4kXyfGb_l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Transformer"
      ],
      "metadata": {
        "id": "wSEO7HUZHppR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multihead Attention layer"
      ],
      "metadata": {
        "id": "UtydO3sOIRE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create mask for padding tokens so translator ignores them\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "\n",
        "# Mask future tokens so translator cannot see future\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "\n",
        "# Create final masks\n",
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead)\n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "      q: query shape == (..., seq_len_q, depth)\n",
        "      k: key shape == (..., seq_len_k, depth)\n",
        "      v: value shape == (..., seq_len_v, depth_v)\n",
        "      mask: Float tensor with shape broadcastable\n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "      output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    # (..., seq_len_q, seq_len_k)\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(\n",
        "        scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# Add attention layers to create multi-head attention\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, *, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # (batch_size, num_heads, seq_len_q, depth)\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        # (batch_size, num_heads, seq_len_k, depth)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        # (batch_size, num_heads, seq_len_v, depth)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        # (batch_size, seq_len_q, num_heads, depth)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        # (batch_size, seq_len_q, d_model)\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "        # Build Transformer encoder layer"
      ],
      "metadata": {
        "id": "BSHsspXXIaUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "yvNLzFWpJw6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ],
      "metadata": {
        "id": "bCdZS_nJcGVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build Transformer encoder from encoder layer\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "              maximum_position_encoding=MAX_TOKENS,rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "metadata": {
        "id": "EBZNB3MacRCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "ndsnLCOwRgcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "metadata": {
        "id": "fnXVTpI_cgzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build Transformer decoder from decoder layer\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "              maximum_position_encoding,rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size,d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ],
      "metadata": {
        "id": "UEuoBDtJcp7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "cjFbGSvJUQY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build full Transformer\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_vocab_size, pe_input=MAX_TOKENS, pe_target=MAX_TOKENS, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.encoder= Encoder(num_layers, d_model, num_heads, dff,input_vocab_size, pe_input, rate)\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff,target_vocab_size, pe_target, rate)\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "  \n",
        "  def call(self, inp, tar, training, enc_padding_mask,look_ahead_mask, dec_padding_mask):\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask) # (batch_size, inp_seq_len, d_model)\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    final_output = self.final_layer(dec_output) # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    return final_output, attention_weights\n"
      ],
      "metadata": {
        "id": "_6sd1ofVcyl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer and Metrics"
      ],
      "metadata": {
        "id": "pPhqoBhwWYZ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y79jHzBnA7wG"
      },
      "outputs": [],
      "source": [
        "# Set learning rate schedule\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set hyperparameters"
      ],
      "metadata": {
        "id": "g11C2K3tXFyP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q51YVNusr5_"
      },
      "outputs": [],
      "source": [
        "num_layers = 6\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate a Transformer\n"
      ],
      "metadata": {
        "id": "5fGafl4DXYct"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DJxw765BYHi"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=tokenizers.fr.get_vocab_size().numpy(),\n",
        "    target_vocab_size=tokenizers.twi.get_vocab_size().numpy(),\n",
        "    rate=dropout_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "-aGTknsnYDCC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BF0wsHaaJVKC"
      },
      "outputs": [],
      "source": [
        "# Instantiate learning rate and set optimizer\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "metadata": {
        "id": "HpLaySRfSy89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PHWy8sdGoR8"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = './checkpoints/train'\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# If a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose number of training epochs\n",
        "EPOCHS = 100\n",
        "train_step_signature = [\n",
        "  tf.TensorSpec(shape=(None, None), dtype=tf.int64), \n",
        "  tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp,True,enc_padding_mask,combined_mask,dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ],
      "metadata": {
        "id": "VAj0P_T0ZcBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7k6GwwEIi_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2707b2ac-2a78-4c2f-803e-2fa4d65e1dc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 7.9555 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 7.1838 Accuracy 0.0544\n",
            "Epoch 1 Batch 100 Loss 6.7439 Accuracy 0.0737\n",
            "Epoch 1 Batch 150 Loss 6.4487 Accuracy 0.0929\n",
            "Epoch 1 Batch 200 Loss 6.1771 Accuracy 0.1185\n",
            "Epoch 1 Batch 250 Loss 5.9704 Accuracy 0.1359\n",
            "Epoch 1 Loss 5.9076 Accuracy 0.1412\n",
            "Time taken for 1 epoch: 77.14 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 5.0366 Accuracy 0.2052\n",
            "Epoch 2 Batch 50 Loss 4.9705 Accuracy 0.2183\n",
            "Epoch 2 Batch 100 Loss 4.9197 Accuracy 0.2222\n",
            "Epoch 2 Batch 150 Loss 4.8662 Accuracy 0.2274\n",
            "Epoch 2 Batch 200 Loss 4.8119 Accuracy 0.2329\n",
            "Epoch 2 Batch 250 Loss 4.7586 Accuracy 0.2378\n",
            "Epoch 2 Loss 4.7379 Accuracy 0.2401\n",
            "Time taken for 1 epoch: 56.03 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4.3325 Accuracy 0.2892\n",
            "Epoch 3 Batch 50 Loss 4.3730 Accuracy 0.2735\n",
            "Epoch 3 Batch 100 Loss 4.3314 Accuracy 0.2783\n",
            "Epoch 3 Batch 150 Loss 4.2944 Accuracy 0.2805\n",
            "Epoch 3 Batch 200 Loss 4.2561 Accuracy 0.2837\n",
            "Epoch 3 Batch 250 Loss 4.2140 Accuracy 0.2878\n",
            "Epoch 3 Loss 4.1969 Accuracy 0.2897\n",
            "Time taken for 1 epoch: 56.03 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 4.0689 Accuracy 0.2953\n",
            "Epoch 4 Batch 50 Loss 3.8887 Accuracy 0.3195\n",
            "Epoch 4 Batch 100 Loss 3.8399 Accuracy 0.3247\n",
            "Epoch 4 Batch 150 Loss 3.8163 Accuracy 0.3276\n",
            "Epoch 4 Batch 200 Loss 3.7910 Accuracy 0.3299\n",
            "Epoch 4 Batch 250 Loss 3.7658 Accuracy 0.3320\n",
            "Epoch 4 Loss 3.7589 Accuracy 0.3324\n",
            "Time taken for 1 epoch: 56.35 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 3.4281 Accuracy 0.3653\n",
            "Epoch 5 Batch 50 Loss 3.5003 Accuracy 0.3566\n",
            "Epoch 5 Batch 100 Loss 3.4879 Accuracy 0.3586\n",
            "Epoch 5 Batch 150 Loss 3.4804 Accuracy 0.3578\n",
            "Epoch 5 Batch 200 Loss 3.4720 Accuracy 0.3578\n",
            "Epoch 5 Batch 250 Loss 3.4520 Accuracy 0.3595\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
            "Epoch 5 Loss 3.4437 Accuracy 0.3606\n",
            "Time taken for 1 epoch: 59.30 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 3.3789 Accuracy 0.3576\n",
            "Epoch 6 Batch 50 Loss 3.2380 Accuracy 0.3774\n",
            "Epoch 6 Batch 100 Loss 3.2217 Accuracy 0.3820\n",
            "Epoch 6 Batch 150 Loss 3.2207 Accuracy 0.3820\n",
            "Epoch 6 Batch 200 Loss 3.2109 Accuracy 0.3826\n",
            "Epoch 6 Batch 250 Loss 3.2007 Accuracy 0.3840\n",
            "Epoch 6 Loss 3.1962 Accuracy 0.3844\n",
            "Time taken for 1 epoch: 55.95 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.9815 Accuracy 0.4266\n",
            "Epoch 7 Batch 50 Loss 2.9762 Accuracy 0.4113\n",
            "Epoch 7 Batch 100 Loss 2.9736 Accuracy 0.4110\n",
            "Epoch 7 Batch 150 Loss 2.9836 Accuracy 0.4084\n",
            "Epoch 7 Batch 200 Loss 2.9781 Accuracy 0.4092\n",
            "Epoch 7 Batch 250 Loss 2.9746 Accuracy 0.4092\n",
            "Epoch 7 Loss 2.9726 Accuracy 0.4095\n",
            "Time taken for 1 epoch: 56.86 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.8517 Accuracy 0.4236\n",
            "Epoch 8 Batch 50 Loss 2.7601 Accuracy 0.4334\n",
            "Epoch 8 Batch 100 Loss 2.7786 Accuracy 0.4314\n",
            "Epoch 8 Batch 150 Loss 2.7861 Accuracy 0.4309\n",
            "Epoch 8 Batch 200 Loss 2.7838 Accuracy 0.4313\n",
            "Epoch 8 Batch 250 Loss 2.7886 Accuracy 0.4296\n",
            "Epoch 8 Loss 2.7904 Accuracy 0.4293\n",
            "Time taken for 1 epoch: 55.77 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.2551 Accuracy 0.4920\n",
            "Epoch 9 Batch 50 Loss 2.5853 Accuracy 0.4536\n",
            "Epoch 9 Batch 100 Loss 2.6148 Accuracy 0.4508\n",
            "Epoch 9 Batch 150 Loss 2.6214 Accuracy 0.4508\n",
            "Epoch 9 Batch 200 Loss 2.6287 Accuracy 0.4509\n",
            "Epoch 9 Batch 250 Loss 2.6358 Accuracy 0.4497\n",
            "Epoch 9 Loss 2.6382 Accuracy 0.4492\n",
            "Time taken for 1 epoch: 55.94 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 2.4987 Accuracy 0.4686\n",
            "Epoch 10 Batch 50 Loss 2.4364 Accuracy 0.4788\n",
            "Epoch 10 Batch 100 Loss 2.4813 Accuracy 0.4705\n",
            "Epoch 10 Batch 150 Loss 2.4868 Accuracy 0.4705\n",
            "Epoch 10 Batch 200 Loss 2.5017 Accuracy 0.4686\n",
            "Epoch 10 Batch 250 Loss 2.5080 Accuracy 0.4678\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
            "Epoch 10 Loss 2.5132 Accuracy 0.4670\n",
            "Time taken for 1 epoch: 58.96 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 2.1905 Accuracy 0.5111\n",
            "Epoch 11 Batch 50 Loss 2.3243 Accuracy 0.4935\n",
            "Epoch 11 Batch 100 Loss 2.3547 Accuracy 0.4877\n",
            "Epoch 11 Batch 150 Loss 2.3803 Accuracy 0.4839\n",
            "Epoch 11 Batch 200 Loss 2.3913 Accuracy 0.4818\n",
            "Epoch 11 Batch 250 Loss 2.4092 Accuracy 0.4794\n",
            "Epoch 11 Loss 2.4138 Accuracy 0.4786\n",
            "Time taken for 1 epoch: 55.91 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 2.1788 Accuracy 0.5129\n",
            "Epoch 12 Batch 50 Loss 2.2365 Accuracy 0.5031\n",
            "Epoch 12 Batch 100 Loss 2.2557 Accuracy 0.5005\n",
            "Epoch 12 Batch 150 Loss 2.2973 Accuracy 0.4936\n",
            "Epoch 12 Batch 200 Loss 2.3140 Accuracy 0.4916\n",
            "Epoch 12 Batch 250 Loss 2.3318 Accuracy 0.4894\n",
            "Epoch 12 Loss 2.3363 Accuracy 0.4893\n",
            "Time taken for 1 epoch: 56.31 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 2.1267 Accuracy 0.5237\n",
            "Epoch 13 Batch 50 Loss 2.1570 Accuracy 0.5160\n",
            "Epoch 13 Batch 100 Loss 2.2090 Accuracy 0.5077\n",
            "Epoch 13 Batch 150 Loss 2.2282 Accuracy 0.5047\n",
            "Epoch 13 Batch 200 Loss 2.2486 Accuracy 0.5011\n",
            "Epoch 13 Batch 250 Loss 2.2693 Accuracy 0.4978\n",
            "Epoch 13 Loss 2.2792 Accuracy 0.4966\n",
            "Time taken for 1 epoch: 54.96 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 2.0983 Accuracy 0.5156\n",
            "Epoch 14 Batch 50 Loss 2.1209 Accuracy 0.5216\n",
            "Epoch 14 Batch 100 Loss 2.1520 Accuracy 0.5149\n",
            "Epoch 14 Batch 150 Loss 2.1873 Accuracy 0.5092\n",
            "Epoch 14 Batch 200 Loss 2.2223 Accuracy 0.5040\n",
            "Epoch 14 Batch 250 Loss 2.2468 Accuracy 0.5003\n",
            "Epoch 14 Loss 2.2518 Accuracy 0.4997\n",
            "Time taken for 1 epoch: 55.03 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 2.0514 Accuracy 0.5400\n",
            "Epoch 15 Batch 50 Loss 2.1089 Accuracy 0.5181\n",
            "Epoch 15 Batch 100 Loss 2.1288 Accuracy 0.5154\n",
            "Epoch 15 Batch 150 Loss 2.1677 Accuracy 0.5099\n",
            "Epoch 15 Batch 200 Loss 2.1927 Accuracy 0.5061\n",
            "Epoch 15 Batch 250 Loss 2.2144 Accuracy 0.5029\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
            "Epoch 15 Loss 2.2250 Accuracy 0.5011\n",
            "Time taken for 1 epoch: 59.83 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 2.0374 Accuracy 0.5272\n",
            "Epoch 16 Batch 50 Loss 2.0482 Accuracy 0.5312\n",
            "Epoch 16 Batch 100 Loss 2.1025 Accuracy 0.5209\n",
            "Epoch 16 Batch 150 Loss 2.1349 Accuracy 0.5149\n",
            "Epoch 16 Batch 200 Loss 2.1542 Accuracy 0.5113\n",
            "Epoch 16 Batch 250 Loss 2.1761 Accuracy 0.5087\n",
            "Epoch 16 Loss 2.1811 Accuracy 0.5081\n",
            "Time taken for 1 epoch: 54.97 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.9724 Accuracy 0.5336\n",
            "Epoch 17 Batch 50 Loss 1.9941 Accuracy 0.5376\n",
            "Epoch 17 Batch 100 Loss 2.0077 Accuracy 0.5351\n",
            "Epoch 17 Batch 150 Loss 2.0362 Accuracy 0.5304\n",
            "Epoch 17 Batch 200 Loss 2.0498 Accuracy 0.5274\n",
            "Epoch 17 Batch 250 Loss 2.0732 Accuracy 0.5235\n",
            "Epoch 17 Loss 2.0824 Accuracy 0.5222\n",
            "Time taken for 1 epoch: 55.26 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.7613 Accuracy 0.5832\n",
            "Epoch 18 Batch 50 Loss 1.8665 Accuracy 0.5620\n",
            "Epoch 18 Batch 100 Loss 1.8964 Accuracy 0.5548\n",
            "Epoch 18 Batch 150 Loss 1.9358 Accuracy 0.5487\n",
            "Epoch 18 Batch 200 Loss 1.9648 Accuracy 0.5436\n",
            "Epoch 18 Batch 250 Loss 1.9781 Accuracy 0.5415\n",
            "Epoch 18 Loss 1.9898 Accuracy 0.5396\n",
            "Time taken for 1 epoch: 55.62 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.9494 Accuracy 0.5376\n",
            "Epoch 19 Batch 50 Loss 1.8046 Accuracy 0.5685\n",
            "Epoch 19 Batch 100 Loss 1.8381 Accuracy 0.5631\n",
            "Epoch 19 Batch 150 Loss 1.8768 Accuracy 0.5564\n",
            "Epoch 19 Batch 200 Loss 1.8950 Accuracy 0.5538\n",
            "Epoch 19 Batch 250 Loss 1.9159 Accuracy 0.5497\n",
            "Epoch 19 Loss 1.9204 Accuracy 0.5491\n",
            "Time taken for 1 epoch: 55.48 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.7511 Accuracy 0.5630\n",
            "Epoch 20 Batch 50 Loss 1.7204 Accuracy 0.5806\n",
            "Epoch 20 Batch 100 Loss 1.7536 Accuracy 0.5768\n",
            "Epoch 20 Batch 150 Loss 1.7853 Accuracy 0.5709\n",
            "Epoch 20 Batch 200 Loss 1.8057 Accuracy 0.5680\n",
            "Epoch 20 Batch 250 Loss 1.8207 Accuracy 0.5656\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
            "Epoch 20 Loss 1.8292 Accuracy 0.5644\n",
            "Time taken for 1 epoch: 58.65 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.6602 Accuracy 0.5970\n",
            "Epoch 21 Batch 50 Loss 1.6345 Accuracy 0.5971\n",
            "Epoch 21 Batch 100 Loss 1.6644 Accuracy 0.5920\n",
            "Epoch 21 Batch 200 Loss 1.7179 Accuracy 0.5835\n",
            "Epoch 21 Batch 250 Loss 1.7340 Accuracy 0.5808\n",
            "Epoch 21 Loss 1.7462 Accuracy 0.5787\n",
            "Time taken for 1 epoch: 56.22 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 1.6612 Accuracy 0.6023\n",
            "Epoch 22 Batch 50 Loss 1.5768 Accuracy 0.6092\n",
            "Epoch 22 Batch 100 Loss 1.6089 Accuracy 0.6035\n",
            "Epoch 22 Batch 150 Loss 1.6216 Accuracy 0.6009\n",
            "Epoch 22 Batch 200 Loss 1.6406 Accuracy 0.5969\n",
            "Epoch 22 Batch 250 Loss 1.6597 Accuracy 0.5941\n",
            "Epoch 22 Loss 1.6662 Accuracy 0.5929\n",
            "Time taken for 1 epoch: 55.24 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 1.5808 Accuracy 0.6140\n",
            "Epoch 23 Batch 50 Loss 1.5016 Accuracy 0.6236\n",
            "Epoch 23 Batch 100 Loss 1.5265 Accuracy 0.6187\n",
            "Epoch 23 Batch 150 Loss 1.5486 Accuracy 0.6145\n",
            "Epoch 23 Batch 200 Loss 1.5672 Accuracy 0.6104\n",
            "Epoch 23 Batch 250 Loss 1.5925 Accuracy 0.6054\n",
            "Epoch 23 Loss 1.5959 Accuracy 0.6047\n",
            "Time taken for 1 epoch: 55.54 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 1.4316 Accuracy 0.6396\n",
            "Epoch 24 Batch 50 Loss 1.4417 Accuracy 0.6370\n",
            "Epoch 24 Batch 100 Loss 1.4723 Accuracy 0.6288\n",
            "Epoch 24 Batch 150 Loss 1.4817 Accuracy 0.6259\n",
            "Epoch 24 Batch 200 Loss 1.4974 Accuracy 0.6230\n",
            "Epoch 24 Batch 250 Loss 1.5150 Accuracy 0.6197\n",
            "Epoch 24 Loss 1.5223 Accuracy 0.6184\n",
            "Time taken for 1 epoch: 55.99 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 1.4145 Accuracy 0.6349\n",
            "Epoch 25 Batch 50 Loss 1.3650 Accuracy 0.6496\n",
            "Epoch 25 Batch 100 Loss 1.3751 Accuracy 0.6477\n",
            "Epoch 25 Batch 150 Loss 1.4071 Accuracy 0.6401\n",
            "Epoch 25 Batch 200 Loss 1.4322 Accuracy 0.6349\n",
            "Epoch 25 Batch 250 Loss 1.4498 Accuracy 0.6309\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-5\n",
            "Epoch 25 Loss 1.4576 Accuracy 0.6295\n",
            "Time taken for 1 epoch: 57.85 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 1.3036 Accuracy 0.6600\n",
            "Epoch 26 Batch 50 Loss 1.3026 Accuracy 0.6635\n",
            "Epoch 26 Batch 100 Loss 1.3108 Accuracy 0.6612\n",
            "Epoch 26 Batch 150 Loss 1.3328 Accuracy 0.6551\n",
            "Epoch 26 Batch 200 Loss 1.3604 Accuracy 0.6483\n",
            "Epoch 26 Batch 250 Loss 1.3819 Accuracy 0.6433\n",
            "Epoch 26 Loss 1.3916 Accuracy 0.6414\n",
            "Time taken for 1 epoch: 55.43 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 1.3135 Accuracy 0.6738\n",
            "Epoch 27 Batch 50 Loss 1.2839 Accuracy 0.6661\n",
            "Epoch 27 Batch 100 Loss 1.2731 Accuracy 0.6686\n",
            "Epoch 27 Batch 150 Loss 1.2902 Accuracy 0.6649\n",
            "Epoch 27 Batch 200 Loss 1.3064 Accuracy 0.6611\n",
            "Epoch 27 Batch 250 Loss 1.3224 Accuracy 0.6569\n",
            "Epoch 27 Loss 1.3300 Accuracy 0.6548\n",
            "Time taken for 1 epoch: 55.71 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 1.2072 Accuracy 0.6721\n",
            "Epoch 28 Batch 50 Loss 1.1778 Accuracy 0.6858\n",
            "Epoch 28 Batch 100 Loss 1.1991 Accuracy 0.6806\n",
            "Epoch 28 Batch 150 Loss 1.2326 Accuracy 0.6735\n",
            "Epoch 28 Batch 200 Loss 1.2474 Accuracy 0.6703\n",
            "Epoch 28 Batch 250 Loss 1.2648 Accuracy 0.6671\n",
            "Epoch 28 Loss 1.2704 Accuracy 0.6661\n",
            "Time taken for 1 epoch: 56.06 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 1.2568 Accuracy 0.6677\n",
            "Epoch 29 Batch 50 Loss 1.2052 Accuracy 0.6825\n",
            "Epoch 29 Batch 100 Loss 1.1975 Accuracy 0.6821\n",
            "Epoch 29 Batch 150 Loss 1.1949 Accuracy 0.6811\n",
            "Epoch 29 Batch 200 Loss 1.2028 Accuracy 0.6792\n",
            "Epoch 29 Batch 250 Loss 1.2178 Accuracy 0.6753\n",
            "Epoch 29 Loss 1.2218 Accuracy 0.6748\n",
            "Time taken for 1 epoch: 55.34 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.9538 Accuracy 0.7370\n",
            "Epoch 30 Batch 50 Loss 1.0657 Accuracy 0.7097\n",
            "Epoch 30 Batch 100 Loss 1.0754 Accuracy 0.7063\n",
            "Epoch 30 Batch 150 Loss 1.1015 Accuracy 0.7008\n",
            "Epoch 30 Batch 200 Loss 1.1287 Accuracy 0.6949\n",
            "Epoch 30 Batch 250 Loss 1.1526 Accuracy 0.6896\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-6\n",
            "Epoch 30 Loss 1.1613 Accuracy 0.6875\n",
            "Time taken for 1 epoch: 58.13 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 1.0214 Accuracy 0.7287\n",
            "Epoch 31 Batch 50 Loss 1.0068 Accuracy 0.7225\n",
            "Epoch 31 Batch 100 Loss 1.0290 Accuracy 0.7184\n",
            "Epoch 31 Batch 150 Loss 1.0575 Accuracy 0.7113\n",
            "Epoch 31 Batch 200 Loss 1.0780 Accuracy 0.7060\n",
            "Epoch 31 Batch 250 Loss 1.0941 Accuracy 0.7017\n",
            "Epoch 31 Loss 1.0987 Accuracy 0.7008\n",
            "Time taken for 1 epoch: 55.38 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.8716 Accuracy 0.7621\n",
            "Epoch 32 Batch 50 Loss 0.9622 Accuracy 0.7361\n",
            "Epoch 32 Batch 100 Loss 1.0002 Accuracy 0.7262\n",
            "Epoch 32 Batch 150 Loss 1.0224 Accuracy 0.7205\n",
            "Epoch 32 Batch 200 Loss 1.0474 Accuracy 0.7140\n",
            "Epoch 32 Batch 250 Loss 1.0640 Accuracy 0.7099\n",
            "Epoch 32 Loss 1.0692 Accuracy 0.7085\n",
            "Time taken for 1 epoch: 56.02 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.8311 Accuracy 0.7683\n",
            "Epoch 33 Batch 50 Loss 0.9048 Accuracy 0.7458\n",
            "Epoch 33 Batch 100 Loss 0.9339 Accuracy 0.7383\n",
            "Epoch 33 Batch 150 Loss 0.9558 Accuracy 0.7334\n",
            "Epoch 33 Batch 200 Loss 0.9793 Accuracy 0.7276\n",
            "Epoch 33 Batch 250 Loss 0.9977 Accuracy 0.7233\n",
            "Epoch 33 Loss 1.0059 Accuracy 0.7216\n",
            "Time taken for 1 epoch: 55.03 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.8040 Accuracy 0.7881\n",
            "Epoch 34 Batch 50 Loss 0.8780 Accuracy 0.7543\n",
            "Epoch 34 Batch 100 Loss 0.8889 Accuracy 0.7500\n",
            "Epoch 34 Batch 150 Loss 0.9123 Accuracy 0.7434\n",
            "Epoch 34 Batch 200 Loss 0.9312 Accuracy 0.7383\n",
            "Epoch 34 Batch 250 Loss 0.9520 Accuracy 0.7335\n",
            "Epoch 34 Loss 0.9602 Accuracy 0.7314\n",
            "Time taken for 1 epoch: 55.01 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.8944 Accuracy 0.7701\n",
            "Epoch 35 Batch 50 Loss 0.8226 Accuracy 0.7690\n",
            "Epoch 35 Batch 100 Loss 0.8472 Accuracy 0.7619\n",
            "Epoch 35 Batch 150 Loss 0.8805 Accuracy 0.7519\n",
            "Epoch 35 Batch 200 Loss 0.8960 Accuracy 0.7477\n",
            "Epoch 35 Batch 250 Loss 0.9117 Accuracy 0.7435\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-7\n",
            "Epoch 35 Loss 0.9189 Accuracy 0.7417\n",
            "Time taken for 1 epoch: 58.57 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.8426 Accuracy 0.7655\n",
            "Epoch 36 Batch 50 Loss 0.8099 Accuracy 0.7694\n",
            "Epoch 36 Batch 100 Loss 0.8180 Accuracy 0.7673\n",
            "Epoch 36 Batch 150 Loss 0.8367 Accuracy 0.7616\n",
            "Epoch 36 Batch 200 Loss 0.8576 Accuracy 0.7557\n",
            "Epoch 36 Batch 250 Loss 0.8736 Accuracy 0.7513\n",
            "Epoch 36 Loss 0.8789 Accuracy 0.7502\n",
            "Time taken for 1 epoch: 55.51 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.9555 Accuracy 0.7349\n",
            "Epoch 37 Batch 50 Loss 0.7669 Accuracy 0.7793\n",
            "Epoch 37 Batch 100 Loss 0.7859 Accuracy 0.7752\n",
            "Epoch 37 Batch 150 Loss 0.8068 Accuracy 0.7689\n",
            "Epoch 37 Batch 200 Loss 0.8243 Accuracy 0.7649\n",
            "Epoch 37 Batch 250 Loss 0.8396 Accuracy 0.7606\n",
            "Epoch 37 Loss 0.8425 Accuracy 0.7598\n",
            "Time taken for 1 epoch: 55.85 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.7242 Accuracy 0.7757\n",
            "Epoch 38 Batch 50 Loss 0.7436 Accuracy 0.7826\n",
            "Epoch 38 Batch 100 Loss 0.7556 Accuracy 0.7799\n",
            "Epoch 38 Batch 150 Loss 0.7689 Accuracy 0.7759\n",
            "Epoch 38 Batch 200 Loss 0.7808 Accuracy 0.7728\n",
            "Epoch 38 Batch 250 Loss 0.8002 Accuracy 0.7676\n",
            "Epoch 38 Loss 0.8081 Accuracy 0.7657\n",
            "Time taken for 1 epoch: 55.72 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.7601 Accuracy 0.7914\n",
            "Epoch 39 Batch 50 Loss 0.7063 Accuracy 0.7963\n",
            "Epoch 39 Batch 100 Loss 0.7232 Accuracy 0.7906\n",
            "Epoch 39 Batch 150 Loss 0.7334 Accuracy 0.7862\n",
            "Epoch 39 Batch 200 Loss 0.7489 Accuracy 0.7822\n",
            "Epoch 39 Batch 250 Loss 0.7645 Accuracy 0.7780\n",
            "Epoch 39 Loss 0.7706 Accuracy 0.7764\n",
            "Time taken for 1 epoch: 54.67 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.6058 Accuracy 0.8035\n",
            "Epoch 40 Batch 50 Loss 0.6572 Accuracy 0.8056\n",
            "Epoch 40 Batch 100 Loss 0.6805 Accuracy 0.7995\n",
            "Epoch 40 Batch 150 Loss 0.7035 Accuracy 0.7932\n",
            "Epoch 40 Batch 200 Loss 0.7197 Accuracy 0.7888\n",
            "Epoch 40 Batch 250 Loss 0.7339 Accuracy 0.7845\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-8\n",
            "Epoch 40 Loss 0.7368 Accuracy 0.7838\n",
            "Time taken for 1 epoch: 57.90 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.6331 Accuracy 0.8176\n",
            "Epoch 41 Batch 50 Loss 0.6369 Accuracy 0.8129\n",
            "Epoch 41 Batch 100 Loss 0.6444 Accuracy 0.8096\n",
            "Epoch 41 Batch 150 Loss 0.6647 Accuracy 0.8040\n",
            "Epoch 41 Batch 200 Loss 0.6819 Accuracy 0.7998\n",
            "Epoch 41 Batch 250 Loss 0.6971 Accuracy 0.7956\n",
            "Epoch 41 Loss 0.7039 Accuracy 0.7935\n",
            "Time taken for 1 epoch: 56.00 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.5864 Accuracy 0.8355\n",
            "Epoch 42 Batch 50 Loss 0.5950 Accuracy 0.8242\n",
            "Epoch 42 Batch 100 Loss 0.6149 Accuracy 0.8174\n",
            "Epoch 42 Batch 150 Loss 0.6413 Accuracy 0.8097\n",
            "Epoch 42 Batch 200 Loss 0.6588 Accuracy 0.8043\n",
            "Epoch 42 Batch 250 Loss 0.6713 Accuracy 0.8001\n",
            "Epoch 42 Loss 0.6747 Accuracy 0.7994\n",
            "Time taken for 1 epoch: 55.51 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.5105 Accuracy 0.8349\n",
            "Epoch 43 Batch 50 Loss 0.5854 Accuracy 0.8231\n",
            "Epoch 43 Batch 100 Loss 0.5986 Accuracy 0.8212\n",
            "Epoch 43 Batch 150 Loss 0.6082 Accuracy 0.8182\n",
            "Epoch 43 Batch 200 Loss 0.6249 Accuracy 0.8135\n",
            "Epoch 43 Batch 250 Loss 0.6407 Accuracy 0.8089\n",
            "Epoch 43 Loss 0.6461 Accuracy 0.8073\n",
            "Time taken for 1 epoch: 55.25 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.6042 Accuracy 0.8294\n",
            "Epoch 44 Batch 50 Loss 0.5488 Accuracy 0.8356\n",
            "Epoch 44 Batch 100 Loss 0.5692 Accuracy 0.8296\n",
            "Epoch 44 Batch 150 Loss 0.5862 Accuracy 0.8247\n",
            "Epoch 44 Batch 200 Loss 0.6054 Accuracy 0.8193\n",
            "Epoch 44 Batch 250 Loss 0.6173 Accuracy 0.8158\n",
            "Epoch 44 Loss 0.6232 Accuracy 0.8144\n",
            "Time taken for 1 epoch: 55.87 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.5197 Accuracy 0.8510\n",
            "Epoch 45 Batch 50 Loss 0.5507 Accuracy 0.8343\n",
            "Epoch 45 Batch 100 Loss 0.5623 Accuracy 0.8309\n",
            "Epoch 45 Batch 200 Loss 0.5905 Accuracy 0.8222\n",
            "Epoch 45 Batch 250 Loss 0.6054 Accuracy 0.8179\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/train/ckpt-9\n",
            "Epoch 45 Loss 0.6096 Accuracy 0.8169\n",
            "Time taken for 1 epoch: 57.92 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.4516 Accuracy 0.8677\n",
            "Epoch 46 Batch 50 Loss 0.5178 Accuracy 0.8451\n",
            "Epoch 46 Batch 100 Loss 0.5333 Accuracy 0.8400\n",
            "Epoch 46 Batch 150 Loss 0.5439 Accuracy 0.8360\n",
            "Epoch 46 Batch 200 Loss 0.5560 Accuracy 0.8319\n",
            "Epoch 46 Batch 250 Loss 0.5703 Accuracy 0.8278\n",
            "Epoch 46 Loss 0.5762 Accuracy 0.8260\n",
            "Time taken for 1 epoch: 55.61 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.4824 Accuracy 0.8654\n",
            "Epoch 47 Batch 50 Loss 0.4934 Accuracy 0.8487\n",
            "Epoch 47 Batch 100 Loss 0.5135 Accuracy 0.8440\n",
            "Epoch 47 Batch 150 Loss 0.5296 Accuracy 0.8398\n",
            "Epoch 47 Batch 200 Loss 0.5431 Accuracy 0.8361\n",
            "Epoch 47 Batch 250 Loss 0.5517 Accuracy 0.8337\n",
            "Epoch 47 Loss 0.5545 Accuracy 0.8327\n",
            "Time taken for 1 epoch: 55.12 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.4980 Accuracy 0.8309\n",
            "Epoch 48 Batch 50 Loss 0.4775 Accuracy 0.8546\n",
            "Epoch 48 Batch 100 Loss 0.4975 Accuracy 0.8494\n",
            "Epoch 48 Batch 150 Loss 0.5146 Accuracy 0.8443\n",
            "Epoch 48 Batch 200 Loss 0.5257 Accuracy 0.8408\n",
            "Epoch 48 Batch 250 Loss 0.5331 Accuracy 0.8385\n",
            "Epoch 48 Loss 0.5383 Accuracy 0.8369\n",
            "Time taken for 1 epoch: 55.61 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.3892 Accuracy 0.8915\n",
            "Epoch 49 Batch 50 Loss 0.4957 Accuracy 0.8499\n",
            "Epoch 49 Batch 100 Loss 0.4956 Accuracy 0.8493\n",
            "Epoch 49 Batch 150 Loss 0.4965 Accuracy 0.8483\n",
            "Epoch 49 Batch 200 Loss 0.5075 Accuracy 0.8451\n",
            "Epoch 49 Batch 250 Loss 0.5210 Accuracy 0.8414\n",
            "Epoch 49 Loss 0.5242 Accuracy 0.8407\n",
            "Time taken for 1 epoch: 54.27 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.4584 Accuracy 0.8621\n",
            "Epoch 50 Batch 50 Loss 0.4570 Accuracy 0.8597\n",
            "Epoch 50 Batch 100 Loss 0.4765 Accuracy 0.8545\n",
            "Epoch 50 Batch 150 Loss 0.4811 Accuracy 0.8529\n",
            "Epoch 50 Batch 200 Loss 0.4959 Accuracy 0.8487\n",
            "Epoch 50 Batch 250 Loss 0.5056 Accuracy 0.8458\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-10\n",
            "Epoch 50 Loss 0.5108 Accuracy 0.8443\n",
            "Time taken for 1 epoch: 58.87 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.3453 Accuracy 0.9005\n",
            "Epoch 51 Batch 50 Loss 0.4382 Accuracy 0.8666\n",
            "Epoch 51 Batch 100 Loss 0.4432 Accuracy 0.8647\n",
            "Epoch 51 Batch 150 Loss 0.4572 Accuracy 0.8598\n",
            "Epoch 51 Batch 200 Loss 0.4710 Accuracy 0.8559\n",
            "Epoch 51 Batch 250 Loss 0.4837 Accuracy 0.8521\n",
            "Epoch 51 Loss 0.4861 Accuracy 0.8513\n",
            "Time taken for 1 epoch: 55.05 secs\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.4523 Accuracy 0.8713\n",
            "Epoch 52 Batch 50 Loss 0.4335 Accuracy 0.8687\n",
            "Epoch 52 Batch 100 Loss 0.4459 Accuracy 0.8636\n",
            "Epoch 52 Batch 150 Loss 0.4554 Accuracy 0.8602\n",
            "Epoch 52 Batch 200 Loss 0.4648 Accuracy 0.8576\n",
            "Epoch 52 Batch 250 Loss 0.4745 Accuracy 0.8547\n",
            "Epoch 52 Loss 0.4772 Accuracy 0.8539\n",
            "Time taken for 1 epoch: 54.61 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.4391 Accuracy 0.8649\n",
            "Epoch 53 Batch 50 Loss 0.4036 Accuracy 0.8773\n",
            "Epoch 53 Batch 100 Loss 0.4167 Accuracy 0.8727\n",
            "Epoch 53 Batch 150 Loss 0.4307 Accuracy 0.8685\n",
            "Epoch 53 Batch 200 Loss 0.4438 Accuracy 0.8643\n",
            "Epoch 53 Batch 250 Loss 0.4537 Accuracy 0.8612\n",
            "Epoch 53 Loss 0.4563 Accuracy 0.8605\n",
            "Time taken for 1 epoch: 55.39 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.3819 Accuracy 0.8733\n",
            "Epoch 54 Batch 50 Loss 0.4004 Accuracy 0.8769\n",
            "Epoch 54 Batch 100 Loss 0.4072 Accuracy 0.8752\n",
            "Epoch 54 Batch 150 Loss 0.4195 Accuracy 0.8713\n",
            "Epoch 54 Batch 200 Loss 0.4307 Accuracy 0.8674\n",
            "Epoch 54 Batch 250 Loss 0.4394 Accuracy 0.8645\n",
            "Epoch 54 Loss 0.4423 Accuracy 0.8636\n",
            "Time taken for 1 epoch: 54.69 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.3227 Accuracy 0.8873\n",
            "Epoch 55 Batch 50 Loss 0.4023 Accuracy 0.8786\n",
            "Epoch 55 Batch 100 Loss 0.4275 Accuracy 0.8703\n",
            "Epoch 55 Batch 150 Loss 0.4281 Accuracy 0.8696\n",
            "Epoch 55 Batch 200 Loss 0.4279 Accuracy 0.8692\n",
            "Epoch 55 Batch 250 Loss 0.4311 Accuracy 0.8680\n",
            "Saving checkpoint for epoch 55 at ./checkpoints/train/ckpt-11\n",
            "Epoch 55 Loss 0.4336 Accuracy 0.8671\n",
            "Time taken for 1 epoch: 58.93 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.2926 Accuracy 0.9123\n",
            "Epoch 56 Batch 50 Loss 0.3689 Accuracy 0.8860\n",
            "Epoch 56 Batch 100 Loss 0.3794 Accuracy 0.8814\n",
            "Epoch 56 Batch 150 Loss 0.3891 Accuracy 0.8784\n",
            "Epoch 56 Batch 200 Loss 0.4004 Accuracy 0.8756\n",
            "Epoch 56 Batch 250 Loss 0.4064 Accuracy 0.8732\n",
            "Epoch 56 Loss 0.4079 Accuracy 0.8725\n",
            "Time taken for 1 epoch: 55.94 secs\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.4169 Accuracy 0.8872\n",
            "Epoch 57 Batch 50 Loss 0.3616 Accuracy 0.8903\n",
            "Epoch 57 Batch 100 Loss 0.3643 Accuracy 0.8882\n",
            "Epoch 57 Batch 150 Loss 0.3786 Accuracy 0.8830\n",
            "Epoch 57 Batch 200 Loss 0.3899 Accuracy 0.8789\n",
            "Epoch 57 Batch 250 Loss 0.3996 Accuracy 0.8762\n",
            "Epoch 57 Loss 0.4020 Accuracy 0.8756\n",
            "Time taken for 1 epoch: 54.43 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.3691 Accuracy 0.8889\n",
            "Epoch 58 Batch 50 Loss 0.3380 Accuracy 0.8963\n",
            "Epoch 58 Batch 100 Loss 0.3489 Accuracy 0.8933\n",
            "Epoch 58 Batch 150 Loss 0.3651 Accuracy 0.8875\n",
            "Epoch 58 Batch 200 Loss 0.3768 Accuracy 0.8833\n",
            "Epoch 58 Batch 250 Loss 0.3868 Accuracy 0.8803\n",
            "Epoch 58 Loss 0.3897 Accuracy 0.8794\n",
            "Time taken for 1 epoch: 55.44 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.3441 Accuracy 0.8869\n",
            "Epoch 59 Batch 50 Loss 0.3385 Accuracy 0.8945\n",
            "Epoch 59 Batch 100 Loss 0.3487 Accuracy 0.8911\n",
            "Epoch 59 Batch 150 Loss 0.3593 Accuracy 0.8878\n",
            "Epoch 59 Batch 200 Loss 0.3691 Accuracy 0.8849\n",
            "Epoch 59 Batch 250 Loss 0.3774 Accuracy 0.8824\n",
            "Epoch 59 Loss 0.3810 Accuracy 0.8811\n",
            "Time taken for 1 epoch: 55.01 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.3137 Accuracy 0.9103\n",
            "Epoch 60 Batch 50 Loss 0.3356 Accuracy 0.8952\n",
            "Epoch 60 Batch 100 Loss 0.3396 Accuracy 0.8938\n",
            "Epoch 60 Batch 150 Loss 0.3474 Accuracy 0.8918\n",
            "Epoch 60 Batch 200 Loss 0.3544 Accuracy 0.8896\n",
            "Epoch 60 Batch 250 Loss 0.3618 Accuracy 0.8872\n",
            "Saving checkpoint for epoch 60 at ./checkpoints/train/ckpt-12\n",
            "Epoch 60 Loss 0.3657 Accuracy 0.8861\n",
            "Time taken for 1 epoch: 57.83 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.2903 Accuracy 0.9154\n",
            "Epoch 61 Batch 50 Loss 0.3141 Accuracy 0.9031\n",
            "Epoch 61 Batch 100 Loss 0.3208 Accuracy 0.9017\n",
            "Epoch 61 Batch 150 Loss 0.3332 Accuracy 0.8983\n",
            "Epoch 61 Batch 200 Loss 0.3441 Accuracy 0.8947\n",
            "Epoch 61 Batch 250 Loss 0.3532 Accuracy 0.8918\n",
            "Epoch 61 Loss 0.3560 Accuracy 0.8909\n",
            "Time taken for 1 epoch: 55.47 secs\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.3196 Accuracy 0.9104\n",
            "Epoch 62 Batch 50 Loss 0.3037 Accuracy 0.9057\n",
            "Epoch 62 Batch 100 Loss 0.3157 Accuracy 0.9015\n",
            "Epoch 62 Batch 150 Loss 0.3275 Accuracy 0.8974\n",
            "Epoch 62 Batch 200 Loss 0.3353 Accuracy 0.8949\n",
            "Epoch 62 Batch 250 Loss 0.3434 Accuracy 0.8921\n",
            "Epoch 62 Loss 0.3460 Accuracy 0.8913\n",
            "Time taken for 1 epoch: 54.85 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.2926 Accuracy 0.9027\n",
            "Epoch 63 Batch 50 Loss 0.3046 Accuracy 0.9059\n",
            "Epoch 63 Batch 100 Loss 0.3119 Accuracy 0.9029\n",
            "Epoch 63 Batch 150 Loss 0.3254 Accuracy 0.8990\n",
            "Epoch 63 Batch 200 Loss 0.3309 Accuracy 0.8972\n",
            "Epoch 63 Batch 250 Loss 0.3375 Accuracy 0.8952\n",
            "Epoch 63 Loss 0.3394 Accuracy 0.8943\n",
            "Time taken for 1 epoch: 55.22 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.2755 Accuracy 0.9094\n",
            "Epoch 64 Batch 50 Loss 0.2911 Accuracy 0.9100\n",
            "Epoch 64 Batch 100 Loss 0.3024 Accuracy 0.9059\n",
            "Epoch 64 Batch 150 Loss 0.3131 Accuracy 0.9031\n",
            "Epoch 64 Batch 200 Loss 0.3196 Accuracy 0.9010\n",
            "Epoch 64 Batch 250 Loss 0.3285 Accuracy 0.8982\n",
            "Epoch 64 Loss 0.3318 Accuracy 0.8973\n",
            "Time taken for 1 epoch: 55.97 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.3436 Accuracy 0.8986\n",
            "Epoch 65 Batch 50 Loss 0.3016 Accuracy 0.9070\n",
            "Epoch 65 Batch 100 Loss 0.3038 Accuracy 0.9058\n",
            "Epoch 65 Batch 150 Loss 0.3081 Accuracy 0.9040\n",
            "Epoch 65 Batch 200 Loss 0.3165 Accuracy 0.9015\n",
            "Epoch 65 Batch 250 Loss 0.3201 Accuracy 0.9000\n",
            "Saving checkpoint for epoch 65 at ./checkpoints/train/ckpt-13\n",
            "Epoch 65 Loss 0.3224 Accuracy 0.8994\n",
            "Time taken for 1 epoch: 58.61 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.2844 Accuracy 0.9022\n",
            "Epoch 66 Batch 50 Loss 0.2800 Accuracy 0.9152\n",
            "Epoch 66 Batch 100 Loss 0.2878 Accuracy 0.9114\n",
            "Epoch 66 Batch 150 Loss 0.2968 Accuracy 0.9084\n",
            "Epoch 66 Batch 200 Loss 0.3057 Accuracy 0.9051\n",
            "Epoch 66 Batch 250 Loss 0.3148 Accuracy 0.9025\n",
            "Epoch 66 Loss 0.3184 Accuracy 0.9014\n",
            "Time taken for 1 epoch: 55.49 secs\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.2527 Accuracy 0.9234\n",
            "Epoch 67 Batch 50 Loss 0.2793 Accuracy 0.9134\n",
            "Epoch 67 Batch 100 Loss 0.2854 Accuracy 0.9112\n",
            "Epoch 67 Batch 150 Loss 0.2946 Accuracy 0.9079\n",
            "Epoch 67 Batch 200 Loss 0.3000 Accuracy 0.9061\n",
            "Epoch 67 Batch 250 Loss 0.3043 Accuracy 0.9047\n",
            "Epoch 67 Loss 0.3059 Accuracy 0.9042\n",
            "Time taken for 1 epoch: 54.73 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.2317 Accuracy 0.9259\n",
            "Epoch 68 Batch 50 Loss 0.2630 Accuracy 0.9214\n",
            "Epoch 68 Batch 100 Loss 0.2723 Accuracy 0.9166\n",
            "Epoch 68 Batch 150 Loss 0.2831 Accuracy 0.9130\n",
            "Epoch 68 Batch 200 Loss 0.2913 Accuracy 0.9104\n",
            "Epoch 68 Batch 250 Loss 0.2968 Accuracy 0.9082\n",
            "Epoch 68 Loss 0.2980 Accuracy 0.9077\n",
            "Time taken for 1 epoch: 55.25 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.2849 Accuracy 0.9077\n",
            "Epoch 69 Batch 50 Loss 0.2549 Accuracy 0.9228\n",
            "Epoch 69 Batch 100 Loss 0.2692 Accuracy 0.9179\n",
            "Epoch 69 Batch 150 Loss 0.2776 Accuracy 0.9145\n",
            "Epoch 69 Batch 200 Loss 0.2864 Accuracy 0.9122\n",
            "Epoch 69 Batch 250 Loss 0.2934 Accuracy 0.9098\n",
            "Epoch 69 Loss 0.2949 Accuracy 0.9090\n",
            "Time taken for 1 epoch: 55.80 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.2182 Accuracy 0.9334\n",
            "Epoch 70 Batch 50 Loss 0.2590 Accuracy 0.9185\n",
            "Epoch 70 Batch 100 Loss 0.2659 Accuracy 0.9171\n",
            "Epoch 70 Batch 150 Loss 0.2739 Accuracy 0.9147\n",
            "Epoch 70 Batch 200 Loss 0.2838 Accuracy 0.9113\n",
            "Epoch 70 Batch 250 Loss 0.2878 Accuracy 0.9100\n",
            "Saving checkpoint for epoch 70 at ./checkpoints/train/ckpt-14\n",
            "Epoch 70 Loss 0.2891 Accuracy 0.9095\n",
            "Time taken for 1 epoch: 58.21 secs\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.2378 Accuracy 0.9266\n",
            "Epoch 71 Batch 50 Loss 0.2432 Accuracy 0.9263\n",
            "Epoch 71 Batch 100 Loss 0.2532 Accuracy 0.9226\n",
            "Epoch 71 Batch 150 Loss 0.2605 Accuracy 0.9200\n",
            "Epoch 71 Batch 200 Loss 0.2697 Accuracy 0.9170\n",
            "Epoch 71 Batch 250 Loss 0.2758 Accuracy 0.9150\n",
            "Epoch 71 Loss 0.2777 Accuracy 0.9145\n",
            "Time taken for 1 epoch: 55.06 secs\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.2142 Accuracy 0.9323\n",
            "Epoch 72 Batch 50 Loss 0.2498 Accuracy 0.9223\n",
            "Epoch 72 Batch 100 Loss 0.2555 Accuracy 0.9206\n",
            "Epoch 72 Batch 150 Loss 0.2621 Accuracy 0.9184\n",
            "Epoch 72 Batch 200 Loss 0.2677 Accuracy 0.9168\n",
            "Epoch 72 Batch 250 Loss 0.2725 Accuracy 0.9151\n",
            "Epoch 72 Loss 0.2735 Accuracy 0.9148\n",
            "Time taken for 1 epoch: 54.82 secs\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.2280 Accuracy 0.9239\n",
            "Epoch 73 Batch 50 Loss 0.2379 Accuracy 0.9261\n",
            "Epoch 73 Batch 100 Loss 0.2469 Accuracy 0.9234\n",
            "Epoch 73 Batch 150 Loss 0.2549 Accuracy 0.9207\n",
            "Epoch 73 Batch 200 Loss 0.2620 Accuracy 0.9185\n",
            "Epoch 73 Batch 250 Loss 0.2693 Accuracy 0.9160\n",
            "Epoch 73 Loss 0.2717 Accuracy 0.9154\n",
            "Time taken for 1 epoch: 55.25 secs\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.2812 Accuracy 0.9205\n",
            "Epoch 74 Batch 50 Loss 0.2291 Accuracy 0.9298\n",
            "Epoch 74 Batch 100 Loss 0.2431 Accuracy 0.9253\n",
            "Epoch 74 Batch 150 Loss 0.2506 Accuracy 0.9229\n",
            "Epoch 74 Batch 200 Loss 0.2552 Accuracy 0.9212\n",
            "Epoch 74 Batch 250 Loss 0.2597 Accuracy 0.9194\n",
            "Epoch 74 Loss 0.2616 Accuracy 0.9189\n",
            "Time taken for 1 epoch: 56.61 secs\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.2528 Accuracy 0.9127\n",
            "Epoch 75 Batch 50 Loss 0.2349 Accuracy 0.9269\n",
            "Epoch 75 Batch 100 Loss 0.2417 Accuracy 0.9239\n",
            "Epoch 75 Batch 150 Loss 0.2489 Accuracy 0.9216\n",
            "Epoch 75 Batch 200 Loss 0.2523 Accuracy 0.9206\n",
            "Epoch 75 Batch 250 Loss 0.2559 Accuracy 0.9195\n",
            "Saving checkpoint for epoch 75 at ./checkpoints/train/ckpt-15\n",
            "Epoch 75 Loss 0.2568 Accuracy 0.9191\n",
            "Time taken for 1 epoch: 60.42 secs\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.2161 Accuracy 0.9400\n",
            "Epoch 76 Batch 50 Loss 0.2291 Accuracy 0.9289\n",
            "Epoch 76 Batch 100 Loss 0.2345 Accuracy 0.9275\n",
            "Epoch 76 Batch 150 Loss 0.2377 Accuracy 0.9262\n",
            "Epoch 76 Batch 200 Loss 0.2439 Accuracy 0.9244\n",
            "Epoch 76 Batch 250 Loss 0.2501 Accuracy 0.9223\n",
            "Epoch 76 Loss 0.2521 Accuracy 0.9216\n",
            "Time taken for 1 epoch: 55.31 secs\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.1724 Accuracy 0.9443\n",
            "Epoch 77 Batch 50 Loss 0.2159 Accuracy 0.9336\n",
            "Epoch 77 Batch 100 Loss 0.2246 Accuracy 0.9307\n",
            "Epoch 77 Batch 150 Loss 0.2301 Accuracy 0.9288\n",
            "Epoch 77 Batch 200 Loss 0.2382 Accuracy 0.9264\n",
            "Epoch 77 Batch 250 Loss 0.2435 Accuracy 0.9246\n",
            "Epoch 77 Loss 0.2450 Accuracy 0.9244\n",
            "Time taken for 1 epoch: 54.73 secs\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.2046 Accuracy 0.9424\n",
            "Epoch 78 Batch 50 Loss 0.2211 Accuracy 0.9331\n",
            "Epoch 78 Batch 100 Loss 0.2267 Accuracy 0.9305\n",
            "Epoch 78 Batch 150 Loss 0.2316 Accuracy 0.9290\n",
            "Epoch 78 Batch 200 Loss 0.2360 Accuracy 0.9268\n",
            "Epoch 78 Batch 250 Loss 0.2426 Accuracy 0.9250\n",
            "Epoch 78 Loss 0.2451 Accuracy 0.9243\n",
            "Time taken for 1 epoch: 55.24 secs\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.2228 Accuracy 0.9209\n",
            "Epoch 79 Batch 50 Loss 0.2165 Accuracy 0.9325\n",
            "Epoch 79 Batch 100 Loss 0.2237 Accuracy 0.9307\n",
            "Epoch 79 Batch 150 Loss 0.2297 Accuracy 0.9289\n",
            "Epoch 79 Batch 200 Loss 0.2339 Accuracy 0.9276\n",
            "Epoch 79 Batch 250 Loss 0.2372 Accuracy 0.9265\n",
            "Epoch 79 Loss 0.2381 Accuracy 0.9261\n",
            "Time taken for 1 epoch: 55.08 secs\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.1622 Accuracy 0.9559\n",
            "Epoch 80 Batch 50 Loss 0.2049 Accuracy 0.9369\n",
            "Epoch 80 Batch 100 Loss 0.2180 Accuracy 0.9337\n",
            "Epoch 80 Batch 150 Loss 0.2218 Accuracy 0.9320\n",
            "Epoch 80 Batch 200 Loss 0.2257 Accuracy 0.9304\n",
            "Epoch 80 Batch 250 Loss 0.2290 Accuracy 0.9293\n",
            "Saving checkpoint for epoch 80 at ./checkpoints/train/ckpt-16\n",
            "Epoch 80 Loss 0.2308 Accuracy 0.9286\n",
            "Time taken for 1 epoch: 58.82 secs\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.1488 Accuracy 0.9514\n",
            "Epoch 81 Batch 50 Loss 0.2042 Accuracy 0.9376\n",
            "Epoch 81 Batch 100 Loss 0.2110 Accuracy 0.9349\n",
            "Epoch 81 Batch 150 Loss 0.2146 Accuracy 0.9338\n",
            "Epoch 81 Batch 200 Loss 0.2199 Accuracy 0.9320\n",
            "Epoch 81 Batch 250 Loss 0.2262 Accuracy 0.9301\n",
            "Epoch 81 Loss 0.2288 Accuracy 0.9293\n",
            "Time taken for 1 epoch: 55.23 secs\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.1970 Accuracy 0.9390\n",
            "Epoch 82 Batch 50 Loss 0.2023 Accuracy 0.9364\n",
            "Epoch 82 Batch 100 Loss 0.2060 Accuracy 0.9358\n",
            "Epoch 82 Batch 150 Loss 0.2094 Accuracy 0.9353\n",
            "Epoch 82 Batch 200 Loss 0.2147 Accuracy 0.9336\n",
            "Epoch 82 Batch 250 Loss 0.2197 Accuracy 0.9316\n",
            "Epoch 82 Loss 0.2212 Accuracy 0.9312\n",
            "Time taken for 1 epoch: 55.59 secs\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.2116 Accuracy 0.9303\n",
            "Epoch 83 Batch 50 Loss 0.1935 Accuracy 0.9416\n",
            "Epoch 83 Batch 100 Loss 0.2005 Accuracy 0.9391\n",
            "Epoch 83 Batch 150 Loss 0.2094 Accuracy 0.9359\n",
            "Epoch 83 Batch 200 Loss 0.2158 Accuracy 0.9336\n",
            "Epoch 83 Batch 250 Loss 0.2198 Accuracy 0.9322\n",
            "Epoch 83 Loss 0.2214 Accuracy 0.9318\n",
            "Time taken for 1 epoch: 54.88 secs\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.2088 Accuracy 0.9395\n",
            "Epoch 84 Batch 50 Loss 0.1974 Accuracy 0.9394\n",
            "Epoch 84 Batch 100 Loss 0.2004 Accuracy 0.9383\n",
            "Epoch 84 Batch 150 Loss 0.2065 Accuracy 0.9362\n",
            "Epoch 84 Batch 200 Loss 0.2100 Accuracy 0.9350\n",
            "Epoch 84 Batch 250 Loss 0.2143 Accuracy 0.9333\n",
            "Epoch 84 Loss 0.2160 Accuracy 0.9328\n",
            "Time taken for 1 epoch: 54.92 secs\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.1732 Accuracy 0.9364\n",
            "Epoch 85 Batch 50 Loss 0.1882 Accuracy 0.9421\n",
            "Epoch 85 Batch 100 Loss 0.2006 Accuracy 0.9394\n",
            "Epoch 85 Batch 150 Loss 0.2103 Accuracy 0.9361\n",
            "Epoch 85 Batch 200 Loss 0.2118 Accuracy 0.9355\n",
            "Epoch 85 Batch 250 Loss 0.2136 Accuracy 0.9344\n",
            "Saving checkpoint for epoch 85 at ./checkpoints/train/ckpt-17\n",
            "Epoch 85 Loss 0.2147 Accuracy 0.9341\n",
            "Time taken for 1 epoch: 57.97 secs\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.1403 Accuracy 0.9531\n",
            "Epoch 86 Batch 50 Loss 0.1834 Accuracy 0.9433\n",
            "Epoch 86 Batch 100 Loss 0.1919 Accuracy 0.9404\n",
            "Epoch 86 Batch 150 Loss 0.1979 Accuracy 0.9382\n",
            "Epoch 86 Batch 200 Loss 0.2034 Accuracy 0.9366\n",
            "Epoch 86 Batch 250 Loss 0.2053 Accuracy 0.9356\n",
            "Epoch 86 Loss 0.2069 Accuracy 0.9351\n",
            "Time taken for 1 epoch: 55.65 secs\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.2001 Accuracy 0.9344\n",
            "Epoch 87 Batch 50 Loss 0.1904 Accuracy 0.9399\n",
            "Epoch 87 Batch 100 Loss 0.1897 Accuracy 0.9405\n",
            "Epoch 87 Batch 150 Loss 0.1934 Accuracy 0.9397\n",
            "Epoch 87 Batch 200 Loss 0.1975 Accuracy 0.9386\n",
            "Epoch 87 Batch 250 Loss 0.2027 Accuracy 0.9369\n",
            "Epoch 87 Loss 0.2038 Accuracy 0.9367\n",
            "Time taken for 1 epoch: 55.22 secs\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.1949 Accuracy 0.9449\n",
            "Epoch 88 Batch 50 Loss 0.1825 Accuracy 0.9436\n",
            "Epoch 88 Batch 100 Loss 0.1813 Accuracy 0.9438\n",
            "Epoch 88 Batch 150 Loss 0.1867 Accuracy 0.9421\n",
            "Epoch 88 Batch 200 Loss 0.1914 Accuracy 0.9406\n",
            "Epoch 88 Batch 250 Loss 0.1973 Accuracy 0.9387\n",
            "Epoch 88 Loss 0.1998 Accuracy 0.9379\n",
            "Time taken for 1 epoch: 55.81 secs\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.1724 Accuracy 0.9482\n",
            "Epoch 89 Batch 50 Loss 0.1878 Accuracy 0.9415\n",
            "Epoch 89 Batch 100 Loss 0.1855 Accuracy 0.9425\n",
            "Epoch 89 Batch 150 Loss 0.1873 Accuracy 0.9419\n",
            "Epoch 89 Batch 200 Loss 0.1919 Accuracy 0.9405\n",
            "Epoch 89 Batch 250 Loss 0.1963 Accuracy 0.9391\n",
            "Epoch 89 Loss 0.1978 Accuracy 0.9385\n",
            "Time taken for 1 epoch: 56.43 secs\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.2133 Accuracy 0.9281\n",
            "Epoch 90 Batch 50 Loss 0.1890 Accuracy 0.9416\n",
            "Epoch 90 Batch 100 Loss 0.1860 Accuracy 0.9419\n",
            "Epoch 90 Batch 150 Loss 0.1882 Accuracy 0.9416\n",
            "Epoch 90 Batch 200 Loss 0.1920 Accuracy 0.9404\n",
            "Epoch 90 Batch 250 Loss 0.1945 Accuracy 0.9396\n",
            "Saving checkpoint for epoch 90 at ./checkpoints/train/ckpt-18\n",
            "Epoch 90 Loss 0.1955 Accuracy 0.9393\n",
            "Time taken for 1 epoch: 60.84 secs\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.1524 Accuracy 0.9515\n",
            "Epoch 91 Batch 50 Loss 0.1785 Accuracy 0.9460\n",
            "Epoch 91 Batch 100 Loss 0.1784 Accuracy 0.9454\n",
            "Epoch 91 Batch 150 Loss 0.1790 Accuracy 0.9446\n",
            "Epoch 91 Batch 200 Loss 0.1839 Accuracy 0.9429\n",
            "Epoch 91 Batch 250 Loss 0.1884 Accuracy 0.9415\n",
            "Epoch 91 Loss 0.1899 Accuracy 0.9412\n",
            "Time taken for 1 epoch: 55.56 secs\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.1599 Accuracy 0.9589\n",
            "Epoch 92 Batch 50 Loss 0.1746 Accuracy 0.9460\n",
            "Epoch 92 Batch 100 Loss 0.1764 Accuracy 0.9452\n",
            "Epoch 92 Batch 150 Loss 0.1781 Accuracy 0.9451\n",
            "Epoch 92 Batch 200 Loss 0.1812 Accuracy 0.9442\n",
            "Epoch 92 Batch 250 Loss 0.1851 Accuracy 0.9430\n",
            "Epoch 92 Loss 0.1867 Accuracy 0.9425\n",
            "Time taken for 1 epoch: 55.40 secs\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.1910 Accuracy 0.9384\n",
            "Epoch 93 Batch 50 Loss 0.1725 Accuracy 0.9489\n",
            "Epoch 93 Batch 100 Loss 0.1757 Accuracy 0.9469\n",
            "Epoch 93 Batch 150 Loss 0.1799 Accuracy 0.9452\n",
            "Epoch 93 Batch 200 Loss 0.1821 Accuracy 0.9444\n",
            "Epoch 93 Batch 250 Loss 0.1839 Accuracy 0.9436\n",
            "Epoch 93 Loss 0.1842 Accuracy 0.9436\n",
            "Time taken for 1 epoch: 55.25 secs\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.1418 Accuracy 0.9560\n",
            "Epoch 94 Batch 50 Loss 0.1642 Accuracy 0.9495\n",
            "Epoch 94 Batch 100 Loss 0.1683 Accuracy 0.9487\n",
            "Epoch 94 Batch 150 Loss 0.1720 Accuracy 0.9472\n",
            "Epoch 94 Batch 200 Loss 0.1786 Accuracy 0.9453\n",
            "Epoch 94 Batch 250 Loss 0.1823 Accuracy 0.9440\n",
            "Epoch 94 Loss 0.1834 Accuracy 0.9437\n",
            "Time taken for 1 epoch: 56.19 secs\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.1517 Accuracy 0.9542\n",
            "Epoch 95 Batch 50 Loss 0.1679 Accuracy 0.9490\n",
            "Epoch 95 Batch 100 Loss 0.1717 Accuracy 0.9472\n",
            "Epoch 95 Batch 150 Loss 0.1745 Accuracy 0.9463\n",
            "Epoch 95 Batch 200 Loss 0.1759 Accuracy 0.9456\n",
            "Epoch 95 Batch 250 Loss 0.1795 Accuracy 0.9446\n",
            "Saving checkpoint for epoch 95 at ./checkpoints/train/ckpt-19\n",
            "Epoch 95 Loss 0.1810 Accuracy 0.9443\n",
            "Time taken for 1 epoch: 57.92 secs\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.1801 Accuracy 0.9497\n",
            "Epoch 96 Batch 50 Loss 0.1688 Accuracy 0.9495\n",
            "Epoch 96 Batch 100 Loss 0.1708 Accuracy 0.9489\n",
            "Epoch 96 Batch 150 Loss 0.1720 Accuracy 0.9482\n",
            "Epoch 96 Batch 200 Loss 0.1747 Accuracy 0.9470\n",
            "Epoch 96 Batch 250 Loss 0.1786 Accuracy 0.9453\n",
            "Epoch 96 Loss 0.1797 Accuracy 0.9450\n",
            "Time taken for 1 epoch: 55.20 secs\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.1723 Accuracy 0.9309\n",
            "Epoch 97 Batch 50 Loss 0.1655 Accuracy 0.9485\n",
            "Epoch 97 Batch 100 Loss 0.1689 Accuracy 0.9479\n",
            "Epoch 97 Batch 150 Loss 0.1683 Accuracy 0.9480\n",
            "Epoch 97 Batch 200 Loss 0.1701 Accuracy 0.9475\n",
            "Epoch 97 Batch 250 Loss 0.1728 Accuracy 0.9465\n",
            "Epoch 97 Loss 0.1735 Accuracy 0.9462\n",
            "Time taken for 1 epoch: 55.54 secs\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.1249 Accuracy 0.9586\n",
            "Epoch 98 Batch 50 Loss 0.1553 Accuracy 0.9514\n",
            "Epoch 98 Batch 100 Loss 0.1597 Accuracy 0.9503\n",
            "Epoch 98 Batch 150 Loss 0.1680 Accuracy 0.9485\n",
            "Epoch 98 Batch 200 Loss 0.1708 Accuracy 0.9475\n",
            "Epoch 98 Batch 250 Loss 0.1745 Accuracy 0.9462\n",
            "Epoch 98 Loss 0.1758 Accuracy 0.9460\n",
            "Time taken for 1 epoch: 56.05 secs\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.1347 Accuracy 0.9609\n",
            "Epoch 99 Batch 50 Loss 0.1530 Accuracy 0.9541\n",
            "Epoch 99 Batch 100 Loss 0.1557 Accuracy 0.9529\n",
            "Epoch 99 Batch 150 Loss 0.1590 Accuracy 0.9518\n",
            "Epoch 99 Batch 200 Loss 0.1635 Accuracy 0.9502\n",
            "Epoch 99 Batch 250 Loss 0.1667 Accuracy 0.9492\n",
            "Epoch 99 Loss 0.1681 Accuracy 0.9487\n",
            "Time taken for 1 epoch: 55.07 secs\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.1713 Accuracy 0.9446\n",
            "Epoch 100 Batch 50 Loss 0.1576 Accuracy 0.9515\n",
            "Epoch 100 Batch 100 Loss 0.1600 Accuracy 0.9509\n",
            "Epoch 100 Batch 150 Loss 0.1623 Accuracy 0.9499\n",
            "Epoch 100 Batch 200 Loss 0.1653 Accuracy 0.9487\n",
            "Epoch 100 Batch 250 Loss 0.1688 Accuracy 0.9478\n",
            "Saving checkpoint for epoch 100 at ./checkpoints/train/ckpt-20\n",
            "Epoch 100 Loss 0.1706 Accuracy 0.9472\n",
            "Time taken for 1 epoch: 58.81 secs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run training! Each epoch takes several mins with GPU\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  # inp -> twi, tar -> french\n",
        "  for (batch, (inp,tar)) in enumerate(train_batches):\n",
        "    train_step(inp, tar)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a Translator\n"
      ],
      "metadata": {
        "id": "Aij6BWpIBU52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(tf.Module):\n",
        "    def __init__(self, tokenizers, transformer):\n",
        "        self.tokenizers = tokenizers\n",
        "        self.transformer = transformer\n",
        "\n",
        "    def __call__(self, sentence, max_length=MAX_TOKENS):\n",
        "        # The input sentence is French, hence adding the `[START]` and `[END]` tokens.\n",
        "        sentence = tf.convert_to_tensor([sentence])\n",
        "        sentence = self.tokenizers.fr.tokenize(sentence).to_tensor()\n",
        "\n",
        "        encoder_input = sentence\n",
        "\n",
        "        # As the output language is Twi, initialize the output with the\n",
        "        # English `[START]` token.\n",
        "        start_end = self.tokenizers.twi.tokenize([''])[0]\n",
        "        start = start_end[0][tf.newaxis]\n",
        "        end = start_end[1][tf.newaxis]\n",
        "\n",
        "        # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "        # dynamic-loop can be traced by `tf.function`.\n",
        "        output_array = tf.TensorArray(\n",
        "            dtype=tf.int64, size=0, dynamic_size=True)\n",
        "        output_array = output_array.write(0, start)\n",
        "\n",
        "        for i in tf.range(max_length):\n",
        "            output = tf.transpose(output_array.stack())\n",
        "\n",
        "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "                encoder_input, output)\n",
        "\n",
        "            predictions, attention_weights = self.transformer(\n",
        "                encoder_input, output, False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "\n",
        "            # Select the last token from the `seq_len` dimension.\n",
        "            # Shape `(batch_size, 1, vocab_size)`.\n",
        "            predictions = predictions[:, -1:, :]\n",
        "\n",
        "            predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "            # Concatenate the `predicted_id` to the output which is given to the\n",
        "            # decoder as its input.\n",
        "            output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "            if predicted_id == end:\n",
        "                break\n",
        "\n",
        "        output = tf.transpose(output_array.stack())\n",
        "        # The output shape is `(1, tokens)`.\n",
        "        text = self.tokenizers.twi.detokenize(output)[0]  # Shape: `()`.\n",
        "\n",
        "        tokens = self.tokenizers.twi.lookup(output)[0]\n",
        "        _, attention_weights = self.transformer(\n",
        "            encoder_input, output[:, :-1], False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "\n",
        "        return text, tokens, attention_weights"
      ],
      "metadata": {
        "id": "z0clY8WkaGZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt1OVm3ecSNO"
      },
      "outputs": [],
      "source": [
        "#Create an instance of this Translator class\n",
        "translator = Translator(tokenizers, transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## translate example sentecnes"
      ],
      "metadata": {
        "id": "3dMj9N3fBh1P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysGcUmAzc_Yi"
      },
      "outputs": [],
      "source": [
        "def print_translation(sentence, tokens, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKmyWMcGduft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d16ba389-8c11-4fdb-a353-2dfb17fb927f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : Asamoah a attaché la ficelle au cerf-volant.\n",
            "Prediction     : asamoah de hama no bɔɔ asangoli no mu .\n",
            "Ground truth   : Asamoah de hama no bɔɔ asangoli no mu.\n"
          ]
        }
      ],
      "source": [
        "sentence =\"Asamoah a attaché la ficelle au cerf-volant.\"\n",
        "ground_truth= \"Asamoah de hama no bɔɔ asangoli no mu.\"\n",
        "translated_text, translated_tokens, attention_weights = translator(sentence)\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYL7f6tu9Y0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fe03c63-84ab-499b-daed-328a8723b658"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : je ne sais pas qui tu es\n",
            "Prediction     : minnim nipa ko a woyɛ .\n",
            "Ground truth   : minnim onipa ko a woyɛ\n"
          ]
        }
      ],
      "source": [
        "sentence=\"je ne sais pas qui tu es\"\n",
        "ground_truth=\"minnim onipa ko a woyɛ\"\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Attentions"
      ],
      "metadata": {
        "id": "qWa9iysqBrFW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9pfpPIb9kUK"
      },
      "outputs": [],
      "source": [
        "def plot_attention_head(in_tokens, translated_tokens, attention):\n",
        "  # The model didn't generate `<START>` in the output. Skip it.\n",
        "  translated_tokens = translated_tokens[1:]\n",
        "\n",
        "  ax = plt.gca()\n",
        "  ax.matshow(attention)\n",
        "  ax.set_xticks(range(len(in_tokens)))\n",
        "  ax.set_yticks(range(len(translated_tokens)))\n",
        "\n",
        "  labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n",
        "  ax.set_xticklabels(\n",
        "      labels, rotation=90)\n",
        "\n",
        "  labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n",
        "  ax.set_yticklabels(labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "head = 0\n",
        "# Shape: `(batch=1, num_attention_heads, seq_len_q, seq_len_k)`.\n",
        "attention_heads = tf.squeeze(\n",
        "  attention_weights['decoder_layer4_block2'], 0)\n",
        "attention = attention_heads[head]\n",
        "attention.shape"
      ],
      "metadata": {
        "id": "4Jp-67xoVB9A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f22bf94-ed74-4616-f941-a24933734718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([7, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize last French input sentence\n",
        "in_tokens = tf.convert_to_tensor([sentence])\n",
        "in_tokens = tokenizers.fr.tokenize(in_tokens).to_tensor()\n",
        "in_tokens = tokenizers.fr.lookup(in_tokens)[0]\n",
        "np.char.decode(in_tokens.numpy().astype(np.bytes_), 'UTF-8')"
      ],
      "metadata": {
        "id": "qYbkydJST7Mu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d96a7582-33da-4a48-9e07-feef696e48f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['[START]', 'je', 'asamoah', 'dire', 'le', 'son', 'dans', 'trop',\n",
              "       '[END]'], dtype='<U7')"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# View translated tokens\n",
        "translated_tokens"
      ],
      "metadata": {
        "id": "AcL3yO8aVKdg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9682795b-868d-4120-d5b1-aab13b44891c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(8,), dtype=string, numpy=\n",
              "array([b'[START]', b'k\\xc9\\x94\\xc9\\x94', b'\\xc9\\x94hy\\xc9\\x9b\\xc9\\x9b',\n",
              "       b'aku', b'a', b'##di', b'.', b'[END]'], dtype=object)>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize Transformer attention mechanism operation\n",
        "plot_attention_head(in_tokens, translated_tokens, attention)"
      ],
      "metadata": {
        "id": "CA7BYz3iVbTv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "ca41877d-d0be-4db6-b237-d8a856536e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUoAAAEfCAYAAAAnao9BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAYmUlEQVR4nO3de5xdZX3v8c+XECCQEMJF7lcBo42AYThFQAgochAvYPUg1oMKNUdbi8ceUPCgWI89aGmPrVKtUYpSL1WLIj0HrRzlWrGQQAiEi7UoCt7ABAhJyPXbP9Ya2QwzeTKTvfbK7Pm+X695zd5r7b1/z+yZ+e5nrfWsZ8k2ERExsi3abkBExOYuQRkRUZCgjIgoSFBGRBQkKCMiChKUEREFCcqIiIIEZUREQYIyIqJgy7YbEM2QdDBwHrAvHb9n2ye01qiIcUo5hbE/SboT+FtgAbBucLntBa01KmKcSlD2KUkLbB/edjsi+kGCss9I2rG+eQ7wa+AbwKrB9baXtNGuiPEsQdlnJP0YMKBhVtv2AT1uUsS4l6CMiCjIUe8+JmkW8AJgm8Fltq9or0UR41Nf9yglfXwjHvaE7Qsbb0yPSboImEMVlNcAJwM3235dm+2KGI/6PSgfBD5QeNj5tp/fi/b0kqS7gEOBO2wfKmlX4Au2T2y5aRHjTr9ven/M9uc39ABJM3rVmB5baXu9pLWStqc6Ar53242KGI/6PSjXlh5g+6960ZAWzJe0A/AZqkHnTwK3tNukiPGp3ze9b7c9u+12tE3SfsD2the13JSIcanfe5QTmqRXA8fWd28AEpQRY9DvPcq1wIrhVlENvt6+x03qGUkfAY4AvlgvOgO4zfb72mtVxPjU70F5h+0Xtd2ONkhaBBxme319fxLVEfBD2m1ZxPiT+Sj72w4dt6e31oqIca7f91F+bbiFkl4OnNfnYwovBu6QdB3VroZjgfPbbVJE9/TyhJJ+3/Q+gWpOxj2Aq4CPApdTBcef2f56i81rnKTdqfZTAtxq+5dttieim3p5Qkm/9yj/EphLNX7w5Pr7+bYvbbVVvbNL/X1L4ChJ9PuHQ0woPTuhpN97lM84mCPpftvPa7NNvSLp74BDgMXA+nqxbZ/VXqsixqd+71FOl/Tajvtbdt7v897VkbZf0HYjIpok6Xjgj4HBDtC9wKW2r+9qnT7vUV6+gdV93buSdBnwl7bvabst0TxJRwMLbS+X9CZgNvDXth9suWmNkXQKcCnwIeB2qmMPs4ELgXfavqZrtfo5KDdE0q62f9V2O5oi6TjgauCXVJeCGBxkn3GUfageN3so1e6WzwGfBf6L7ePabFeTJF0PvMv2nUOWHwJ8ops/e79vej9DPUnE7wFvBJ5PdTS8X10G/FfgLp7eRxn9a61tS3oN1abnZZLObrtRDdttaEgC2F5UTyvYNX0flJKmAK+hCscXAdOAU4Eb22xXDzxi++q2GxE9s0zSBcCbgGMlbQFMbrlNTVs+xnWj1teb3pK+BLwE+A7wD8D3gB/Z3r/VhvWApE9SnZnzTzzzKoz9fABrwpK0G1Vn4DbbN0naB5jTq0t/SJoNHEN1Ybt/sX17D2o+xvAdHgHH2O7aXLP9HpQLqU7TvAL4B9sPSXpgIlyJcIQDWT05gCVpW+B/APvYfpukg4Dn2f6/TdeO3pP0AeD1wOCH8KnA12x/uOG6G9wHafuGrtXq56AEkDSTauac04FHqYYRzOrnAzltk/QVqsmCz7Q9qw7O79s+rOWm9a162NtHgedQ9ah6NkOWpPuBQ20/Vd+fQnUEvm/GLPf1PkpJR9r+AXARcJGkw6mnG5P0kO2j2m1hcyRtA5wN/A7PvApjL4ZEPdf26ZLOqGuukDTcdcb7iqSDgfOAfen437J9Qg/K/znwKtv39qDWUD+n+ht7qr6/NfBw00XreQxG6unZ9ku7VauvgxL4JNW4KgBsLwAWSDqPat9lP/t74D7gJKpxZr9PNRi3F1bXvQoDSHouHftJ+9jXqOYW+Aywrse1f9VSSAI8DiyWdC3V7/xE4NbBSStsn9NQ3XOHWXYk8B6qa0R1TV9vek/kS0EMnr4paZHtQyRNBm6yfWQPap9INej3BVQH0o4G3tLtsyU2N5IW2D68pdp/DexGNflLTw/eSXrzhtaXzsfuUhuOA95P1bP9M9vf6ubr93uP8gBJIw6Rsf3qXjamx9bU3x+TNItq4Plzmi5aD0uZAbyW6tNdVIOCH2269mbgnyT9IfANnhlWS3pQe3uq2fxf3rHMPH2ApTG2Py9pK+DgetH9ttds6DndIukkqg/lVVQBeV0jdfq8R/lvwB+MtL6bR8U2N5L+ALgSeCHVmRpTgQ/Y/tse1J5ve6DpOpsbST8eZrH7fZSFpDnA54GfUH0w7g282XajY5Ul3UY1Q9YlDHOF0W4OUer3oJywl4JoU329nkeBr9Ax8LdHPasJqc2Dd5IWAG+0fX99/2Dgy03vhqhPYdzQwZyuHUTr903v4T7hJwRJ76KapHgZ1cGF2VRzcX6nB+VPr7//UccyA/3es5oMvIOnr3x5PfDpHm2GtnnwbvJgSALY/mH9XjTK9pymawzq92vmXFyfsQCApDMlfVPSxyXt2HRxScdIemt9exdJvTwj6CzbT1Dts9qJ6rzvj/SisO39h/nq65CsfQo4nGq0xSfr25/qUe0Dbb8fWF4fPDkF+N0e1V4g6bOS5tRfnwHmN11U0ns6br9+yLr/3c1a/R6UnwZWA0g6lioorqAazjCvycKSLgLeC1xQL5oMfKHJmkObUH9/BXCF7cUdy5opWF16A0mvHe6rydpD2tHWB9QRtt9s+3v111t5+lIcTRt68G46PTh4V3s7cA9wTv11D1XPumlv6Lh9wZB1/7mbhfp903tSx36x04F5tq8ErqxPb2zSaVSTcNwOYPvnkqY1XLPTAknfAfYHLqhrNz2L0LFU59O/impTW0O+92KoykXAANUZWJfz9AfU0U3XBtZJeq7tf6/bcgC9G085r77swYVU0+tNpRou0yhVl0G+0/ZM4P80XW9o+RFuD3d/k/R9UEra0vZa4KVU188Z1PTPvrqe9mpw0PV2Ddcb6mzgMKqgGAB2pjr63aRlkv4EuJunAxJG3uHehDY/oM4FrpP0QH1/P+CtTRas3+9Bg7X+pv7e+N+c7XWS7pe0j+2fNl1vaPkRbg93f5P0e1B+GbhB0qPASuAmAEkHUm1+N+mrkj4N7CDpbVTB9dmGa3Y6C3gXsBewkGpM4y3AJxqsObX+/jyqTc5vUoXlq4BbG6zbqc0PqJ2AWVQBeSrwYpr/Oxv8EBh8zwfHDffyPZ9BdWbOrTxzlEPT45QPlfQE1d/YlPo29f1tRn7aGNju6y+qgDgN2K5j2cHA7B7UPpFqjNclwMt6/HPfVf+xLKzvzwS+3qPaNwLTOu5PA27sUe1zqfZNPwC8jerD4Y97VHtR/f0Y4DqqAyr/OgHe81uB4zq+5vTq5+7VV1/3KEc6hdH2D0uP2YSaN9s+RtIynrn5+XZJ64ElwCW2P9mtmiN4yvZTkpC0te37JPVqNpddqQ+i1VbXyxpn+y/qUyifoOplfcD2tb2ozdP7I08BPmP7/0lqdKqxDq2958CWHnLyRn2uf6M25n+3W//ffR2UwPNVXUtkJKI6Otg1to+pvw+7X0zSTsD3qYaPNOkhVZe+uAq4VtJSoFcXmrqCalKEb9T3T6X5/aO/VQdjr8Kx08P17pYTgY9K2prejSzp+Xsu6R3AH1KdKtz5fzYN+Jcma9d69v/d72fm7LsRD1tn+6HGG9NB0u62f9HDesdR/cF82/bq0uO7VHM2T8/QdKPtOxquN9iDf9Yqejcv47ZUw1Lusv1vknYHXujeDPJv4z2fTrV/8mLg/I5Vy9yDs7B6+f/d10EZEdEN/T7gPCJik02ooJQ0t/yo1E7t1B6v9ZuqPaGCkmcOOE/t1E7t/qufoIyIaMO4O5gzbcZk77Tn1mN67pNL1zB1xthnf/rFkzuM+bnrli1n0rSxnySyxVNjP3V13YrlTNp27LX3ec4jY37u479Zx/SdJo35+Q8/tMuYn7tm1XImb71pJ+bsvMdjY3resqVrmLYJf2sAK9eP7fkrl65iyoyx/Y/89jXWjb3tax5fyeTpYx9GufNWT475ucuWrGXajmMb9fjIw6tYtmTtsP9o424c5U57bs0Hvn5oK7X/9KbXtFIXYNq9jU/vN6JL39n0kM+RnX/B21urDXD2B69qrfY9K/Zorfbix3dvrfZZe93cSt33nTby9J3Z9I6IKEhQRkQUJCgjIgoSlBERBQnKiIiCBGVEREGCMiKiIEEZEVGQoIyIKEhQRkQUNBKUkvaTdHcTrx0R0WvpUUZEFDQelJIOkHSHpPMkLZS0WNKHVdlZ0q2SFkm6SdKsptsTETFajQZlfXnUK4G3AJfZPgx4EXA4cAawDDje9iHAhcBXJT2rTZLmSpovaf6TS9c02eSIiGdpMih3Ab4J/L7tOwevylZfBXAecJLtVbaX18tvANYAz7qymu15tgdsD2zKfJIREWPRZFA+DvwUOKbBGhERjWty4t7VwGnAP0t6kuqa0kskbUV1XYsv1BeIn2R7RX3t6a2ABxtsU0TEqDU6w7nt5ZJeCVwLHCjp9VQXpP8m8CVgZ6og3ZKqB3q67fVNtikiYrQaCUrbPwFm1bcfA46oV31oyEMfAWY30YaIiG7JOMqIiIIEZUREQYIyIqIgQRkRUZCgjIgoSFBGRBQkKCMiChKUEREFCcqIiIJGT2FswtQtVnPUlB+3U1xupy4w7aH2zuw8dpvWSrP9VXe0VxyY9Kftve/HbX9fa7V/vHyn1mofPPnXrdTdRiNP4ZgeZUREQYIyIqIgQRkRUZCgjIgoSFBGRBQkKCMiChKUEREFCcqIiIIEZUREQYIyIqIgQRkRUZCgjIgoGFNQSvqcpNd1uzEREZuj9CgjIgqKQSnpBEkLJd0l6auSZtSrjpX0fUkPDPYuJV0h6dSO535R0msk7SHp5vp1bpV0fL3+v0u6T9ICSV9s5CeMiNhEGzMf5e3AbNvrJb0f+Gi9fHfgGGAmcDXwj8BlwLuBqyRNB44C3gz8N+B+22cPee33AUfYfnBDDZA0F5gLsMeekzbm54qI6Jpij9L2Y7YHZy+9FDipvn2V7fW27wF2rR97A3CQpF2AM4Arba8Fvg4cKelnks7sePmLgfsk3VJowzzbA7YHZuyYvQUR0VubMsP5qo7b6rh9BfAm4A3AW+tlLwe+Zfvc3z5BmkwVpgfafngT2hER0ahiUNab0MvqXuU7ge8AkzfwlM8BtwK/rHubAIcAKzpecwawHjiQOnAl7WD7sTH8DBERjdqY7dgjgEWS7gJeCLx3Qw+2/SvgXuDyjsV/BRwt6e76dV5n+3Hgg8DNku4Evisp29URsdkp9iht/39g1pDFbxnymKmDtyVtCxwEfLlj/c+AE4Z57Y8DHx9ViyMieqyrPThJL6PqTX6i7jFGRIx7Xb1cbd373LebrxkR0bbsE4yIKEhQRkQUJCgjIgoSlBERBQnKiIiCBGVEREGCMiKioKvjKHthPbBifTvN/p0D25u740c/36+12lc8sXNrtbeYNrX8oAZ9+eH/1FrtM/fa4KRajZq+1VOt1V60as9W6q5cv2TEdelRRkQUJCgjIgoSlBERBQnKiIiCBGVEREGCMiKiIEEZEVGQoIyIKEhQRkQUJCgjIgoSlBERBQnKiIiCngSlpCd7USciognpUUZEFHQ9KCVdJWmBpMWS5g5Zt7OkWySdIumDks7tWHe3pP263Z6IiE3VxMSOZ9leImkKcJukKwEk7QpcDVxo+1pJRzRQOyKi65oIynMknVbf3hs4CJgMfBf4I9s3jPYF657pXIDd9pzUrXZGRGyUrm56S5oDvAx4se1DgTuAbYC1wALgpI6Hrx1Sf5uRXtf2PNsDtgdm7JjdqhHRW91OnenAUtsrJM0EjqyXGzgLmCnpvfWynwCzASTNBvbvclsiIrqi25ve3wbeLule4H7gB4MrbK+TdAZwtaRlwOXAmZIWA/8K/LDLbYmI6IquBqXtVcDJw6ya2rG+c/P75d2sHxHRhOzwi4goSFBGRBQkKCMiChKUEREFCcqIiIIEZUREQYIyIqIgQRkRUZCgjIgoaGL2oEYJ2FrrWqm9+Ed7tlIXYK+F61ur/bqzft5a7S89fkBrtQHO3Gtha7V32/Kx1mqvXDe5tdqHbfNQK3WnbLF6xHXpUUZEFCQoIyIKEpQREQUJyoiIggRlRERBgjIioiBBGRFRkKCMiChIUEZEFCQoIyIKEpQREQUJyoiIggRlRERBgjIioiBBGRFR0HpQSrpK0gJJiyXNbbs9ERFDbQ4T955le4mkKcBtkq60/ZvOB9QBOhdg9z0ntdHGiJjAWu9RAudIuhP4AbA3cNDQB9ieZ3vA9sCMHTeHJkfERNJqj1LSHOBlwIttr5B0PbBNm22KiBiq7e7ZdGBpHZIzgSNbbk9ExLO0HZTfBraUdC/wEarN74iIzUqrm962VwEnt9mGiIiStnuUERGbvQRlRERBgjIioiBBGRFRkKCMiChIUEZEFCQoIyIKEpQREQUJyoiIgs1hmrVRMbDK7Uy1pi3dSl2AdZPVWu1tt9iqtdpes7q12gDr3N773qa169vrQz3V0v+3Gfl3nR5lRERBgjIioiBBGRFRkKCMiChIUEZEFCQoIyIKEpQREQUJyoiIggRlRERBgjIioiBBGRFRkKCMiCgYc1BKuljS8ZJOlXTBkHXPk/R5SVtIumUDr3G9pIH69jWSdhhreyIimrIpPcrfBX4AHAfcOGTdS+plLwTu3pgXs/0K249tQnsiIhox6mnWJF0CnATsD9wCPBd4qaR/BK4DPgHsA/wKmAaslzTf9oCkKcDlwKHAfcCUjtf9CTBg+9FN+okiIrps1EFp+zxJXwXOBP4EuN720R0POaze3D4K+DvgL2wvrte9A1hh+/mSDgFu35iakuYCcwF227OdueoiYuIa66b3bOBOYCZwb+cKSdsCq2wbOAi4v2P1scAXAGwvAhZtTDHb82wP2B6YsWOOP0VEb42qRynpMOBzwF7Ao8C21WItBF4MfIUqPHeQtAjYD5gv6WLbX+liuyMiemZUQWl7IdWm9feBY6g2rf/c9j31Q14t6TzgAeA3wCtsv6fjJW4E3gh8T9Is4JBN/QEiIpo26u1YSbsAS22vB2Z2hOSgY4GbqY583zBk3aeAqZLuBT4ELBh9kyMiemssB3MeAU6pbx85zPpX1Tf/1zDrVgJvGOF19xttWyIieiFHRiIiChKUEREFCcqIiIIEZUREQYIyIqIgQRkRUZCgjIgoSFBGRBQkKCMiCkZ9Zk7bnlg3hX9+clYrtafusKKVugCPHTSjtdpXPrl9a7WXveFZJ3/11O3L57dW+xdbtfc7X72+vWi4YfnMVuouW790xHXpUUZEFCQoIyIKEpQREQUJyoiIggRlRERBgjIioiBBGRFRkKCMiChIUEZEFCQoIyIKEpQREQUJyoiIggRlRERBgjIioiBBGRFRMC6CUtJcSfMlzV++dHXbzYmICWZcBKXtebYHbA9sN2OrtpsTERPMuAjKiIg2JSgjIgo2m6CUdI2kPdpuR0TEUJvNxcVsv6LtNkREDGez6VFGRGyuEpQREQUJyoiIggRlRERBgjIioiBBGRFRkKCMiChIUEZEFCQoIyIKNpszczaWEevcTr7baqUuwJppbq32A6t3aa326qntvecA61v8nS9Zu11rtdu0Zn07sbSh/+/0KCMiChKUEREFCcqIiIIEZUREQYIyIqIgQRkRUZCgjIgoSFBGRBQkKCMiChKUEREFCcqIiIIEZUREQYIyIqKgGJSS9pO0UtLC+v46SQs7vs6vl18vaX7H8wYkXV/fniPpcUl3SLpf0o2SXtnx2HdL+qmkS7v+E0ZEbKKNnc/o320fVt9e2XF7qOdIOtn2t4ZZd5PtVwJIOgy4StJK29+1/TFJS4GB0TU/IqJ53d70vgT4n6UH2V4IfAh458a8qKS5kuZLmr986epNbGJExOiMJSinDNn0Pr1j3S3AaknHb8Tr3A7M3JiCtufZHrA9sN2MrcbQ5IiIsRvLVMIb2vQG+DBwIfDewuu0O3V1RMRG6vpRb9vfA6YARxYe+iLg3m7Xj4jotqaGB30YeM9IKyUdArwf+JuG6kdEdM1YNr2nDA4Vqn3b9vmdD7B9jaRHhjzvJZLuALYFfg2cY/u7Y6gfEdFTow5K25NGWD5nyP3DO25fD0wfba2IiM3Bxmx6rwOmD+lFdpWkdwMXAE80VSMiYqyKPUrbPwP2brIRtj8GfKzJGhERY5VzvSMiChKUEREFCcqIiIIEZUREQYIyIqIgQRkRUSDbbbdhVOozfh4c49N3Bh7tYnNSO7VTe/Oqvym197W9y3Arxl1QbgpJ8223Mjlwaqf2RKjddv2mamfTOyKiIEEZEVEw0YJyXmqndmr3df1Gak+ofZQREWMx0XqUERGjlqCMiChIUEZEFCQoIyIKEpQREQX/AUE01bqMQBjuAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to plot individual attention weights\n",
        "def plot_attention_weights(sentence, translated_tokens, attention_heads):\n",
        "  in_tokens = tf.convert_to_tensor([sentence])\n",
        "  in_tokens = tokenizers.fr.tokenize(in_tokens).to_tensor()\n",
        "  in_tokens = tokenizers.fr.lookup(in_tokens)[0]\n",
        "\n",
        "  fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "  for h, head in enumerate(attention_heads):\n",
        "    ax = fig.add_subplot(2, 4, h+1)\n",
        "\n",
        "    plot_attention_head(in_tokens, translated_tokens, head)\n",
        "\n",
        "    ax.set_xlabel(f'Head {h+1}')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "7ve6yy7VVfd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the 8 attention heads in decoder layer 4 for last french input\n",
        "plot_attention_weights(sentence,\n",
        "                       translated_tokens,\n",
        "                       attention_weights['decoder_layer4_block2'][0])"
      ],
      "metadata": {
        "id": "duWodUGoVo2D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "6c49a9ce-fb64-4335-d3f2-573ebe01eb0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1152x576 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Translator"
      ],
      "metadata": {
        "id": "tBe9E_dXBzne"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMpmh77M9ufZ"
      },
      "outputs": [],
      "source": [
        "# class to export translator\n",
        "class ExportTranslator(tf.Module):\n",
        "  def __init__(self, translator):\n",
        "    self.translator = translator\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def __call__(self, sentence):\n",
        "    (result,\n",
        "     tokens,\n",
        "     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89tEraoTixKG"
      },
      "outputs": [],
      "source": [
        "translator = ExportTranslator(translator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZurB6qDFDzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c120e69b-7786-47b7-f27a-3d2518e070eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'wonte ase .'"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "translator(tf.constant(\"Vous n'a pas pas compris.\")).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "module_no_signatures_path = '/content/drive/MyDrive/french_twi_translator'\n",
        "print('Saving model...')\n",
        "tf.saved_model.save(translator, module_no_signatures_path)"
      ],
      "metadata": {
        "id": "YMhMt9_z2ZXH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ffe4a8-8b04-4ef3-e8bd-d7d19a018b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn while saving (showing 5 of 332). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded = tf.saved_model.load(module_no_signatures_path)"
      ],
      "metadata": {
        "id": "P0jO3gtH9bih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded(\"Kwaku a essayé de briser le combat entre Abena et John.\").numpy()"
      ],
      "metadata": {
        "id": "adjW9Tug-ZOa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f243843d-8efa-4685-a2d3-305bfb745b84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'so nekongua no nyanmum w\\xc9\\x94 mu ?'"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}