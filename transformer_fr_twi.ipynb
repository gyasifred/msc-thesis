{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/transformer_fr_twi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qFzQxA29DJp"
      },
      "source": [
        "This notebook is based on the implementation of the paper [Attention is All you Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017). The code are adapted from the Tenssorflow repository at https://www.tensorflow.org/text/tutorials/transformer#build_the_transformer and a big thanks to [[Crater David]](https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:37370749) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hblEgiSfLEck"
      },
      "source": [
        "# Install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naWgZqfLsOiN",
        "outputId": "1c56b0f3-bb15-44b9-86f1-ab32fd58e87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: googletrans==3.1.0a0 in /usr/local/lib/python3.7/dist-packages (3.1.0a0)\n",
            "Requirement already satisfied: httpx==0.13.3 in /usr/local/lib/python3.7/dist-packages (from googletrans==3.1.0a0) (0.13.3)\n",
            "Requirement already satisfied: hstspreload in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2022.9.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.0)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Requirement already satisfied: httpcore==0.9.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2022.6.15)\n",
            "Requirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: rfc3986<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
            "Requirement already satisfied: h2==3.* in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
            "Requirement already satisfied: h11<0.10,>=0.8 in /usr/local/lib/python3.7/dist-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
            "Requirement already satisfied: hpack<4,>=3.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
            "Requirement already satisfied: hyperframe<6,>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U tensorflow-text-nightly==2.11.0.dev20220817\n",
        "!pip install -U -q tf-nightly==2.11.0.dev20220817\n",
        "!pip3 install googletrans==3.1.0a0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY7seog2LQOL"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gwubBvId4uGK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W18p7g2lLatA"
      },
      "source": [
        "# preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ovdl-UER8hMX"
      },
      "outputs": [],
      "source": [
        "# This code was adapted from https://github.com/GhanaNLP/kasa/blob/master/Kasa/Preprocessing.py\n",
        "# A subclass of the kasafrench for preprocessing data\n",
        "# import required library\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "class Preprocessing:\n",
        "    # dummy initialization method\n",
        "    def __init__(self):\n",
        "        # initialize with some default parameters here later\n",
        "        pass\n",
        "\n",
        "    # read in parallel twi - english dataset\n",
        "    def read_parallel_dataset(self, filepath_1, filepath_2, filepath_3=None):\n",
        "        if filepath_3 != None:\n",
        "            # read first language data\n",
        "            lang_1 = []\n",
        "            with open(filepath_1, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_1.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            # read second language data\n",
        "            lang_2 = []\n",
        "            with open(filepath_2, encoding='utf-8') as file:\n",
        "\n",
        "                # twi=file.read()\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_2.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "            # Read third Language data\n",
        "            lang_3 = []\n",
        "            with open(filepath_3, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_3.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            return lang_1, lang_2, lang_3\n",
        "            \n",
        "        else:\n",
        "            # read first language data\n",
        "            lang_1 = []\n",
        "            with open(filepath_1, encoding='utf-8') as file:\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_1.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            # read second language data\n",
        "            lang_2 = []\n",
        "            with open(filepath_2, encoding='utf-8') as file:\n",
        "\n",
        "                # twi=file.read()\n",
        "                line = file.readline()\n",
        "                cnt = 1\n",
        "                while line:\n",
        "                    lang_2.append(line.strip())\n",
        "                    line = file.readline()\n",
        "                    cnt += 1\n",
        "\n",
        "            return lang_1, lang_2\n",
        "\n",
        "    # Define a helper function to remove string accents\n",
        "\n",
        "    def removeStringAccent(self, s):\n",
        "        return ''.join(\n",
        "            c for c in unicodedata.normalize('NFD', s)\n",
        "            if unicodedata.category(c) != 'Mn'\n",
        "        )\n",
        "\n",
        "    # normalize input twi sentence\n",
        "    def normalize_twi(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = s.lower()\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.ƆɔɛƐ!?’]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "\n",
        "    # normalize input french sentence\n",
        "    def normalize_FrEn(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = s.lower()\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "    \n",
        "    def writeTotxt(self,destination,data):\n",
        "        with open(destination, 'w') as f:\n",
        "            for line in data:\n",
        "                 f.write(f\"{line}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "B4asafNW9HIt"
      },
      "outputs": [],
      "source": [
        "# import preprocessing class \n",
        "# Create an instance of tft preprocessing class\n",
        "preprocessor = Preprocessing()\n",
        "\n",
        "# Read raw parallel dataset\n",
        "raw_data_twi, raw_data_fr = preprocessor.read_parallel_dataset(\n",
        "    filepath_1='/content/drive/MyDrive/verified_twi.txt',\n",
        "    filepath_2='/content/drive/MyDrive/verified_english_french.txt'\n",
        ")\n",
        "\n",
        "# Normalize the raw data\n",
        "raw_data_fr = [preprocessor.normalize_FrEn(data) for data in raw_data_fr]\n",
        "raw_data_twi = [preprocessor.normalize_twi(data) for data in raw_data_twi]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au81TkNOLwnB"
      },
      "source": [
        "# split data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5l08UvTL9mtx"
      },
      "outputs": [],
      "source": [
        "# split the dataset into training and test sets\n",
        "# Keep 10% of the data as test set\n",
        "train_twi,test_twi,train_fr,test_fr = train_test_split(raw_data_twi,raw_data_fr, test_size=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3q4djANP-9g6"
      },
      "outputs": [],
      "source": [
        "# write the preprocess traning and test dataset to a file\n",
        "preprocessor.writeTotxt('train_twi.txt',train_twi)\n",
        "preprocessor.writeTotxt('train_fr.txt',train_fr)\n",
        "preprocessor.writeTotxt('test_twi.txt',test_twi)\n",
        "preprocessor.writeTotxt('test_fr.txt',test_fr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao2l87AzL_kE"
      },
      "source": [
        "# Build tf dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "0WZrSEP9_ISJ"
      },
      "outputs": [],
      "source": [
        "# build tf datasets from traning and test sentences in both languages\n",
        "train_dataset_fr = tf.data.TextLineDataset('/content/train_fr.txt')\n",
        "train_dataset_tw = tf.data.TextLineDataset('/content/train_twi.txt')\n",
        "\n",
        "val_dataset_fr = tf.data.TextLineDataset('/content/test_fr.txt')\n",
        "val_dataset_tw = tf.data.TextLineDataset('/content/test_twi.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "mmksbF-CAEXg"
      },
      "outputs": [],
      "source": [
        "# combine languages into single dataset\n",
        "trained_combined = tf.data.Dataset.zip((train_dataset_fr, train_dataset_tw))\n",
        "val_combined = tf.data.Dataset.zip((val_dataset_fr, val_dataset_tw))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZJ-hiEUBLDu",
        "outputId": "05b46c4a-12c5-40c6-bc85-54a904509597"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "French:  les premiers colons americains sont arrives au eme siecle .\n",
            "Twi:  amerika atubrafo a wodi kan no duu hɔ wɔ afeha a ɛto so no mu .\n",
            "French:  vous devez vous laver les mains .\n",
            "Twi:  ɛsɛ sɛ wohohoro wo nsa .\n",
            "French:  appiah horrow est le jour de paie .\n",
            "Twi:  da a wɔde tua ka no yɛ da koro akatua .\n",
            "French:  je comprends pourquoi tu aimes asamoah .\n",
            "Twi:  mitumi hu nea enti a w ani gye asamoa ho\n",
            "French:  il avait un jean .\n",
            "Twi:  ɔhyɛ jeans attade\n"
          ]
        }
      ],
      "source": [
        "# verify trained_combined dataset is correct\n",
        "for  fr,tw in trained_combined.take(5):\n",
        "  print(\"French: \", fr.numpy().decode('utf-8'))\n",
        "  print(\"Twi: \", tw.numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaxFg3TZMIK8"
      },
      "source": [
        "# load tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vjb-3AU1BnrK"
      },
      "source": [
        "\n",
        "\n",
        "1. The tokenizer is which was build from the parallel corpus.\n",
        "2. The code are available on the link https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/build_subword_tokenizer.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "aR_vgyQgBo_T"
      },
      "outputs": [],
      "source": [
        "model_named = '/content/drive/MyDrive/translate_frengtwi_converter'\n",
        "# Verify tokenizer model can be reloaded\n",
        "tokenizers = tf.saved_model.load(model_named)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdHMl3icEjos",
        "outputId": "34b88989-50a3-42c8-88aa-5631a0272248"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'je', b'suis', b'etudiant', b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "# Verify tokenizer works on french sentence\n",
        "tokens = tokenizers.fr.tokenize(['je suis étudiant'])\n",
        "text_tokens = tokenizers.fr.lookup(tokens)\n",
        "text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSOMXh_de04Q",
        "outputId": "e60f831d-fd1f-462f-9691-0742225c66a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "je suis etudiant\n"
          ]
        }
      ],
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.fr.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBNqSqeSfChq",
        "outputId": "23e92718-8bdf-499d-83b2-069b234136de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'na', b'mmarimaa', b'ne', b'mmeawa', b'pii', b'w\\xc9\\x94',\n",
              "  b'agu', b'##di', b'##bea', b'h\\xc9\\x94', b'.', b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Verify tokenizer works on twi sentence\n",
        "tokens = tokenizers.twi.tokenize(['Na mmarimaa ne mmeawa pii wɔ agudibea hɔ .'])\n",
        "text_tokens = tokenizers.twi.lookup(tokens)\n",
        "text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lS1xMYQfQR9",
        "outputId": "6d88656a-da20-43d4-91ba-7cdcf3e5ba4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "na mmarimaa ne mmeawa pii wɔ agudibea hɔ .\n"
          ]
        }
      ],
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.twi.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQVo6G3OMfac"
      },
      "source": [
        "# Check sentence with maximum length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvoh1vRgCxSb"
      },
      "source": [
        "By checking the maximum length of sentence, you can suitably select the MAX_TOKENS. The MAX_TOKKENS help drops examples longer than the maximum number of tokens (MAX_TOKENS). Without limiting the size of sequences, the performance may be negatively affected.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VJxJuNufTCB",
        "outputId": "cac2336a-cd5a-4e05-fcc1-86e3a194c01e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".........................................................................................................................................................................................................................................................................................................................................................................................................................................................................."
          ]
        }
      ],
      "source": [
        "#The distribution of tokens per example in the dataset is as follows:\n",
        "\n",
        "lengths = []\n",
        "\n",
        "for fr_examples,twi_examples in trained_combined.batch(50):\n",
        "  twi_tokens = tokenizers.twi.tokenize(twi_examples)\n",
        "  lengths.append(twi_tokens.row_lengths())\n",
        "\n",
        "  fr_tokens = tokenizers.fr.tokenize(fr_examples)\n",
        "  lengths.append(fr_tokens.row_lengths())\n",
        "  print('.', end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "ri-qmhyyiBSa",
        "outputId": "d9aa6e6b-2f9b-44ef-95b6-599e4915af94"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "all_lengths = np.concatenate(lengths)\n",
        "\n",
        "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
        "plt.ylim(plt.ylim())\n",
        "max_length = max(all_lengths)\n",
        "plt.plot([max_length, max_length], plt.ylim())\n",
        "plt.title(f'Maximum tokens per example: {max_length}');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DYwhdWAMpnW"
      },
      "source": [
        "# Tokenize the training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "GC7UlwEeiNTO"
      },
      "outputs": [],
      "source": [
        "MAX_TOKENS = 55\n",
        "\n",
        "def filter_max_tokens(l1, l2):\n",
        "  num_tokens = tf.maximum(tf.shape(l1)[1],tf.shape(l2)[1])\n",
        "  return num_tokens < MAX_TOKENS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Routine to tokenize sentence pairs; needed to make sentence batches for training\n",
        "def tokenize_pairs(fr,twi):\n",
        "  fr = tokenizers.fr.tokenize(fr)\n",
        "  # Convert from ragged to dense, padding with zeros.\n",
        "  fr = fr.to_tensor()\n",
        "  tw = tokenizers.twi.tokenize(twi)\n",
        "  # Convert from ragged to dense, padding with zeros.\n",
        "  tw = tw.to_tensor()\n",
        "  return fr,tw"
      ],
      "metadata": {
        "id": "AORJvDkcOhxC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "EWMySNLzklc_"
      },
      "outputs": [],
      "source": [
        "# create training batches\n",
        "BUFFER_SIZE = len(train_fr)\n",
        "BATCH_SIZE = 64\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .filter(filter_max_tokens)\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "# Create training and validation set batches.\n",
        "train_batches = make_batches(trained_combined)\n",
        "val_batches = make_batches(val_combined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2MhJY21RqcJ"
      },
      "source": [
        "Write the test batch to file. This will come in handy later if you prefer to translate from txt file and estimate the translator [BLEU](https://aclanthology.org/P02-1040.pdf) score. The translator will run to error for any for any sentence with the shape greater than the MAX_TOKENS used for training. It adivisable to use the trimmed sentences for testting to avoid such occurrance\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "2V_Rh8fFSvPV"
      },
      "outputs": [],
      "source": [
        "french_test = []\n",
        "twi_test= []\n",
        "\n",
        "for fr_batches, twi_batches in val_batches:\n",
        "    for fr in tokenizers.fr.detokenize(fr_batches):\n",
        "      french_test.append(fr.numpy().decode(\"utf-8\"))\n",
        "    for tw in tokenizers.twi.detokenize(twi_batches):\n",
        "      twi_test.append(tw.numpy().decode(\"utf-8\"))\n",
        "\n",
        "\n",
        "preprocessor.writeTotxt('/content/drive/MyDrive/twi_testing_set.txt',test_twi)\n",
        "preprocessor.writeTotxt('/content/drive/MyDrive/french_testing_set.txt',french_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNFMseSdM3wr"
      },
      "source": [
        "# Positional encoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5du74jWFF5WK"
      },
      "source": [
        "Positional encodings are added to the embeddings to give the model some information about the relative position of the tokens in the sentence so the model can learn to recognize the word order.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "KXtWVxfeb6px"
      },
      "outputs": [],
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "  \n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # Apply the sine function to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # Apply the cosine function to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXL_HTJWG44o"
      },
      "source": [
        "# Create a point-wise feed-forward network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "db4kXyfGb_l5"
      },
      "outputs": [],
      "source": [
        "def point_wise_feed_forward_network(\n",
        "  d_model, # Input/output dimensionality.\n",
        "  dff # Inner-layer dimensionality.\n",
        "  ):\n",
        "\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # Shape `(batch_size, seq_len, dff)`.\n",
        "      tf.keras.layers.Dense(d_model)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "  ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSEO7HUZHppR"
      },
      "source": [
        "# Build Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtydO3sOIRE-"
      },
      "source": [
        "### Multihead Attention layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "BSHsspXXIaUR"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create mask for padding tokens so translator ignores them\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "\n",
        "# Mask future tokens so translator cannot see future\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "\n",
        "# Create final masks\n",
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead)\n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "      q: query shape == (..., seq_len_q, depth)\n",
        "      k: key shape == (..., seq_len_k, depth)\n",
        "      v: value shape == (..., seq_len_v, depth_v)\n",
        "      mask: Float tensor with shape broadcastable\n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "      output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    # (..., seq_len_q, seq_len_k)\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(\n",
        "        scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# Add attention layers to create multi-head attention\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, *, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # (batch_size, num_heads, seq_len_q, depth)\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        # (batch_size, num_heads, seq_len_k, depth)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        # (batch_size, num_heads, seq_len_v, depth)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        # (batch_size, seq_len_q, num_heads, depth)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        # (batch_size, seq_len_q, d_model)\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "        # Build Transformer encoder layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvNLzFWpJw6m"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "bCdZS_nJcGVW"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "EBZNB3MacRCF"
      },
      "outputs": [],
      "source": [
        "#Build Transformer encoder from encoder layer\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "              maximum_position_encoding=MAX_TOKENS,rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndsnLCOwRgcY"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "fnXVTpI_cgzR"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "UEuoBDtJcp7Q"
      },
      "outputs": [],
      "source": [
        "#Build Transformer decoder from decoder layer\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "              maximum_position_encoding,rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size,d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjFbGSvJUQY6"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "_6sd1ofVcyl4"
      },
      "outputs": [],
      "source": [
        "# Build full Transformer\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_vocab_size, pe_input=MAX_TOKENS, pe_target=MAX_TOKENS, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.encoder= Encoder(num_layers, d_model, num_heads, dff,input_vocab_size, pe_input, rate)\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff,target_vocab_size, pe_target, rate)\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "  \n",
        "  def call(self, inp, tar, training, enc_padding_mask,look_ahead_mask, dec_padding_mask):\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask) # (batch_size, inp_seq_len, d_model)\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    final_output = self.final_layer(dec_output) # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    return final_output, attention_weights\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPhqoBhwWYZ9"
      },
      "source": [
        "# Optimizer and Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "y79jHzBnA7wG"
      },
      "outputs": [],
      "source": [
        "# Set learning rate schedule\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g11C2K3tXFyP"
      },
      "source": [
        "# Set hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "-q51YVNusr5_"
      },
      "outputs": [],
      "source": [
        "num_layers = 6\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fGafl4DXYct"
      },
      "source": [
        "# Instantiate a Transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "9DJxw765BYHi"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=tokenizers.fr.get_vocab_size().numpy(),\n",
        "    target_vocab_size=tokenizers.twi.get_vocab_size().numpy(),\n",
        "    rate=dropout_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aGTknsnYDCC"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "BF0wsHaaJVKC"
      },
      "outputs": [],
      "source": [
        "# Instantiate learning rate and set optimizer\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "HpLaySRfSy89"
      },
      "outputs": [],
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "-PHWy8sdGoR8"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = './checkpoints/train'\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# If a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "VAj0P_T0ZcBV"
      },
      "outputs": [],
      "source": [
        "# Choose number of training epochs\n",
        "EPOCHS = 120\n",
        "train_step_signature = [\n",
        "  tf.TensorSpec(shape=(None, None), dtype=tf.int64), \n",
        "  tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp,True,enc_padding_mask,combined_mask,dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7k6GwwEIi_A",
        "outputId": "fc4fe824-12cd-4f72-8467-6a3cb360244e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 7.9081 Accuracy 0.0015\n",
            "Epoch 1 Batch 50 Loss 7.0870 Accuracy 0.0564\n",
            "Epoch 1 Batch 100 Loss 6.6120 Accuracy 0.0808\n",
            "Epoch 1 Batch 150 Loss 6.3261 Accuracy 0.1043\n",
            "Epoch 1 Batch 200 Loss 6.0895 Accuracy 0.1305\n",
            "Epoch 1 Batch 250 Loss 5.9069 Accuracy 0.1491\n",
            "Epoch 1 Batch 300 Loss 5.7688 Accuracy 0.1629\n",
            "Epoch 1 Batch 350 Loss 5.6573 Accuracy 0.1740\n",
            "Epoch 1 Loss 5.6440 Accuracy 0.1752\n",
            "Time taken for 1 epoch: 87.56 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 4.9767 Accuracy 0.2442\n",
            "Epoch 2 Batch 50 Loss 4.8706 Accuracy 0.2504\n",
            "Epoch 2 Batch 100 Loss 4.8208 Accuracy 0.2543\n",
            "Epoch 2 Batch 150 Loss 4.7760 Accuracy 0.2583\n",
            "Epoch 2 Batch 200 Loss 4.7389 Accuracy 0.2618\n",
            "Epoch 2 Batch 250 Loss 4.6969 Accuracy 0.2655\n",
            "Epoch 2 Batch 300 Loss 4.6522 Accuracy 0.2705\n",
            "Epoch 2 Batch 350 Loss 4.6078 Accuracy 0.2748\n",
            "Epoch 2 Loss 4.6019 Accuracy 0.2753\n",
            "Time taken for 1 epoch: 70.83 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4.2435 Accuracy 0.2959\n",
            "Epoch 3 Batch 50 Loss 4.1978 Accuracy 0.3159\n",
            "Epoch 3 Batch 100 Loss 4.1800 Accuracy 0.3167\n",
            "Epoch 3 Batch 150 Loss 4.1472 Accuracy 0.3198\n",
            "Epoch 3 Batch 200 Loss 4.1181 Accuracy 0.3224\n",
            "Epoch 3 Batch 250 Loss 4.0879 Accuracy 0.3250\n",
            "Epoch 3 Batch 300 Loss 4.0585 Accuracy 0.3270\n",
            "Epoch 3 Batch 350 Loss 4.0326 Accuracy 0.3292\n",
            "Epoch 3 Loss 4.0294 Accuracy 0.3294\n",
            "Time taken for 1 epoch: 67.90 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 3.6615 Accuracy 0.3656\n",
            "Epoch 4 Batch 50 Loss 3.7083 Accuracy 0.3556\n",
            "Epoch 4 Batch 100 Loss 3.6855 Accuracy 0.3587\n",
            "Epoch 4 Batch 150 Loss 3.6765 Accuracy 0.3598\n",
            "Epoch 4 Batch 200 Loss 3.6577 Accuracy 0.3606\n",
            "Epoch 4 Batch 250 Loss 3.6432 Accuracy 0.3618\n",
            "Epoch 4 Batch 300 Loss 3.6244 Accuracy 0.3634\n",
            "Epoch 4 Batch 350 Loss 3.6104 Accuracy 0.3641\n",
            "Epoch 4 Loss 3.6071 Accuracy 0.3645\n",
            "Time taken for 1 epoch: 68.82 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 3.2812 Accuracy 0.4014\n",
            "Epoch 5 Batch 50 Loss 3.3269 Accuracy 0.3920\n",
            "Epoch 5 Batch 100 Loss 3.3376 Accuracy 0.3904\n",
            "Epoch 5 Batch 150 Loss 3.3354 Accuracy 0.3906\n",
            "Epoch 5 Batch 200 Loss 3.3300 Accuracy 0.3912\n",
            "Epoch 5 Batch 250 Loss 3.3219 Accuracy 0.3922\n",
            "Epoch 5 Batch 300 Loss 3.3155 Accuracy 0.3928\n",
            "Epoch 5 Batch 350 Loss 3.3023 Accuracy 0.3941\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
            "Epoch 5 Loss 3.2989 Accuracy 0.3944\n",
            "Time taken for 1 epoch: 72.12 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 3.0220 Accuracy 0.4073\n",
            "Epoch 6 Batch 50 Loss 3.0371 Accuracy 0.4210\n",
            "Epoch 6 Batch 100 Loss 3.0480 Accuracy 0.4186\n",
            "Epoch 6 Batch 150 Loss 3.0477 Accuracy 0.4188\n",
            "Epoch 6 Batch 200 Loss 3.0468 Accuracy 0.4189\n",
            "Epoch 6 Batch 250 Loss 3.0441 Accuracy 0.4196\n",
            "Epoch 6 Batch 300 Loss 3.0445 Accuracy 0.4190\n",
            "Epoch 6 Batch 350 Loss 3.0416 Accuracy 0.4198\n",
            "Epoch 6 Loss 3.0439 Accuracy 0.4194\n",
            "Time taken for 1 epoch: 68.30 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 3.0083 Accuracy 0.4061\n",
            "Epoch 7 Batch 50 Loss 2.8019 Accuracy 0.4450\n",
            "Epoch 7 Batch 100 Loss 2.8191 Accuracy 0.4421\n",
            "Epoch 7 Batch 150 Loss 2.8241 Accuracy 0.4415\n",
            "Epoch 7 Batch 200 Loss 2.8254 Accuracy 0.4419\n",
            "Epoch 7 Batch 250 Loss 2.8297 Accuracy 0.4407\n",
            "Epoch 7 Batch 300 Loss 2.8256 Accuracy 0.4418\n",
            "Epoch 7 Batch 350 Loss 2.8330 Accuracy 0.4408\n",
            "Epoch 7 Loss 2.8344 Accuracy 0.4407\n",
            "Time taken for 1 epoch: 67.73 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.7404 Accuracy 0.4542\n",
            "Epoch 8 Batch 50 Loss 2.6297 Accuracy 0.4647\n",
            "Epoch 8 Batch 100 Loss 2.6130 Accuracy 0.4667\n",
            "Epoch 8 Batch 150 Loss 2.6355 Accuracy 0.4638\n",
            "Epoch 8 Batch 200 Loss 2.6527 Accuracy 0.4618\n",
            "Epoch 8 Batch 250 Loss 2.6687 Accuracy 0.4591\n",
            "Epoch 8 Batch 300 Loss 2.6745 Accuracy 0.4585\n",
            "Epoch 8 Batch 350 Loss 2.6785 Accuracy 0.4580\n",
            "Epoch 8 Loss 2.6789 Accuracy 0.4579\n",
            "Time taken for 1 epoch: 67.52 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.7407 Accuracy 0.4266\n",
            "Epoch 9 Batch 50 Loss 2.4743 Accuracy 0.4806\n",
            "Epoch 9 Batch 100 Loss 2.4905 Accuracy 0.4788\n",
            "Epoch 9 Batch 150 Loss 2.5036 Accuracy 0.4768\n",
            "Epoch 9 Batch 200 Loss 2.5125 Accuracy 0.4766\n",
            "Epoch 9 Batch 250 Loss 2.5344 Accuracy 0.4738\n",
            "Epoch 9 Batch 300 Loss 2.5502 Accuracy 0.4713\n",
            "Epoch 9 Batch 350 Loss 2.5610 Accuracy 0.4701\n",
            "Epoch 9 Loss 2.5623 Accuracy 0.4699\n",
            "Time taken for 1 epoch: 67.59 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 2.3825 Accuracy 0.4902\n",
            "Epoch 10 Batch 50 Loss 2.3721 Accuracy 0.4930\n",
            "Epoch 10 Batch 100 Loss 2.3945 Accuracy 0.4901\n",
            "Epoch 10 Batch 150 Loss 2.4203 Accuracy 0.4860\n",
            "Epoch 10 Batch 200 Loss 2.4389 Accuracy 0.4832\n",
            "Epoch 10 Batch 250 Loss 2.4548 Accuracy 0.4813\n",
            "Epoch 10 Batch 300 Loss 2.4713 Accuracy 0.4791\n",
            "Epoch 10 Batch 350 Loss 2.4862 Accuracy 0.4768\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
            "Epoch 10 Loss 2.4883 Accuracy 0.4765\n",
            "Time taken for 1 epoch: 70.54 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 2.3930 Accuracy 0.4770\n",
            "Epoch 11 Batch 50 Loss 2.3131 Accuracy 0.5017\n",
            "Epoch 11 Batch 100 Loss 2.3358 Accuracy 0.4962\n",
            "Epoch 11 Batch 150 Loss 2.3564 Accuracy 0.4922\n",
            "Epoch 11 Batch 200 Loss 2.4016 Accuracy 0.4855\n",
            "Epoch 11 Batch 250 Loss 2.4222 Accuracy 0.4823\n",
            "Epoch 11 Batch 300 Loss 2.4334 Accuracy 0.4812\n",
            "Epoch 11 Batch 350 Loss 2.4532 Accuracy 0.4788\n",
            "Epoch 11 Loss 2.4540 Accuracy 0.4787\n",
            "Time taken for 1 epoch: 67.62 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 2.1253 Accuracy 0.5240\n",
            "Epoch 12 Batch 50 Loss 2.2812 Accuracy 0.5036\n",
            "Epoch 12 Batch 100 Loss 2.3062 Accuracy 0.4995\n",
            "Epoch 12 Batch 150 Loss 2.3450 Accuracy 0.4927\n",
            "Epoch 12 Batch 200 Loss 2.3697 Accuracy 0.4891\n",
            "Epoch 12 Batch 250 Loss 2.3794 Accuracy 0.4879\n",
            "Epoch 12 Batch 300 Loss 2.3988 Accuracy 0.4852\n",
            "Epoch 12 Batch 350 Loss 2.4164 Accuracy 0.4827\n",
            "Epoch 12 Loss 2.4156 Accuracy 0.4827\n",
            "Time taken for 1 epoch: 67.09 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 2.2647 Accuracy 0.5118\n",
            "Epoch 13 Batch 50 Loss 2.1763 Accuracy 0.5159\n",
            "Epoch 13 Batch 100 Loss 2.2042 Accuracy 0.5112\n",
            "Epoch 13 Batch 150 Loss 2.2409 Accuracy 0.5053\n",
            "Epoch 13 Batch 200 Loss 2.2678 Accuracy 0.5014\n",
            "Epoch 13 Batch 250 Loss 2.2903 Accuracy 0.4983\n",
            "Epoch 13 Batch 300 Loss 2.2989 Accuracy 0.4974\n",
            "Epoch 13 Batch 350 Loss 2.3033 Accuracy 0.4971\n",
            "Epoch 13 Loss 2.3059 Accuracy 0.4968\n",
            "Time taken for 1 epoch: 67.57 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 2.1450 Accuracy 0.5243\n",
            "Epoch 14 Batch 50 Loss 2.0824 Accuracy 0.5273\n",
            "Epoch 14 Batch 100 Loss 2.1024 Accuracy 0.5252\n",
            "Epoch 14 Batch 150 Loss 2.1244 Accuracy 0.5224\n",
            "Epoch 14 Batch 200 Loss 2.1396 Accuracy 0.5197\n",
            "Epoch 14 Batch 250 Loss 2.1605 Accuracy 0.5169\n",
            "Epoch 14 Batch 300 Loss 2.1718 Accuracy 0.5155\n",
            "Epoch 14 Batch 350 Loss 2.1909 Accuracy 0.5129\n",
            "Epoch 14 Loss 2.1935 Accuracy 0.5123\n",
            "Time taken for 1 epoch: 67.28 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 2.0415 Accuracy 0.5327\n",
            "Epoch 15 Batch 50 Loss 1.9407 Accuracy 0.5526\n",
            "Epoch 15 Batch 100 Loss 1.9662 Accuracy 0.5473\n",
            "Epoch 15 Batch 150 Loss 2.0018 Accuracy 0.5418\n",
            "Epoch 15 Batch 200 Loss 2.0163 Accuracy 0.5400\n",
            "Epoch 15 Batch 250 Loss 2.0441 Accuracy 0.5349\n",
            "Epoch 15 Batch 300 Loss 2.0588 Accuracy 0.5330\n",
            "Epoch 15 Batch 350 Loss 2.0749 Accuracy 0.5307\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
            "Epoch 15 Loss 2.0758 Accuracy 0.5306\n",
            "Time taken for 1 epoch: 70.52 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.9029 Accuracy 0.5525\n",
            "Epoch 16 Batch 50 Loss 1.8615 Accuracy 0.5636\n",
            "Epoch 16 Batch 100 Loss 1.8952 Accuracy 0.5574\n",
            "Epoch 16 Batch 150 Loss 1.9227 Accuracy 0.5519\n",
            "Epoch 16 Batch 200 Loss 1.9379 Accuracy 0.5496\n",
            "Epoch 16 Batch 250 Loss 1.9496 Accuracy 0.5484\n",
            "Epoch 16 Batch 300 Loss 1.9585 Accuracy 0.5466\n",
            "Epoch 16 Batch 350 Loss 1.9729 Accuracy 0.5443\n",
            "Epoch 16 Loss 1.9745 Accuracy 0.5442\n",
            "Time taken for 1 epoch: 67.51 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.5355 Accuracy 0.6576\n",
            "Epoch 17 Batch 50 Loss 1.7460 Accuracy 0.5828\n",
            "Epoch 17 Batch 100 Loss 1.7637 Accuracy 0.5815\n",
            "Epoch 17 Batch 150 Loss 1.7959 Accuracy 0.5742\n",
            "Epoch 17 Batch 200 Loss 1.8179 Accuracy 0.5696\n",
            "Epoch 17 Batch 250 Loss 1.8419 Accuracy 0.5653\n",
            "Epoch 17 Batch 300 Loss 1.8557 Accuracy 0.5624\n",
            "Epoch 17 Batch 350 Loss 1.8694 Accuracy 0.5600\n",
            "Epoch 17 Loss 1.8710 Accuracy 0.5597\n",
            "Time taken for 1 epoch: 67.58 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.5381 Accuracy 0.6371\n",
            "Epoch 18 Batch 50 Loss 1.6650 Accuracy 0.5923\n",
            "Epoch 18 Batch 100 Loss 1.7040 Accuracy 0.5855\n",
            "Epoch 18 Batch 150 Loss 1.7121 Accuracy 0.5843\n",
            "Epoch 18 Batch 200 Loss 1.7291 Accuracy 0.5818\n",
            "Epoch 18 Batch 250 Loss 1.7517 Accuracy 0.5784\n",
            "Epoch 18 Batch 300 Loss 1.7693 Accuracy 0.5746\n",
            "Epoch 18 Batch 350 Loss 1.7815 Accuracy 0.5732\n",
            "Epoch 18 Loss 1.7824 Accuracy 0.5730\n",
            "Time taken for 1 epoch: 67.48 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.6045 Accuracy 0.6110\n",
            "Epoch 19 Batch 50 Loss 1.5598 Accuracy 0.6142\n",
            "Epoch 19 Batch 100 Loss 1.5990 Accuracy 0.6071\n",
            "Epoch 19 Batch 150 Loss 1.6278 Accuracy 0.6008\n",
            "Epoch 19 Batch 200 Loss 1.6524 Accuracy 0.5954\n",
            "Epoch 19 Batch 250 Loss 1.6734 Accuracy 0.5914\n",
            "Epoch 19 Batch 300 Loss 1.6815 Accuracy 0.5900\n",
            "Epoch 19 Batch 350 Loss 1.6911 Accuracy 0.5888\n",
            "Epoch 19 Loss 1.6933 Accuracy 0.5883\n",
            "Time taken for 1 epoch: 71.68 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.6063 Accuracy 0.5981\n",
            "Epoch 20 Batch 50 Loss 1.4779 Accuracy 0.6342\n",
            "Epoch 20 Batch 100 Loss 1.5095 Accuracy 0.6243\n",
            "Epoch 20 Batch 150 Loss 1.5375 Accuracy 0.6172\n",
            "Epoch 20 Batch 200 Loss 1.5577 Accuracy 0.6132\n",
            "Epoch 20 Batch 250 Loss 1.5737 Accuracy 0.6102\n",
            "Epoch 20 Batch 300 Loss 1.5919 Accuracy 0.6066\n",
            "Epoch 20 Batch 350 Loss 1.6064 Accuracy 0.6040\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
            "Epoch 20 Loss 1.6068 Accuracy 0.6041\n",
            "Time taken for 1 epoch: 70.51 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.3815 Accuracy 0.6365\n",
            "Epoch 21 Batch 50 Loss 1.4512 Accuracy 0.6328\n",
            "Epoch 21 Batch 100 Loss 1.4426 Accuracy 0.6347\n",
            "Epoch 21 Batch 150 Loss 1.4509 Accuracy 0.6319\n",
            "Epoch 21 Batch 200 Loss 1.4676 Accuracy 0.6284\n",
            "Epoch 21 Batch 250 Loss 1.4899 Accuracy 0.6242\n",
            "Epoch 21 Batch 300 Loss 1.5040 Accuracy 0.6215\n",
            "Epoch 21 Batch 350 Loss 1.5220 Accuracy 0.6177\n",
            "Epoch 21 Loss 1.5242 Accuracy 0.6173\n",
            "Time taken for 1 epoch: 67.86 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 1.1470 Accuracy 0.7105\n",
            "Epoch 22 Batch 50 Loss 1.3070 Accuracy 0.6663\n",
            "Epoch 22 Batch 100 Loss 1.3315 Accuracy 0.6578\n",
            "Epoch 22 Batch 150 Loss 1.3531 Accuracy 0.6529\n",
            "Epoch 22 Batch 200 Loss 1.3838 Accuracy 0.6460\n",
            "Epoch 22 Batch 250 Loss 1.4061 Accuracy 0.6415\n",
            "Epoch 22 Batch 300 Loss 1.4292 Accuracy 0.6368\n",
            "Epoch 22 Batch 350 Loss 1.4458 Accuracy 0.6334\n",
            "Epoch 22 Loss 1.4480 Accuracy 0.6330\n",
            "Time taken for 1 epoch: 67.17 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 1.2937 Accuracy 0.6430\n",
            "Epoch 23 Batch 50 Loss 1.2788 Accuracy 0.6670\n",
            "Epoch 23 Batch 100 Loss 1.2845 Accuracy 0.6648\n",
            "Epoch 23 Batch 150 Loss 1.3059 Accuracy 0.6602\n",
            "Epoch 23 Batch 200 Loss 1.3237 Accuracy 0.6562\n",
            "Epoch 23 Batch 250 Loss 1.3491 Accuracy 0.6508\n",
            "Epoch 23 Batch 300 Loss 1.3638 Accuracy 0.6475\n",
            "Epoch 23 Batch 350 Loss 1.3763 Accuracy 0.6453\n",
            "Epoch 23 Loss 1.3799 Accuracy 0.6445\n",
            "Time taken for 1 epoch: 67.53 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 1.2542 Accuracy 0.6812\n",
            "Epoch 24 Batch 50 Loss 1.1826 Accuracy 0.6887\n",
            "Epoch 24 Batch 100 Loss 1.1893 Accuracy 0.6862\n",
            "Epoch 24 Batch 150 Loss 1.2115 Accuracy 0.6805\n",
            "Epoch 24 Batch 200 Loss 1.2362 Accuracy 0.6743\n",
            "Epoch 24 Batch 250 Loss 1.2671 Accuracy 0.6676\n",
            "Epoch 24 Batch 300 Loss 1.2876 Accuracy 0.6628\n",
            "Epoch 24 Batch 350 Loss 1.3057 Accuracy 0.6587\n",
            "Epoch 24 Loss 1.3074 Accuracy 0.6584\n",
            "Time taken for 1 epoch: 66.87 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 1.1747 Accuracy 0.6770\n",
            "Epoch 25 Batch 50 Loss 1.1410 Accuracy 0.6970\n",
            "Epoch 25 Batch 100 Loss 1.1505 Accuracy 0.6942\n",
            "Epoch 25 Batch 150 Loss 1.1693 Accuracy 0.6889\n",
            "Epoch 25 Batch 200 Loss 1.1865 Accuracy 0.6848\n",
            "Epoch 25 Batch 250 Loss 1.2012 Accuracy 0.6821\n",
            "Epoch 25 Batch 300 Loss 1.2170 Accuracy 0.6783\n",
            "Epoch 25 Batch 350 Loss 1.2312 Accuracy 0.6748\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-5\n",
            "Epoch 25 Loss 1.2321 Accuracy 0.6745\n",
            "Time taken for 1 epoch: 70.62 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 1.0700 Accuracy 0.7076\n",
            "Epoch 26 Batch 50 Loss 1.0659 Accuracy 0.7125\n",
            "Epoch 26 Batch 100 Loss 1.0694 Accuracy 0.7094\n",
            "Epoch 26 Batch 150 Loss 1.0855 Accuracy 0.7060\n",
            "Epoch 26 Batch 200 Loss 1.1130 Accuracy 0.6992\n",
            "Epoch 26 Batch 250 Loss 1.1285 Accuracy 0.6959\n",
            "Epoch 26 Batch 300 Loss 1.1458 Accuracy 0.6916\n",
            "Epoch 26 Batch 350 Loss 1.1610 Accuracy 0.6882\n",
            "Epoch 26 Loss 1.1621 Accuracy 0.6879\n",
            "Time taken for 1 epoch: 67.27 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 1.0298 Accuracy 0.7309\n",
            "Epoch 27 Batch 50 Loss 0.9811 Accuracy 0.7302\n",
            "Epoch 27 Batch 100 Loss 1.0078 Accuracy 0.7241\n",
            "Epoch 27 Batch 150 Loss 1.0325 Accuracy 0.7176\n",
            "Epoch 27 Batch 200 Loss 1.0517 Accuracy 0.7131\n",
            "Epoch 27 Batch 250 Loss 1.0731 Accuracy 0.7083\n",
            "Epoch 27 Batch 300 Loss 1.0851 Accuracy 0.7056\n",
            "Epoch 27 Batch 350 Loss 1.0984 Accuracy 0.7025\n",
            "Epoch 27 Loss 1.1005 Accuracy 0.7020\n",
            "Time taken for 1 epoch: 67.12 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 1.1078 Accuracy 0.7031\n",
            "Epoch 28 Batch 50 Loss 0.9516 Accuracy 0.7377\n",
            "Epoch 28 Batch 100 Loss 0.9737 Accuracy 0.7314\n",
            "Epoch 28 Batch 150 Loss 0.9840 Accuracy 0.7289\n",
            "Epoch 28 Batch 200 Loss 0.9975 Accuracy 0.7257\n",
            "Epoch 28 Batch 250 Loss 1.0124 Accuracy 0.7217\n",
            "Epoch 28 Batch 300 Loss 1.0295 Accuracy 0.7173\n",
            "Epoch 28 Batch 350 Loss 1.0413 Accuracy 0.7140\n",
            "Epoch 28 Loss 1.0432 Accuracy 0.7136\n",
            "Time taken for 1 epoch: 66.93 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.7791 Accuracy 0.7845\n",
            "Epoch 29 Batch 50 Loss 0.8947 Accuracy 0.7464\n",
            "Epoch 29 Batch 100 Loss 0.9085 Accuracy 0.7442\n",
            "Epoch 29 Batch 150 Loss 0.9325 Accuracy 0.7383\n",
            "Epoch 29 Batch 200 Loss 0.9514 Accuracy 0.7335\n",
            "Epoch 29 Batch 250 Loss 0.9629 Accuracy 0.7312\n",
            "Epoch 29 Batch 300 Loss 0.9804 Accuracy 0.7269\n",
            "Epoch 29 Batch 350 Loss 0.9951 Accuracy 0.7234\n",
            "Epoch 29 Loss 0.9959 Accuracy 0.7231\n",
            "Time taken for 1 epoch: 67.50 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.9273 Accuracy 0.7361\n",
            "Epoch 30 Batch 50 Loss 0.8454 Accuracy 0.7592\n",
            "Epoch 30 Batch 100 Loss 0.8640 Accuracy 0.7557\n",
            "Epoch 30 Batch 150 Loss 0.8792 Accuracy 0.7520\n",
            "Epoch 30 Batch 200 Loss 0.8970 Accuracy 0.7477\n",
            "Epoch 30 Batch 250 Loss 0.9153 Accuracy 0.7428\n",
            "Epoch 30 Batch 300 Loss 0.9368 Accuracy 0.7382\n",
            "Epoch 30 Batch 350 Loss 0.9481 Accuracy 0.7354\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-6\n",
            "Epoch 30 Loss 0.9499 Accuracy 0.7350\n",
            "Time taken for 1 epoch: 70.66 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.6491 Accuracy 0.8086\n",
            "Epoch 31 Batch 50 Loss 0.8041 Accuracy 0.7716\n",
            "Epoch 31 Batch 100 Loss 0.8095 Accuracy 0.7694\n",
            "Epoch 31 Batch 150 Loss 0.8289 Accuracy 0.7642\n",
            "Epoch 31 Batch 200 Loss 0.8522 Accuracy 0.7582\n",
            "Epoch 31 Batch 250 Loss 0.8696 Accuracy 0.7537\n",
            "Epoch 31 Batch 300 Loss 0.8819 Accuracy 0.7503\n",
            "Epoch 31 Batch 350 Loss 0.8908 Accuracy 0.7480\n",
            "Epoch 31 Loss 0.8919 Accuracy 0.7476\n",
            "Time taken for 1 epoch: 67.29 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.6181 Accuracy 0.8215\n",
            "Epoch 32 Batch 50 Loss 0.7588 Accuracy 0.7845\n",
            "Epoch 32 Batch 100 Loss 0.7687 Accuracy 0.7811\n",
            "Epoch 32 Batch 150 Loss 0.7931 Accuracy 0.7746\n",
            "Epoch 32 Batch 200 Loss 0.8151 Accuracy 0.7680\n",
            "Epoch 32 Batch 250 Loss 0.8282 Accuracy 0.7648\n",
            "Epoch 32 Batch 300 Loss 0.8394 Accuracy 0.7616\n",
            "Epoch 32 Batch 350 Loss 0.8496 Accuracy 0.7588\n",
            "Epoch 32 Loss 0.8510 Accuracy 0.7584\n",
            "Time taken for 1 epoch: 67.51 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.7161 Accuracy 0.7732\n",
            "Epoch 33 Batch 50 Loss 0.7305 Accuracy 0.7886\n",
            "Epoch 33 Batch 100 Loss 0.7287 Accuracy 0.7890\n",
            "Epoch 33 Batch 150 Loss 0.7496 Accuracy 0.7823\n",
            "Epoch 33 Batch 200 Loss 0.7715 Accuracy 0.7763\n",
            "Epoch 33 Batch 250 Loss 0.7828 Accuracy 0.7735\n",
            "Epoch 33 Batch 300 Loss 0.7984 Accuracy 0.7696\n",
            "Epoch 33 Batch 350 Loss 0.8109 Accuracy 0.7661\n",
            "Epoch 33 Loss 0.8114 Accuracy 0.7659\n",
            "Time taken for 1 epoch: 66.83 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.7274 Accuracy 0.7903\n",
            "Epoch 34 Batch 50 Loss 0.6844 Accuracy 0.8016\n",
            "Epoch 34 Batch 100 Loss 0.7155 Accuracy 0.7923\n",
            "Epoch 34 Batch 150 Loss 0.7328 Accuracy 0.7869\n",
            "Epoch 34 Batch 200 Loss 0.7430 Accuracy 0.7838\n",
            "Epoch 34 Batch 250 Loss 0.7562 Accuracy 0.7803\n",
            "Epoch 34 Batch 300 Loss 0.7667 Accuracy 0.7773\n",
            "Epoch 34 Batch 350 Loss 0.7762 Accuracy 0.7748\n",
            "Epoch 34 Loss 0.7767 Accuracy 0.7746\n",
            "Time taken for 1 epoch: 66.78 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.6190 Accuracy 0.8183\n",
            "Epoch 35 Batch 50 Loss 0.6417 Accuracy 0.8111\n",
            "Epoch 35 Batch 100 Loss 0.6573 Accuracy 0.8070\n",
            "Epoch 35 Batch 150 Loss 0.6791 Accuracy 0.8010\n",
            "Epoch 35 Batch 200 Loss 0.6991 Accuracy 0.7953\n",
            "Epoch 35 Batch 250 Loss 0.7136 Accuracy 0.7919\n",
            "Epoch 35 Batch 300 Loss 0.7273 Accuracy 0.7882\n",
            "Epoch 35 Batch 350 Loss 0.7374 Accuracy 0.7854\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-7\n",
            "Epoch 35 Loss 0.7384 Accuracy 0.7851\n",
            "Time taken for 1 epoch: 69.69 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.5567 Accuracy 0.8497\n",
            "Epoch 36 Batch 50 Loss 0.6345 Accuracy 0.8130\n",
            "Epoch 36 Batch 100 Loss 0.6443 Accuracy 0.8109\n",
            "Epoch 36 Batch 150 Loss 0.6654 Accuracy 0.8048\n",
            "Epoch 36 Batch 200 Loss 0.6803 Accuracy 0.8003\n",
            "Epoch 36 Batch 250 Loss 0.6884 Accuracy 0.7980\n",
            "Epoch 36 Batch 300 Loss 0.7009 Accuracy 0.7949\n",
            "Epoch 36 Batch 350 Loss 0.7092 Accuracy 0.7922\n",
            "Epoch 36 Loss 0.7103 Accuracy 0.7918\n",
            "Time taken for 1 epoch: 67.46 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.4311 Accuracy 0.8815\n",
            "Epoch 37 Batch 50 Loss 0.5984 Accuracy 0.8234\n",
            "Epoch 37 Batch 100 Loss 0.6189 Accuracy 0.8175\n",
            "Epoch 37 Batch 150 Loss 0.6267 Accuracy 0.8151\n",
            "Epoch 37 Batch 200 Loss 0.6435 Accuracy 0.8102\n",
            "Epoch 37 Batch 250 Loss 0.6575 Accuracy 0.8055\n",
            "Epoch 37 Batch 300 Loss 0.6668 Accuracy 0.8028\n",
            "Epoch 37 Batch 350 Loss 0.6744 Accuracy 0.8008\n",
            "Epoch 37 Loss 0.6752 Accuracy 0.8005\n",
            "Time taken for 1 epoch: 67.01 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.5368 Accuracy 0.8285\n",
            "Epoch 38 Batch 50 Loss 0.5739 Accuracy 0.8274\n",
            "Epoch 38 Batch 100 Loss 0.5831 Accuracy 0.8254\n",
            "Epoch 38 Batch 150 Loss 0.5979 Accuracy 0.8216\n",
            "Epoch 38 Batch 200 Loss 0.6183 Accuracy 0.8156\n",
            "Epoch 38 Batch 250 Loss 0.6294 Accuracy 0.8130\n",
            "Epoch 38 Batch 300 Loss 0.6422 Accuracy 0.8098\n",
            "Epoch 38 Batch 350 Loss 0.6534 Accuracy 0.8067\n",
            "Epoch 38 Loss 0.6550 Accuracy 0.8063\n",
            "Time taken for 1 epoch: 67.47 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.5632 Accuracy 0.8325\n",
            "Epoch 39 Batch 50 Loss 0.5601 Accuracy 0.8313\n",
            "Epoch 39 Batch 100 Loss 0.5670 Accuracy 0.8294\n",
            "Epoch 39 Batch 150 Loss 0.5778 Accuracy 0.8262\n",
            "Epoch 39 Batch 200 Loss 0.5924 Accuracy 0.8227\n",
            "Epoch 39 Batch 250 Loss 0.6023 Accuracy 0.8201\n",
            "Epoch 39 Batch 300 Loss 0.6149 Accuracy 0.8169\n",
            "Epoch 39 Batch 350 Loss 0.6247 Accuracy 0.8142\n",
            "Epoch 39 Loss 0.6259 Accuracy 0.8139\n",
            "Time taken for 1 epoch: 66.85 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.5800 Accuracy 0.8249\n",
            "Epoch 40 Batch 50 Loss 0.5331 Accuracy 0.8398\n",
            "Epoch 40 Batch 100 Loss 0.5446 Accuracy 0.8362\n",
            "Epoch 40 Batch 150 Loss 0.5577 Accuracy 0.8330\n",
            "Epoch 40 Batch 200 Loss 0.5718 Accuracy 0.8291\n",
            "Epoch 40 Batch 250 Loss 0.5820 Accuracy 0.8262\n",
            "Epoch 40 Batch 300 Loss 0.5910 Accuracy 0.8235\n",
            "Epoch 40 Batch 350 Loss 0.6007 Accuracy 0.8203\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-8\n",
            "Epoch 40 Loss 0.6020 Accuracy 0.8200\n",
            "Time taken for 1 epoch: 69.51 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.6743 Accuracy 0.8158\n",
            "Epoch 41 Batch 50 Loss 0.5073 Accuracy 0.8494\n",
            "Epoch 41 Batch 100 Loss 0.5208 Accuracy 0.8444\n",
            "Epoch 41 Batch 150 Loss 0.5374 Accuracy 0.8385\n",
            "Epoch 41 Batch 200 Loss 0.5504 Accuracy 0.8349\n",
            "Epoch 41 Batch 250 Loss 0.5599 Accuracy 0.8321\n",
            "Epoch 41 Batch 300 Loss 0.5707 Accuracy 0.8290\n",
            "Epoch 41 Batch 350 Loss 0.5782 Accuracy 0.8269\n",
            "Epoch 41 Loss 0.5795 Accuracy 0.8265\n",
            "Time taken for 1 epoch: 67.19 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.4179 Accuracy 0.8635\n",
            "Epoch 42 Batch 50 Loss 0.4801 Accuracy 0.8551\n",
            "Epoch 42 Batch 100 Loss 0.4941 Accuracy 0.8511\n",
            "Epoch 42 Batch 150 Loss 0.5064 Accuracy 0.8471\n",
            "Epoch 42 Batch 200 Loss 0.5180 Accuracy 0.8431\n",
            "Epoch 42 Batch 250 Loss 0.5318 Accuracy 0.8391\n",
            "Epoch 42 Batch 300 Loss 0.5419 Accuracy 0.8362\n",
            "Epoch 42 Batch 350 Loss 0.5532 Accuracy 0.8331\n",
            "Epoch 42 Loss 0.5549 Accuracy 0.8327\n",
            "Time taken for 1 epoch: 66.74 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.4513 Accuracy 0.8548\n",
            "Epoch 43 Batch 50 Loss 0.4729 Accuracy 0.8567\n",
            "Epoch 43 Batch 100 Loss 0.4823 Accuracy 0.8539\n",
            "Epoch 43 Batch 150 Loss 0.4933 Accuracy 0.8507\n",
            "Epoch 43 Batch 200 Loss 0.5094 Accuracy 0.8458\n",
            "Epoch 43 Batch 250 Loss 0.5203 Accuracy 0.8424\n",
            "Epoch 43 Batch 300 Loss 0.5302 Accuracy 0.8395\n",
            "Epoch 43 Batch 350 Loss 0.5403 Accuracy 0.8365\n",
            "Epoch 43 Loss 0.5415 Accuracy 0.8363\n",
            "Time taken for 1 epoch: 67.78 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.4907 Accuracy 0.8481\n",
            "Epoch 44 Batch 50 Loss 0.4708 Accuracy 0.8592\n",
            "Epoch 44 Batch 100 Loss 0.4702 Accuracy 0.8587\n",
            "Epoch 44 Batch 150 Loss 0.4780 Accuracy 0.8554\n",
            "Epoch 44 Batch 200 Loss 0.4887 Accuracy 0.8523\n",
            "Epoch 44 Batch 250 Loss 0.4993 Accuracy 0.8490\n",
            "Epoch 44 Batch 300 Loss 0.5086 Accuracy 0.8465\n",
            "Epoch 44 Batch 350 Loss 0.5159 Accuracy 0.8442\n",
            "Epoch 44 Loss 0.5164 Accuracy 0.8440\n",
            "Time taken for 1 epoch: 67.28 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.4292 Accuracy 0.8732\n",
            "Epoch 45 Batch 50 Loss 0.4918 Accuracy 0.8521\n",
            "Epoch 45 Batch 100 Loss 0.4754 Accuracy 0.8558\n",
            "Epoch 45 Batch 150 Loss 0.4768 Accuracy 0.8552\n",
            "Epoch 45 Batch 200 Loss 0.4801 Accuracy 0.8534\n",
            "Epoch 45 Batch 250 Loss 0.4879 Accuracy 0.8505\n",
            "Epoch 45 Batch 300 Loss 0.4921 Accuracy 0.8492\n",
            "Epoch 45 Batch 350 Loss 0.5012 Accuracy 0.8466\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/train/ckpt-9\n",
            "Epoch 45 Loss 0.5024 Accuracy 0.8464\n",
            "Time taken for 1 epoch: 69.97 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.4291 Accuracy 0.8767\n",
            "Epoch 46 Batch 50 Loss 0.4227 Accuracy 0.8702\n",
            "Epoch 46 Batch 100 Loss 0.4352 Accuracy 0.8668\n",
            "Epoch 46 Batch 150 Loss 0.4453 Accuracy 0.8640\n",
            "Epoch 46 Batch 200 Loss 0.4584 Accuracy 0.8602\n",
            "Epoch 46 Batch 250 Loss 0.4672 Accuracy 0.8571\n",
            "Epoch 46 Batch 300 Loss 0.4784 Accuracy 0.8543\n",
            "Epoch 46 Batch 350 Loss 0.4855 Accuracy 0.8524\n",
            "Epoch 46 Loss 0.4867 Accuracy 0.8520\n",
            "Time taken for 1 epoch: 66.90 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.4468 Accuracy 0.8644\n",
            "Epoch 47 Batch 50 Loss 0.4231 Accuracy 0.8712\n",
            "Epoch 47 Batch 100 Loss 0.4266 Accuracy 0.8706\n",
            "Epoch 47 Batch 150 Loss 0.4406 Accuracy 0.8664\n",
            "Epoch 47 Batch 200 Loss 0.4500 Accuracy 0.8631\n",
            "Epoch 47 Batch 250 Loss 0.4578 Accuracy 0.8604\n",
            "Epoch 47 Batch 300 Loss 0.4655 Accuracy 0.8580\n",
            "Epoch 47 Batch 350 Loss 0.4718 Accuracy 0.8558\n",
            "Epoch 47 Loss 0.4727 Accuracy 0.8555\n",
            "Time taken for 1 epoch: 66.93 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.4039 Accuracy 0.8766\n",
            "Epoch 48 Batch 50 Loss 0.4520 Accuracy 0.8656\n",
            "Epoch 48 Batch 100 Loss 0.4449 Accuracy 0.8672\n",
            "Epoch 48 Batch 150 Loss 0.4401 Accuracy 0.8672\n",
            "Epoch 48 Batch 200 Loss 0.4452 Accuracy 0.8652\n",
            "Epoch 48 Batch 250 Loss 0.4509 Accuracy 0.8632\n",
            "Epoch 48 Batch 300 Loss 0.4583 Accuracy 0.8606\n",
            "Epoch 48 Loss 0.4626 Accuracy 0.8592\n",
            "Time taken for 1 epoch: 67.18 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.4092 Accuracy 0.8744\n",
            "Epoch 49 Batch 100 Loss 0.3996 Accuracy 0.8770\n",
            "Epoch 49 Batch 150 Loss 0.4077 Accuracy 0.8749\n",
            "Epoch 49 Batch 200 Loss 0.4156 Accuracy 0.8721\n",
            "Epoch 49 Batch 250 Loss 0.4249 Accuracy 0.8702\n",
            "Epoch 49 Batch 300 Loss 0.4315 Accuracy 0.8675\n",
            "Epoch 49 Batch 350 Loss 0.4411 Accuracy 0.8649\n",
            "Epoch 49 Loss 0.4419 Accuracy 0.8647\n",
            "Time taken for 1 epoch: 67.59 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.3873 Accuracy 0.8874\n",
            "Epoch 50 Batch 50 Loss 0.3834 Accuracy 0.8814\n",
            "Epoch 50 Batch 100 Loss 0.3922 Accuracy 0.8796\n",
            "Epoch 50 Batch 150 Loss 0.4059 Accuracy 0.8757\n",
            "Epoch 50 Batch 200 Loss 0.4102 Accuracy 0.8743\n",
            "Epoch 50 Batch 250 Loss 0.4188 Accuracy 0.8719\n",
            "Epoch 50 Batch 300 Loss 0.4276 Accuracy 0.8694\n",
            "Epoch 50 Batch 350 Loss 0.4343 Accuracy 0.8674\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-10\n",
            "Epoch 50 Loss 0.4352 Accuracy 0.8672\n",
            "Time taken for 1 epoch: 69.70 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.4057 Accuracy 0.8815\n",
            "Epoch 51 Batch 50 Loss 0.3653 Accuracy 0.8880\n",
            "Epoch 51 Batch 100 Loss 0.3756 Accuracy 0.8847\n",
            "Epoch 51 Batch 150 Loss 0.3837 Accuracy 0.8815\n",
            "Epoch 51 Batch 200 Loss 0.3903 Accuracy 0.8792\n",
            "Epoch 51 Batch 250 Loss 0.4010 Accuracy 0.8761\n",
            "Epoch 51 Batch 300 Loss 0.4082 Accuracy 0.8741\n",
            "Epoch 51 Batch 350 Loss 0.4153 Accuracy 0.8720\n",
            "Epoch 51 Loss 0.4162 Accuracy 0.8717\n",
            "Time taken for 1 epoch: 66.88 secs\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.4092 Accuracy 0.8645\n",
            "Epoch 52 Batch 50 Loss 0.3694 Accuracy 0.8875\n",
            "Epoch 52 Batch 100 Loss 0.3682 Accuracy 0.8878\n",
            "Epoch 52 Batch 150 Loss 0.3772 Accuracy 0.8849\n",
            "Epoch 52 Batch 200 Loss 0.3845 Accuracy 0.8822\n",
            "Epoch 52 Batch 250 Loss 0.3936 Accuracy 0.8794\n",
            "Epoch 52 Batch 300 Loss 0.4023 Accuracy 0.8767\n",
            "Epoch 52 Batch 350 Loss 0.4089 Accuracy 0.8747\n",
            "Epoch 52 Loss 0.4101 Accuracy 0.8742\n",
            "Time taken for 1 epoch: 67.19 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.3602 Accuracy 0.8805\n",
            "Epoch 53 Batch 50 Loss 0.3527 Accuracy 0.8897\n",
            "Epoch 53 Batch 100 Loss 0.3730 Accuracy 0.8840\n",
            "Epoch 53 Batch 150 Loss 0.3726 Accuracy 0.8837\n",
            "Epoch 53 Batch 200 Loss 0.3758 Accuracy 0.8829\n",
            "Epoch 53 Batch 250 Loss 0.3773 Accuracy 0.8825\n",
            "Epoch 53 Batch 300 Loss 0.3812 Accuracy 0.8812\n",
            "Epoch 53 Batch 350 Loss 0.3874 Accuracy 0.8795\n",
            "Epoch 53 Loss 0.3885 Accuracy 0.8791\n",
            "Time taken for 1 epoch: 66.60 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.3227 Accuracy 0.9002\n",
            "Epoch 54 Batch 50 Loss 0.3566 Accuracy 0.8922\n",
            "Epoch 54 Batch 100 Loss 0.3521 Accuracy 0.8923\n",
            "Epoch 54 Batch 150 Loss 0.3586 Accuracy 0.8909\n",
            "Epoch 54 Batch 200 Loss 0.3627 Accuracy 0.8887\n",
            "Epoch 54 Batch 250 Loss 0.3682 Accuracy 0.8877\n",
            "Epoch 54 Batch 300 Loss 0.3753 Accuracy 0.8852\n",
            "Epoch 54 Batch 350 Loss 0.3810 Accuracy 0.8834\n",
            "Epoch 54 Loss 0.3817 Accuracy 0.8831\n",
            "Time taken for 1 epoch: 66.97 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.2905 Accuracy 0.9095\n",
            "Epoch 55 Batch 50 Loss 0.3244 Accuracy 0.8994\n",
            "Epoch 55 Batch 100 Loss 0.3316 Accuracy 0.8966\n",
            "Epoch 55 Batch 150 Loss 0.3405 Accuracy 0.8941\n",
            "Epoch 55 Batch 200 Loss 0.3449 Accuracy 0.8933\n",
            "Epoch 55 Batch 250 Loss 0.3517 Accuracy 0.8915\n",
            "Epoch 55 Batch 300 Loss 0.3583 Accuracy 0.8897\n",
            "Epoch 55 Batch 350 Loss 0.3640 Accuracy 0.8876\n",
            "Saving checkpoint for epoch 55 at ./checkpoints/train/ckpt-11\n",
            "Epoch 55 Loss 0.3651 Accuracy 0.8873\n",
            "Time taken for 1 epoch: 69.72 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.2414 Accuracy 0.9238\n",
            "Epoch 56 Batch 50 Loss 0.3216 Accuracy 0.8989\n",
            "Epoch 56 Batch 100 Loss 0.3330 Accuracy 0.8963\n",
            "Epoch 56 Batch 150 Loss 0.3372 Accuracy 0.8950\n",
            "Epoch 56 Batch 200 Loss 0.3442 Accuracy 0.8933\n",
            "Epoch 56 Batch 250 Loss 0.3499 Accuracy 0.8912\n",
            "Epoch 56 Batch 300 Loss 0.3533 Accuracy 0.8903\n",
            "Epoch 56 Batch 350 Loss 0.3602 Accuracy 0.8886\n",
            "Epoch 56 Loss 0.3600 Accuracy 0.8886\n",
            "Time taken for 1 epoch: 66.95 secs\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.2975 Accuracy 0.9109\n",
            "Epoch 57 Batch 50 Loss 0.2990 Accuracy 0.9086\n",
            "Epoch 57 Batch 100 Loss 0.3091 Accuracy 0.9049\n",
            "Epoch 57 Batch 150 Loss 0.3202 Accuracy 0.9016\n",
            "Epoch 57 Batch 200 Loss 0.3317 Accuracy 0.8979\n",
            "Epoch 57 Batch 250 Loss 0.3396 Accuracy 0.8954\n",
            "Epoch 57 Batch 300 Loss 0.3447 Accuracy 0.8937\n",
            "Epoch 57 Batch 350 Loss 0.3505 Accuracy 0.8918\n",
            "Epoch 57 Loss 0.3511 Accuracy 0.8917\n",
            "Time taken for 1 epoch: 66.78 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.2665 Accuracy 0.9088\n",
            "Epoch 58 Batch 50 Loss 0.3005 Accuracy 0.9069\n",
            "Epoch 58 Batch 100 Loss 0.3055 Accuracy 0.9050\n",
            "Epoch 58 Batch 150 Loss 0.3125 Accuracy 0.9025\n",
            "Epoch 58 Batch 200 Loss 0.3214 Accuracy 0.8999\n",
            "Epoch 58 Batch 250 Loss 0.3276 Accuracy 0.8978\n",
            "Epoch 58 Batch 300 Loss 0.3364 Accuracy 0.8949\n",
            "Epoch 58 Batch 350 Loss 0.3428 Accuracy 0.8930\n",
            "Epoch 58 Loss 0.3429 Accuracy 0.8929\n",
            "Time taken for 1 epoch: 67.04 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.3382 Accuracy 0.8981\n",
            "Epoch 59 Batch 50 Loss 0.2872 Accuracy 0.9115\n",
            "Epoch 59 Batch 100 Loss 0.2973 Accuracy 0.9070\n",
            "Epoch 59 Batch 150 Loss 0.3018 Accuracy 0.9053\n",
            "Epoch 59 Batch 200 Loss 0.3110 Accuracy 0.9025\n",
            "Epoch 59 Batch 250 Loss 0.3172 Accuracy 0.9008\n",
            "Epoch 59 Batch 300 Loss 0.3227 Accuracy 0.8994\n",
            "Epoch 59 Batch 350 Loss 0.3268 Accuracy 0.8983\n",
            "Epoch 59 Loss 0.3274 Accuracy 0.8980\n",
            "Time taken for 1 epoch: 67.56 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.3146 Accuracy 0.9049\n",
            "Epoch 60 Batch 50 Loss 0.2991 Accuracy 0.9093\n",
            "Epoch 60 Batch 100 Loss 0.2966 Accuracy 0.9092\n",
            "Epoch 60 Batch 150 Loss 0.2992 Accuracy 0.9078\n",
            "Epoch 60 Batch 200 Loss 0.3081 Accuracy 0.9049\n",
            "Epoch 60 Batch 250 Loss 0.3144 Accuracy 0.9025\n",
            "Epoch 60 Batch 300 Loss 0.3176 Accuracy 0.9015\n",
            "Epoch 60 Batch 350 Loss 0.3229 Accuracy 0.8998\n",
            "Saving checkpoint for epoch 60 at ./checkpoints/train/ckpt-12\n",
            "Epoch 60 Loss 0.3240 Accuracy 0.8994\n",
            "Time taken for 1 epoch: 69.59 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.2696 Accuracy 0.9105\n",
            "Epoch 61 Batch 50 Loss 0.2866 Accuracy 0.9102\n",
            "Epoch 61 Batch 100 Loss 0.2881 Accuracy 0.9098\n",
            "Epoch 61 Batch 150 Loss 0.2951 Accuracy 0.9077\n",
            "Epoch 61 Batch 200 Loss 0.3002 Accuracy 0.9063\n",
            "Epoch 61 Batch 250 Loss 0.3071 Accuracy 0.9043\n",
            "Epoch 61 Batch 300 Loss 0.3116 Accuracy 0.9032\n",
            "Epoch 61 Batch 350 Loss 0.3164 Accuracy 0.9020\n",
            "Epoch 61 Loss 0.3179 Accuracy 0.9016\n",
            "Time taken for 1 epoch: 67.40 secs\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.3333 Accuracy 0.9040\n",
            "Epoch 62 Batch 50 Loss 0.2664 Accuracy 0.9177\n",
            "Epoch 62 Batch 100 Loss 0.2708 Accuracy 0.9158\n",
            "Epoch 62 Batch 150 Loss 0.2781 Accuracy 0.9133\n",
            "Epoch 62 Batch 200 Loss 0.2869 Accuracy 0.9110\n",
            "Epoch 62 Batch 250 Loss 0.2940 Accuracy 0.9090\n",
            "Epoch 62 Batch 300 Loss 0.2993 Accuracy 0.9070\n",
            "Epoch 62 Batch 350 Loss 0.3042 Accuracy 0.9052\n",
            "Epoch 62 Loss 0.3044 Accuracy 0.9052\n",
            "Time taken for 1 epoch: 66.77 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.2762 Accuracy 0.9220\n",
            "Epoch 63 Batch 50 Loss 0.2589 Accuracy 0.9171\n",
            "Epoch 63 Batch 100 Loss 0.2684 Accuracy 0.9157\n",
            "Epoch 63 Batch 150 Loss 0.2759 Accuracy 0.9137\n",
            "Epoch 63 Batch 200 Loss 0.2834 Accuracy 0.9117\n",
            "Epoch 63 Batch 250 Loss 0.2899 Accuracy 0.9096\n",
            "Epoch 63 Batch 300 Loss 0.2937 Accuracy 0.9086\n",
            "Epoch 63 Batch 350 Loss 0.2973 Accuracy 0.9077\n",
            "Epoch 63 Loss 0.2977 Accuracy 0.9075\n",
            "Time taken for 1 epoch: 66.69 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.2149 Accuracy 0.9405\n",
            "Epoch 64 Batch 50 Loss 0.2602 Accuracy 0.9208\n",
            "Epoch 64 Batch 100 Loss 0.2706 Accuracy 0.9178\n",
            "Epoch 64 Batch 150 Loss 0.2738 Accuracy 0.9167\n",
            "Epoch 64 Batch 200 Loss 0.2790 Accuracy 0.9152\n",
            "Epoch 64 Batch 250 Loss 0.2840 Accuracy 0.9136\n",
            "Epoch 64 Batch 300 Loss 0.2890 Accuracy 0.9117\n",
            "Epoch 64 Batch 350 Loss 0.2935 Accuracy 0.9100\n",
            "Epoch 64 Loss 0.2938 Accuracy 0.9100\n",
            "Time taken for 1 epoch: 66.64 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.1916 Accuracy 0.9381\n",
            "Epoch 65 Batch 50 Loss 0.2515 Accuracy 0.9211\n",
            "Epoch 65 Batch 100 Loss 0.2561 Accuracy 0.9210\n",
            "Epoch 65 Batch 150 Loss 0.2607 Accuracy 0.9197\n",
            "Epoch 65 Batch 200 Loss 0.2651 Accuracy 0.9179\n",
            "Epoch 65 Batch 250 Loss 0.2731 Accuracy 0.9153\n",
            "Epoch 65 Batch 300 Loss 0.2767 Accuracy 0.9142\n",
            "Epoch 65 Batch 350 Loss 0.2825 Accuracy 0.9121\n",
            "Saving checkpoint for epoch 65 at ./checkpoints/train/ckpt-13\n",
            "Epoch 65 Loss 0.2830 Accuracy 0.9120\n",
            "Time taken for 1 epoch: 70.10 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.2736 Accuracy 0.9106\n",
            "Epoch 66 Batch 50 Loss 0.2534 Accuracy 0.9214\n",
            "Epoch 66 Batch 100 Loss 0.2557 Accuracy 0.9213\n",
            "Epoch 66 Batch 150 Loss 0.2575 Accuracy 0.9203\n",
            "Epoch 66 Batch 200 Loss 0.2630 Accuracy 0.9187\n",
            "Epoch 66 Batch 250 Loss 0.2675 Accuracy 0.9173\n",
            "Epoch 66 Batch 300 Loss 0.2716 Accuracy 0.9161\n",
            "Epoch 66 Batch 350 Loss 0.2740 Accuracy 0.9154\n",
            "Epoch 66 Loss 0.2747 Accuracy 0.9151\n",
            "Time taken for 1 epoch: 67.22 secs\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.2522 Accuracy 0.9222\n",
            "Epoch 67 Batch 50 Loss 0.2409 Accuracy 0.9263\n",
            "Epoch 67 Batch 100 Loss 0.2483 Accuracy 0.9238\n",
            "Epoch 67 Batch 150 Loss 0.2522 Accuracy 0.9229\n",
            "Epoch 67 Batch 250 Loss 0.2657 Accuracy 0.9183\n",
            "Epoch 67 Batch 300 Loss 0.2678 Accuracy 0.9175\n",
            "Epoch 67 Batch 350 Loss 0.2711 Accuracy 0.9163\n",
            "Epoch 67 Loss 0.2717 Accuracy 0.9163\n",
            "Time taken for 1 epoch: 66.74 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.2281 Accuracy 0.9287\n",
            "Epoch 68 Batch 50 Loss 0.2371 Accuracy 0.9259\n",
            "Epoch 68 Batch 100 Loss 0.2372 Accuracy 0.9260\n",
            "Epoch 68 Batch 150 Loss 0.2485 Accuracy 0.9225\n",
            "Epoch 68 Batch 200 Loss 0.2528 Accuracy 0.9212\n",
            "Epoch 68 Batch 250 Loss 0.2577 Accuracy 0.9197\n",
            "Epoch 68 Batch 300 Loss 0.2612 Accuracy 0.9184\n",
            "Epoch 68 Batch 350 Loss 0.2658 Accuracy 0.9170\n",
            "Epoch 68 Loss 0.2661 Accuracy 0.9168\n",
            "Time taken for 1 epoch: 67.27 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.2437 Accuracy 0.9333\n",
            "Epoch 69 Batch 50 Loss 0.2389 Accuracy 0.9261\n",
            "Epoch 69 Batch 100 Loss 0.2452 Accuracy 0.9246\n",
            "Epoch 69 Batch 150 Loss 0.2458 Accuracy 0.9239\n",
            "Epoch 69 Batch 200 Loss 0.2507 Accuracy 0.9224\n",
            "Epoch 69 Batch 250 Loss 0.2558 Accuracy 0.9207\n",
            "Epoch 69 Batch 300 Loss 0.2571 Accuracy 0.9202\n",
            "Epoch 69 Batch 350 Loss 0.2608 Accuracy 0.9190\n",
            "Epoch 69 Loss 0.2614 Accuracy 0.9188\n",
            "Time taken for 1 epoch: 66.77 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.1812 Accuracy 0.9450\n",
            "Epoch 70 Batch 50 Loss 0.2234 Accuracy 0.9313\n",
            "Epoch 70 Batch 100 Loss 0.2287 Accuracy 0.9292\n",
            "Epoch 70 Batch 150 Loss 0.2362 Accuracy 0.9268\n",
            "Epoch 70 Batch 200 Loss 0.2388 Accuracy 0.9259\n",
            "Epoch 70 Batch 250 Loss 0.2458 Accuracy 0.9238\n",
            "Epoch 70 Batch 300 Loss 0.2498 Accuracy 0.9224\n",
            "Epoch 70 Batch 350 Loss 0.2509 Accuracy 0.9221\n",
            "Saving checkpoint for epoch 70 at ./checkpoints/train/ckpt-14\n",
            "Epoch 70 Loss 0.2510 Accuracy 0.9220\n",
            "Time taken for 1 epoch: 70.15 secs\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.2968 Accuracy 0.9152\n",
            "Epoch 71 Batch 50 Loss 0.2290 Accuracy 0.9294\n",
            "Epoch 71 Batch 100 Loss 0.2285 Accuracy 0.9291\n",
            "Epoch 71 Batch 150 Loss 0.2340 Accuracy 0.9277\n",
            "Epoch 71 Batch 200 Loss 0.2362 Accuracy 0.9269\n",
            "Epoch 71 Batch 250 Loss 0.2406 Accuracy 0.9254\n",
            "Epoch 71 Batch 300 Loss 0.2438 Accuracy 0.9242\n",
            "Epoch 71 Batch 350 Loss 0.2460 Accuracy 0.9233\n",
            "Epoch 71 Loss 0.2465 Accuracy 0.9232\n",
            "Time taken for 1 epoch: 66.80 secs\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.2106 Accuracy 0.9407\n",
            "Epoch 72 Batch 50 Loss 0.2101 Accuracy 0.9336\n",
            "Epoch 72 Batch 100 Loss 0.2130 Accuracy 0.9331\n",
            "Epoch 72 Batch 150 Loss 0.2221 Accuracy 0.9308\n",
            "Epoch 72 Batch 200 Loss 0.2286 Accuracy 0.9291\n",
            "Epoch 72 Batch 250 Loss 0.2355 Accuracy 0.9269\n",
            "Epoch 72 Batch 300 Loss 0.2389 Accuracy 0.9257\n",
            "Epoch 72 Batch 350 Loss 0.2431 Accuracy 0.9245\n",
            "Epoch 72 Loss 0.2436 Accuracy 0.9244\n",
            "Time taken for 1 epoch: 67.10 secs\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.1598 Accuracy 0.9560\n",
            "Epoch 73 Batch 50 Loss 0.2087 Accuracy 0.9352\n",
            "Epoch 73 Batch 100 Loss 0.2169 Accuracy 0.9329\n",
            "Epoch 73 Batch 150 Loss 0.2218 Accuracy 0.9309\n",
            "Epoch 73 Batch 200 Loss 0.2247 Accuracy 0.9300\n",
            "Epoch 73 Batch 250 Loss 0.2278 Accuracy 0.9290\n",
            "Epoch 73 Batch 300 Loss 0.2311 Accuracy 0.9277\n",
            "Epoch 73 Batch 350 Loss 0.2339 Accuracy 0.9266\n",
            "Epoch 73 Loss 0.2341 Accuracy 0.9266\n",
            "Time taken for 1 epoch: 67.08 secs\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.1642 Accuracy 0.9473\n",
            "Epoch 74 Batch 50 Loss 0.2087 Accuracy 0.9361\n",
            "Epoch 74 Batch 100 Loss 0.2093 Accuracy 0.9359\n",
            "Epoch 74 Batch 150 Loss 0.2138 Accuracy 0.9343\n",
            "Epoch 74 Batch 200 Loss 0.2187 Accuracy 0.9326\n",
            "Epoch 74 Batch 250 Loss 0.2231 Accuracy 0.9309\n",
            "Epoch 74 Batch 300 Loss 0.2254 Accuracy 0.9301\n",
            "Epoch 74 Batch 350 Loss 0.2274 Accuracy 0.9297\n",
            "Epoch 74 Loss 0.2276 Accuracy 0.9296\n",
            "Time taken for 1 epoch: 66.81 secs\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.3012 Accuracy 0.9123\n",
            "Epoch 75 Batch 50 Loss 0.2138 Accuracy 0.9337\n",
            "Epoch 75 Batch 100 Loss 0.2205 Accuracy 0.9316\n",
            "Epoch 75 Batch 150 Loss 0.2206 Accuracy 0.9315\n",
            "Epoch 75 Batch 200 Loss 0.2211 Accuracy 0.9314\n",
            "Epoch 75 Batch 250 Loss 0.2236 Accuracy 0.9308\n",
            "Epoch 75 Batch 300 Loss 0.2256 Accuracy 0.9302\n",
            "Epoch 75 Batch 350 Loss 0.2285 Accuracy 0.9295\n",
            "Saving checkpoint for epoch 75 at ./checkpoints/train/ckpt-15\n",
            "Epoch 75 Loss 0.2291 Accuracy 0.9292\n",
            "Time taken for 1 epoch: 70.31 secs\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.2389 Accuracy 0.9187\n",
            "Epoch 76 Batch 50 Loss 0.2076 Accuracy 0.9360\n",
            "Epoch 76 Batch 100 Loss 0.2100 Accuracy 0.9360\n",
            "Epoch 76 Batch 150 Loss 0.2117 Accuracy 0.9344\n",
            "Epoch 76 Batch 200 Loss 0.2151 Accuracy 0.9336\n",
            "Epoch 76 Batch 250 Loss 0.2200 Accuracy 0.9320\n",
            "Epoch 76 Batch 300 Loss 0.2217 Accuracy 0.9313\n",
            "Epoch 76 Batch 350 Loss 0.2238 Accuracy 0.9305\n",
            "Epoch 76 Loss 0.2239 Accuracy 0.9305\n",
            "Time taken for 1 epoch: 66.79 secs\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.1735 Accuracy 0.9563\n",
            "Epoch 77 Batch 50 Loss 0.1897 Accuracy 0.9414\n",
            "Epoch 77 Batch 100 Loss 0.1974 Accuracy 0.9386\n",
            "Epoch 77 Batch 150 Loss 0.2066 Accuracy 0.9358\n",
            "Epoch 77 Batch 200 Loss 0.2091 Accuracy 0.9348\n",
            "Epoch 77 Batch 250 Loss 0.2118 Accuracy 0.9337\n",
            "Epoch 77 Batch 300 Loss 0.2156 Accuracy 0.9327\n",
            "Epoch 77 Batch 350 Loss 0.2192 Accuracy 0.9316\n",
            "Epoch 77 Loss 0.2199 Accuracy 0.9314\n",
            "Time taken for 1 epoch: 66.82 secs\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.2423 Accuracy 0.9304\n",
            "Epoch 78 Batch 50 Loss 0.1916 Accuracy 0.9417\n",
            "Epoch 78 Batch 100 Loss 0.2029 Accuracy 0.9385\n",
            "Epoch 78 Batch 150 Loss 0.2067 Accuracy 0.9374\n",
            "Epoch 78 Batch 200 Loss 0.2092 Accuracy 0.9361\n",
            "Epoch 78 Batch 250 Loss 0.2114 Accuracy 0.9352\n",
            "Epoch 78 Batch 300 Loss 0.2135 Accuracy 0.9346\n",
            "Epoch 78 Batch 350 Loss 0.2178 Accuracy 0.9332\n",
            "Epoch 78 Loss 0.2185 Accuracy 0.9330\n",
            "Time taken for 1 epoch: 66.92 secs\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.1599 Accuracy 0.9485\n",
            "Epoch 79 Batch 50 Loss 0.1963 Accuracy 0.9398\n",
            "Epoch 79 Batch 100 Loss 0.1979 Accuracy 0.9396\n",
            "Epoch 79 Batch 150 Loss 0.1992 Accuracy 0.9387\n",
            "Epoch 79 Batch 200 Loss 0.2023 Accuracy 0.9379\n",
            "Epoch 79 Batch 250 Loss 0.2060 Accuracy 0.9363\n",
            "Epoch 79 Batch 300 Loss 0.2105 Accuracy 0.9346\n",
            "Epoch 79 Batch 350 Loss 0.2148 Accuracy 0.9335\n",
            "Epoch 79 Loss 0.2155 Accuracy 0.9333\n",
            "Time taken for 1 epoch: 67.31 secs\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.1857 Accuracy 0.9325\n",
            "Epoch 80 Batch 50 Loss 0.1860 Accuracy 0.9412\n",
            "Epoch 80 Batch 100 Loss 0.1867 Accuracy 0.9414\n",
            "Epoch 80 Batch 150 Loss 0.1885 Accuracy 0.9409\n",
            "Epoch 80 Batch 200 Loss 0.1929 Accuracy 0.9395\n",
            "Epoch 80 Batch 250 Loss 0.1976 Accuracy 0.9379\n",
            "Epoch 80 Batch 300 Loss 0.2019 Accuracy 0.9366\n",
            "Epoch 80 Batch 350 Loss 0.2065 Accuracy 0.9353\n",
            "Saving checkpoint for epoch 80 at ./checkpoints/train/ckpt-16\n",
            "Epoch 80 Loss 0.2070 Accuracy 0.9352\n",
            "Time taken for 1 epoch: 70.02 secs\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.1660 Accuracy 0.9460\n",
            "Epoch 81 Batch 50 Loss 0.1883 Accuracy 0.9405\n",
            "Epoch 81 Batch 100 Loss 0.1872 Accuracy 0.9415\n",
            "Epoch 81 Batch 150 Loss 0.1927 Accuracy 0.9401\n",
            "Epoch 81 Batch 200 Loss 0.1984 Accuracy 0.9381\n",
            "Epoch 81 Batch 250 Loss 0.2045 Accuracy 0.9368\n",
            "Epoch 81 Batch 300 Loss 0.2078 Accuracy 0.9361\n",
            "Epoch 81 Batch 350 Loss 0.2106 Accuracy 0.9352\n",
            "Epoch 81 Loss 0.2109 Accuracy 0.9350\n",
            "Time taken for 1 epoch: 66.91 secs\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.1821 Accuracy 0.9423\n",
            "Epoch 82 Batch 50 Loss 0.1810 Accuracy 0.9443\n",
            "Epoch 82 Batch 100 Loss 0.1837 Accuracy 0.9433\n",
            "Epoch 82 Batch 150 Loss 0.1892 Accuracy 0.9413\n",
            "Epoch 82 Batch 200 Loss 0.1917 Accuracy 0.9404\n",
            "Epoch 82 Batch 250 Loss 0.1987 Accuracy 0.9385\n",
            "Epoch 82 Batch 300 Loss 0.2023 Accuracy 0.9372\n",
            "Epoch 82 Batch 350 Loss 0.2054 Accuracy 0.9363\n",
            "Epoch 82 Loss 0.2054 Accuracy 0.9363\n",
            "Time taken for 1 epoch: 66.71 secs\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.1301 Accuracy 0.9661\n",
            "Epoch 83 Batch 50 Loss 0.1773 Accuracy 0.9468\n",
            "Epoch 83 Batch 100 Loss 0.1918 Accuracy 0.9426\n",
            "Epoch 83 Batch 150 Loss 0.1964 Accuracy 0.9401\n",
            "Epoch 83 Batch 200 Loss 0.1993 Accuracy 0.9388\n",
            "Epoch 83 Batch 250 Loss 0.2002 Accuracy 0.9384\n",
            "Epoch 83 Batch 300 Loss 0.2015 Accuracy 0.9383\n",
            "Epoch 83 Batch 350 Loss 0.2033 Accuracy 0.9377\n",
            "Epoch 83 Loss 0.2035 Accuracy 0.9376\n",
            "Time taken for 1 epoch: 66.45 secs\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.1733 Accuracy 0.9438\n",
            "Epoch 84 Batch 50 Loss 0.1724 Accuracy 0.9476\n",
            "Epoch 84 Batch 100 Loss 0.1716 Accuracy 0.9477\n",
            "Epoch 84 Batch 150 Loss 0.1791 Accuracy 0.9450\n",
            "Epoch 84 Batch 200 Loss 0.1830 Accuracy 0.9436\n",
            "Epoch 84 Batch 250 Loss 0.1869 Accuracy 0.9423\n",
            "Epoch 84 Batch 300 Loss 0.1903 Accuracy 0.9412\n",
            "Epoch 84 Batch 350 Loss 0.1933 Accuracy 0.9402\n",
            "Epoch 84 Loss 0.1938 Accuracy 0.9400\n",
            "Time taken for 1 epoch: 66.93 secs\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.1752 Accuracy 0.9324\n",
            "Epoch 85 Batch 50 Loss 0.1728 Accuracy 0.9459\n",
            "Epoch 85 Batch 100 Loss 0.1753 Accuracy 0.9452\n",
            "Epoch 85 Batch 150 Loss 0.1791 Accuracy 0.9441\n",
            "Epoch 85 Batch 200 Loss 0.1836 Accuracy 0.9430\n",
            "Epoch 85 Batch 250 Loss 0.1874 Accuracy 0.9418\n",
            "Epoch 85 Batch 300 Loss 0.1909 Accuracy 0.9408\n",
            "Epoch 85 Batch 350 Loss 0.1923 Accuracy 0.9403\n",
            "Saving checkpoint for epoch 85 at ./checkpoints/train/ckpt-17\n",
            "Epoch 85 Loss 0.1924 Accuracy 0.9403\n",
            "Time taken for 1 epoch: 70.63 secs\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.1588 Accuracy 0.9533\n",
            "Epoch 86 Batch 50 Loss 0.1621 Accuracy 0.9499\n",
            "Epoch 86 Batch 100 Loss 0.1711 Accuracy 0.9475\n",
            "Epoch 86 Batch 150 Loss 0.1767 Accuracy 0.9456\n",
            "Epoch 86 Batch 200 Loss 0.1795 Accuracy 0.9445\n",
            "Epoch 86 Batch 250 Loss 0.1812 Accuracy 0.9438\n",
            "Epoch 86 Batch 300 Loss 0.1830 Accuracy 0.9433\n",
            "Epoch 86 Batch 350 Loss 0.1862 Accuracy 0.9422\n",
            "Epoch 86 Loss 0.1863 Accuracy 0.9423\n",
            "Time taken for 1 epoch: 66.67 secs\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.1847 Accuracy 0.9482\n",
            "Epoch 87 Batch 50 Loss 0.1753 Accuracy 0.9455\n",
            "Epoch 87 Batch 100 Loss 0.1721 Accuracy 0.9475\n",
            "Epoch 87 Batch 150 Loss 0.1748 Accuracy 0.9461\n",
            "Epoch 87 Batch 200 Loss 0.1765 Accuracy 0.9457\n",
            "Epoch 87 Batch 250 Loss 0.1808 Accuracy 0.9439\n",
            "Epoch 87 Batch 300 Loss 0.1840 Accuracy 0.9430\n",
            "Epoch 87 Batch 350 Loss 0.1869 Accuracy 0.9421\n",
            "Epoch 87 Loss 0.1869 Accuracy 0.9420\n",
            "Time taken for 1 epoch: 66.83 secs\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.1629 Accuracy 0.9517\n",
            "Epoch 88 Batch 50 Loss 0.1745 Accuracy 0.9465\n",
            "Epoch 88 Batch 100 Loss 0.1734 Accuracy 0.9470\n",
            "Epoch 88 Batch 150 Loss 0.1743 Accuracy 0.9465\n",
            "Epoch 88 Batch 200 Loss 0.1790 Accuracy 0.9449\n",
            "Epoch 88 Batch 250 Loss 0.1808 Accuracy 0.9443\n",
            "Epoch 88 Batch 300 Loss 0.1834 Accuracy 0.9437\n",
            "Epoch 88 Batch 350 Loss 0.1849 Accuracy 0.9431\n",
            "Epoch 88 Loss 0.1849 Accuracy 0.9431\n",
            "Time taken for 1 epoch: 66.87 secs\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.1579 Accuracy 0.9435\n",
            "Epoch 89 Batch 50 Loss 0.1587 Accuracy 0.9500\n",
            "Epoch 89 Batch 100 Loss 0.1629 Accuracy 0.9487\n",
            "Epoch 89 Batch 150 Loss 0.1662 Accuracy 0.9475\n",
            "Epoch 89 Batch 200 Loss 0.1693 Accuracy 0.9466\n",
            "Epoch 89 Batch 250 Loss 0.1735 Accuracy 0.9453\n",
            "Epoch 89 Batch 300 Loss 0.1754 Accuracy 0.9448\n",
            "Epoch 89 Batch 350 Loss 0.1781 Accuracy 0.9441\n",
            "Epoch 89 Loss 0.1784 Accuracy 0.9439\n",
            "Time taken for 1 epoch: 66.99 secs\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.1464 Accuracy 0.9522\n",
            "Epoch 90 Batch 50 Loss 0.1565 Accuracy 0.9525\n",
            "Epoch 90 Batch 100 Loss 0.1628 Accuracy 0.9502\n",
            "Epoch 90 Batch 150 Loss 0.1671 Accuracy 0.9487\n",
            "Epoch 90 Batch 200 Loss 0.1701 Accuracy 0.9477\n",
            "Epoch 90 Batch 250 Loss 0.1727 Accuracy 0.9468\n",
            "Epoch 90 Batch 300 Loss 0.1768 Accuracy 0.9453\n",
            "Epoch 90 Batch 350 Loss 0.1784 Accuracy 0.9447\n",
            "Saving checkpoint for epoch 90 at ./checkpoints/train/ckpt-18\n",
            "Epoch 90 Loss 0.1784 Accuracy 0.9447\n",
            "Time taken for 1 epoch: 68.99 secs\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.1536 Accuracy 0.9489\n",
            "Epoch 91 Batch 50 Loss 0.1503 Accuracy 0.9540\n",
            "Epoch 91 Batch 100 Loss 0.1568 Accuracy 0.9521\n",
            "Epoch 91 Batch 150 Loss 0.1603 Accuracy 0.9511\n",
            "Epoch 91 Batch 200 Loss 0.1649 Accuracy 0.9497\n",
            "Epoch 91 Batch 250 Loss 0.1680 Accuracy 0.9483\n",
            "Epoch 91 Batch 300 Loss 0.1720 Accuracy 0.9470\n",
            "Epoch 91 Batch 350 Loss 0.1733 Accuracy 0.9465\n",
            "Epoch 91 Loss 0.1734 Accuracy 0.9465\n",
            "Time taken for 1 epoch: 66.93 secs\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.1059 Accuracy 0.9645\n",
            "Epoch 92 Batch 50 Loss 0.1522 Accuracy 0.9533\n",
            "Epoch 92 Batch 100 Loss 0.1546 Accuracy 0.9519\n",
            "Epoch 92 Batch 150 Loss 0.1568 Accuracy 0.9513\n",
            "Epoch 92 Batch 200 Loss 0.1592 Accuracy 0.9505\n",
            "Epoch 92 Batch 250 Loss 0.1638 Accuracy 0.9491\n",
            "Epoch 92 Batch 300 Loss 0.1670 Accuracy 0.9483\n",
            "Epoch 92 Batch 350 Loss 0.1696 Accuracy 0.9474\n",
            "Epoch 92 Loss 0.1695 Accuracy 0.9474\n",
            "Time taken for 1 epoch: 66.98 secs\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.1335 Accuracy 0.9535\n",
            "Epoch 93 Batch 50 Loss 0.1569 Accuracy 0.9518\n",
            "Epoch 93 Batch 100 Loss 0.1574 Accuracy 0.9519\n",
            "Epoch 93 Batch 150 Loss 0.1632 Accuracy 0.9501\n",
            "Epoch 93 Batch 200 Loss 0.1660 Accuracy 0.9494\n",
            "Epoch 93 Batch 250 Loss 0.1668 Accuracy 0.9493\n",
            "Epoch 93 Batch 300 Loss 0.1682 Accuracy 0.9485\n",
            "Epoch 93 Batch 350 Loss 0.1689 Accuracy 0.9482\n",
            "Epoch 93 Loss 0.1692 Accuracy 0.9480\n",
            "Time taken for 1 epoch: 67.00 secs\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.1393 Accuracy 0.9578\n",
            "Epoch 94 Batch 50 Loss 0.1518 Accuracy 0.9542\n",
            "Epoch 94 Batch 100 Loss 0.1580 Accuracy 0.9522\n",
            "Epoch 94 Batch 150 Loss 0.1605 Accuracy 0.9510\n",
            "Epoch 94 Batch 200 Loss 0.1618 Accuracy 0.9508\n",
            "Epoch 94 Batch 250 Loss 0.1638 Accuracy 0.9501\n",
            "Epoch 94 Batch 300 Loss 0.1659 Accuracy 0.9495\n",
            "Epoch 94 Batch 350 Loss 0.1680 Accuracy 0.9486\n",
            "Epoch 94 Loss 0.1684 Accuracy 0.9485\n",
            "Time taken for 1 epoch: 66.92 secs\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.1098 Accuracy 0.9608\n",
            "Epoch 95 Batch 50 Loss 0.1499 Accuracy 0.9548\n",
            "Epoch 95 Batch 100 Loss 0.1561 Accuracy 0.9523\n",
            "Epoch 95 Batch 150 Loss 0.1565 Accuracy 0.9521\n",
            "Epoch 95 Batch 200 Loss 0.1599 Accuracy 0.9512\n",
            "Epoch 95 Batch 250 Loss 0.1611 Accuracy 0.9505\n",
            "Epoch 95 Batch 300 Loss 0.1626 Accuracy 0.9501\n",
            "Epoch 95 Batch 350 Loss 0.1649 Accuracy 0.9494\n",
            "Saving checkpoint for epoch 95 at ./checkpoints/train/ckpt-19\n",
            "Epoch 95 Loss 0.1652 Accuracy 0.9494\n",
            "Time taken for 1 epoch: 69.73 secs\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.1492 Accuracy 0.9575\n",
            "Epoch 96 Batch 50 Loss 0.1497 Accuracy 0.9539\n",
            "Epoch 96 Batch 100 Loss 0.1549 Accuracy 0.9520\n",
            "Epoch 96 Batch 150 Loss 0.1552 Accuracy 0.9522\n",
            "Epoch 96 Batch 200 Loss 0.1575 Accuracy 0.9515\n",
            "Epoch 96 Batch 250 Loss 0.1629 Accuracy 0.9499\n",
            "Epoch 96 Batch 300 Loss 0.1650 Accuracy 0.9492\n",
            "Epoch 96 Batch 350 Loss 0.1667 Accuracy 0.9488\n",
            "Epoch 96 Loss 0.1664 Accuracy 0.9489\n",
            "Time taken for 1 epoch: 67.15 secs\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.1389 Accuracy 0.9538\n",
            "Epoch 97 Batch 50 Loss 0.1452 Accuracy 0.9558\n",
            "Epoch 97 Batch 100 Loss 0.1476 Accuracy 0.9546\n",
            "Epoch 97 Batch 150 Loss 0.1501 Accuracy 0.9534\n",
            "Epoch 97 Batch 200 Loss 0.1538 Accuracy 0.9523\n",
            "Epoch 97 Batch 250 Loss 0.1566 Accuracy 0.9513\n",
            "Epoch 97 Batch 300 Loss 0.1601 Accuracy 0.9502\n",
            "Epoch 97 Batch 350 Loss 0.1619 Accuracy 0.9496\n",
            "Epoch 97 Loss 0.1619 Accuracy 0.9496\n",
            "Time taken for 1 epoch: 66.89 secs\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.1248 Accuracy 0.9642\n",
            "Epoch 98 Batch 50 Loss 0.1491 Accuracy 0.9548\n",
            "Epoch 98 Batch 100 Loss 0.1473 Accuracy 0.9546\n",
            "Epoch 98 Batch 150 Loss 0.1500 Accuracy 0.9536\n",
            "Epoch 98 Batch 200 Loss 0.1529 Accuracy 0.9526\n",
            "Epoch 98 Batch 250 Loss 0.1530 Accuracy 0.9526\n",
            "Epoch 98 Batch 300 Loss 0.1539 Accuracy 0.9523\n",
            "Epoch 98 Batch 350 Loss 0.1565 Accuracy 0.9512\n",
            "Epoch 98 Loss 0.1566 Accuracy 0.9511\n",
            "Time taken for 1 epoch: 66.91 secs\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.1103 Accuracy 0.9689\n",
            "Epoch 99 Batch 50 Loss 0.1423 Accuracy 0.9564\n",
            "Epoch 99 Batch 100 Loss 0.1459 Accuracy 0.9554\n",
            "Epoch 99 Batch 150 Loss 0.1501 Accuracy 0.9539\n",
            "Epoch 99 Batch 200 Loss 0.1521 Accuracy 0.9534\n",
            "Epoch 99 Batch 250 Loss 0.1531 Accuracy 0.9531\n",
            "Epoch 99 Batch 300 Loss 0.1546 Accuracy 0.9527\n",
            "Epoch 99 Batch 350 Loss 0.1583 Accuracy 0.9515\n",
            "Epoch 99 Loss 0.1586 Accuracy 0.9515\n",
            "Time taken for 1 epoch: 67.11 secs\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.1508 Accuracy 0.9512\n",
            "Epoch 100 Batch 50 Loss 0.1470 Accuracy 0.9561\n",
            "Epoch 100 Batch 100 Loss 0.1441 Accuracy 0.9568\n",
            "Epoch 100 Batch 150 Loss 0.1454 Accuracy 0.9562\n",
            "Epoch 100 Batch 200 Loss 0.1462 Accuracy 0.9557\n",
            "Epoch 100 Batch 250 Loss 0.1480 Accuracy 0.9551\n",
            "Epoch 100 Batch 300 Loss 0.1498 Accuracy 0.9543\n",
            "Epoch 100 Batch 350 Loss 0.1528 Accuracy 0.9533\n",
            "Saving checkpoint for epoch 100 at ./checkpoints/train/ckpt-20\n",
            "Epoch 100 Loss 0.1532 Accuracy 0.9532\n",
            "Time taken for 1 epoch: 69.69 secs\n",
            "\n",
            "Epoch 101 Batch 0 Loss 0.2087 Accuracy 0.9337\n",
            "Epoch 101 Batch 50 Loss 0.1412 Accuracy 0.9565\n",
            "Epoch 101 Batch 100 Loss 0.1439 Accuracy 0.9560\n",
            "Epoch 101 Batch 150 Loss 0.1433 Accuracy 0.9565\n",
            "Epoch 101 Batch 200 Loss 0.1444 Accuracy 0.9560\n",
            "Epoch 101 Batch 250 Loss 0.1475 Accuracy 0.9551\n",
            "Epoch 101 Batch 300 Loss 0.1502 Accuracy 0.9543\n",
            "Epoch 101 Batch 350 Loss 0.1527 Accuracy 0.9534\n",
            "Epoch 101 Loss 0.1528 Accuracy 0.9533\n",
            "Time taken for 1 epoch: 67.32 secs\n",
            "\n",
            "Epoch 102 Batch 0 Loss 0.1598 Accuracy 0.9522\n",
            "Epoch 102 Batch 50 Loss 0.1390 Accuracy 0.9579\n",
            "Epoch 102 Batch 100 Loss 0.1398 Accuracy 0.9570\n",
            "Epoch 102 Batch 150 Loss 0.1427 Accuracy 0.9562\n",
            "Epoch 102 Batch 200 Loss 0.1457 Accuracy 0.9557\n",
            "Epoch 102 Batch 250 Loss 0.1488 Accuracy 0.9549\n",
            "Epoch 102 Batch 300 Loss 0.1514 Accuracy 0.9542\n",
            "Epoch 102 Batch 350 Loss 0.1531 Accuracy 0.9536\n",
            "Epoch 102 Loss 0.1537 Accuracy 0.9534\n",
            "Time taken for 1 epoch: 66.70 secs\n",
            "\n",
            "Epoch 103 Batch 0 Loss 0.0856 Accuracy 0.9755\n",
            "Epoch 103 Batch 50 Loss 0.1388 Accuracy 0.9586\n",
            "Epoch 103 Batch 100 Loss 0.1410 Accuracy 0.9572\n",
            "Epoch 103 Batch 150 Loss 0.1453 Accuracy 0.9554\n",
            "Epoch 103 Batch 200 Loss 0.1467 Accuracy 0.9551\n",
            "Epoch 103 Batch 250 Loss 0.1477 Accuracy 0.9547\n",
            "Epoch 103 Batch 300 Loss 0.1494 Accuracy 0.9541\n",
            "Epoch 103 Batch 350 Loss 0.1512 Accuracy 0.9535\n",
            "Epoch 103 Loss 0.1514 Accuracy 0.9534\n",
            "Time taken for 1 epoch: 67.20 secs\n",
            "\n",
            "Epoch 104 Batch 0 Loss 0.1419 Accuracy 0.9569\n",
            "Epoch 104 Batch 50 Loss 0.1278 Accuracy 0.9612\n",
            "Epoch 104 Batch 100 Loss 0.1346 Accuracy 0.9592\n",
            "Epoch 104 Batch 150 Loss 0.1349 Accuracy 0.9585\n",
            "Epoch 104 Batch 200 Loss 0.1382 Accuracy 0.9578\n",
            "Epoch 104 Batch 250 Loss 0.1409 Accuracy 0.9566\n",
            "Epoch 104 Batch 300 Loss 0.1431 Accuracy 0.9558\n",
            "Epoch 104 Batch 350 Loss 0.1454 Accuracy 0.9552\n",
            "Epoch 104 Loss 0.1460 Accuracy 0.9551\n",
            "Time taken for 1 epoch: 67.01 secs\n",
            "\n",
            "Epoch 105 Batch 0 Loss 0.1722 Accuracy 0.9485\n",
            "Epoch 105 Batch 50 Loss 0.1338 Accuracy 0.9584\n",
            "Epoch 105 Batch 100 Loss 0.1345 Accuracy 0.9586\n",
            "Epoch 105 Batch 150 Loss 0.1328 Accuracy 0.9595\n",
            "Epoch 105 Batch 200 Loss 0.1343 Accuracy 0.9590\n",
            "Epoch 105 Batch 250 Loss 0.1370 Accuracy 0.9582\n",
            "Epoch 105 Batch 300 Loss 0.1402 Accuracy 0.9573\n",
            "Epoch 105 Batch 350 Loss 0.1424 Accuracy 0.9566\n",
            "Saving checkpoint for epoch 105 at ./checkpoints/train/ckpt-21\n",
            "Epoch 105 Loss 0.1423 Accuracy 0.9567\n",
            "Time taken for 1 epoch: 70.03 secs\n",
            "\n",
            "Epoch 106 Batch 0 Loss 0.1437 Accuracy 0.9580\n",
            "Epoch 106 Batch 50 Loss 0.1193 Accuracy 0.9625\n",
            "Epoch 106 Batch 100 Loss 0.1298 Accuracy 0.9600\n",
            "Epoch 106 Batch 150 Loss 0.1360 Accuracy 0.9586\n",
            "Epoch 106 Batch 200 Loss 0.1383 Accuracy 0.9580\n",
            "Epoch 106 Batch 250 Loss 0.1409 Accuracy 0.9571\n",
            "Epoch 106 Batch 300 Loss 0.1420 Accuracy 0.9567\n",
            "Epoch 106 Batch 350 Loss 0.1436 Accuracy 0.9561\n",
            "Epoch 106 Loss 0.1437 Accuracy 0.9561\n",
            "Time taken for 1 epoch: 67.45 secs\n",
            "\n",
            "Epoch 107 Batch 0 Loss 0.1164 Accuracy 0.9676\n",
            "Epoch 107 Batch 50 Loss 0.1289 Accuracy 0.9608\n",
            "Epoch 107 Batch 100 Loss 0.1302 Accuracy 0.9605\n",
            "Epoch 107 Batch 150 Loss 0.1326 Accuracy 0.9596\n",
            "Epoch 107 Batch 200 Loss 0.1331 Accuracy 0.9594\n",
            "Epoch 107 Batch 250 Loss 0.1362 Accuracy 0.9586\n",
            "Epoch 107 Batch 300 Loss 0.1386 Accuracy 0.9577\n",
            "Epoch 107 Batch 350 Loss 0.1406 Accuracy 0.9570\n",
            "Epoch 107 Loss 0.1408 Accuracy 0.9570\n",
            "Time taken for 1 epoch: 66.36 secs\n",
            "\n",
            "Epoch 108 Batch 0 Loss 0.1152 Accuracy 0.9632\n",
            "Epoch 108 Batch 50 Loss 0.1317 Accuracy 0.9605\n",
            "Epoch 108 Batch 100 Loss 0.1299 Accuracy 0.9606\n",
            "Epoch 108 Batch 150 Loss 0.1354 Accuracy 0.9589\n",
            "Epoch 108 Batch 200 Loss 0.1347 Accuracy 0.9588\n",
            "Epoch 108 Batch 250 Loss 0.1359 Accuracy 0.9584\n",
            "Epoch 108 Batch 300 Loss 0.1366 Accuracy 0.9582\n",
            "Epoch 108 Batch 350 Loss 0.1377 Accuracy 0.9579\n",
            "Epoch 108 Loss 0.1378 Accuracy 0.9579\n",
            "Time taken for 1 epoch: 67.35 secs\n",
            "\n",
            "Epoch 109 Batch 0 Loss 0.1361 Accuracy 0.9567\n",
            "Epoch 109 Batch 50 Loss 0.1288 Accuracy 0.9590\n",
            "Epoch 109 Batch 100 Loss 0.1293 Accuracy 0.9594\n",
            "Epoch 109 Batch 150 Loss 0.1306 Accuracy 0.9592\n",
            "Epoch 109 Batch 200 Loss 0.1301 Accuracy 0.9594\n",
            "Epoch 109 Batch 250 Loss 0.1323 Accuracy 0.9589\n",
            "Epoch 109 Batch 300 Loss 0.1353 Accuracy 0.9580\n",
            "Epoch 109 Batch 350 Loss 0.1370 Accuracy 0.9575\n",
            "Epoch 109 Loss 0.1374 Accuracy 0.9574\n",
            "Time taken for 1 epoch: 66.70 secs\n",
            "\n",
            "Epoch 110 Batch 0 Loss 0.1836 Accuracy 0.9448\n",
            "Epoch 110 Batch 50 Loss 0.1219 Accuracy 0.9635\n",
            "Epoch 110 Batch 100 Loss 0.1237 Accuracy 0.9622\n",
            "Epoch 110 Batch 150 Loss 0.1253 Accuracy 0.9619\n",
            "Epoch 110 Batch 200 Loss 0.1277 Accuracy 0.9613\n",
            "Epoch 110 Batch 250 Loss 0.1293 Accuracy 0.9607\n",
            "Epoch 110 Batch 300 Loss 0.1319 Accuracy 0.9596\n",
            "Epoch 110 Batch 350 Loss 0.1331 Accuracy 0.9591\n",
            "Saving checkpoint for epoch 110 at ./checkpoints/train/ckpt-22\n",
            "Epoch 110 Loss 0.1332 Accuracy 0.9591\n",
            "Time taken for 1 epoch: 70.17 secs\n",
            "\n",
            "Epoch 111 Batch 0 Loss 0.0997 Accuracy 0.9690\n",
            "Epoch 111 Batch 50 Loss 0.1253 Accuracy 0.9622\n",
            "Epoch 111 Batch 100 Loss 0.1223 Accuracy 0.9632\n",
            "Epoch 111 Batch 150 Loss 0.1241 Accuracy 0.9627\n",
            "Epoch 111 Batch 200 Loss 0.1293 Accuracy 0.9612\n",
            "Epoch 111 Batch 250 Loss 0.1297 Accuracy 0.9609\n",
            "Epoch 111 Batch 300 Loss 0.1323 Accuracy 0.9602\n",
            "Epoch 111 Batch 350 Loss 0.1330 Accuracy 0.9599\n",
            "Epoch 111 Loss 0.1334 Accuracy 0.9598\n",
            "Time taken for 1 epoch: 67.24 secs\n",
            "\n",
            "Epoch 112 Batch 0 Loss 0.1042 Accuracy 0.9678\n",
            "Epoch 112 Batch 50 Loss 0.1188 Accuracy 0.9635\n",
            "Epoch 112 Batch 100 Loss 0.1233 Accuracy 0.9614\n",
            "Epoch 112 Batch 150 Loss 0.1230 Accuracy 0.9621\n",
            "Epoch 112 Batch 200 Loss 0.1260 Accuracy 0.9612\n",
            "Epoch 112 Batch 250 Loss 0.1283 Accuracy 0.9605\n",
            "Epoch 112 Batch 300 Loss 0.1304 Accuracy 0.9599\n",
            "Epoch 112 Batch 350 Loss 0.1321 Accuracy 0.9593\n",
            "Epoch 112 Loss 0.1323 Accuracy 0.9593\n",
            "Time taken for 1 epoch: 66.58 secs\n",
            "\n",
            "Epoch 113 Batch 0 Loss 0.1257 Accuracy 0.9669\n",
            "Epoch 113 Batch 50 Loss 0.1137 Accuracy 0.9659\n",
            "Epoch 113 Batch 100 Loss 0.1169 Accuracy 0.9643\n",
            "Epoch 113 Batch 150 Loss 0.1206 Accuracy 0.9634\n",
            "Epoch 113 Batch 200 Loss 0.1252 Accuracy 0.9616\n",
            "Epoch 113 Batch 250 Loss 0.1266 Accuracy 0.9612\n",
            "Epoch 113 Batch 300 Loss 0.1274 Accuracy 0.9609\n",
            "Epoch 113 Batch 350 Loss 0.1289 Accuracy 0.9606\n",
            "Epoch 113 Loss 0.1292 Accuracy 0.9605\n",
            "Time taken for 1 epoch: 67.24 secs\n",
            "\n",
            "Epoch 114 Batch 0 Loss 0.1707 Accuracy 0.9528\n",
            "Epoch 114 Batch 50 Loss 0.1262 Accuracy 0.9616\n",
            "Epoch 114 Batch 100 Loss 0.1259 Accuracy 0.9623\n",
            "Epoch 114 Batch 150 Loss 0.1268 Accuracy 0.9617\n",
            "Epoch 114 Batch 200 Loss 0.1276 Accuracy 0.9613\n",
            "Epoch 114 Batch 250 Loss 0.1286 Accuracy 0.9607\n",
            "Epoch 114 Batch 300 Loss 0.1300 Accuracy 0.9602\n",
            "Epoch 114 Batch 350 Loss 0.1312 Accuracy 0.9598\n",
            "Epoch 114 Loss 0.1312 Accuracy 0.9598\n",
            "Time taken for 1 epoch: 66.67 secs\n",
            "\n",
            "Epoch 115 Batch 0 Loss 0.1135 Accuracy 0.9713\n",
            "Epoch 115 Batch 50 Loss 0.1165 Accuracy 0.9648\n",
            "Epoch 115 Batch 100 Loss 0.1250 Accuracy 0.9621\n",
            "Epoch 115 Batch 150 Loss 0.1251 Accuracy 0.9620\n",
            "Epoch 115 Batch 200 Loss 0.1268 Accuracy 0.9612\n",
            "Epoch 115 Batch 250 Loss 0.1280 Accuracy 0.9604\n",
            "Epoch 115 Batch 300 Loss 0.1295 Accuracy 0.9601\n",
            "Epoch 115 Batch 350 Loss 0.1299 Accuracy 0.9600\n",
            "Saving checkpoint for epoch 115 at ./checkpoints/train/ckpt-23\n",
            "Epoch 115 Loss 0.1297 Accuracy 0.9600\n",
            "Time taken for 1 epoch: 70.02 secs\n",
            "\n",
            "Epoch 116 Batch 0 Loss 0.1521 Accuracy 0.9565\n",
            "Epoch 116 Batch 50 Loss 0.1139 Accuracy 0.9648\n",
            "Epoch 116 Batch 100 Loss 0.1140 Accuracy 0.9648\n",
            "Epoch 116 Batch 150 Loss 0.1164 Accuracy 0.9642\n",
            "Epoch 116 Batch 200 Loss 0.1192 Accuracy 0.9635\n",
            "Epoch 116 Batch 250 Loss 0.1214 Accuracy 0.9629\n",
            "Epoch 116 Batch 300 Loss 0.1241 Accuracy 0.9621\n"
          ]
        }
      ],
      "source": [
        "# Run training! Each epoch takes several mins with GPU\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  # inp -> twi, tar -> french\n",
        "  for (batch, (inp,tar)) in enumerate(train_batches):\n",
        "    train_step(inp, tar)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aij6BWpIBU52"
      },
      "source": [
        "# Build a Translator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0clY8WkaGZZ"
      },
      "outputs": [],
      "source": [
        "class Translator(tf.Module):\n",
        "    def __init__(self, tokenizers, transformer):\n",
        "        self.tokenizers = tokenizers\n",
        "        self.transformer = transformer\n",
        "\n",
        "    def __call__(self, sentence, max_length=MAX_TOKENS):\n",
        "        # The input sentence is French, hence adding the `[START]` and `[END]` tokens.\n",
        "        sentence = tf.convert_to_tensor([sentence])\n",
        "        sentence = self.tokenizers.fr.tokenize(sentence).to_tensor()\n",
        "\n",
        "        encoder_input = sentence\n",
        "\n",
        "        # As the output language is Twi, initialize the output with the\n",
        "        # English `[START]` token.\n",
        "        start_end = self.tokenizers.twi.tokenize([''])[0]\n",
        "        start = start_end[0][tf.newaxis]\n",
        "        end = start_end[1][tf.newaxis]\n",
        "\n",
        "        # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "        # dynamic-loop can be traced by `tf.function`.\n",
        "        output_array = tf.TensorArray(\n",
        "            dtype=tf.int64, size=0, dynamic_size=True)\n",
        "        output_array = output_array.write(0, start)\n",
        "        output = tf.transpose(output_array.stack())\n",
        "\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "                encoder_input, output)\n",
        "\n",
        "        for i in tf.range(max_length):\n",
        "            output = tf.transpose(output_array.stack())\n",
        "\n",
        "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "                encoder_input, output)\n",
        "\n",
        "            predictions, attention_weights = self.transformer(\n",
        "                encoder_input, output, False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "\n",
        "            # Select the last token from the `seq_len` dimension.\n",
        "            # Shape `(batch_size, 1, vocab_size)`.\n",
        "            predictions = predictions[:, -1:, :]\n",
        "\n",
        "            predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "            # Concatenate the `predicted_id` to the output which is given to the\n",
        "            # decoder as its input.\n",
        "            output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "            if predicted_id == end:\n",
        "                break\n",
        "\n",
        "        output = tf.transpose(output_array.stack())\n",
        "        # The output shape is `(1, tokens)`.\n",
        "        text = self.tokenizers.twi.detokenize(output)[0]  # Shape: `()`.\n",
        "\n",
        "        tokens = self.tokenizers.twi.lookup(output)[0]\n",
        "        _, attention_weights = self.transformer(\n",
        "            encoder_input, output[:, :-1], False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "\n",
        "        return text, tokens, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt1OVm3ecSNO"
      },
      "outputs": [],
      "source": [
        "#Create an instance of this Translator class\n",
        "translator = Translator(tokenizers, transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dMj9N3fBh1P"
      },
      "source": [
        "## translate example sentecnes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysGcUmAzc_Yi"
      },
      "outputs": [],
      "source": [
        "def print_translation(sentence, tokens, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKmyWMcGduft"
      },
      "outputs": [],
      "source": [
        "sentence =\"Asamoah a attaché la ficelle au cerf-volant.\"\n",
        "ground_truth= \"Asamoah de hama no bɔɔ asangoli no mu.\"\n",
        "translated_text, translated_tokens, attention_weights = translator(sentence)\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYL7f6tu9Y0N"
      },
      "outputs": [],
      "source": [
        "sentence=\"je ne sais pas qui tu es\"\n",
        "ground_truth=\"minnim onipa ko a woyɛ\"\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBe9E_dXBzne"
      },
      "source": [
        "# Save Translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMpmh77M9ufZ"
      },
      "outputs": [],
      "source": [
        "# class to export translator\n",
        "class ExportTranslator(tf.Module):\n",
        "  def __init__(self, translator):\n",
        "    self.translator = translator\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def __call__(self, sentence):\n",
        "    (result,\n",
        "     tokens,\n",
        "     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89tEraoTixKG"
      },
      "outputs": [],
      "source": [
        "translator = ExportTranslator(translator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZurB6qDFDzh"
      },
      "outputs": [],
      "source": [
        "translator(tf.constant(\"Vous n'a pas pas compris.\")).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMhMt9_z2ZXH"
      },
      "outputs": [],
      "source": [
        "module_no_signatures_path = '/content/drive/MyDrive/french_twi_translator'\n",
        "print('Saving model...')\n",
        "tf.saved_model.save(translator, module_no_signatures_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0jO3gtH9bih"
      },
      "outputs": [],
      "source": [
        "reloaded = tf.saved_model.load(module_no_signatures_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adjW9Tug-ZOa"
      },
      "outputs": [],
      "source": [
        "reloaded(\"Kwaku a essayé de briser le combat entre Abena et John.\").numpy().decode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbDLGyb8clxZ"
      },
      "source": [
        "#BLEU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8k01SazA8Cl"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "\n",
        "class Bleu():\n",
        "    def __init__(self, translator):\n",
        "        self.translator = translator\n",
        "\n",
        "    def get_bleuscore(self, testfile, referencefile, smothingfunction=None):\n",
        "        if type(testfile) == str and type(referencefile) == str:\n",
        "            # Open test file and read lines\n",
        "            f = open(testfile, \"r\")\n",
        "            hypothesis = f.readlines()\n",
        "            f.close()\n",
        "            # open refernce file and read lines\n",
        "            f = open(referencefile, \"r\")\n",
        "            reference = f.readlines()\n",
        "            f.close()\n",
        "        elif type(testfile) == list and type(referencefile) == list:\n",
        "            hypothesis = testfile\n",
        "            reference = referencefile\n",
        "        else:\n",
        "            print(f'File must be txt or python list')\n",
        "\n",
        "        # check the length of our input sentence\n",
        "        length = len(hypothesis)\n",
        "        bleu_total = np.array([0., 0., 0.])\n",
        "        weights = [(1./2, 1./2), (1./3, 1./3, 1./3),\n",
        "                   (1./4, 1./4, 1./4, 1./4)]\n",
        "        for i in range(length):\n",
        "            hypothesis[i] = hypothesis[i]\n",
        "            reference[i] = reference[i]\n",
        "            groundtruth = reference[i].lower().replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            groundtruth = [groundtruth]\n",
        "            translated_text = self.translator(\n",
        "                hypothesis[i]).numpy().decode(\"utf-8\")\n",
        "            # print(\"Translated Text: \", translated_text)\n",
        "            # print(\"Ground Truth: \", reference[i])\n",
        "            candidate = translated_text.replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            bleu = np.array(sentence_bleu(\n",
        "                groundtruth, candidate, weights, smoothing_function=smothingfunction))\n",
        "            bleu_total += bleu\n",
        "\n",
        "        return f'2-GRAMS: {bleu_total[0]/length}',f'3-GRAMS: {bleu_total[1]/length}',f'4-GRAMS: {bleu_total[2]/length}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYHXXY10dMh5"
      },
      "outputs": [],
      "source": [
        "#iNSTANTIATE OBJECT OF BLEU CLASS AND IT SMOOTHING FUNCTION\n",
        "smooth= SmoothingFunction()\n",
        "bleu = Bleu(reloaded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2y6aHDfdbZK"
      },
      "outputs": [],
      "source": [
        "# ESTIMATE BLEU SCORE FROM THE TEST DATA\n",
        "# from list\n",
        "%%time\n",
        "bleu.get_bleuscore(french_test,twi_test,smooth.method2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GOOGLE *API* BLEU\n"
      ],
      "metadata": {
        "id": "VRgT8luKPPuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use google API for bidirectional pivot translation of Twi and French\n",
        "# pivot language = English\n",
        "# import libraries\n",
        "from googletrans import Translator, constants\n",
        "# instantiate a translator object\n",
        "# initiate translator object\n",
        "translator = Translator()\n",
        "# Add Akan to the language supported by this package\n",
        "# Note the googletrans package has not  been updated to capture the new additions by google since May 2022\n",
        "# from https://translate.google.com/?sl=en&tl=ak&op=translate , the key and value for Twi is 'ak':'akan'\n",
        "constants.LANGUAGES['ak'] = 'akan'\n",
        "\n",
        "\n",
        "class GooglePivot:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def evaluate(self, sentences, src_key, dest_key):\n",
        "        eng_text = translator.translate(sentences, src=src_key, dest='en').text\n",
        "        print(eng_text)\n",
        "        output = translator.translate(eng_text, dest=dest_key).text\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class GoogleDirect:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def evaluate(self, sentences, src_key, dest_key):\n",
        "\n",
        "        return translator.translate(sentences, src=src_key, dest=dest_key).text"
      ],
      "metadata": {
        "id": "836KH3HJPNyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "\n",
        "class GoogleBleu():\n",
        "    def __init__(self, translator):\n",
        "        self.translator = translator\n",
        "\n",
        "    def get_bleuscore(self, testfile, referencefile,src_lang,dest_lang, smothingfunction=None):\n",
        "        if type(testfile) == str and type(referencefile) == str:\n",
        "            # Open test file and read lines\n",
        "            f = open(testfile, \"r\")\n",
        "            hypothesis = f.readlines()\n",
        "            f.close()\n",
        "            # open refernce file and read lines\n",
        "            f = open(referencefile, \"r\")\n",
        "            reference = f.readlines()\n",
        "            f.close()\n",
        "        elif type(testfile) == list and type(referencefile) == list:\n",
        "            hypothesis = testfile\n",
        "            reference = referencefile\n",
        "        else:\n",
        "            print(f'File must be txt or python list')\n",
        "\n",
        "        # check the length of our input sentence\n",
        "        length = len(hypothesis)\n",
        "        bleu_total = np.array([0., 0., 0.])\n",
        "        weights = [(1./2, 1./2), (1./3, 1./3, 1./3),\n",
        "                   (1./4, 1./4, 1./4, 1./4)]\n",
        "        for i in range(length):\n",
        "            hypothesis[i] = hypothesis[i]\n",
        "            reference[i] = reference[i]\n",
        "            groundtruth = reference[i].lower().replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            groundtruth = [groundtruth]\n",
        "            translated_text = self.translator.evaluate(hypothesis[i],src_key=src_lang, dest_key=dest_lang)\n",
        "            # print(\"Translated Text: \", translated_text)\n",
        "            # print(\"Ground Truth: \", reference[i])\n",
        "            candidate = translated_text.replace(\" ' \", \"'\").replace(\" .\", \".\").replace(\" ?\", \"?\").replace(\" !\", \"!\")\\\n",
        "                .replace(' \" ', '\" ').replace(' \"', '\"').replace(\" : \", \": \").replace(\" ( \", \" (\")\\\n",
        "                .replace(\" ) \", \") \").replace(\" , \", \", \").split()\n",
        "            bleu = np.array(sentence_bleu(\n",
        "                groundtruth, candidate, weights, smoothing_function=smothingfunction))\n",
        "            bleu_total += bleu\n",
        "\n",
        "        return f'2-GRAMS: {bleu_total[0]/length}',f'3-GRAMS: {bleu_total[1]/length}',f'4-GRAMS: {bleu_total[2]/length}'"
      ],
      "metadata": {
        "id": "QHT4GJc7Sqpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate GoogleDirect class\n",
        "google_translate = GoogleDirect()"
      ],
      "metadata": {
        "id": "DxmDJjRkQLiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "google_bleu = GoogleBleu(google_translate)"
      ],
      "metadata": {
        "id": "UHjJ24E4RQ22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "google_bleu.get_bleuscore(french_test,twi_test,'fr','ak',smooth.method2)\n"
      ],
      "metadata": {
        "id": "v8QDG1k8UMhp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}