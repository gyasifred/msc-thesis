{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/transformer_fr_twi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is based on the implementation of the paper [Attention is All you Need](https://arxiv.org/abs/1706.03762) by Vaswani et al. (2017). The code are adapted from the Tenssorflow repository at https://www.tensorflow.org/text/tutorials/transformer#build_the_transformer and a big thanks to [[Crater David]](https://nrs.harvard.edu/URN-3:HUL.INSTREPOS:37370749) \n"
      ],
      "metadata": {
        "id": "6qFzQxA29DJp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hblEgiSfLEck"
      },
      "source": [
        "# Install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U tensorflow-text-nightly==2.11.0.dev20220817\n",
        "!pip install -U -q tf-nightly==2.11.0.dev20220817"
      ],
      "metadata": {
        "id": "naWgZqfLsOiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY7seog2LQOL"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwubBvId4uGK"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W18p7g2lLatA"
      },
      "source": [
        "# preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovdl-UER8hMX"
      },
      "outputs": [],
      "source": [
        "# This code was adapted from https://github.com/GhanaNLP/kasa/blob/master/Kasa/Preprocessing.py\n",
        "# A subclass of the TFT  for preprocessing data\n",
        "# import required library\n",
        "import re\n",
        "import unicodedata\n",
        "\n",
        "\n",
        "class Preprocessing:\n",
        "    # dummy initialization method\n",
        "    def __init__(self):\n",
        "        # initialize with some default parameters here later\n",
        "        pass\n",
        "\n",
        "    # read in parallel twi - french dataset\n",
        "    def read_parallel_dataset(self, filepath_twi, filepath_french):\n",
        "\n",
        "        # read french data\n",
        "        french_data = []\n",
        "        with open(filepath_french, encoding='utf-8') as file:\n",
        "            line = file.readline()\n",
        "            cnt = 1\n",
        "            while line:\n",
        "                french_data.append(line.strip())\n",
        "                line = file.readline()\n",
        "                cnt += 1\n",
        "\n",
        "        # read twi data\n",
        "        twi_data = []\n",
        "        with open(filepath_twi, encoding='utf-8') as file:\n",
        "\n",
        "            # twi=file.read()\n",
        "            line = file.readline()\n",
        "            cnt = 1\n",
        "            while line:\n",
        "                twi_data.append(line.strip())\n",
        "                line = file.readline()\n",
        "                cnt += 1\n",
        "\n",
        "        return twi_data, french_data\n",
        "\n",
        "    # Define a helper function to remove string accents\n",
        "\n",
        "    def removeStringAccent(self, s):\n",
        "        return ''.join(\n",
        "            c for c in unicodedata.normalize('NFD', s)\n",
        "            if unicodedata.category(c) != 'Mn'\n",
        "        )\n",
        "\n",
        "    # normalize input twi sentence\n",
        "    def normalize_twi(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.ƆɔɛƐ!?’]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n",
        "\n",
        "    # normalize input french sentence\n",
        "    def normalize_fr(self, s):\n",
        "        s = self.removeStringAccent(s)\n",
        "        s = re.sub(r'([!.?])', r' \\1', s)\n",
        "        s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
        "        s = re.sub(r'\\s+', r' ', s)\n",
        "        return s\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4asafNW9HIt"
      },
      "outputs": [],
      "source": [
        "# Create an instance of tft preprocessing class\n",
        "\n",
        "TwiFrPreprocessor = Preprocessing()\n",
        "\n",
        "# Read raw parallel dataset\n",
        "raw_data_twi,raw_data_fr = TwiFrPreprocessor.read_parallel_dataset(\n",
        "        filepath_twi='/content/verified_twi.txt',\n",
        "        filepath_french='/content/verified_french.txt')\n",
        "\n",
        "# Normalize the raw data\n",
        "raw_data_fr = [TwiFrPreprocessor.normalize_fr(data) for data in raw_data_fr]\n",
        "raw_data_twi = [TwiFrPreprocessor.normalize_twi(data) for data in raw_data_twi]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "au81TkNOLwnB"
      },
      "source": [
        "# split data into training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5l08UvTL9mtx"
      },
      "outputs": [],
      "source": [
        "# split the dataset into training and test sets\n",
        "# Keep 20% of the data as test set\n",
        "train_twi,test_twi,train_fr,test_fr = train_test_split(raw_data_twi,raw_data_fr, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEbWCsNi9wao"
      },
      "outputs": [],
      "source": [
        "# define function to write text to txt file\n",
        "def writeTotxt(destination,data):\n",
        "  with open(destination, 'w') as f:\n",
        "    for line in data:\n",
        "        f.write(f\"{line}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q4djANP-9g6"
      },
      "outputs": [],
      "source": [
        "# write the preprocess traning and test dataset to a file\n",
        "writeTotxt('train_twi.txt',train_twi)\n",
        "writeTotxt('train_fr.txt',train_fr)\n",
        "writeTotxt('test_twi.txt',test_twi)\n",
        "writeTotxt('test_fr.txt',test_fr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ao2l87AzL_kE"
      },
      "source": [
        "# Build tf dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WZrSEP9_ISJ"
      },
      "outputs": [],
      "source": [
        "# build tf datasets from traning and test sentences in both languages\n",
        "train_dataset_fr = tf.data.TextLineDataset('/content/train_fr.txt')\n",
        "train_dataset_tw = tf.data.TextLineDataset('/content/train_twi.txt')\n",
        "\n",
        "val_dataset_fr = tf.data.TextLineDataset('/content/test_fr.txt')\n",
        "val_dataset_tw = tf.data.TextLineDataset('/content/test_twi.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmksbF-CAEXg"
      },
      "outputs": [],
      "source": [
        "# combine languages into single dataset\n",
        "trained_combined = tf.data.Dataset.zip((train_dataset_fr, train_dataset_tw))\n",
        "val_combined = tf.data.Dataset.zip((val_dataset_fr, val_dataset_tw))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZJ-hiEUBLDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bab8cf4-a755-4762-dd44-0aba94ed1d70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "French:  Par quoi dois je commencer ?\n",
            "Twi:  Dɛn na mefi ase ?\n",
            "French:  J etudie l economie au college .\n",
            "Twi:  Meresua sikasɛm ho ade wɔ suapɔn\n",
            "French:  Cette riviere devient peu profonde a cet endroit .\n",
            "Twi:  Saa asubɔnten yi mu nnɔ pii saa bere no .\n",
            "French:  Asamoah se demanda combien de temps il devrait attendre Araba .\n",
            "Twi:  Asamoah ntumi nhu bere tenten a ɛsɛ sɛ ɔtwɛn Araba .\n",
            "French:  Plus je vieillis plus je me souviens clairement de choses qui ne se sont jamais produites .\n",
            "Twi:  Dodow a me mfe rekɔ anim no dodow no ara na mekae nneɛma a amma saa da .\n"
          ]
        }
      ],
      "source": [
        "# verify trained_combined dataset is correct\n",
        "for  fr,tw in trained_combined.take(5):\n",
        "  print(\"French: \", fr.numpy().decode('utf-8'))\n",
        "  print(\"Twi: \", tw.numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaxFg3TZMIK8"
      },
      "source": [
        "# load tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1. The tokenizer is which was build from the parallel corpus.\n",
        "2. The code are available on the link https://colab.research.google.com/github/gyasifred/msc-thesis/blob/main/build_subword_tokenizer.ipynb\n"
      ],
      "metadata": {
        "id": "Vjb-3AU1BnrK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aR_vgyQgBo_T"
      },
      "outputs": [],
      "source": [
        "model_named = '/content/drive/MyDrive/translate_fr_twi_converter'\n",
        "# Verify tokenizer model can be reloaded\n",
        "tokenizers = tf.saved_model.load(model_named)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdHMl3icEjos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e635d7d1-d734-4c81-c54e-8ad20b5c1a5b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'je', b'##e', b'##he', b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# Verify tokenizer works on french sentence\n",
        "tokens = tokenizers.fr.tokenize(['je suis étudiant'])\n",
        "text_tokens = tokenizers.fr.lookup(tokens)\n",
        "text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSOMXh_de04Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10997606-b17c-4422-830c-8e99b16d5c63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "je suis etudiant\n"
          ]
        }
      ],
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.fr.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBNqSqeSfChq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee33629c-8ba9-49df-da25-c1a22736028a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'[START]', b'na', b'atwa', b'biara', b'mu',\n",
              "  b'\\xc9\\x94b\\xc9\\x9bk\\xc9\\x94', b'wob\\xc9\\x9by\\xc9\\x9b', b'pii',\n",
              "  b'w\\xc9\\x94', b'a', b'nufusu', b'anaa', b'polisifo', b'h\\xc9\\x94', b'.',\n",
              "  b'[END]']]>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Verify tokenizer works on twi sentence\n",
        "tokens = tokenizers.twi.tokenize(['Na mmarimaa ne mmeawa pii wɔ agudibea hɔ .'])\n",
        "text_tokens = tokenizers.twi.lookup(tokens)\n",
        "text_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lS1xMYQfQR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e5c6be-4fe7-4c05-d196-39d026f634fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "na mmarimaa ne mmeawa pii wɔ agudibea hɔ .\n"
          ]
        }
      ],
      "source": [
        "# Remove token markers to get original sentence\n",
        "round_trip = tokenizers.twi.detokenize(tokens)\n",
        "print(round_trip.numpy()[0].decode('utf-8'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQVo6G3OMfac"
      },
      "source": [
        "# Check sentence with maximum length"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By checking the maximum length of sentence, you can suitably select the MAX_TOKENS. The MAX_TOKKENS help drops examples longer than the maximum number of tokens (MAX_TOKENS). Without limiting the size of sequences, the performance may be negatively affected.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vvoh1vRgCxSb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7VJxJuNufTCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d18e1cb-3760-4092-9e3b-69803a3fe536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "......................................................................................................................................................................................................................................................................................................................................................................................................................."
          ]
        }
      ],
      "source": [
        "#The distribution of tokens per example in the dataset is as follows:\n",
        "\n",
        "lengths = []\n",
        "\n",
        "for fr_examples,twi_examples in trained_combined.batch(50):\n",
        "  twi_tokens = tokenizers.twi.tokenize(twi_examples)\n",
        "  lengths.append(twi_tokens.row_lengths())\n",
        "\n",
        "  fr_tokens = tokenizers.fr.tokenize(fr_examples)\n",
        "  lengths.append(fr_tokens.row_lengths())\n",
        "  print('.', end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri-qmhyyiBSa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "c8de9a2c-fb15-46ad-fed3-77e24c204aa8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "all_lengths = np.concatenate(lengths)\n",
        "\n",
        "plt.hist(all_lengths, np.linspace(0, 500, 101))\n",
        "plt.ylim(plt.ylim())\n",
        "max_length = max(all_lengths)\n",
        "plt.plot([max_length, max_length], plt.ylim())\n",
        "plt.title(f'Maximum tokens per example: {max_length}');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DYwhdWAMpnW"
      },
      "source": [
        "# Tokenize the training and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5piHJYkWjKii"
      },
      "outputs": [],
      "source": [
        "# function to drops examples longer than the maximum number of tokens (MAX_TOKENS).\n",
        "MAX_TOKENS = 50\n",
        "\n",
        "def filter_max_tokens(l1, l2):\n",
        "  num_tokens = tf.maximum(tf.shape(l1)[1],tf.shape(l2)[1])\n",
        "  return num_tokens < MAX_TOKENS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GC7UlwEeiNTO"
      },
      "outputs": [],
      "source": [
        "# Routine to tokenize sentence pairs; needed to make sentence batches for training\n",
        "def tokenize_pairs(fr,twi):\n",
        "  fr = tokenizers.fr.tokenize(fr)\n",
        "  # Convert from ragged to dense, padding with zeros.\n",
        "  fr = fr.to_tensor()\n",
        "  tw = tokenizers.twi.tokenize(twi)\n",
        "  # Convert from ragged to dense, padding with zeros.\n",
        "  tw = tw.to_tensor()\n",
        "  return fr,tw"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWMySNLzklc_"
      },
      "outputs": [],
      "source": [
        "# create training batches\n",
        "BUFFER_SIZE = len(train_fr)\n",
        "BATCH_SIZE = 64\n",
        "def make_batches(ds):\n",
        "  return (\n",
        "      ds\n",
        "      .cache()\n",
        "      .shuffle(BUFFER_SIZE)\n",
        "      .batch(BATCH_SIZE)\n",
        "      .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "      .filter(filter_max_tokens)\n",
        "      .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
        "\n",
        "# Create training and validation set batches.\n",
        "train_batches = make_batches(trained_combined)\n",
        "val_batches = make_batches(val_combined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNFMseSdM3wr"
      },
      "source": [
        "# Positional encoding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positional encodings are added to the embeddings to give the model some information about the relative position of the tokens in the sentence so the model can learn to recognize the word order.\n",
        "\n"
      ],
      "metadata": {
        "id": "5du74jWFF5WK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "  \n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # Apply the sine function to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # Apply the cosine function to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "KXtWVxfeb6px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a point-wise feed-forward network"
      ],
      "metadata": {
        "id": "mXL_HTJWG44o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def point_wise_feed_forward_network(\n",
        "  d_model, # Input/output dimensionality.\n",
        "  dff # Inner-layer dimensionality.\n",
        "  ):\n",
        "\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # Shape `(batch_size, seq_len, dff)`.\n",
        "      tf.keras.layers.Dense(d_model)  # Shape `(batch_size, seq_len, d_model)`.\n",
        "  ])"
      ],
      "metadata": {
        "id": "db4kXyfGb_l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build Transformer"
      ],
      "metadata": {
        "id": "wSEO7HUZHppR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multihead Attention layer"
      ],
      "metadata": {
        "id": "UtydO3sOIRE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create mask for padding tokens so translator ignores them\n",
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "    # add extra dimensions to add the padding\n",
        "    # to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "\n",
        "# Mask future tokens so translator cannot see future\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)\n",
        "\n",
        "\n",
        "# Create final masks\n",
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by\n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "    q, k, v must have matching leading dimensions.\n",
        "    k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "    The mask has different shapes depending on its type(padding or look ahead)\n",
        "    but it must be broadcastable for addition.\n",
        "\n",
        "    Args:\n",
        "      q: query shape == (..., seq_len_q, depth)\n",
        "      k: key shape == (..., seq_len_k, depth)\n",
        "      v: value shape == (..., seq_len_v, depth_v)\n",
        "      mask: Float tensor with shape broadcastable\n",
        "            to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "      output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    # (..., seq_len_q, seq_len_k)\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(\n",
        "        scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "# Add attention layers to create multi-head attention\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, *, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        # (batch_size, num_heads, seq_len_q, depth)\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        # (batch_size, num_heads, seq_len_k, depth)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        # (batch_size, num_heads, seq_len_v, depth)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "            q, k, v, mask)\n",
        "\n",
        "        # (batch_size, seq_len_q, num_heads, depth)\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention,\n",
        "                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        # (batch_size, seq_len_q, d_model)\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "        # Build Transformer encoder layer"
      ],
      "metadata": {
        "id": "BSHsspXXIaUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder"
      ],
      "metadata": {
        "id": "yvNLzFWpJw6m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    return out2"
      ],
      "metadata": {
        "id": "bCdZS_nJcGVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build Transformer encoder from encoder layer\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "              maximum_position_encoding=MAX_TOKENS,rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "    self.enc_layers = [\n",
        "        EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "\n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "metadata": {
        "id": "EBZNB3MacRCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder"
      ],
      "metadata": {
        "id": "ndsnLCOwRgcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "    return out3, attn_weights_block1, attn_weights_block2"
      ],
      "metadata": {
        "id": "fnXVTpI_cgzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Build Transformer decoder from decoder layer\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "              maximum_position_encoding,rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size,d_model)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "    self.dec_layers = [\n",
        "        DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n",
        "        for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "  def call(self, x, enc_output, training,\n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "\n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "\n",
        "      attention_weights[f'decoder_layer{i+1}_block1'] = block1\n",
        "      attention_weights[f'decoder_layer{i+1}_block2'] = block2\n",
        "\n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ],
      "metadata": {
        "id": "UEuoBDtJcp7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transformer"
      ],
      "metadata": {
        "id": "cjFbGSvJUQY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build full Transformer\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               target_vocab_size, pe_input=MAX_TOKENS, pe_target=MAX_TOKENS, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "    self.encoder= Encoder(num_layers, d_model, num_heads, dff,input_vocab_size, pe_input, rate)\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff,target_vocab_size, pe_target, rate)\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "  \n",
        "  def call(self, inp, tar, training, enc_padding_mask,look_ahead_mask, dec_padding_mask):\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask) # (batch_size, inp_seq_len, d_model)\n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    final_output = self.final_layer(dec_output) # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    return final_output, attention_weights\n"
      ],
      "metadata": {
        "id": "_6sd1ofVcyl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer and Metrics"
      ],
      "metadata": {
        "id": "pPhqoBhwWYZ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y79jHzBnA7wG"
      },
      "outputs": [],
      "source": [
        "# Set learning rate schedule\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "  accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "  accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "  mask = tf.cast(mask, dtype=tf.float32)\n",
        "  return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set hyperparameters"
      ],
      "metadata": {
        "id": "g11C2K3tXFyP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q51YVNusr5_"
      },
      "outputs": [],
      "source": [
        "num_layers = 6\n",
        "d_model = 512\n",
        "dff = 2048\n",
        "num_heads = 8\n",
        "dropout_rate = 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate a Transformer\n"
      ],
      "metadata": {
        "id": "5fGafl4DXYct"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DJxw765BYHi"
      },
      "outputs": [],
      "source": [
        "transformer = Transformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    dff=dff,\n",
        "    input_vocab_size=tokenizers.fr.get_vocab_size().numpy(),\n",
        "    target_vocab_size=tokenizers.twi.get_vocab_size().numpy(),\n",
        "    rate=dropout_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "-aGTknsnYDCC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BF0wsHaaJVKC"
      },
      "outputs": [],
      "source": [
        "# Instantiate learning rate and set optimizer\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "metadata": {
        "id": "HpLaySRfSy89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PHWy8sdGoR8"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = './checkpoints/train'\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# If a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print('Latest checkpoint restored!!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose number of training epochs\n",
        "EPOCHS = 100\n",
        "train_step_signature = [\n",
        "  tf.TensorSpec(shape=(None, None), dtype=tf.int64), \n",
        "  tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp,True,enc_padding_mask,combined_mask,dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  train_loss(loss)\n",
        "  train_accuracy(accuracy_function(tar_real, predictions))"
      ],
      "metadata": {
        "id": "VAj0P_T0ZcBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7k6GwwEIi_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5039c62a-5d34-47d9-daee-df92f6e52c86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 7.7942 Accuracy 0.0000\n",
            "Epoch 1 Batch 50 Loss 7.1101 Accuracy 0.0582\n",
            "Epoch 1 Batch 100 Loss 6.6873 Accuracy 0.0751\n",
            "Epoch 1 Batch 150 Loss 6.4286 Accuracy 0.0856\n",
            "Epoch 1 Batch 200 Loss 6.1776 Accuracy 0.1090\n",
            "Epoch 1 Batch 250 Loss 5.9686 Accuracy 0.1286\n",
            "Epoch 1 Batch 300 Loss 5.8084 Accuracy 0.1429\n",
            "Epoch 1 Loss 5.7952 Accuracy 0.1441\n",
            "Time taken for 1 epoch: 92.48 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 5.1041 Accuracy 0.2047\n",
            "Epoch 2 Batch 50 Loss 4.9156 Accuracy 0.2211\n",
            "Epoch 2 Batch 100 Loss 4.8591 Accuracy 0.2270\n",
            "Epoch 2 Batch 150 Loss 4.8064 Accuracy 0.2328\n",
            "Epoch 2 Batch 200 Loss 4.7512 Accuracy 0.2391\n",
            "Epoch 2 Batch 250 Loss 4.6991 Accuracy 0.2438\n",
            "Epoch 2 Batch 300 Loss 4.6477 Accuracy 0.2485\n",
            "Epoch 2 Loss 4.6422 Accuracy 0.2490\n",
            "Time taken for 1 epoch: 62.06 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 4.2727 Accuracy 0.2979\n",
            "Epoch 3 Batch 50 Loss 4.2444 Accuracy 0.2862\n",
            "Epoch 3 Batch 100 Loss 4.2102 Accuracy 0.2891\n",
            "Epoch 3 Batch 150 Loss 4.1649 Accuracy 0.2937\n",
            "Epoch 3 Batch 200 Loss 4.1263 Accuracy 0.2977\n",
            "Epoch 3 Batch 250 Loss 4.0892 Accuracy 0.3009\n",
            "Epoch 3 Batch 300 Loss 4.0461 Accuracy 0.3049\n",
            "Epoch 3 Loss 4.0437 Accuracy 0.3051\n",
            "Time taken for 1 epoch: 61.89 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 3.7920 Accuracy 0.3177\n",
            "Epoch 4 Batch 50 Loss 3.7027 Accuracy 0.3360\n",
            "Epoch 4 Batch 100 Loss 3.7011 Accuracy 0.3354\n",
            "Epoch 4 Batch 150 Loss 3.6736 Accuracy 0.3384\n",
            "Epoch 4 Batch 200 Loss 3.6535 Accuracy 0.3409\n",
            "Epoch 4 Batch 250 Loss 3.6294 Accuracy 0.3432\n",
            "Epoch 4 Batch 300 Loss 3.6087 Accuracy 0.3451\n",
            "Epoch 4 Loss 3.6068 Accuracy 0.3451\n",
            "Time taken for 1 epoch: 61.88 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 3.4771 Accuracy 0.3413\n",
            "Epoch 5 Batch 50 Loss 3.3512 Accuracy 0.3703\n",
            "Epoch 5 Batch 100 Loss 3.3503 Accuracy 0.3692\n",
            "Epoch 5 Batch 150 Loss 3.3307 Accuracy 0.3708\n",
            "Epoch 5 Batch 200 Loss 3.3224 Accuracy 0.3715\n",
            "Epoch 5 Batch 250 Loss 3.3172 Accuracy 0.3716\n",
            "Epoch 5 Batch 300 Loss 3.2952 Accuracy 0.3739\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train/ckpt-1\n",
            "Epoch 5 Loss 3.2939 Accuracy 0.3740\n",
            "Time taken for 1 epoch: 65.02 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 3.1366 Accuracy 0.3978\n",
            "Epoch 6 Batch 50 Loss 3.0561 Accuracy 0.4020\n",
            "Epoch 6 Batch 100 Loss 3.0621 Accuracy 0.3982\n",
            "Epoch 6 Batch 150 Loss 3.0567 Accuracy 0.3985\n",
            "Epoch 6 Batch 200 Loss 3.0589 Accuracy 0.3982\n",
            "Epoch 6 Batch 250 Loss 3.0556 Accuracy 0.3984\n",
            "Epoch 6 Batch 300 Loss 3.0521 Accuracy 0.3989\n",
            "Epoch 6 Loss 3.0519 Accuracy 0.3989\n",
            "Time taken for 1 epoch: 62.09 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.9084 Accuracy 0.4119\n",
            "Epoch 7 Batch 50 Loss 2.8406 Accuracy 0.4203\n",
            "Epoch 7 Batch 100 Loss 2.8654 Accuracy 0.4177\n",
            "Epoch 7 Batch 150 Loss 2.8578 Accuracy 0.4197\n",
            "Epoch 7 Batch 200 Loss 2.8532 Accuracy 0.4210\n",
            "Epoch 7 Batch 250 Loss 2.8455 Accuracy 0.4220\n",
            "Epoch 7 Batch 300 Loss 2.8436 Accuracy 0.4225\n",
            "Epoch 7 Loss 2.8428 Accuracy 0.4227\n",
            "Time taken for 1 epoch: 61.10 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 2.6353 Accuracy 0.4277\n",
            "Epoch 8 Batch 50 Loss 2.6222 Accuracy 0.4493\n",
            "Epoch 8 Batch 100 Loss 2.6402 Accuracy 0.4462\n",
            "Epoch 8 Batch 150 Loss 2.6601 Accuracy 0.4436\n",
            "Epoch 8 Batch 200 Loss 2.6694 Accuracy 0.4431\n",
            "Epoch 8 Batch 250 Loss 2.6683 Accuracy 0.4434\n",
            "Epoch 8 Batch 300 Loss 2.6723 Accuracy 0.4427\n",
            "Epoch 8 Loss 2.6717 Accuracy 0.4429\n",
            "Time taken for 1 epoch: 61.65 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.5156 Accuracy 0.4526\n",
            "Epoch 9 Batch 50 Loss 2.4588 Accuracy 0.4738\n",
            "Epoch 9 Batch 100 Loss 2.4804 Accuracy 0.4699\n",
            "Epoch 9 Batch 150 Loss 2.5008 Accuracy 0.4675\n",
            "Epoch 9 Batch 200 Loss 2.5076 Accuracy 0.4666\n",
            "Epoch 9 Batch 250 Loss 2.5195 Accuracy 0.4644\n",
            "Epoch 9 Batch 300 Loss 2.5305 Accuracy 0.4629\n",
            "Epoch 9 Loss 2.5306 Accuracy 0.4630\n",
            "Time taken for 1 epoch: 61.64 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 2.2567 Accuracy 0.5087\n",
            "Epoch 10 Batch 50 Loss 2.3042 Accuracy 0.4978\n",
            "Epoch 10 Batch 100 Loss 2.3458 Accuracy 0.4901\n",
            "Epoch 10 Batch 150 Loss 2.3582 Accuracy 0.4883\n",
            "Epoch 10 Batch 200 Loss 2.3800 Accuracy 0.4845\n",
            "Epoch 10 Batch 250 Loss 2.3984 Accuracy 0.4819\n",
            "Epoch 10 Batch 300 Loss 2.4158 Accuracy 0.4791\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train/ckpt-2\n",
            "Epoch 10 Loss 2.4152 Accuracy 0.4792\n",
            "Time taken for 1 epoch: 64.47 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 2.3767 Accuracy 0.4745\n",
            "Epoch 11 Batch 50 Loss 2.2402 Accuracy 0.5030\n",
            "Epoch 11 Batch 100 Loss 2.2723 Accuracy 0.4966\n",
            "Epoch 11 Batch 150 Loss 2.2896 Accuracy 0.4943\n",
            "Epoch 11 Batch 200 Loss 2.3064 Accuracy 0.4920\n",
            "Epoch 11 Batch 250 Loss 2.3284 Accuracy 0.4888\n",
            "Epoch 11 Batch 300 Loss 2.3422 Accuracy 0.4865\n",
            "Epoch 11 Loss 2.3441 Accuracy 0.4863\n",
            "Time taken for 1 epoch: 61.35 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 2.0012 Accuracy 0.5508\n",
            "Epoch 12 Batch 50 Loss 2.1622 Accuracy 0.5136\n",
            "Epoch 12 Batch 100 Loss 2.1874 Accuracy 0.5088\n",
            "Epoch 12 Batch 150 Loss 2.2272 Accuracy 0.5026\n",
            "Epoch 12 Batch 200 Loss 2.2521 Accuracy 0.4984\n",
            "Epoch 12 Batch 250 Loss 2.2670 Accuracy 0.4961\n",
            "Epoch 12 Batch 300 Loss 2.2913 Accuracy 0.4931\n",
            "Epoch 12 Loss 2.2928 Accuracy 0.4929\n",
            "Time taken for 1 epoch: 61.48 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.9481 Accuracy 0.5583\n",
            "Epoch 13 Batch 50 Loss 2.1650 Accuracy 0.5102\n",
            "Epoch 13 Batch 100 Loss 2.1843 Accuracy 0.5074\n",
            "Epoch 13 Batch 150 Loss 2.2144 Accuracy 0.5026\n",
            "Epoch 13 Batch 200 Loss 2.2239 Accuracy 0.5013\n",
            "Epoch 13 Batch 250 Loss 2.2431 Accuracy 0.4979\n",
            "Epoch 13 Batch 300 Loss 2.2622 Accuracy 0.4954\n",
            "Epoch 13 Loss 2.2643 Accuracy 0.4952\n",
            "Time taken for 1 epoch: 61.64 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 2.1571 Accuracy 0.4852\n",
            "Epoch 14 Batch 50 Loss 2.1262 Accuracy 0.5118\n",
            "Epoch 14 Batch 100 Loss 2.1585 Accuracy 0.5088\n",
            "Epoch 14 Batch 150 Loss 2.1844 Accuracy 0.5060\n",
            "Epoch 14 Batch 200 Loss 2.2008 Accuracy 0.5045\n",
            "Epoch 14 Batch 250 Loss 2.2137 Accuracy 0.5026\n",
            "Epoch 14 Batch 300 Loss 2.2285 Accuracy 0.5011\n",
            "Epoch 14 Loss 2.2300 Accuracy 0.5009\n",
            "Time taken for 1 epoch: 61.07 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 2.0915 Accuracy 0.5304\n",
            "Epoch 15 Batch 50 Loss 2.0203 Accuracy 0.5314\n",
            "Epoch 15 Batch 100 Loss 2.0586 Accuracy 0.5250\n",
            "Epoch 15 Batch 150 Loss 2.0812 Accuracy 0.5221\n",
            "Epoch 15 Batch 200 Loss 2.1008 Accuracy 0.5189\n",
            "Epoch 15 Batch 250 Loss 2.1200 Accuracy 0.5164\n",
            "Epoch 15 Batch 300 Loss 2.1292 Accuracy 0.5151\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train/ckpt-3\n",
            "Epoch 15 Loss 2.1306 Accuracy 0.5148\n",
            "Time taken for 1 epoch: 64.07 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.8641 Accuracy 0.5521\n",
            "Epoch 16 Batch 50 Loss 1.8762 Accuracy 0.5576\n",
            "Epoch 16 Batch 100 Loss 1.9438 Accuracy 0.5464\n",
            "Epoch 16 Batch 150 Loss 1.9684 Accuracy 0.5413\n",
            "Epoch 16 Batch 200 Loss 1.9930 Accuracy 0.5375\n",
            "Epoch 16 Batch 250 Loss 2.0113 Accuracy 0.5348\n",
            "Epoch 16 Batch 300 Loss 2.0295 Accuracy 0.5319\n",
            "Epoch 16 Loss 2.0306 Accuracy 0.5318\n",
            "Time taken for 1 epoch: 61.10 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.8722 Accuracy 0.5628\n",
            "Epoch 17 Batch 50 Loss 1.8301 Accuracy 0.5671\n",
            "Epoch 17 Batch 100 Loss 1.8528 Accuracy 0.5616\n",
            "Epoch 17 Batch 150 Loss 1.8892 Accuracy 0.5553\n",
            "Epoch 17 Batch 200 Loss 1.9155 Accuracy 0.5501\n",
            "Epoch 17 Batch 250 Loss 1.9276 Accuracy 0.5477\n",
            "Epoch 17 Batch 300 Loss 1.9414 Accuracy 0.5463\n",
            "Epoch 17 Loss 1.9425 Accuracy 0.5462\n",
            "Time taken for 1 epoch: 61.13 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.6537 Accuracy 0.5866\n",
            "Epoch 18 Batch 50 Loss 1.7240 Accuracy 0.5834\n",
            "Epoch 18 Batch 100 Loss 1.7712 Accuracy 0.5745\n",
            "Epoch 18 Batch 150 Loss 1.8015 Accuracy 0.5680\n",
            "Epoch 18 Batch 200 Loss 1.8160 Accuracy 0.5670\n",
            "Epoch 18 Batch 250 Loss 1.8339 Accuracy 0.5632\n",
            "Epoch 18 Batch 300 Loss 1.8488 Accuracy 0.5611\n",
            "Epoch 18 Loss 1.8499 Accuracy 0.5610\n",
            "Time taken for 1 epoch: 61.31 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.5997 Accuracy 0.6100\n",
            "Epoch 19 Batch 50 Loss 1.6541 Accuracy 0.5956\n",
            "Epoch 19 Batch 100 Loss 1.6986 Accuracy 0.5870\n",
            "Epoch 19 Batch 150 Loss 1.7092 Accuracy 0.5847\n",
            "Epoch 19 Batch 200 Loss 1.7301 Accuracy 0.5808\n",
            "Epoch 19 Batch 250 Loss 1.7495 Accuracy 0.5773\n",
            "Epoch 19 Batch 300 Loss 1.7686 Accuracy 0.5737\n",
            "Epoch 19 Loss 1.7693 Accuracy 0.5737\n",
            "Time taken for 1 epoch: 61.42 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 1.5035 Accuracy 0.6154\n",
            "Epoch 20 Batch 50 Loss 1.5688 Accuracy 0.6092\n",
            "Epoch 20 Batch 100 Loss 1.5914 Accuracy 0.6058\n",
            "Epoch 20 Batch 150 Loss 1.6305 Accuracy 0.5987\n",
            "Epoch 20 Batch 200 Loss 1.6437 Accuracy 0.5973\n",
            "Epoch 20 Batch 250 Loss 1.6651 Accuracy 0.5933\n",
            "Epoch 20 Batch 300 Loss 1.6797 Accuracy 0.5903\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train/ckpt-4\n",
            "Epoch 20 Loss 1.6803 Accuracy 0.5901\n",
            "Time taken for 1 epoch: 63.95 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.5490 Accuracy 0.6157\n",
            "Epoch 21 Batch 50 Loss 1.4918 Accuracy 0.6238\n",
            "Epoch 21 Batch 100 Loss 1.4969 Accuracy 0.6232\n",
            "Epoch 21 Batch 150 Loss 1.5310 Accuracy 0.6156\n",
            "Epoch 21 Batch 200 Loss 1.5621 Accuracy 0.6102\n",
            "Epoch 21 Batch 250 Loss 1.5830 Accuracy 0.6063\n",
            "Epoch 21 Batch 300 Loss 1.6033 Accuracy 0.6023\n",
            "Epoch 21 Loss 1.6028 Accuracy 0.6024\n",
            "Time taken for 1 epoch: 61.40 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 1.2799 Accuracy 0.6877\n",
            "Epoch 22 Batch 50 Loss 1.4233 Accuracy 0.6395\n",
            "Epoch 22 Batch 100 Loss 1.4539 Accuracy 0.6313\n",
            "Epoch 22 Batch 150 Loss 1.4824 Accuracy 0.6254\n",
            "Epoch 22 Batch 200 Loss 1.5071 Accuracy 0.6208\n",
            "Epoch 22 Batch 250 Loss 1.5249 Accuracy 0.6174\n",
            "Epoch 22 Batch 300 Loss 1.5335 Accuracy 0.6155\n",
            "Epoch 22 Loss 1.5353 Accuracy 0.6152\n",
            "Time taken for 1 epoch: 61.25 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 1.3841 Accuracy 0.6358\n",
            "Epoch 23 Batch 50 Loss 1.3537 Accuracy 0.6505\n",
            "Epoch 23 Batch 100 Loss 1.3696 Accuracy 0.6469\n",
            "Epoch 23 Batch 150 Loss 1.4081 Accuracy 0.6391\n",
            "Epoch 23 Batch 200 Loss 1.4372 Accuracy 0.6327\n",
            "Epoch 23 Batch 250 Loss 1.4593 Accuracy 0.6289\n",
            "Epoch 23 Batch 300 Loss 1.4731 Accuracy 0.6258\n",
            "Epoch 23 Loss 1.4734 Accuracy 0.6258\n",
            "Time taken for 1 epoch: 61.30 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 1.2555 Accuracy 0.6613\n",
            "Epoch 24 Batch 50 Loss 1.2726 Accuracy 0.6684\n",
            "Epoch 24 Batch 100 Loss 1.3048 Accuracy 0.6609\n",
            "Epoch 24 Batch 150 Loss 1.3371 Accuracy 0.6534\n",
            "Epoch 24 Batch 200 Loss 1.3578 Accuracy 0.6492\n",
            "Epoch 24 Batch 250 Loss 1.3813 Accuracy 0.6438\n",
            "Epoch 24 Batch 300 Loss 1.3975 Accuracy 0.6403\n",
            "Epoch 24 Loss 1.4001 Accuracy 0.6399\n",
            "Time taken for 1 epoch: 61.25 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 1.0149 Accuracy 0.7226\n",
            "Epoch 25 Batch 50 Loss 1.2310 Accuracy 0.6758\n",
            "Epoch 25 Batch 100 Loss 1.2530 Accuracy 0.6691\n",
            "Epoch 25 Batch 150 Loss 1.2714 Accuracy 0.6652\n",
            "Epoch 25 Batch 200 Loss 1.2953 Accuracy 0.6610\n",
            "Epoch 25 Batch 250 Loss 1.3134 Accuracy 0.6567\n",
            "Epoch 25 Batch 300 Loss 1.3335 Accuracy 0.6527\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/train/ckpt-5\n",
            "Epoch 25 Loss 1.3339 Accuracy 0.6526\n",
            "Time taken for 1 epoch: 63.80 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 1.1432 Accuracy 0.6779\n",
            "Epoch 26 Batch 50 Loss 1.1304 Accuracy 0.6953\n",
            "Epoch 26 Batch 100 Loss 1.1764 Accuracy 0.6850\n",
            "Epoch 26 Batch 150 Loss 1.2077 Accuracy 0.6779\n",
            "Epoch 26 Batch 200 Loss 1.2372 Accuracy 0.6718\n",
            "Epoch 26 Batch 250 Loss 1.2581 Accuracy 0.6671\n",
            "Epoch 26 Batch 300 Loss 1.2747 Accuracy 0.6634\n",
            "Epoch 26 Loss 1.2775 Accuracy 0.6629\n",
            "Time taken for 1 epoch: 61.35 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 1.1860 Accuracy 0.6783\n",
            "Epoch 27 Batch 50 Loss 1.1141 Accuracy 0.6990\n",
            "Epoch 27 Batch 100 Loss 1.1409 Accuracy 0.6932\n",
            "Epoch 27 Batch 150 Loss 1.1656 Accuracy 0.6876\n",
            "Epoch 27 Batch 200 Loss 1.1790 Accuracy 0.6840\n",
            "Epoch 27 Batch 250 Loss 1.2078 Accuracy 0.6778\n",
            "Epoch 27 Batch 300 Loss 1.2219 Accuracy 0.6744\n",
            "Epoch 27 Loss 1.2229 Accuracy 0.6743\n",
            "Time taken for 1 epoch: 60.96 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 1.1375 Accuracy 0.6833\n",
            "Epoch 28 Batch 50 Loss 1.0594 Accuracy 0.7091\n",
            "Epoch 28 Batch 100 Loss 1.0757 Accuracy 0.7063\n",
            "Epoch 28 Batch 150 Loss 1.0976 Accuracy 0.7013\n",
            "Epoch 28 Batch 200 Loss 1.1217 Accuracy 0.6958\n",
            "Epoch 28 Batch 250 Loss 1.1449 Accuracy 0.6902\n",
            "Epoch 28 Batch 300 Loss 1.1620 Accuracy 0.6867\n",
            "Epoch 28 Loss 1.1636 Accuracy 0.6862\n",
            "Time taken for 1 epoch: 61.29 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.9339 Accuracy 0.7301\n",
            "Epoch 29 Batch 50 Loss 0.9998 Accuracy 0.7253\n",
            "Epoch 29 Batch 100 Loss 1.0324 Accuracy 0.7177\n",
            "Epoch 29 Batch 150 Loss 1.0603 Accuracy 0.7100\n",
            "Epoch 29 Batch 200 Loss 1.0775 Accuracy 0.7053\n",
            "Epoch 29 Batch 250 Loss 1.1003 Accuracy 0.6999\n",
            "Epoch 29 Batch 300 Loss 1.1164 Accuracy 0.6966\n",
            "Epoch 29 Loss 1.1180 Accuracy 0.6962\n",
            "Time taken for 1 epoch: 61.20 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 1.0847 Accuracy 0.7230\n",
            "Epoch 30 Batch 50 Loss 0.9683 Accuracy 0.7322\n",
            "Epoch 30 Batch 100 Loss 0.9781 Accuracy 0.7284\n",
            "Epoch 30 Batch 150 Loss 0.9915 Accuracy 0.7240\n",
            "Epoch 30 Batch 200 Loss 1.0093 Accuracy 0.7199\n",
            "Epoch 30 Batch 250 Loss 1.0296 Accuracy 0.7147\n",
            "Epoch 30 Batch 300 Loss 1.0494 Accuracy 0.7106\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/train/ckpt-6\n",
            "Epoch 30 Loss 1.0506 Accuracy 0.7103\n",
            "Time taken for 1 epoch: 63.92 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.8455 Accuracy 0.7704\n",
            "Epoch 31 Batch 50 Loss 0.9209 Accuracy 0.7455\n",
            "Epoch 31 Batch 100 Loss 0.9366 Accuracy 0.7395\n",
            "Epoch 31 Batch 150 Loss 0.9597 Accuracy 0.7326\n",
            "Epoch 31 Batch 200 Loss 0.9769 Accuracy 0.7279\n",
            "Epoch 31 Batch 250 Loss 0.9928 Accuracy 0.7240\n",
            "Epoch 31 Batch 300 Loss 1.0098 Accuracy 0.7199\n",
            "Epoch 31 Loss 1.0133 Accuracy 0.7191\n",
            "Time taken for 1 epoch: 61.09 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.7471 Accuracy 0.7844\n",
            "Epoch 32 Batch 50 Loss 0.8401 Accuracy 0.7621\n",
            "Epoch 32 Batch 100 Loss 0.8717 Accuracy 0.7533\n",
            "Epoch 32 Batch 150 Loss 0.8972 Accuracy 0.7462\n",
            "Epoch 32 Batch 200 Loss 0.9243 Accuracy 0.7393\n",
            "Epoch 32 Batch 250 Loss 0.9420 Accuracy 0.7351\n",
            "Epoch 32 Batch 300 Loss 0.9611 Accuracy 0.7304\n",
            "Epoch 32 Loss 0.9624 Accuracy 0.7300\n",
            "Time taken for 1 epoch: 60.80 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.7180 Accuracy 0.7856\n",
            "Epoch 33 Batch 50 Loss 0.8152 Accuracy 0.7703\n",
            "Epoch 33 Batch 100 Loss 0.8398 Accuracy 0.7623\n",
            "Epoch 33 Batch 150 Loss 0.8673 Accuracy 0.7539\n",
            "Epoch 33 Batch 200 Loss 0.8850 Accuracy 0.7486\n",
            "Epoch 33 Batch 250 Loss 0.9054 Accuracy 0.7435\n",
            "Epoch 33 Batch 300 Loss 0.9202 Accuracy 0.7396\n",
            "Epoch 33 Loss 0.9219 Accuracy 0.7392\n",
            "Time taken for 1 epoch: 61.41 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.6933 Accuracy 0.8015\n",
            "Epoch 34 Batch 50 Loss 0.7780 Accuracy 0.7750\n",
            "Epoch 34 Batch 100 Loss 0.8015 Accuracy 0.7701\n",
            "Epoch 34 Batch 150 Loss 0.8261 Accuracy 0.7629\n",
            "Epoch 34 Batch 200 Loss 0.8462 Accuracy 0.7579\n",
            "Epoch 34 Batch 250 Loss 0.8670 Accuracy 0.7526\n",
            "Epoch 34 Batch 300 Loss 0.8797 Accuracy 0.7490\n",
            "Epoch 34 Loss 0.8814 Accuracy 0.7486\n",
            "Time taken for 1 epoch: 61.09 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.8197 Accuracy 0.7708\n",
            "Epoch 35 Batch 50 Loss 0.7567 Accuracy 0.7802\n",
            "Epoch 35 Batch 100 Loss 0.7725 Accuracy 0.7769\n",
            "Epoch 35 Batch 150 Loss 0.7933 Accuracy 0.7707\n",
            "Epoch 35 Batch 200 Loss 0.8103 Accuracy 0.7662\n",
            "Epoch 35 Batch 250 Loss 0.8206 Accuracy 0.7638\n",
            "Epoch 35 Batch 300 Loss 0.8358 Accuracy 0.7601\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/train/ckpt-7\n",
            "Epoch 35 Loss 0.8362 Accuracy 0.7600\n",
            "Time taken for 1 epoch: 63.53 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.7000 Accuracy 0.7881\n",
            "Epoch 36 Batch 50 Loss 0.7065 Accuracy 0.7943\n",
            "Epoch 36 Batch 100 Loss 0.7334 Accuracy 0.7864\n",
            "Epoch 36 Batch 150 Loss 0.7503 Accuracy 0.7819\n",
            "Epoch 36 Batch 200 Loss 0.7669 Accuracy 0.7779\n",
            "Epoch 36 Batch 250 Loss 0.7848 Accuracy 0.7731\n",
            "Epoch 36 Batch 300 Loss 0.7980 Accuracy 0.7689\n",
            "Epoch 36 Loss 0.7989 Accuracy 0.7688\n",
            "Time taken for 1 epoch: 61.23 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.7522 Accuracy 0.7680\n",
            "Epoch 37 Batch 50 Loss 0.6700 Accuracy 0.8017\n",
            "Epoch 37 Batch 100 Loss 0.6942 Accuracy 0.7950\n",
            "Epoch 37 Batch 150 Loss 0.7185 Accuracy 0.7885\n",
            "Epoch 37 Batch 200 Loss 0.7361 Accuracy 0.7843\n",
            "Epoch 37 Batch 250 Loss 0.7547 Accuracy 0.7791\n",
            "Epoch 37 Batch 300 Loss 0.7687 Accuracy 0.7749\n",
            "Epoch 37 Loss 0.7692 Accuracy 0.7748\n",
            "Time taken for 1 epoch: 60.95 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.6447 Accuracy 0.8294\n",
            "Epoch 38 Batch 50 Loss 0.6422 Accuracy 0.8118\n",
            "Epoch 38 Batch 100 Loss 0.6700 Accuracy 0.8031\n",
            "Epoch 38 Batch 150 Loss 0.6886 Accuracy 0.7970\n",
            "Epoch 38 Batch 200 Loss 0.7033 Accuracy 0.7928\n",
            "Epoch 38 Batch 250 Loss 0.7166 Accuracy 0.7895\n",
            "Epoch 38 Batch 300 Loss 0.7325 Accuracy 0.7853\n",
            "Epoch 38 Loss 0.7339 Accuracy 0.7849\n",
            "Time taken for 1 epoch: 61.10 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.5724 Accuracy 0.8388\n",
            "Epoch 39 Batch 50 Loss 0.6244 Accuracy 0.8169\n",
            "Epoch 39 Batch 100 Loss 0.6393 Accuracy 0.8111\n",
            "Epoch 39 Batch 150 Loss 0.6573 Accuracy 0.8050\n",
            "Epoch 39 Batch 200 Loss 0.6765 Accuracy 0.8004\n",
            "Epoch 39 Batch 250 Loss 0.6900 Accuracy 0.7967\n",
            "Epoch 39 Batch 300 Loss 0.7027 Accuracy 0.7930\n",
            "Epoch 39 Loss 0.7028 Accuracy 0.7930\n",
            "Time taken for 1 epoch: 61.07 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.5618 Accuracy 0.8364\n",
            "Epoch 40 Batch 50 Loss 0.5897 Accuracy 0.8238\n",
            "Epoch 40 Batch 100 Loss 0.6139 Accuracy 0.8177\n",
            "Epoch 40 Batch 150 Loss 0.6301 Accuracy 0.8127\n",
            "Epoch 40 Batch 200 Loss 0.6396 Accuracy 0.8097\n",
            "Epoch 40 Batch 250 Loss 0.6578 Accuracy 0.8043\n",
            "Epoch 40 Batch 300 Loss 0.6711 Accuracy 0.7998\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/train/ckpt-8\n",
            "Epoch 40 Loss 0.6715 Accuracy 0.7997\n",
            "Time taken for 1 epoch: 64.16 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.6106 Accuracy 0.8066\n",
            "Epoch 41 Batch 50 Loss 0.5555 Accuracy 0.8325\n",
            "Epoch 41 Batch 100 Loss 0.5727 Accuracy 0.8277\n",
            "Epoch 41 Batch 150 Loss 0.5973 Accuracy 0.8208\n",
            "Epoch 41 Batch 200 Loss 0.6142 Accuracy 0.8158\n",
            "Epoch 41 Batch 250 Loss 0.6320 Accuracy 0.8111\n",
            "Epoch 41 Batch 300 Loss 0.6428 Accuracy 0.8078\n",
            "Epoch 41 Loss 0.6443 Accuracy 0.8073\n",
            "Time taken for 1 epoch: 60.90 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.5627 Accuracy 0.8333\n",
            "Epoch 42 Batch 50 Loss 0.5350 Accuracy 0.8403\n",
            "Epoch 42 Batch 100 Loss 0.5566 Accuracy 0.8334\n",
            "Epoch 42 Batch 150 Loss 0.5689 Accuracy 0.8295\n",
            "Epoch 42 Batch 200 Loss 0.5841 Accuracy 0.8245\n",
            "Epoch 42 Batch 250 Loss 0.5992 Accuracy 0.8201\n",
            "Epoch 42 Batch 300 Loss 0.6121 Accuracy 0.8162\n",
            "Epoch 42 Loss 0.6127 Accuracy 0.8159\n",
            "Time taken for 1 epoch: 60.80 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.5269 Accuracy 0.8569\n",
            "Epoch 43 Batch 50 Loss 0.5506 Accuracy 0.8358\n",
            "Epoch 43 Batch 100 Loss 0.5496 Accuracy 0.8341\n",
            "Epoch 43 Batch 150 Loss 0.5582 Accuracy 0.8319\n",
            "Epoch 43 Batch 200 Loss 0.5746 Accuracy 0.8271\n",
            "Epoch 43 Batch 250 Loss 0.5848 Accuracy 0.8236\n",
            "Epoch 43 Batch 300 Loss 0.5996 Accuracy 0.8198\n",
            "Epoch 43 Loss 0.6007 Accuracy 0.8195\n",
            "Time taken for 1 epoch: 60.96 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.5031 Accuracy 0.8554\n",
            "Epoch 44 Batch 50 Loss 0.5114 Accuracy 0.8467\n",
            "Epoch 44 Batch 100 Loss 0.5316 Accuracy 0.8402\n",
            "Epoch 44 Batch 150 Loss 0.5403 Accuracy 0.8366\n",
            "Epoch 44 Batch 200 Loss 0.5475 Accuracy 0.8343\n",
            "Epoch 44 Batch 250 Loss 0.5611 Accuracy 0.8304\n",
            "Epoch 44 Batch 300 Loss 0.5706 Accuracy 0.8274\n",
            "Epoch 44 Loss 0.5727 Accuracy 0.8268\n",
            "Time taken for 1 epoch: 60.70 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.5288 Accuracy 0.8469\n",
            "Epoch 45 Batch 50 Loss 0.5114 Accuracy 0.8453\n",
            "Epoch 45 Batch 100 Loss 0.5171 Accuracy 0.8433\n",
            "Epoch 45 Batch 150 Loss 0.5234 Accuracy 0.8410\n",
            "Epoch 45 Batch 200 Loss 0.5337 Accuracy 0.8379\n",
            "Epoch 45 Batch 250 Loss 0.5460 Accuracy 0.8341\n",
            "Epoch 45 Batch 300 Loss 0.5541 Accuracy 0.8315\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/train/ckpt-9\n",
            "Epoch 45 Loss 0.5545 Accuracy 0.8314\n",
            "Time taken for 1 epoch: 64.13 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.4396 Accuracy 0.8687\n",
            "Epoch 46 Batch 50 Loss 0.4593 Accuracy 0.8616\n",
            "Epoch 46 Batch 100 Loss 0.4759 Accuracy 0.8560\n",
            "Epoch 46 Batch 150 Loss 0.4900 Accuracy 0.8509\n",
            "Epoch 46 Batch 200 Loss 0.5073 Accuracy 0.8452\n",
            "Epoch 46 Batch 250 Loss 0.5181 Accuracy 0.8418\n",
            "Epoch 46 Batch 300 Loss 0.5279 Accuracy 0.8388\n",
            "Epoch 46 Loss 0.5292 Accuracy 0.8385\n",
            "Time taken for 1 epoch: 61.35 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.3991 Accuracy 0.8788\n",
            "Epoch 47 Batch 50 Loss 0.4483 Accuracy 0.8617\n",
            "Epoch 47 Batch 100 Loss 0.4528 Accuracy 0.8603\n",
            "Epoch 47 Batch 150 Loss 0.4709 Accuracy 0.8552\n",
            "Epoch 47 Batch 200 Loss 0.4839 Accuracy 0.8515\n",
            "Epoch 47 Batch 250 Loss 0.4935 Accuracy 0.8488\n",
            "Epoch 47 Batch 300 Loss 0.5047 Accuracy 0.8457\n",
            "Epoch 47 Loss 0.5060 Accuracy 0.8453\n",
            "Time taken for 1 epoch: 60.95 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.4429 Accuracy 0.8746\n",
            "Epoch 48 Batch 50 Loss 0.4364 Accuracy 0.8686\n",
            "Epoch 48 Batch 100 Loss 0.4853 Accuracy 0.8538\n",
            "Epoch 48 Batch 150 Loss 0.4813 Accuracy 0.8538\n",
            "Epoch 48 Batch 200 Loss 0.4788 Accuracy 0.8542\n",
            "Epoch 48 Batch 250 Loss 0.4840 Accuracy 0.8524\n",
            "Epoch 48 Batch 300 Loss 0.4895 Accuracy 0.8505\n",
            "Epoch 48 Loss 0.4910 Accuracy 0.8502\n",
            "Time taken for 1 epoch: 61.30 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.4333 Accuracy 0.8707\n",
            "Epoch 49 Batch 50 Loss 0.4179 Accuracy 0.8727\n",
            "Epoch 49 Batch 100 Loss 0.4154 Accuracy 0.8722\n",
            "Epoch 49 Batch 150 Loss 0.4308 Accuracy 0.8675\n",
            "Epoch 49 Batch 200 Loss 0.4441 Accuracy 0.8631\n",
            "Epoch 49 Batch 250 Loss 0.4554 Accuracy 0.8596\n",
            "Epoch 49 Batch 300 Loss 0.4646 Accuracy 0.8570\n",
            "Epoch 49 Loss 0.4656 Accuracy 0.8567\n",
            "Time taken for 1 epoch: 61.36 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.3947 Accuracy 0.8799\n",
            "Epoch 50 Batch 50 Loss 0.4151 Accuracy 0.8737\n",
            "Epoch 50 Batch 100 Loss 0.4187 Accuracy 0.8716\n",
            "Epoch 50 Batch 150 Loss 0.4323 Accuracy 0.8676\n",
            "Epoch 50 Batch 200 Loss 0.4416 Accuracy 0.8651\n",
            "Epoch 50 Batch 250 Loss 0.4526 Accuracy 0.8619\n",
            "Epoch 50 Batch 300 Loss 0.4632 Accuracy 0.8585\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/train/ckpt-10\n",
            "Epoch 50 Loss 0.4641 Accuracy 0.8583\n",
            "Time taken for 1 epoch: 64.32 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.3332 Accuracy 0.9017\n",
            "Epoch 51 Batch 50 Loss 0.4028 Accuracy 0.8758\n",
            "Epoch 51 Batch 100 Loss 0.4077 Accuracy 0.8746\n",
            "Epoch 51 Batch 150 Loss 0.4165 Accuracy 0.8713\n",
            "Epoch 51 Batch 200 Loss 0.4300 Accuracy 0.8673\n",
            "Epoch 51 Batch 250 Loss 0.4362 Accuracy 0.8654\n",
            "Epoch 51 Batch 300 Loss 0.4460 Accuracy 0.8621\n",
            "Epoch 51 Loss 0.4468 Accuracy 0.8620\n",
            "Time taken for 1 epoch: 61.17 secs\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.3694 Accuracy 0.8969\n",
            "Epoch 52 Batch 50 Loss 0.3863 Accuracy 0.8820\n",
            "Epoch 52 Batch 100 Loss 0.3995 Accuracy 0.8782\n",
            "Epoch 52 Batch 150 Loss 0.4068 Accuracy 0.8752\n",
            "Epoch 52 Batch 200 Loss 0.4151 Accuracy 0.8726\n",
            "Epoch 52 Batch 250 Loss 0.4256 Accuracy 0.8691\n",
            "Epoch 52 Batch 300 Loss 0.4325 Accuracy 0.8670\n",
            "Epoch 52 Loss 0.4332 Accuracy 0.8669\n",
            "Time taken for 1 epoch: 61.12 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.3161 Accuracy 0.9014\n",
            "Epoch 53 Batch 50 Loss 0.3674 Accuracy 0.8876\n",
            "Epoch 53 Batch 100 Loss 0.3760 Accuracy 0.8848\n",
            "Epoch 53 Batch 150 Loss 0.3872 Accuracy 0.8806\n",
            "Epoch 53 Batch 200 Loss 0.3990 Accuracy 0.8769\n",
            "Epoch 53 Batch 250 Loss 0.4061 Accuracy 0.8745\n",
            "Epoch 53 Batch 300 Loss 0.4141 Accuracy 0.8722\n",
            "Epoch 53 Loss 0.4147 Accuracy 0.8720\n",
            "Time taken for 1 epoch: 61.15 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.3110 Accuracy 0.9174\n",
            "Epoch 54 Batch 50 Loss 0.3498 Accuracy 0.8915\n",
            "Epoch 54 Batch 100 Loss 0.3636 Accuracy 0.8879\n",
            "Epoch 54 Batch 150 Loss 0.3743 Accuracy 0.8843\n",
            "Epoch 54 Batch 200 Loss 0.3867 Accuracy 0.8806\n",
            "Epoch 54 Batch 250 Loss 0.3958 Accuracy 0.8779\n",
            "Epoch 54 Batch 300 Loss 0.4034 Accuracy 0.8758\n",
            "Epoch 54 Loss 0.4043 Accuracy 0.8754\n",
            "Time taken for 1 epoch: 61.45 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.3210 Accuracy 0.9026\n",
            "Epoch 55 Batch 50 Loss 0.3470 Accuracy 0.8929\n",
            "Epoch 55 Batch 100 Loss 0.3525 Accuracy 0.8916\n",
            "Epoch 55 Batch 150 Loss 0.3646 Accuracy 0.8873\n",
            "Epoch 55 Batch 200 Loss 0.3785 Accuracy 0.8833\n",
            "Epoch 55 Batch 250 Loss 0.3870 Accuracy 0.8805\n",
            "Epoch 55 Batch 300 Loss 0.3945 Accuracy 0.8783\n",
            "Saving checkpoint for epoch 55 at ./checkpoints/train/ckpt-11\n",
            "Epoch 55 Loss 0.3954 Accuracy 0.8780\n",
            "Time taken for 1 epoch: 63.78 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.3020 Accuracy 0.9083\n",
            "Epoch 56 Batch 50 Loss 0.3415 Accuracy 0.8942\n",
            "Epoch 56 Batch 100 Loss 0.3506 Accuracy 0.8914\n",
            "Epoch 56 Batch 150 Loss 0.3616 Accuracy 0.8879\n",
            "Epoch 56 Batch 200 Loss 0.3727 Accuracy 0.8844\n",
            "Epoch 56 Batch 250 Loss 0.3803 Accuracy 0.8819\n",
            "Epoch 56 Batch 300 Loss 0.3875 Accuracy 0.8797\n",
            "Epoch 56 Loss 0.3884 Accuracy 0.8794\n",
            "Time taken for 1 epoch: 61.21 secs\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.3014 Accuracy 0.9017\n",
            "Epoch 57 Batch 50 Loss 0.3472 Accuracy 0.8915\n",
            "Epoch 57 Batch 100 Loss 0.3548 Accuracy 0.8905\n",
            "Epoch 57 Batch 150 Loss 0.3571 Accuracy 0.8892\n",
            "Epoch 57 Batch 200 Loss 0.3637 Accuracy 0.8873\n",
            "Epoch 57 Batch 250 Loss 0.3674 Accuracy 0.8858\n",
            "Epoch 57 Batch 300 Loss 0.3735 Accuracy 0.8840\n",
            "Epoch 57 Loss 0.3747 Accuracy 0.8836\n",
            "Time taken for 1 epoch: 61.52 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.2713 Accuracy 0.9178\n",
            "Epoch 58 Batch 50 Loss 0.3191 Accuracy 0.9015\n",
            "Epoch 58 Batch 100 Loss 0.3347 Accuracy 0.8958\n",
            "Epoch 58 Batch 150 Loss 0.3442 Accuracy 0.8922\n",
            "Epoch 58 Batch 200 Loss 0.3508 Accuracy 0.8903\n",
            "Epoch 58 Batch 250 Loss 0.3612 Accuracy 0.8871\n",
            "Epoch 58 Batch 300 Loss 0.3648 Accuracy 0.8861\n",
            "Epoch 58 Loss 0.3654 Accuracy 0.8859\n",
            "Time taken for 1 epoch: 61.28 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.2962 Accuracy 0.9100\n",
            "Epoch 59 Batch 50 Loss 0.3216 Accuracy 0.9002\n",
            "Epoch 59 Batch 100 Loss 0.3310 Accuracy 0.8970\n",
            "Epoch 59 Batch 150 Loss 0.3366 Accuracy 0.8953\n",
            "Epoch 59 Batch 200 Loss 0.3473 Accuracy 0.8922\n",
            "Epoch 59 Batch 250 Loss 0.3563 Accuracy 0.8899\n",
            "Epoch 59 Batch 300 Loss 0.3625 Accuracy 0.8877\n",
            "Epoch 59 Loss 0.3636 Accuracy 0.8874\n",
            "Time taken for 1 epoch: 61.27 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.3039 Accuracy 0.9033\n",
            "Epoch 60 Batch 50 Loss 0.2913 Accuracy 0.9105\n",
            "Epoch 60 Batch 100 Loss 0.3083 Accuracy 0.9045\n",
            "Epoch 60 Batch 150 Loss 0.3183 Accuracy 0.9012\n",
            "Epoch 60 Batch 200 Loss 0.3295 Accuracy 0.8975\n",
            "Epoch 60 Batch 250 Loss 0.3387 Accuracy 0.8947\n",
            "Epoch 60 Batch 300 Loss 0.3454 Accuracy 0.8920\n",
            "Saving checkpoint for epoch 60 at ./checkpoints/train/ckpt-12\n",
            "Epoch 60 Loss 0.3462 Accuracy 0.8918\n",
            "Time taken for 1 epoch: 63.87 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.3512 Accuracy 0.8898\n",
            "Epoch 61 Batch 50 Loss 0.3106 Accuracy 0.9045\n",
            "Epoch 61 Batch 100 Loss 0.3098 Accuracy 0.9037\n",
            "Epoch 61 Batch 150 Loss 0.3151 Accuracy 0.9022\n",
            "Epoch 61 Batch 200 Loss 0.3238 Accuracy 0.8993\n",
            "Epoch 61 Batch 250 Loss 0.3314 Accuracy 0.8969\n",
            "Epoch 61 Batch 300 Loss 0.3388 Accuracy 0.8944\n",
            "Epoch 61 Loss 0.3391 Accuracy 0.8943\n",
            "Time taken for 1 epoch: 60.81 secs\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.2776 Accuracy 0.9111\n",
            "Epoch 62 Batch 50 Loss 0.3034 Accuracy 0.9063\n",
            "Epoch 62 Batch 100 Loss 0.3076 Accuracy 0.9049\n",
            "Epoch 62 Batch 150 Loss 0.3181 Accuracy 0.9018\n",
            "Epoch 62 Batch 200 Loss 0.3215 Accuracy 0.9005\n",
            "Epoch 62 Batch 250 Loss 0.3263 Accuracy 0.8987\n",
            "Epoch 62 Batch 300 Loss 0.3317 Accuracy 0.8969\n",
            "Epoch 62 Loss 0.3325 Accuracy 0.8966\n",
            "Time taken for 1 epoch: 61.10 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.2589 Accuracy 0.9233\n",
            "Epoch 63 Batch 50 Loss 0.2886 Accuracy 0.9103\n",
            "Epoch 63 Batch 100 Loss 0.2898 Accuracy 0.9095\n",
            "Epoch 63 Batch 150 Loss 0.2983 Accuracy 0.9073\n",
            "Epoch 63 Batch 200 Loss 0.3084 Accuracy 0.9037\n",
            "Epoch 63 Batch 250 Loss 0.3155 Accuracy 0.9009\n",
            "Epoch 63 Batch 300 Loss 0.3208 Accuracy 0.8990\n",
            "Epoch 63 Loss 0.3215 Accuracy 0.8987\n",
            "Time taken for 1 epoch: 61.27 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.2946 Accuracy 0.9064\n",
            "Epoch 64 Batch 50 Loss 0.2793 Accuracy 0.9138\n",
            "Epoch 64 Batch 100 Loss 0.2832 Accuracy 0.9123\n",
            "Epoch 64 Batch 150 Loss 0.2918 Accuracy 0.9089\n",
            "Epoch 64 Batch 200 Loss 0.2991 Accuracy 0.9066\n",
            "Epoch 64 Batch 250 Loss 0.3051 Accuracy 0.9045\n",
            "Epoch 64 Batch 300 Loss 0.3098 Accuracy 0.9034\n",
            "Epoch 64 Loss 0.3102 Accuracy 0.9035\n",
            "Time taken for 1 epoch: 61.41 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.2724 Accuracy 0.9225\n",
            "Epoch 65 Batch 50 Loss 0.2655 Accuracy 0.9179\n",
            "Epoch 65 Batch 100 Loss 0.2750 Accuracy 0.9150\n",
            "Epoch 65 Batch 150 Loss 0.2840 Accuracy 0.9124\n",
            "Epoch 65 Batch 200 Loss 0.2914 Accuracy 0.9097\n",
            "Epoch 65 Batch 250 Loss 0.2946 Accuracy 0.9083\n",
            "Epoch 65 Batch 300 Loss 0.3003 Accuracy 0.9065\n",
            "Saving checkpoint for epoch 65 at ./checkpoints/train/ckpt-13\n",
            "Epoch 65 Loss 0.3005 Accuracy 0.9064\n",
            "Time taken for 1 epoch: 63.73 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.2705 Accuracy 0.9134\n",
            "Epoch 66 Batch 50 Loss 0.2589 Accuracy 0.9177\n",
            "Epoch 66 Batch 100 Loss 0.2689 Accuracy 0.9145\n",
            "Epoch 66 Batch 150 Loss 0.2777 Accuracy 0.9120\n",
            "Epoch 66 Batch 200 Loss 0.2859 Accuracy 0.9096\n",
            "Epoch 66 Batch 250 Loss 0.2909 Accuracy 0.9081\n",
            "Epoch 66 Batch 300 Loss 0.2954 Accuracy 0.9067\n",
            "Epoch 66 Loss 0.2958 Accuracy 0.9066\n",
            "Time taken for 1 epoch: 61.10 secs\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.2016 Accuracy 0.9322\n",
            "Epoch 67 Batch 50 Loss 0.2585 Accuracy 0.9200\n",
            "Epoch 67 Batch 100 Loss 0.2639 Accuracy 0.9181\n",
            "Epoch 67 Batch 150 Loss 0.2700 Accuracy 0.9153\n",
            "Epoch 67 Batch 200 Loss 0.2754 Accuracy 0.9138\n",
            "Epoch 67 Batch 250 Loss 0.2818 Accuracy 0.9115\n",
            "Epoch 67 Batch 300 Loss 0.2866 Accuracy 0.9100\n",
            "Epoch 67 Loss 0.2869 Accuracy 0.9099\n",
            "Time taken for 1 epoch: 60.78 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.2205 Accuracy 0.9262\n",
            "Epoch 68 Batch 50 Loss 0.2458 Accuracy 0.9230\n",
            "Epoch 68 Batch 100 Loss 0.2501 Accuracy 0.9215\n",
            "Epoch 68 Batch 150 Loss 0.2584 Accuracy 0.9192\n",
            "Epoch 68 Batch 200 Loss 0.2657 Accuracy 0.9164\n",
            "Epoch 68 Batch 250 Loss 0.2715 Accuracy 0.9149\n",
            "Epoch 68 Batch 300 Loss 0.2773 Accuracy 0.9129\n",
            "Epoch 68 Loss 0.2778 Accuracy 0.9129\n",
            "Time taken for 1 epoch: 60.95 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.2969 Accuracy 0.9053\n",
            "Epoch 69 Batch 50 Loss 0.2474 Accuracy 0.9228\n",
            "Epoch 69 Batch 100 Loss 0.2503 Accuracy 0.9224\n",
            "Epoch 69 Batch 150 Loss 0.2572 Accuracy 0.9200\n",
            "Epoch 69 Batch 200 Loss 0.2658 Accuracy 0.9172\n",
            "Epoch 69 Batch 250 Loss 0.2703 Accuracy 0.9154\n",
            "Epoch 69 Batch 300 Loss 0.2759 Accuracy 0.9138\n",
            "Epoch 69 Loss 0.2761 Accuracy 0.9137\n",
            "Time taken for 1 epoch: 60.79 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.2832 Accuracy 0.9248\n",
            "Epoch 70 Batch 50 Loss 0.2341 Accuracy 0.9280\n",
            "Epoch 70 Batch 100 Loss 0.2378 Accuracy 0.9263\n",
            "Epoch 70 Batch 150 Loss 0.2465 Accuracy 0.9236\n",
            "Epoch 70 Batch 200 Loss 0.2542 Accuracy 0.9208\n",
            "Epoch 70 Batch 250 Loss 0.2607 Accuracy 0.9190\n",
            "Epoch 70 Batch 300 Loss 0.2654 Accuracy 0.9173\n",
            "Saving checkpoint for epoch 70 at ./checkpoints/train/ckpt-14\n",
            "Epoch 70 Loss 0.2658 Accuracy 0.9172\n",
            "Time taken for 1 epoch: 64.11 secs\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.2715 Accuracy 0.9168\n",
            "Epoch 71 Batch 50 Loss 0.2356 Accuracy 0.9264\n",
            "Epoch 71 Batch 100 Loss 0.2425 Accuracy 0.9247\n",
            "Epoch 71 Batch 150 Loss 0.2480 Accuracy 0.9227\n",
            "Epoch 71 Batch 200 Loss 0.2522 Accuracy 0.9216\n",
            "Epoch 71 Batch 250 Loss 0.2570 Accuracy 0.9198\n",
            "Epoch 71 Batch 300 Loss 0.2599 Accuracy 0.9190\n",
            "Epoch 71 Loss 0.2604 Accuracy 0.9188\n",
            "Time taken for 1 epoch: 61.94 secs\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.2246 Accuracy 0.9339\n",
            "Epoch 72 Batch 50 Loss 0.2207 Accuracy 0.9304\n",
            "Epoch 72 Batch 100 Loss 0.2339 Accuracy 0.9268\n",
            "Epoch 72 Batch 150 Loss 0.2418 Accuracy 0.9240\n",
            "Epoch 72 Batch 200 Loss 0.2480 Accuracy 0.9220\n",
            "Epoch 72 Batch 250 Loss 0.2518 Accuracy 0.9209\n",
            "Epoch 72 Batch 300 Loss 0.2564 Accuracy 0.9195\n",
            "Epoch 72 Loss 0.2568 Accuracy 0.9194\n",
            "Time taken for 1 epoch: 61.05 secs\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.1789 Accuracy 0.9392\n",
            "Epoch 73 Batch 50 Loss 0.2231 Accuracy 0.9299\n",
            "Epoch 73 Batch 100 Loss 0.2279 Accuracy 0.9280\n",
            "Epoch 73 Batch 150 Loss 0.2361 Accuracy 0.9261\n",
            "Epoch 73 Batch 200 Loss 0.2415 Accuracy 0.9241\n",
            "Epoch 73 Batch 250 Loss 0.2467 Accuracy 0.9225\n",
            "Epoch 73 Batch 300 Loss 0.2498 Accuracy 0.9215\n",
            "Epoch 73 Loss 0.2501 Accuracy 0.9215\n",
            "Time taken for 1 epoch: 60.77 secs\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.2611 Accuracy 0.9195\n",
            "Epoch 74 Batch 50 Loss 0.2370 Accuracy 0.9270\n",
            "Epoch 74 Batch 100 Loss 0.2354 Accuracy 0.9273\n",
            "Epoch 74 Batch 150 Loss 0.2385 Accuracy 0.9262\n",
            "Epoch 74 Batch 200 Loss 0.2424 Accuracy 0.9248\n",
            "Epoch 74 Batch 250 Loss 0.2451 Accuracy 0.9243\n",
            "Epoch 74 Batch 300 Loss 0.2489 Accuracy 0.9228\n",
            "Epoch 74 Loss 0.2488 Accuracy 0.9228\n",
            "Time taken for 1 epoch: 60.85 secs\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.2262 Accuracy 0.9375\n",
            "Epoch 75 Batch 50 Loss 0.2144 Accuracy 0.9330\n",
            "Epoch 75 Batch 100 Loss 0.2239 Accuracy 0.9300\n",
            "Epoch 75 Batch 150 Loss 0.2299 Accuracy 0.9285\n",
            "Epoch 75 Batch 200 Loss 0.2330 Accuracy 0.9274\n",
            "Epoch 75 Batch 250 Loss 0.2389 Accuracy 0.9257\n",
            "Epoch 75 Batch 300 Loss 0.2418 Accuracy 0.9248\n",
            "Saving checkpoint for epoch 75 at ./checkpoints/train/ckpt-15\n",
            "Epoch 75 Loss 0.2418 Accuracy 0.9249\n",
            "Time taken for 1 epoch: 63.88 secs\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.1766 Accuracy 0.9478\n",
            "Epoch 76 Batch 50 Loss 0.2097 Accuracy 0.9353\n",
            "Epoch 76 Batch 100 Loss 0.2172 Accuracy 0.9330\n",
            "Epoch 76 Batch 150 Loss 0.2221 Accuracy 0.9313\n",
            "Epoch 76 Batch 200 Loss 0.2274 Accuracy 0.9296\n",
            "Epoch 76 Batch 250 Loss 0.2307 Accuracy 0.9282\n",
            "Epoch 76 Batch 300 Loss 0.2356 Accuracy 0.9266\n",
            "Epoch 76 Loss 0.2360 Accuracy 0.9265\n",
            "Time taken for 1 epoch: 61.20 secs\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.1647 Accuracy 0.9361\n",
            "Epoch 77 Batch 50 Loss 0.2096 Accuracy 0.9340\n",
            "Epoch 77 Batch 100 Loss 0.2140 Accuracy 0.9333\n",
            "Epoch 77 Batch 150 Loss 0.2156 Accuracy 0.9322\n",
            "Epoch 77 Batch 200 Loss 0.2224 Accuracy 0.9303\n",
            "Epoch 77 Batch 250 Loss 0.2278 Accuracy 0.9284\n",
            "Epoch 77 Batch 300 Loss 0.2340 Accuracy 0.9265\n",
            "Epoch 77 Loss 0.2345 Accuracy 0.9264\n",
            "Time taken for 1 epoch: 60.63 secs\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.2211 Accuracy 0.9358\n",
            "Epoch 78 Batch 50 Loss 0.2101 Accuracy 0.9348\n",
            "Epoch 78 Batch 100 Loss 0.2085 Accuracy 0.9348\n",
            "Epoch 78 Batch 150 Loss 0.2131 Accuracy 0.9331\n",
            "Epoch 78 Batch 200 Loss 0.2167 Accuracy 0.9320\n",
            "Epoch 78 Batch 250 Loss 0.2205 Accuracy 0.9309\n",
            "Epoch 78 Batch 300 Loss 0.2233 Accuracy 0.9298\n",
            "Epoch 78 Loss 0.2236 Accuracy 0.9296\n",
            "Time taken for 1 epoch: 61.15 secs\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.2068 Accuracy 0.9469\n",
            "Epoch 79 Batch 50 Loss 0.2056 Accuracy 0.9363\n",
            "Epoch 79 Batch 100 Loss 0.2069 Accuracy 0.9353\n",
            "Epoch 79 Batch 150 Loss 0.2138 Accuracy 0.9336\n",
            "Epoch 79 Batch 200 Loss 0.2167 Accuracy 0.9328\n",
            "Epoch 79 Batch 250 Loss 0.2175 Accuracy 0.9322\n",
            "Epoch 79 Batch 300 Loss 0.2200 Accuracy 0.9313\n",
            "Epoch 79 Loss 0.2206 Accuracy 0.9312\n",
            "Time taken for 1 epoch: 61.15 secs\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.1908 Accuracy 0.9335\n",
            "Epoch 80 Batch 50 Loss 0.1971 Accuracy 0.9394\n",
            "Epoch 80 Batch 100 Loss 0.2067 Accuracy 0.9355\n",
            "Epoch 80 Batch 150 Loss 0.2105 Accuracy 0.9346\n",
            "Epoch 80 Batch 200 Loss 0.2156 Accuracy 0.9330\n",
            "Epoch 80 Batch 250 Loss 0.2180 Accuracy 0.9323\n",
            "Epoch 80 Batch 300 Loss 0.2213 Accuracy 0.9312\n",
            "Saving checkpoint for epoch 80 at ./checkpoints/train/ckpt-16\n",
            "Epoch 80 Loss 0.2213 Accuracy 0.9312\n",
            "Time taken for 1 epoch: 64.06 secs\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.1893 Accuracy 0.9446\n",
            "Epoch 81 Batch 50 Loss 0.2009 Accuracy 0.9385\n",
            "Epoch 81 Batch 100 Loss 0.2015 Accuracy 0.9381\n",
            "Epoch 81 Batch 150 Loss 0.2037 Accuracy 0.9374\n",
            "Epoch 81 Batch 200 Loss 0.2084 Accuracy 0.9358\n",
            "Epoch 81 Batch 250 Loss 0.2119 Accuracy 0.9343\n",
            "Epoch 81 Batch 300 Loss 0.2149 Accuracy 0.9331\n",
            "Epoch 81 Loss 0.2153 Accuracy 0.9330\n",
            "Time taken for 1 epoch: 61.17 secs\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.1434 Accuracy 0.9550\n",
            "Epoch 82 Batch 50 Loss 0.1955 Accuracy 0.9403\n",
            "Epoch 82 Batch 100 Loss 0.1952 Accuracy 0.9396\n",
            "Epoch 82 Batch 150 Loss 0.1980 Accuracy 0.9384\n",
            "Epoch 82 Batch 200 Loss 0.2025 Accuracy 0.9368\n",
            "Epoch 82 Batch 250 Loss 0.2054 Accuracy 0.9363\n",
            "Epoch 82 Batch 300 Loss 0.2098 Accuracy 0.9347\n",
            "Epoch 82 Loss 0.2099 Accuracy 0.9346\n",
            "Time taken for 1 epoch: 60.87 secs\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.2085 Accuracy 0.9328\n",
            "Epoch 83 Batch 50 Loss 0.2025 Accuracy 0.9375\n",
            "Epoch 83 Batch 100 Loss 0.1991 Accuracy 0.9380\n",
            "Epoch 83 Batch 150 Loss 0.1998 Accuracy 0.9379\n",
            "Epoch 83 Batch 200 Loss 0.2023 Accuracy 0.9370\n",
            "Epoch 83 Batch 250 Loss 0.2055 Accuracy 0.9362\n",
            "Epoch 83 Batch 300 Loss 0.2083 Accuracy 0.9351\n",
            "Epoch 83 Loss 0.2087 Accuracy 0.9349\n",
            "Time taken for 1 epoch: 61.44 secs\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.1386 Accuracy 0.9487\n",
            "Epoch 84 Batch 50 Loss 0.1819 Accuracy 0.9438\n",
            "Epoch 84 Batch 100 Loss 0.1898 Accuracy 0.9408\n",
            "Epoch 84 Batch 150 Loss 0.1966 Accuracy 0.9387\n",
            "Epoch 84 Batch 200 Loss 0.2006 Accuracy 0.9376\n",
            "Epoch 84 Batch 250 Loss 0.2044 Accuracy 0.9365\n",
            "Epoch 84 Batch 300 Loss 0.2078 Accuracy 0.9353\n",
            "Epoch 84 Loss 0.2084 Accuracy 0.9352\n",
            "Time taken for 1 epoch: 61.37 secs\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.1845 Accuracy 0.9432\n",
            "Epoch 85 Batch 50 Loss 0.1783 Accuracy 0.9435\n",
            "Epoch 85 Batch 100 Loss 0.1782 Accuracy 0.9440\n",
            "Epoch 85 Batch 150 Loss 0.1856 Accuracy 0.9417\n",
            "Epoch 85 Batch 200 Loss 0.1893 Accuracy 0.9408\n",
            "Epoch 85 Batch 250 Loss 0.1945 Accuracy 0.9393\n",
            "Epoch 85 Batch 300 Loss 0.1968 Accuracy 0.9386\n",
            "Saving checkpoint for epoch 85 at ./checkpoints/train/ckpt-17\n",
            "Epoch 85 Loss 0.1971 Accuracy 0.9385\n",
            "Time taken for 1 epoch: 64.24 secs\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.1367 Accuracy 0.9580\n",
            "Epoch 86 Batch 50 Loss 0.1826 Accuracy 0.9439\n",
            "Epoch 86 Batch 100 Loss 0.1784 Accuracy 0.9443\n",
            "Epoch 86 Batch 150 Loss 0.1847 Accuracy 0.9427\n",
            "Epoch 86 Batch 200 Loss 0.1895 Accuracy 0.9414\n",
            "Epoch 86 Batch 250 Loss 0.1959 Accuracy 0.9393\n",
            "Epoch 86 Batch 300 Loss 0.1990 Accuracy 0.9383\n",
            "Epoch 86 Loss 0.1992 Accuracy 0.9382\n",
            "Time taken for 1 epoch: 60.89 secs\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.1909 Accuracy 0.9406\n",
            "Epoch 87 Batch 50 Loss 0.1731 Accuracy 0.9474\n",
            "Epoch 87 Batch 100 Loss 0.1719 Accuracy 0.9471\n",
            "Epoch 87 Batch 150 Loss 0.1797 Accuracy 0.9443\n",
            "Epoch 87 Batch 200 Loss 0.1842 Accuracy 0.9432\n",
            "Epoch 87 Batch 250 Loss 0.1883 Accuracy 0.9416\n",
            "Epoch 87 Batch 300 Loss 0.1917 Accuracy 0.9406\n",
            "Epoch 87 Loss 0.1918 Accuracy 0.9406\n",
            "Time taken for 1 epoch: 60.72 secs\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.1360 Accuracy 0.9611\n",
            "Epoch 88 Batch 50 Loss 0.1755 Accuracy 0.9445\n",
            "Epoch 88 Batch 100 Loss 0.1762 Accuracy 0.9453\n",
            "Epoch 88 Batch 150 Loss 0.1794 Accuracy 0.9444\n",
            "Epoch 88 Batch 200 Loss 0.1811 Accuracy 0.9436\n",
            "Epoch 88 Batch 250 Loss 0.1858 Accuracy 0.9421\n",
            "Epoch 88 Batch 300 Loss 0.1898 Accuracy 0.9410\n",
            "Epoch 88 Loss 0.1901 Accuracy 0.9409\n",
            "Time taken for 1 epoch: 60.78 secs\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.2182 Accuracy 0.9364\n",
            "Epoch 89 Batch 50 Loss 0.1754 Accuracy 0.9454\n",
            "Epoch 89 Batch 100 Loss 0.1741 Accuracy 0.9459\n",
            "Epoch 89 Batch 150 Loss 0.1781 Accuracy 0.9444\n",
            "Epoch 89 Batch 200 Loss 0.1831 Accuracy 0.9434\n",
            "Epoch 89 Batch 250 Loss 0.1860 Accuracy 0.9423\n",
            "Epoch 89 Batch 300 Loss 0.1897 Accuracy 0.9411\n",
            "Epoch 89 Loss 0.1902 Accuracy 0.9411\n",
            "Time taken for 1 epoch: 61.48 secs\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.1666 Accuracy 0.9542\n",
            "Epoch 90 Batch 50 Loss 0.1604 Accuracy 0.9505\n",
            "Epoch 90 Batch 100 Loss 0.1679 Accuracy 0.9474\n",
            "Epoch 90 Batch 150 Loss 0.1726 Accuracy 0.9462\n",
            "Epoch 90 Batch 200 Loss 0.1766 Accuracy 0.9450\n",
            "Epoch 90 Batch 250 Loss 0.1792 Accuracy 0.9441\n",
            "Epoch 90 Batch 300 Loss 0.1800 Accuracy 0.9440\n",
            "Saving checkpoint for epoch 90 at ./checkpoints/train/ckpt-18\n",
            "Epoch 90 Loss 0.1802 Accuracy 0.9440\n",
            "Time taken for 1 epoch: 63.59 secs\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.2928 Accuracy 0.9209\n",
            "Epoch 91 Batch 50 Loss 0.1681 Accuracy 0.9484\n",
            "Epoch 91 Batch 100 Loss 0.1728 Accuracy 0.9475\n",
            "Epoch 91 Batch 150 Loss 0.1750 Accuracy 0.9462\n",
            "Epoch 91 Batch 200 Loss 0.1785 Accuracy 0.9448\n",
            "Epoch 91 Batch 250 Loss 0.1800 Accuracy 0.9446\n",
            "Epoch 91 Batch 300 Loss 0.1806 Accuracy 0.9444\n",
            "Epoch 91 Loss 0.1809 Accuracy 0.9442\n",
            "Time taken for 1 epoch: 61.42 secs\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.1345 Accuracy 0.9477\n",
            "Epoch 92 Batch 50 Loss 0.1622 Accuracy 0.9487\n",
            "Epoch 92 Batch 100 Loss 0.1637 Accuracy 0.9484\n",
            "Epoch 92 Batch 150 Loss 0.1665 Accuracy 0.9480\n",
            "Epoch 92 Batch 200 Loss 0.1703 Accuracy 0.9468\n",
            "Epoch 92 Batch 250 Loss 0.1751 Accuracy 0.9452\n",
            "Epoch 92 Batch 300 Loss 0.1777 Accuracy 0.9444\n",
            "Epoch 92 Loss 0.1779 Accuracy 0.9444\n",
            "Time taken for 1 epoch: 60.86 secs\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.1396 Accuracy 0.9567\n",
            "Epoch 93 Batch 50 Loss 0.1585 Accuracy 0.9511\n",
            "Epoch 93 Batch 100 Loss 0.1627 Accuracy 0.9499\n",
            "Epoch 93 Batch 150 Loss 0.1667 Accuracy 0.9488\n",
            "Epoch 93 Batch 200 Loss 0.1700 Accuracy 0.9480\n",
            "Epoch 93 Batch 250 Loss 0.1716 Accuracy 0.9472\n",
            "Epoch 93 Batch 300 Loss 0.1757 Accuracy 0.9456\n",
            "Epoch 93 Loss 0.1760 Accuracy 0.9454\n",
            "Time taken for 1 epoch: 60.61 secs\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.1410 Accuracy 0.9456\n",
            "Epoch 94 Batch 50 Loss 0.1557 Accuracy 0.9524\n",
            "Epoch 94 Batch 100 Loss 0.1574 Accuracy 0.9517\n",
            "Epoch 94 Batch 150 Loss 0.1643 Accuracy 0.9494\n",
            "Epoch 94 Batch 200 Loss 0.1686 Accuracy 0.9478\n",
            "Epoch 94 Batch 250 Loss 0.1714 Accuracy 0.9469\n",
            "Epoch 94 Batch 300 Loss 0.1727 Accuracy 0.9465\n",
            "Epoch 94 Loss 0.1732 Accuracy 0.9463\n",
            "Time taken for 1 epoch: 60.67 secs\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.1146 Accuracy 0.9623\n",
            "Epoch 95 Batch 50 Loss 0.1593 Accuracy 0.9505\n",
            "Epoch 95 Batch 100 Loss 0.1599 Accuracy 0.9509\n",
            "Epoch 95 Batch 150 Loss 0.1603 Accuracy 0.9505\n",
            "Epoch 95 Batch 200 Loss 0.1628 Accuracy 0.9497\n",
            "Epoch 95 Batch 250 Loss 0.1656 Accuracy 0.9486\n",
            "Epoch 95 Batch 300 Loss 0.1687 Accuracy 0.9480\n",
            "Saving checkpoint for epoch 95 at ./checkpoints/train/ckpt-19\n",
            "Epoch 95 Loss 0.1686 Accuracy 0.9480\n",
            "Time taken for 1 epoch: 64.35 secs\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.1247 Accuracy 0.9531\n",
            "Epoch 96 Batch 50 Loss 0.1425 Accuracy 0.9556\n",
            "Epoch 96 Batch 100 Loss 0.1535 Accuracy 0.9528\n",
            "Epoch 96 Batch 150 Loss 0.1580 Accuracy 0.9518\n",
            "Epoch 96 Batch 200 Loss 0.1613 Accuracy 0.9507\n",
            "Epoch 96 Batch 250 Loss 0.1640 Accuracy 0.9495\n",
            "Epoch 96 Batch 300 Loss 0.1664 Accuracy 0.9487\n",
            "Epoch 96 Loss 0.1666 Accuracy 0.9486\n",
            "Time taken for 1 epoch: 61.36 secs\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.1034 Accuracy 0.9682\n",
            "Epoch 97 Batch 50 Loss 0.1516 Accuracy 0.9528\n",
            "Epoch 97 Batch 100 Loss 0.1562 Accuracy 0.9510\n",
            "Epoch 97 Batch 150 Loss 0.1595 Accuracy 0.9499\n",
            "Epoch 97 Batch 200 Loss 0.1634 Accuracy 0.9491\n",
            "Epoch 97 Batch 250 Loss 0.1656 Accuracy 0.9483\n",
            "Epoch 97 Batch 300 Loss 0.1678 Accuracy 0.9478\n",
            "Epoch 97 Loss 0.1683 Accuracy 0.9476\n",
            "Time taken for 1 epoch: 61.19 secs\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.1528 Accuracy 0.9582\n",
            "Epoch 98 Batch 50 Loss 0.1473 Accuracy 0.9543\n",
            "Epoch 98 Batch 100 Loss 0.1494 Accuracy 0.9535\n",
            "Epoch 98 Batch 150 Loss 0.1524 Accuracy 0.9523\n",
            "Epoch 98 Batch 200 Loss 0.1545 Accuracy 0.9516\n",
            "Epoch 98 Batch 250 Loss 0.1575 Accuracy 0.9509\n",
            "Epoch 98 Batch 300 Loss 0.1615 Accuracy 0.9496\n",
            "Epoch 98 Loss 0.1620 Accuracy 0.9495\n",
            "Time taken for 1 epoch: 60.66 secs\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.1039 Accuracy 0.9649\n",
            "Epoch 99 Batch 50 Loss 0.1454 Accuracy 0.9548\n",
            "Epoch 99 Batch 100 Loss 0.1496 Accuracy 0.9532\n",
            "Epoch 99 Batch 150 Loss 0.1528 Accuracy 0.9525\n",
            "Epoch 99 Batch 200 Loss 0.1561 Accuracy 0.9516\n",
            "Epoch 99 Batch 250 Loss 0.1588 Accuracy 0.9508\n",
            "Epoch 99 Batch 300 Loss 0.1612 Accuracy 0.9500\n",
            "Epoch 99 Loss 0.1614 Accuracy 0.9500\n",
            "Time taken for 1 epoch: 60.64 secs\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.1117 Accuracy 0.9668\n",
            "Epoch 100 Batch 50 Loss 0.1438 Accuracy 0.9560\n",
            "Epoch 100 Batch 100 Loss 0.1468 Accuracy 0.9550\n",
            "Epoch 100 Batch 150 Loss 0.1490 Accuracy 0.9541\n",
            "Epoch 100 Batch 200 Loss 0.1515 Accuracy 0.9533\n",
            "Epoch 100 Batch 250 Loss 0.1528 Accuracy 0.9526\n",
            "Epoch 100 Batch 300 Loss 0.1562 Accuracy 0.9514\n",
            "Saving checkpoint for epoch 100 at ./checkpoints/train/ckpt-20\n",
            "Epoch 100 Loss 0.1564 Accuracy 0.9512\n",
            "Time taken for 1 epoch: 63.83 secs\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run training! Each epoch takes several mins with GPU\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "\n",
        "  # inp -> twi, tar -> french\n",
        "  for (batch, (inp,tar)) in enumerate(train_batches):\n",
        "    train_step(inp, tar)\n",
        "\n",
        "    if batch % 50 == 0:\n",
        "      print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "  print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "  print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build a Translator\n"
      ],
      "metadata": {
        "id": "Aij6BWpIBU52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Translator(tf.Module):\n",
        "    def __init__(self, tokenizers, transformer):\n",
        "        self.tokenizers = tokenizers\n",
        "        self.transformer = transformer\n",
        "\n",
        "    def __call__(self, sentence, max_length=MAX_TOKENS):\n",
        "        # The input sentence is French, hence adding the `[START]` and `[END]` tokens.\n",
        "        sentence = tf.convert_to_tensor([sentence])\n",
        "        sentence = self.tokenizers.fr.tokenize(sentence).to_tensor()\n",
        "\n",
        "        encoder_input = sentence\n",
        "\n",
        "        # As the output language is Twi, initialize the output with the\n",
        "        # English `[START]` token.\n",
        "        start_end = self.tokenizers.twi.tokenize([''])[0]\n",
        "        start = start_end[0][tf.newaxis]\n",
        "        end = start_end[1][tf.newaxis]\n",
        "\n",
        "        # `tf.TensorArray` is required here (instead of a Python list), so that the\n",
        "        # dynamic-loop can be traced by `tf.function`.\n",
        "        output_array = tf.TensorArray(\n",
        "            dtype=tf.int64, size=0, dynamic_size=True)\n",
        "        output_array = output_array.write(0, start)\n",
        "        output = tf.transpose(output_array.stack())\n",
        "\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "                encoder_input, output)\n",
        "\n",
        "        for i in tf.range(max_length):\n",
        "            output = tf.transpose(output_array.stack())\n",
        "\n",
        "            enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "                encoder_input, output)\n",
        "\n",
        "            predictions, attention_weights = self.transformer(\n",
        "                encoder_input, output, False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "\n",
        "            # Select the last token from the `seq_len` dimension.\n",
        "            # Shape `(batch_size, 1, vocab_size)`.\n",
        "            predictions = predictions[:, -1:, :]\n",
        "\n",
        "            predicted_id = tf.argmax(predictions, axis=-1)\n",
        "\n",
        "            # Concatenate the `predicted_id` to the output which is given to the\n",
        "            # decoder as its input.\n",
        "            output_array = output_array.write(i+1, predicted_id[0])\n",
        "\n",
        "            if predicted_id == end:\n",
        "                break\n",
        "\n",
        "        output = tf.transpose(output_array.stack())\n",
        "        # The output shape is `(1, tokens)`.\n",
        "        text = self.tokenizers.twi.detokenize(output)[0]  # Shape: `()`.\n",
        "\n",
        "        tokens = self.tokenizers.twi.lookup(output)[0]\n",
        "        _, attention_weights = self.transformer(\n",
        "            encoder_input, output[:, :-1], False, enc_padding_mask, combined_mask, dec_padding_mask)\n",
        "\n",
        "        return text, tokens, attention_weights"
      ],
      "metadata": {
        "id": "z0clY8WkaGZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tt1OVm3ecSNO"
      },
      "outputs": [],
      "source": [
        "#Create an instance of this Translator class\n",
        "translator = Translator(tokenizers, transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## translate example sentecnes"
      ],
      "metadata": {
        "id": "3dMj9N3fBh1P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysGcUmAzc_Yi"
      },
      "outputs": [],
      "source": [
        "def print_translation(sentence, tokens, ground_truth):\n",
        "  print(f'{\"Input:\":15s}: {sentence}')\n",
        "  print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n",
        "  print(f'{\"Ground truth\":15s}: {ground_truth}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKmyWMcGduft",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a7b25a2-1ec5-4deb-b196-1e5082a463b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : Asamoah a attaché la ficelle au cerf-volant.\n",
            "Prediction     : asamoah de hama no bɔɔ asangoli no mu .\n",
            "Ground truth   : Asamoah de hama no bɔɔ asangoli no mu.\n"
          ]
        }
      ],
      "source": [
        "sentence =\"Asamoah a attaché la ficelle au cerf-volant.\"\n",
        "ground_truth= \"Asamoah de hama no bɔɔ asangoli no mu.\"\n",
        "translated_text, translated_tokens, attention_weights = translator(sentence)\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYL7f6tu9Y0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9915beb-0295-43a4-82b8-c4f0d675d5b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:         : je ne sais pas qui tu es\n",
            "Prediction     : minnim nipa ko a woyɛ .\n",
            "Ground truth   : minnim onipa ko a woyɛ\n"
          ]
        }
      ],
      "source": [
        "sentence=\"je ne sais pas qui tu es\"\n",
        "ground_truth=\"minnim onipa ko a woyɛ\"\n",
        "translated_text, translated_tokens, attention_weights = translator(tf.constant(sentence))\n",
        "print_translation(sentence, translated_text, ground_truth)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Translator"
      ],
      "metadata": {
        "id": "tBe9E_dXBzne"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMpmh77M9ufZ"
      },
      "outputs": [],
      "source": [
        "# class to export translator\n",
        "class ExportTranslator(tf.Module):\n",
        "  def __init__(self, translator):\n",
        "    self.translator = translator\n",
        "\n",
        "  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.string)])\n",
        "  def __call__(self, sentence):\n",
        "    (result,\n",
        "     tokens,\n",
        "     attention_weights) = self.translator(sentence, max_length=MAX_TOKENS)\n",
        "\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89tEraoTixKG"
      },
      "outputs": [],
      "source": [
        "translator = ExportTranslator(translator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZurB6qDFDzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95a3b3fd-1e00-42c3-b99f-8a33e5c9fc42"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'na wonte ase .'"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ],
      "source": [
        "translator(tf.constant(\"Vous n'a pas pas compris.\")).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "module_no_signatures_path = '/content/drive/MyDrive/french_twi_translator'\n",
        "print('Saving model...')\n",
        "tf.saved_model.save(translator, module_no_signatures_path)"
      ],
      "metadata": {
        "id": "YMhMt9_z2ZXH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "226cb48d-3348-4348-a14b-9bda0f0b7891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, dropout_12_layer_call_fn, dropout_12_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn while saving (showing 5 of 332). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded = tf.saved_model.load(module_no_signatures_path)"
      ],
      "metadata": {
        "id": "P0jO3gtH9bih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reloaded(\"Kwaku a essayé de briser le combat entre Abena et John.\").numpy().decode(\"utf-8\")"
      ],
      "metadata": {
        "id": "adjW9Tug-ZOa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9f1f4932-ff13-4155-cb48-910b5f642dca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'kwaku bɔɔ mmɔden sɛ obegyae ɔko a ɛkɔɔ so wɔ abena ne john ntam no .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W8k01SazA8Cl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}